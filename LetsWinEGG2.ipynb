{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from numpy import array\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from gensim.test.utils import common_dictionary, common_corpus\n",
    "from gensim.models import LsiModel\n",
    "from gensim import corpora, models, utils\n",
    "from gensim.test.utils import common_corpus, common_dictionary, get_tmpfile\n",
    "from gensim.models import LsiModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spacy lib\n",
    "# On https://spacy.io/\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datas preprocessing methods.\n",
    "# Lemmatisation without poncutations\n",
    "\n",
    "stemmer = nltk.stem.snowball.FrenchStemmer()\n",
    "fstw = stopwords.words('french')\n",
    "\n",
    "# French Stop Words, extraits depuis le fichier stopwords-fr.txt + stopwords french de nltk\n",
    "sourceFST = [x.replace('\\n', '') for x in open('stopwords-fr.txt', mode=\"r\", encoding=\"utf-8\").readlines()]+fstw\n",
    "\n",
    "def lemmatize(article):\n",
    "    output = []\n",
    "    outPonc = article.translate(article.maketrans(\"\",\"\", string.punctuation))\n",
    "    outLem = nlp(outPonc)\n",
    "    for token in outLem:\n",
    "        if token.lemma_ not in sourceFST:\n",
    "            output.append(token.lemma_)\n",
    "    res = ' '.join(output)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Reading\n",
    "data = pd.read_csv('export_articles_EGC_2004_2018.csv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's process our corpus, and determine a limit to split it in partitions\n",
    "\n",
    "# usable[] correspond to our corpus processed\n",
    "# limits[] let us know when to delimit partitions\n",
    "limits = []\n",
    "usable = []\n",
    "\n",
    "# To create ours delimiters, we must first know the years which will be the limits\n",
    "limit_years = [2007, 2010, 2014]\n",
    "\n",
    "prev_year = data['year'][0]\n",
    "numArti = 0\n",
    "for i in range(0, len(data['abstract']), 1):\n",
    "    if not isinstance(data['abstract'][i], float): #if not null, empty, or whatever (so if there is a abstract)\n",
    "        year = data['year'][i]\n",
    "        if year != prev_year:\n",
    "            prev_year = year\n",
    "            if year in limit_years:\n",
    "                limits.append(numArti)\n",
    "        numArti+=1\n",
    "        usable.append(stemmer.stem(lemmatize(data['abstract'][i])))\n",
    "limits.append(numArti)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'articles = 1096\n",
      "limits = [267, 543, 790, 1096]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'plateforme objectif permettre citoyen danalyserpar euxmême tweet politique dévénement spécifique francepour cas lélection présidentiel 2017 idéo2017 analyser quasitemp réel message candidat fournir principal caractéristiqueslusage lexiqu politique comparaison entrer candidat'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display pre-processed datas\n",
    "print(\"nombre d'articles =\", len(usable))\n",
    "print(\"limits =\", limits)\n",
    "\n",
    "usable[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params\n",
    "nb_concepts = 30\n",
    "min_gram = 1\n",
    "max_gram = 3\n",
    "\n",
    "# Creation of cleandocs, which is usable[] with ngrams\n",
    "cleandocs = []\n",
    "for t in usable:\n",
    "    doc = []\n",
    "    for n in range(min_gram, max_gram+1):\n",
    "        for gram in ngrams(t.split(), n):\n",
    "            doc.append(\" \".join(gram))\n",
    "    cleandocs.append(doc)\n",
    "\n",
    "# Creation of tfidf model, a tool to create ours tfidf\n",
    "corpus = []\n",
    "dictionary = corpora.Dictionary(cleandocs)\n",
    "for doc in cleandocs:\n",
    "    newVec = dictionary.doc2bow(doc)\n",
    "    corpus.append(newVec)\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "# Creation of partitions_lsa[], which give us the LSA of each partition\n",
    "partitions_lsa = []\n",
    "beg = 0\n",
    "for l in limits:\n",
    "    last = l\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    lsi = models.LsiModel(corpus_tfidf, num_topics=nb_concepts, id2word=dictionary)\n",
    "    corpus_lsi = lsi[corpus_tfidf[beg:last]]\n",
    "    partitions_lsa.append(corpus_lsi)\n",
    "    beg = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition numéro: 0\n",
      "document number  0\n",
      "[(0, -0.00442525458643619), (1, -0.02960472232489413), (2, 0.007187271261485128), (3, -0.010163289400516612), (4, 0.027906869287719065), (5, 0.013444671077312129), (6, -0.01601118924958787), (7, 0.05147121427214083), (8, 0.033533235738763094), (9, 0.00368402146607114), (10, -0.019326838155898494), (11, 0.0076580913305048), (12, 0.004982137500364499), (13, -0.03665251238755576), (14, 0.0004213584272149639), (15, -0.021140695382502975), (16, -0.0015540745732613322), (17, -0.009233778724669387), (18, 0.02040349931749458), (19, 0.04948363822768231), (20, 0.019836501575363107), (21, -0.0008547899353163455), (22, 0.06769213815517623), (23, 0.0210695776207959), (24, -0.013398617928773749), (25, 0.004508298536446662), (26, -0.010083838320645534), (27, 0.03349603308043118), (28, 0.020394990040406753), (29, 0.004775044082856077)]\n",
      "document number  1\n",
      "[(0, -0.011020756955887043), (1, -0.09853747400540062), (2, -0.027366706972146194), (3, 0.07079287849658708), (4, -0.03014781554622567), (5, -0.05252657858760343), (6, -0.08786883248748084), (7, 0.036477409425227154), (8, 0.004156598405122663), (9, -0.0020788279854044986), (10, 0.07939556021413767), (11, -0.06641966411842068), (12, -0.024236245342331472), (13, -0.0010903650486864156), (14, 0.014331433770939758), (15, 0.009725303735975407), (16, -0.0034108625157210923), (17, 0.04734627766654424), (18, -0.053633173829566526), (19, 0.010249448672359012), (20, -0.003708350702689149), (21, -0.09005200881076947), (22, 0.005056543969016167), (23, 0.00040709365583010764), (24, -0.06346831943742966), (25, 0.0300000687165377), (26, 0.019588366835581895), (27, 0.03221002599983117), (28, -0.007142112902733393), (29, 0.015921786003783453)]\n",
      "document number  2\n",
      "[(0, -0.008869994212330024), (1, -0.06932803936309455), (2, -0.010189989271687655), (3, -0.018580228915600707), (4, -0.04466420563318469), (5, 0.0828545155465441), (6, 0.007874889334476274), (7, 0.007010717003672192), (8, -0.02898026506019971), (9, 0.14157631775247753), (10, -0.09654060215709588), (11, -0.03934975033926441), (12, -0.05102161713007614), (13, -0.0078053720470481576), (14, 0.002573739227796603), (15, 0.03341385556352802), (16, 0.004065974622951565), (17, 0.01212127329599502), (18, -0.02424020065501148), (19, 0.008408295375390116), (20, -0.008433512967565785), (21, 0.003826503592703389), (22, -0.050706988501646184), (23, -0.05794814881926031), (24, 0.01985613970862055), (25, -0.013972525593188128), (26, 0.045269142413778225), (27, 0.035109749854129374), (28, -0.016715162934360487), (29, -0.02138841000015612)]\n",
      "Partition numéro: 1\n",
      "document number  0\n",
      "[(0, -0.006020132688293421), (1, -0.040595074780619494), (2, 0.021459952484718426), (3, 0.004125686906182281), (4, 0.014730879298167729), (5, -0.011109895844413345), (6, 0.041140317622898805), (7, 0.006804860709822218), (8, -0.039739530168458215), (9, -0.03334310329507557), (10, -0.0027906679302450673), (11, 0.06316046653743751), (12, 0.007367892795397563), (13, 0.03199393415863029), (14, 0.0029357085560102604), (15, 0.025556490788733236), (16, -0.06307031182109588), (17, -0.02913824346369721), (18, -0.060506211317683904), (19, -0.045758625257437166), (20, -0.02700848812783872), (21, 0.008709230871107452), (22, 0.04652999136270038), (23, 0.01748003977924798), (24, 0.03799573180152678), (25, 0.0009445912217147612), (26, 0.044220237061091704), (27, -0.09092706868272628), (28, -0.006892279225620442), (29, 0.027505616358449954)]\n",
      "document number  1\n",
      "[(0, -0.007251877533645466), (1, -0.0639039146204156), (2, -0.0021800967175388576), (3, -0.023325644121214267), (4, 0.011028936344825638), (5, -0.05954540200922921), (6, 0.00482502757788313), (7, -0.011141619746141085), (8, 0.04077141015444874), (9, -0.06752127846669674), (10, -0.04598122023582999), (11, 0.029497357123257465), (12, 0.03985227226293802), (13, -0.008159153473428242), (14, -0.016584498026988533), (15, 0.023143689989449972), (16, -0.03274197060731132), (17, -0.032440754946594326), (18, -0.04361617034426339), (19, 0.015568200063108885), (20, 0.04269596758897592), (21, -0.03467287116260784), (22, -0.06263734258185272), (23, 0.06576426904957179), (24, -0.007237462707970872), (25, -0.007771635389822428), (26, -0.017822427197590217), (27, -0.045226424060619025), (28, 0.006336595294433956), (29, -0.026355669444458277)]\n",
      "document number  2\n",
      "[(0, -0.005261140335366496), (1, -0.045450290731180315), (2, 0.0023792660925475925), (3, -0.0196134799776054), (4, 0.0023222116423461427), (5, 0.0020700301927880026), (6, 0.02181105378262588), (7, 0.023468788538970203), (8, -0.028312514853394814), (9, -0.0007559231165173131), (10, -0.03799506548942295), (11, 0.017921484697268123), (12, -0.058475782215890254), (13, -0.01937966071482827), (14, -0.0012107394346054982), (15, 0.03128312402450678), (16, 0.019781275672243213), (17, -0.01059433472497873), (18, -0.015249425383083364), (19, 0.01304546821165979), (20, -0.022768316857531473), (21, 0.02945831529872497), (22, -0.038032642531352204), (23, -0.002977917738878204), (24, -0.04216000631848423), (25, 0.018943906635890646), (26, 0.046010686821948935), (27, -0.030804775847123014), (28, 0.049761544867902935), (29, -0.03229435427001705)]\n",
      "Partition numéro: 2\n",
      "document number  0\n",
      "[(0, -0.00752999245152098), (1, -0.029809978365935713), (2, -0.025228281099117934), (3, -0.023011017485858827), (4, -0.00901708990359163), (5, 0.033274754886243416), (6, -0.020974234880654696), (7, 0.006154307100382193), (8, 0.029371772787326163), (9, -0.10464506099951855), (10, -0.054313643146764), (11, -0.05681088411461851), (12, 0.017709922408444295), (13, 0.014303063294802196), (14, 0.007173377548277766), (15, 0.015197539781818993), (16, -0.0045885824042476465), (17, 0.021649376741883525), (18, 0.022236096178233602), (19, 0.01310636218933457), (20, 0.06858934682088735), (21, 0.038811811483243024), (22, -0.009393117381246336), (23, 0.03153308327430355), (24, -0.011635793471065085), (25, 0.010287636100329957), (26, -0.0036476582410084448), (27, -0.042421348745561624), (28, 0.04789300147053184), (29, 0.03652116300726383)]\n",
      "document number  1\n",
      "[(0, -0.00833361525258504), (1, -0.054812317297990285), (2, -0.01016013032754766), (3, -0.007343986927958657), (4, -0.014727581347125521), (5, -0.021081407491578677), (6, -0.024119018850880962), (7, -0.014382484078054899), (8, 0.011482914363679078), (9, 0.01610137649002058), (10, -0.012472798805874194), (11, -0.004369552405526728), (12, 0.057804479861632944), (13, -0.03790860741058024), (14, -0.004229118934228864), (15, 0.011762049642652515), (16, -0.006707148804011387), (17, -0.05196911918064008), (18, -0.003424822112753111), (19, -0.01588231301375516), (20, -0.00217637894426677), (21, 0.009221620870334315), (22, -0.015589879578693396), (23, 0.016448155531927095), (24, -0.010935154984031628), (25, 0.008222305386229041), (26, 0.046585872165183534), (27, 0.038798079874884886), (28, 0.02251647805045883), (29, -0.020403052189303255)]\n",
      "document number  2\n",
      "[(0, -0.00547072082697956), (1, -0.03374258167430298), (2, -0.015275251725379841), (3, -0.01830265302421665), (4, 0.00018226731872992148), (5, 0.017217742960028305), (6, -0.05309771501892629), (7, -0.001611791953370325), (8, 0.0320472302803685), (9, 0.03764910704674089), (10, -0.021130104000084776), (11, 0.03888759988245468), (12, -0.009268339640084436), (13, 0.030804966686012625), (14, 0.03595891511890979), (15, -0.03816913044079188), (16, -0.011237158513694882), (17, -0.0358339190863297), (18, -0.01892674981465846), (19, -0.04541394244606453), (20, 0.03914253937758148), (21, -0.037945922743496355), (22, -0.00546144487373656), (23, -0.002211074195833057), (24, 0.018988874988432915), (25, 0.01793489789190046), (26, -0.029774976123071156), (27, -0.018524803724883826), (28, -0.02025631299064632), (29, -0.008546827325793625)]\n",
      "Partition numéro: 3\n",
      "document number  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.00843419546398208), (1, -0.09146406420927741), (2, -5.182104837538671e-05), (3, 0.08922833102519247), (4, -0.09157097353803897), (5, 0.002680083143680682), (6, -0.03890003321671587), (7, 0.012141631761116803), (8, 0.000828875250456516), (9, 0.04545622766806511), (10, 0.028978874715015666), (11, 0.0481188583762124), (12, 0.059078413119139965), (13, 0.04457230887816985), (14, -0.008549044519596434), (15, -0.008787877070289921), (16, -0.006763338680368765), (17, 0.02459017835323135), (18, -0.015989719636662736), (19, -0.04670852952040745), (20, -0.09638697591361216), (21, -0.004135618210438457), (22, 0.045538371001288075), (23, -0.010395535437275383), (24, 0.02956146091692291), (25, -0.022389549076294302), (26, 0.011714135407949031), (27, -0.05987985916321611), (28, 0.04536238181688116), (29, -0.067337245849176)]\n",
      "document number  1\n",
      "[(0, 0.008745546771916043), (1, -0.07340575375494054), (2, 0.01928723496240723), (3, 0.023590707342729833), (4, 0.016565250482938054), (5, -0.022517689402266842), (6, -0.015635072103146117), (7, -0.024650127585179878), (8, 0.01819942174497385), (9, -0.015348979773291914), (10, -0.03238751371223576), (11, 0.04242545961673552), (12, -0.00048516958411223203), (13, 0.052271558942881054), (14, -0.0032156282417673088), (15, -0.026943359308677348), (16, -0.013032215631114223), (17, -0.014216047715509054), (18, -0.006937589011184839), (19, -0.026566410300098257), (20, -0.053743459042531264), (21, -0.0620504658706916), (22, -0.05305878503322069), (23, -0.032742111360091766), (24, -0.011008186287490897), (25, -0.06453364830336378), (26, -0.005174283620818169), (27, -0.03954432625104986), (28, -0.03435650076333777), (29, 0.020414846644821388)]\n",
      "document number  2\n",
      "[(0, 0.006112790066939226), (1, -0.07545397211601378), (2, 0.046615177389953834), (3, -9.543687788132364e-05), (4, -0.014063167997373064), (5, -0.048759901695643336), (6, -0.015802599618615482), (7, -0.027358922538576896), (8, 0.04342104964083606), (9, -0.01527002662987949), (10, -0.055742441242961106), (11, -0.02426822675017363), (12, 0.005862787801815772), (13, 0.027681551238761565), (14, -0.00287593482516738), (15, 0.017871087553183757), (16, -0.05812422815593674), (17, -0.02541913093124234), (18, 0.003244367213132091), (19, 0.0027363172508416923), (20, -0.019586699187066046), (21, -0.056831385923185394), (22, -0.05709874140272258), (23, 0.0749488576075695), (24, 0.062026436104174576), (25, 0.017453496600765055), (26, -0.04139272506766612), (27, 0.05893402683706305), (28, -0.03795881586082081), (29, -0.05147221648045694)]\n"
     ]
    }
   ],
   "source": [
    "num_partition = 0\n",
    "for lsa in partitions_lsa:\n",
    "    print(\"Partition numéro:\",num_partition)\n",
    "    num_partition+=1\n",
    "    i=0\n",
    "    for doc in lsa:\n",
    "        if (i<3):\n",
    "            print(\"document number \", i)\n",
    "            i+=1\n",
    "            print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create ours partitions\n",
    "partitions = []\n",
    "\n",
    "# You must specify a treshold, to know what are the doc you keep, and what are the doc you drop\n",
    "tresh = 0.03\n",
    "\n",
    "for corpus_lsi in partitions_lsa:\n",
    "    # Let's create ours clusters\n",
    "    clusters = []\n",
    "\n",
    "    for i in range(0, nb_concepts):\n",
    "        dic = {}\n",
    "        num_doc = 0\n",
    "        for doc in corpus_lsi:\n",
    "            if abs(doc[i][1]) > tresh:\n",
    "                dic[num_doc] = doc[i][1]\n",
    "            num_doc+=1\n",
    "        clusters.append(dic)\n",
    "    partitions.append(clusters)\n",
    "    \n",
    "# TODO: it would be nice to know how many articles are in no cluster anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.07079287849658708,\n",
       " 3: 0.06278657173783952,\n",
       " 4: -0.041618199069723756,\n",
       " 8: 0.054975517638219194,\n",
       " 10: 0.05221771109005783,\n",
       " 13: -0.03192847468678608,\n",
       " 14: -0.03791024301073565,\n",
       " 15: -0.05955978337700095,\n",
       " 19: -0.03489750315996548,\n",
       " 21: -0.033238977202303434,\n",
       " 22: -0.047209915155047084,\n",
       " 27: -0.09349683388104621,\n",
       " 30: -0.043076981097785585,\n",
       " 34: 0.04103058699427752,\n",
       " 39: 0.06067031860007079,\n",
       " 43: -0.04072268230397381,\n",
       " 44: -0.06695194373437295,\n",
       " 45: 0.04050302608224564,\n",
       " 50: 0.0710103730320674,\n",
       " 51: -0.030669369773278516,\n",
       " 54: -0.04483703035037884,\n",
       " 55: -0.033769739510406106,\n",
       " 56: -0.052527365658544496,\n",
       " 59: 0.03935718679638772,\n",
       " 60: 0.09441085781127445,\n",
       " 63: -0.047920962467156866,\n",
       " 66: 0.031567115707090664,\n",
       " 69: 0.0914453877946898,\n",
       " 78: -0.049187510489126986,\n",
       " 83: -0.05612234523734906,\n",
       " 85: -0.048745281608375345,\n",
       " 86: 0.0436273860780593,\n",
       " 92: -0.033202635275020355,\n",
       " 93: 0.04855058050023327,\n",
       " 94: 0.04032940601720302,\n",
       " 97: -0.05275780041899206,\n",
       " 99: 0.10614847786250386,\n",
       " 100: 0.030585621169715766,\n",
       " 101: -0.049108946721536054,\n",
       " 104: 0.05146521329915087,\n",
       " 112: -0.07293330555006257,\n",
       " 119: 0.08712325186561931,\n",
       " 121: -0.043460214528684635,\n",
       " 132: 0.032860665344900966,\n",
       " 138: 0.05404130649222209,\n",
       " 143: -0.04006737143715523,\n",
       " 155: -0.040231588576253335,\n",
       " 157: 0.15025148166485533,\n",
       " 169: -0.07461728928447998,\n",
       " 171: -0.045935623537268995,\n",
       " 173: 0.054591618197681824,\n",
       " 179: 0.10899121411782826,\n",
       " 190: -0.03343712859987901,\n",
       " 192: -0.05921453193160766,\n",
       " 201: -0.03330180211797378,\n",
       " 202: -0.03617028518503732,\n",
       " 206: -0.031725763761621194,\n",
       " 211: 0.06748721388397481,\n",
       " 212: 0.05204869044732915,\n",
       " 213: -0.06727409443606304,\n",
       " 222: -0.04660152306812483,\n",
       " 227: -0.05695424143398924,\n",
       " 231: -0.06583907414317244,\n",
       " 232: -0.03372836134178883,\n",
       " 234: -0.04153308968466611,\n",
       " 245: 0.030378876444983913,\n",
       " 246: 0.030931113398616,\n",
       " 248: 0.037033409006185104,\n",
       " 253: 0.03828701782247388,\n",
       " 254: 0.030466595348361023,\n",
       " 255: 0.03047589313634153,\n",
       " 259: -0.039256456812508764,\n",
       " 261: 0.04484043023161957}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display clusters 3 of partition 0 \n",
    "partitions[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_labels_by_cluster = 5\n",
    "\n",
    "# Let's labelize our clusters\n",
    "# For this, we will use the tfidf matrix\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=sourceFST, use_idf=True, ngram_range=(min_gram, max_gram))\n",
    "tfidf = vectorizer.fit_transform(usable)\n",
    "\n",
    "# We can access the value in the tfidf using:\n",
    "#tfidf.toarray().item(num_doc, num_word)\n",
    "# To know the number of the word searched, we will use:\n",
    "#vectorizer.vocabulary_[word]\n",
    "\n",
    "# take less than 8h to compute x)\n",
    "labels = []\n",
    "for clusters in partitions:\n",
    "    l = []\n",
    "    for clus in clusters:\n",
    "        first_arti = True\n",
    "        for article in clus:\n",
    "            link = abs(clus[article])\n",
    "            if first_arti:\n",
    "                coef_list = (tfidf.toarray()[article] * link)\n",
    "                first = False\n",
    "            else:\n",
    "                # the more an article have a high coeficient, the more he is implied in the labeling step\n",
    "                coef_list += (tfidf.toarray()[article] * link)\n",
    "        # Now we have coef_list filled by every coeficient in the multiple tfidf\n",
    "        # Let's find the best ones, to finally get the labels\n",
    "        res = dict(zip(vectorizer.get_feature_names(), coef_list))\n",
    "\n",
    "        l.append(Counter(res).most_common(nb_labels_by_cluster))\n",
    "    labels.append(l)\n",
    "\n",
    "# TODO: on observe beaucoup de labels identiques entre deux clusters\n",
    "# Je pense que c'est parce que l'on a trop de clusters, mais j'aimerais en être sûr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[('xplor', 0.03970108413359015),\n",
       "   ('xplor everywhere', 0.03970108413359015),\n",
       "   ('everywhere', 0.03749812918607926),\n",
       "   ('and', 0.028002371100805695),\n",
       "   ('dater', 0.020009476569576027)],\n",
       "  [('hotspot', 0.027371980004046396),\n",
       "   ('relation', 0.006556304832916206),\n",
       "   ('photographie', 0.006463287241834405),\n",
       "   ('détecter', 0.003906684431161257),\n",
       "   ('approcher extraire relation', 0.0034214975005057995)],\n",
       "  [('lien', 0.006192576191291464),\n",
       "   ('rankmerging', 0.005587713002789331),\n",
       "   ('dan grand', 0.004527631369350621),\n",
       "   ('réseau', 0.0039175314170584565),\n",
       "   ('article décrire rankmerging', 0.0027938565013946653)],\n",
       "  [('petl', 0.013622325033983564),\n",
       "   ('donnée massif', 0.0059570712163714715),\n",
       "   ('paramétrage', 0.005222135732499675),\n",
       "   ('massif', 0.004712692377045261),\n",
       "   ('parallèle', 0.004619402760206431)],\n",
       "  [('réseau social', 0.013993670644504144),\n",
       "   ('social', 0.01261600876588804),\n",
       "   ('baser interaction', 0.01164783189524735),\n",
       "   ('baser interaction utilisateur', 0.01164783189524735),\n",
       "   ('baser lanalyse lenrichissement', 0.01164783189524735)],\n",
       "  [('olfactif', 0.00767673981322693),\n",
       "   ('qualité olfactif', 0.00767673981322693),\n",
       "   ('neuroscientifique', 0.005117826542151288),\n",
       "   ('découvrir sousgroupe', 0.004833845851256355),\n",
       "   ('sousgroupe', 0.0046323583445406135)],\n",
       "  [('hotspot', 0.022029307078946972),\n",
       "   ('relation', 0.005276594986776405),\n",
       "   ('photographie', 0.005201733282311402),\n",
       "   ('détecter', 0.0031441478103473026),\n",
       "   ('approcher extraire relation', 0.0027536633848683715)],\n",
       "  [('règle', 0.005196016380230729),\n",
       "   ('dapprentissage', 0.00439220911858711),\n",
       "   ('liaison', 0.0039580168786537),\n",
       "   ('nombre', 0.003948977058985429),\n",
       "   ('attribut', 0.0037795698513938066)],\n",
       "  [('xplor', 0.00852564046664576),\n",
       "   ('xplor everywhere', 0.00852564046664576),\n",
       "   ('everywhere', 0.00805256517773178),\n",
       "   ('and', 0.006013391156164173),\n",
       "   ('dater', 0.004296950747842232)],\n",
       "  [('réseau social', 0.01244506576491576),\n",
       "   ('social', 0.0112198623771307),\n",
       "   ('baser interaction', 0.01035882847592724),\n",
       "   ('baser interaction utilisateur', 0.01035882847592724),\n",
       "   ('baser lanalyse lenrichissement', 0.01035882847592724)],\n",
       "  [('olfactif', 0.01678702965013443),\n",
       "   ('qualité olfactif', 0.01678702965013443),\n",
       "   ('neuroscientifique', 0.011191353100089623),\n",
       "   ('découvrir sousgroupe', 0.010570361325703175),\n",
       "   ('sousgroupe', 0.010129760649939614)],\n",
       "  [('sémiotique', 0.0072806281630653676),\n",
       "   ('style', 0.00687663678139824),\n",
       "   ('indicateur', 0.005782017571162479),\n",
       "   ('action suggéré', 0.0036403140815326838),\n",
       "   ('action suggéré proposer', 0.0036403140815326838)],\n",
       "  [('olfactif', 0.008005488346810292),\n",
       "   ('qualité olfactif', 0.008005488346810292),\n",
       "   ('neuroscientifique', 0.005336992231206862),\n",
       "   ('découvrir sousgroupe', 0.0050408503575743255),\n",
       "   ('sousgroupe', 0.004830734354390065)],\n",
       "  [('hotspot', 0.028392326795007434),\n",
       "   ('relation', 0.006800704565629713),\n",
       "   ('photographie', 0.006704219552733735),\n",
       "   ('détecter', 0.0040523141196983515),\n",
       "   ('approcher extraire relation', 0.0035490408493759292)],\n",
       "  [('olfactif', 0.007401836678503322),\n",
       "   ('qualité olfactif', 0.007401836678503322),\n",
       "   ('neuroscientifique', 0.004934557785668882),\n",
       "   ('découvrir sousgroupe', 0.004660746409356357),\n",
       "   ('sousgroupe', 0.004466474146162139)],\n",
       "  [('réseau social', 0.0075687165348065095),\n",
       "   ('social', 0.006823584502979717),\n",
       "   ('baser interaction', 0.006299929453808373),\n",
       "   ('baser interaction utilisateur', 0.006299929453808373),\n",
       "   ('baser lanalyse lenrichissement', 0.006299929453808373)],\n",
       "  [('xplor', 0.01348486012606504),\n",
       "   ('xplor everywhere', 0.01348486012606504),\n",
       "   ('everywhere', 0.012736605009624194),\n",
       "   ('and', 0.009511278236682847),\n",
       "   ('dater', 0.006796413715770947)],\n",
       "  [('multiplex', 0.016637798781997094),\n",
       "   ('réseau multiplex', 0.016637798781997094),\n",
       "   ('réseau', 0.013608838292351945),\n",
       "   ('communauter dan', 0.008318899390998547),\n",
       "   ('détection communauter dan', 0.008318899390998547)],\n",
       "  [('olfactif', 0.010136086888101631),\n",
       "   ('qualité olfactif', 0.010136086888101631),\n",
       "   ('neuroscientifique', 0.006757391258734422),\n",
       "   ('découvrir sousgroupe', 0.006382433525701107),\n",
       "   ('sousgroupe', 0.006116396780334425)],\n",
       "  [('hotspot', 0.017635977388930985),\n",
       "   ('relation', 0.00422427766537737),\n",
       "   ('photographie', 0.0041643457155202904),\n",
       "   ('détecter', 0.002517106847347669),\n",
       "   ('approcher extraire relation', 0.002204497173616373)],\n",
       "  [('xplor', 0.006469312987806429),\n",
       "   ('xplor everywhere', 0.006469312987806429),\n",
       "   ('everywhere', 0.00611034029563688),\n",
       "   ('and', 0.004563001414325253),\n",
       "   ('dater', 0.0032605549565142503)],\n",
       "  [('xplor', 0.009008377944514217),\n",
       "   ('xplor everywhere', 0.009008377944514217),\n",
       "   ('everywhere', 0.008508516260759208),\n",
       "   ('and', 0.006353880447440287),\n",
       "   ('dater', 0.004540252019418657)],\n",
       "  [('petl', 0.01240000112223354),\n",
       "   ('donnée massif', 0.005422546414356868),\n",
       "   ('paramétrage', 0.004753556296880969),\n",
       "   ('massif', 0.004289825020201701),\n",
       "   ('parallèle', 0.004204906230596521)],\n",
       "  [('xplor', 0.00736350642110064),\n",
       "   ('xplor everywhere', 0.00736350642110064),\n",
       "   ('everywhere', 0.006954916246414018),\n",
       "   ('and', 0.005193702990905693),\n",
       "   ('dater', 0.003711231378648299)],\n",
       "  [('the', 0.011357605893525096),\n",
       "   ('codicum', 0.008243300460943563),\n",
       "   ('codicum stemma', 0.008243300460943563),\n",
       "   ('stemma', 0.008243300460943563),\n",
       "   ('triplet', 0.007003946542820234)],\n",
       "  [('olfactif', 0.008175664598781272),\n",
       "   ('qualité olfactif', 0.008175664598781272),\n",
       "   ('neuroscientifique', 0.0054504430658541825),\n",
       "   ('découvrir sousgroupe', 0.005148005971752487),\n",
       "   ('sousgroupe', 0.004933423438563812)],\n",
       "  [('xplor', 0.008906551127733586),\n",
       "   ('xplor everywhere', 0.008906551127733586),\n",
       "   ('everywhere', 0.008412339664739841),\n",
       "   ('and', 0.006282058924836263),\n",
       "   ('dater', 0.00448893096990599)],\n",
       "  [('xplor', 0.006533512297333038),\n",
       "   ('xplor everywhere', 0.006533512297333038),\n",
       "   ('everywhere', 0.00617097727960904),\n",
       "   ('and', 0.004608283122092487),\n",
       "   ('dater', 0.003292911619003192)],\n",
       "  [('dinterrogation', 0.015375373943147549),\n",
       "   ('graphe dinterrogation', 0.010614578827988889),\n",
       "   ('sousgraphe', 0.008011830137023663),\n",
       "   ('graphe', 0.007884048711614054),\n",
       "   ('graph', 0.006456883710949432)],\n",
       "  [('xplor', 0.009353580230248357),\n",
       "   ('xplor everywhere', 0.009353580230248357),\n",
       "   ('everywhere', 0.008834563777805138),\n",
       "   ('and', 0.006597362022841277),\n",
       "   ('dater', 0.004714235103228634)]],\n",
       " [[('olfactif', 0.06859531518915728),\n",
       "   ('qualité olfactif', 0.06859531518915728),\n",
       "   ('neuroscientifique', 0.04573021012610486),\n",
       "   ('découvrir sousgroupe', 0.043192707817376205),\n",
       "   ('sousgroupe', 0.041392321277502836)],\n",
       "  [('fusion', 0.011864853950380378),\n",
       "   ('formel', 0.009253568776288959),\n",
       "   ('agg', 0.007975397163396803),\n",
       "   ('algébrique spo', 0.007975397163396803),\n",
       "   ('algébrique spo simple', 0.007975397163396803)],\n",
       "  [('protéinearn', 0.004597937145894851),\n",
       "   ('rosettadock', 0.004597937145894851),\n",
       "   ('scor', 0.004161784759611269),\n",
       "   ('fonction', 0.003514593029412953),\n",
       "   ('poids', 0.003289479987044105)],\n",
       "  [('phraser', 0.007422358538176854),\n",
       "   ('syntaxique', 0.006697722488292242),\n",
       "   ('traduire', 0.006249776038797667),\n",
       "   ('analyser presque', 0.004367875396033847),\n",
       "   ('analyser presque instantan', 0.004367875396033847)],\n",
       "  [('document', 0.012563533440947404),\n",
       "   ('document administratif', 0.006165304329935601),\n",
       "   ('administratif', 0.005823200632443607),\n",
       "   ('lhomme', 0.005580474196259953),\n",
       "   ('nest', 0.00361687660857622)],\n",
       "  [('bayésien naïf', 0.006089559362256916),\n",
       "   ('naïf', 0.005338278557661741),\n",
       "   ('bayésien', 0.004869022833043304),\n",
       "   ('logvraisemblance', 0.004641764044408481),\n",
       "   ('variablesexplicative', 0.004384199363494485)],\n",
       "  [('protéinearn', 0.0072010308102400545),\n",
       "   ('rosettadock', 0.0072010308102400545),\n",
       "   ('scor', 0.006517953449256134),\n",
       "   ('fonction', 0.0055043581256550705),\n",
       "   ('poids', 0.005151798727288293)],\n",
       "  [('protéinearn', 0.006994424498170018),\n",
       "   ('rosettadock', 0.006994424498170018),\n",
       "   ('scor', 0.00633094545555612),\n",
       "   ('fonction', 0.005346431411741119),\n",
       "   ('poids', 0.0050039873703283215)],\n",
       "  [('phraser', 0.0064320547513436665),\n",
       "   ('syntaxique', 0.005804100884162241),\n",
       "   ('traduire', 0.005415920217060323),\n",
       "   ('analyser presque', 0.0037851059807786384),\n",
       "   ('analyser presque instantan', 0.0037851059807786384)],\n",
       "  [('phraser', 0.005036329869966134),\n",
       "   ('syntaxique', 0.00454463896550269),\n",
       "   ('traduire', 0.004240691632302461),\n",
       "   ('analyser presque', 0.0029637562254893144),\n",
       "   ('analyser presque instantan', 0.0029637562254893144)],\n",
       "  [('phraser', 0.00501327659534235),\n",
       "   ('syntaxique', 0.004523836354704207),\n",
       "   ('traduire', 0.004221280308715964),\n",
       "   ('analyser presque', 0.00295018994052621),\n",
       "   ('analyser presque instantan', 0.00295018994052621)],\n",
       "  [('bayésien naïf', 0.006000619314325964),\n",
       "   ('naïf', 0.005260311216751997),\n",
       "   ('bayésien', 0.004797909128686989),\n",
       "   ('logvraisemblance', 0.004573969530547161),\n",
       "   ('variablesexplicative', 0.004320166667804738)],\n",
       "  [('bayésien naïf', 0.006854059583692311),\n",
       "   ('naïf', 0.006008460897078771),\n",
       "   ('bayésien', 0.00548029350348078),\n",
       "   ('logvraisemblance', 0.005224504014363539),\n",
       "   ('variablesexplicative', 0.004934603946949639)],\n",
       "  [('phraser', 0.011771718132410123),\n",
       "   ('syntaxique', 0.010622459270291075),\n",
       "   ('traduire', 0.009912024801955724),\n",
       "   ('analyser presque', 0.006927366514987739),\n",
       "   ('analyser presque instantan', 0.006927366514987739)],\n",
       "  [('phraser', 0.006690028766359748),\n",
       "   ('syntaxique', 0.006036889202441549),\n",
       "   ('traduire', 0.005633139556356207),\n",
       "   ('analyser presque', 0.00393691734197995),\n",
       "   ('analyser presque instantan', 0.00393691734197995)],\n",
       "  [('document', 0.01038288139061854),\n",
       "   ('document administratif', 0.005095192677734542),\n",
       "   ('administratif', 0.004812467906789564),\n",
       "   ('lhomme', 0.004611871489459347),\n",
       "   ('nest', 0.0029890954648915755)],\n",
       "  [('protéinearn', 0.020540936363959764),\n",
       "   ('rosettadock', 0.020540936363959764),\n",
       "   ('scor', 0.018592458573296826),\n",
       "   ('fonction', 0.01570117847888454),\n",
       "   ('poids', 0.01469550299197095)],\n",
       "  [('phraser', 0.005873085210362201),\n",
       "   ('syntaxique', 0.005299702875679408),\n",
       "   ('traduire', 0.00494525655595109),\n",
       "   ('analyser presque', 0.003456166157590714),\n",
       "   ('analyser presque instantan', 0.003456166157590714)],\n",
       "  [('fusion', 0.007949466332383717),\n",
       "   ('formel', 0.006199902143687825),\n",
       "   ('agg', 0.005343525634867079),\n",
       "   ('algébrique spo', 0.005343525634867079),\n",
       "   ('algébrique spo simple', 0.005343525634867079)],\n",
       "  [('phraser', 0.0043961681142215235),\n",
       "   ('syntaxique', 0.003966975441766677),\n",
       "   ('traduire', 0.0037016624838951354),\n",
       "   ('analyser presque', 0.002587036781391222),\n",
       "   ('analyser presque instantan', 0.002587036781391222)],\n",
       "  [('fusion', 0.0063090546918904795),\n",
       "   ('formel', 0.004920521714715708),\n",
       "   ('agg', 0.004240862728175891),\n",
       "   ('algébrique spo', 0.004240862728175891),\n",
       "   ('algébrique spo simple', 0.004240862728175891)],\n",
       "  [('phraser', 0.005776334007810888),\n",
       "   ('syntaxique', 0.005212397378139213),\n",
       "   ('traduire', 0.004863790086186807),\n",
       "   ('analyser presque', 0.00339923045514695),\n",
       "   ('analyser presque instantan', 0.00339923045514695)],\n",
       "  [('fusion', 0.009372820692677458),\n",
       "   ('formel', 0.007309996504822936),\n",
       "   ('agg', 0.006300285522099509),\n",
       "   ('algébrique spo', 0.006300285522099509),\n",
       "   ('algébrique spo simple', 0.006300285522099509)],\n",
       "  [('fusion', 0.005579191402511349),\n",
       "   ('formel', 0.0043512909282430465),\n",
       "   ('agg', 0.0037502583235936356),\n",
       "   ('algébrique spo', 0.0037502583235936356),\n",
       "   ('algébrique spo simple', 0.0037502583235936356)],\n",
       "  [('phraser', 0.0072712496813299885),\n",
       "   ('syntaxique', 0.0065613662096945784),\n",
       "   ('traduire', 0.006122539324495312),\n",
       "   ('analyser presque', 0.004278951551335508),\n",
       "   ('analyser presque instantan', 0.004278951551335508)],\n",
       "  [('article méthode dalignemer', 0.006139067635722386),\n",
       "   ('avecagrovoc', 0.006139067635722386),\n",
       "   ('avecagrovoc nalt', 0.006139067635722386),\n",
       "   ('cibl déjà', 0.006139067635722386),\n",
       "   ('cibl déjà publier', 0.006139067635722386)],\n",
       "  [('phraser', 0.004455257538628307),\n",
       "   ('syntaxique', 0.004020296035838486),\n",
       "   ('traduire', 0.0037514169745876008),\n",
       "   ('analyser presque', 0.002621809453946001),\n",
       "   ('analyser presque instantan', 0.002621809453946001)],\n",
       "  [('protéinearn', 0.00815637343145805),\n",
       "   ('rosettadock', 0.00815637343145805),\n",
       "   ('scor', 0.007382673917377788),\n",
       "   ('fonction', 0.0062346074550160965),\n",
       "   ('poids', 0.005835274889217262)],\n",
       "  [('bayésien naïf', 0.0070846163284177775),\n",
       "   ('naïf', 0.006210573406946168),\n",
       "   ('bayésien', 0.0056646395271583265),\n",
       "   ('logvraisemblance', 0.005400245795369169),\n",
       "   ('variablesexplicative', 0.005100594074205764)],\n",
       "  [('fusion', 0.01148172910513869),\n",
       "   ('formel', 0.008954764246526048),\n",
       "   ('agg', 0.007717865733448655),\n",
       "   ('algébrique spo', 0.007717865733448655),\n",
       "   ('algébrique spo simple', 0.007717865733448655)]],\n",
       " [[('risquer', 0.04022749839837969),\n",
       "   ('chimique', 0.033516250509336314),\n",
       "   ('alimentaire', 0.027771549126627985),\n",
       "   ('risquer chimique', 0.027771549126627985),\n",
       "   ('alimentaire entrer', 0.018514366084418657)],\n",
       "  [('lien', 0.011582248812253414),\n",
       "   ('rankmerging', 0.010450946470514503),\n",
       "   ('dan grand', 0.008468228961595726),\n",
       "   ('réseau', 0.007327132069202367),\n",
       "   ('article décrire rankmerging', 0.005225473235257252)],\n",
       "  [('2d', 0.014488739987302103),\n",
       "   ('projection', 0.01208211596344717),\n",
       "   ('loutil', 0.011193902641902178),\n",
       "   ('changement', 0.010074886196486003),\n",
       "   ('lutilisateur', 0.008449595585377355)],\n",
       "  [('lien', 0.0074874667983787615),\n",
       "   ('rankmerging', 0.006756124477901516),\n",
       "   ('dan grand', 0.005474375850390786),\n",
       "   ('réseau', 0.004736701739428031),\n",
       "   ('article décrire rankmerging', 0.003378062238950758)],\n",
       "  [('2d', 0.0069971914624975505),\n",
       "   ('projection', 0.005834936560558704),\n",
       "   ('loutil', 0.005405982857487352),\n",
       "   ('changement', 0.004865565103761176),\n",
       "   ('lutilisateur', 0.004080647326363442)],\n",
       "  [('lien', 0.013743102909839395),\n",
       "   ('rankmerging', 0.012400737989461274),\n",
       "   ('dan grand', 0.010048112760293042),\n",
       "   ('réseau', 0.008694125958898402),\n",
       "   ('article décrire rankmerging', 0.006200368994730637)],\n",
       "  [('lien', 0.009008991401451987),\n",
       "   ('rankmerging', 0.008129033352339292),\n",
       "   ('dan grand', 0.006586821189666686),\n",
       "   ('réseau', 0.005699244669904866),\n",
       "   ('article décrire rankmerging', 0.004064516676169646)],\n",
       "  [('item', 0.0035384272557310806),\n",
       "   ('meilleur', 0.002397277702326447),\n",
       "   ('gestion', 0.0023787002774534488),\n",
       "   ('affinité', 0.0023075619179474412),\n",
       "   ('affinité entrer', 0.0023075619179474412)],\n",
       "  [('2d', 0.006908584749316405),\n",
       "   ('projection', 0.005761047693429283),\n",
       "   ('loutil', 0.005337525909427137),\n",
       "   ('changement', 0.004803951564397017),\n",
       "   ('lutilisateur', 0.004028973344140947)],\n",
       "  [('lien', 0.00850843283685357),\n",
       "   ('rankmerging', 0.007677367113012679),\n",
       "   ('dan grand', 0.006220843511029474),\n",
       "   ('réseau', 0.005382582614838083),\n",
       "   ('article décrire rankmerging', 0.0038386835565063396)],\n",
       "  [('lien', 0.008407582644006963),\n",
       "   ('rankmerging', 0.007586367516641903),\n",
       "   ('dan grand', 0.00614710804413615),\n",
       "   ('réseau', 0.005318783028577264),\n",
       "   ('article décrire rankmerging', 0.0037931837583209515)],\n",
       "  [('lien', 0.01543769971547062),\n",
       "   ('rankmerging', 0.013929814146590655),\n",
       "   ('dan grand', 0.011287097864160966),\n",
       "   ('réseau', 0.009766157375264832),\n",
       "   ('article décrire rankmerging', 0.0069649070732953275)],\n",
       "  [('lien', 0.009514997373946982),\n",
       "   ('rankmerging', 0.00858561492108535),\n",
       "   ('dan grand', 0.00695678167838363),\n",
       "   ('réseau', 0.0060193528499634386),\n",
       "   ('article décrire rankmerging', 0.004292807460542675)],\n",
       "  [('lien', 0.00933952945629246),\n",
       "   ('rankmerging', 0.008427285926049431),\n",
       "   ('dan grand', 0.006828490314055409),\n",
       "   ('réseau', 0.005908348793030866),\n",
       "   ('article décrire rankmerging', 0.004213642963024716)],\n",
       "  [('lien', 0.013979288656986234),\n",
       "   ('rankmerging', 0.012613854167549133),\n",
       "   ('dan grand', 0.010220797272318808),\n",
       "   ('réseau', 0.008843541170940581),\n",
       "   ('article décrire rankmerging', 0.006306927083774567)],\n",
       "  [('2d', 0.007237656440484753),\n",
       "   ('projection', 0.006035459570270754),\n",
       "   ('loutil', 0.005591764475125251),\n",
       "   ('changement', 0.0050327747637857745),\n",
       "   ('lutilisateur', 0.004220882558565774)],\n",
       "  [('lien', 0.006379910457691116),\n",
       "   ('rankmerging', 0.0057567492946158875),\n",
       "   ('dan grand', 0.00466459867906223),\n",
       "   ('réseau', 0.004036042332619623),\n",
       "   ('article décrire rankmerging', 0.0028783746473079437)],\n",
       "  [('lien', 0.01416385257418673),\n",
       "   ('rankmerging', 0.012780390705514904),\n",
       "   ('dan grand', 0.01035573907284802),\n",
       "   ('réseau', 0.008960299515408771),\n",
       "   ('article décrire rankmerging', 0.006390195352757452)],\n",
       "  [('lien', 0.0111427685020418),\n",
       "   ('rankmerging', 0.010054392634440152),\n",
       "   ('dan grand', 0.008146907951201977),\n",
       "   ('réseau', 0.007049109180302951),\n",
       "   ('article décrire rankmerging', 0.005027196317220076)],\n",
       "  [('lien', 0.008514790675593787),\n",
       "   ('rankmerging', 0.00768310394645662),\n",
       "   ('dan grand', 0.006225491972224342),\n",
       "   ('réseau', 0.005386604694218296),\n",
       "   ('article décrire rankmerging', 0.00384155197322831)],\n",
       "  [('biclustering', 0.012504099899609749),\n",
       "   ('the', 0.010846697992719416),\n",
       "   ('of', 0.009013140587212979),\n",
       "   ('dataset', 0.008552558767030904),\n",
       "   ('we', 0.008023477389081898)],\n",
       "  [('motif', 0.010481917285382916),\n",
       "   ('complexité évaluation', 0.00869638192874236),\n",
       "   ('motif lier', 0.00869638192874236),\n",
       "   ('méthode découvrir motif', 0.008213832446443504),\n",
       "   ('complexité', 0.008095030161359365)],\n",
       "  [('approcher fusion', 0.008957646937533929),\n",
       "   ('approcher fusion donnée', 0.008957646937533929),\n",
       "   ('conflit entr', 0.008957646937533929),\n",
       "   ('dinformation qualité', 0.008957646937533929),\n",
       "   ('dinformation qualité donnée', 0.008957646937533929)],\n",
       "  [('lien', 0.012922061337371565),\n",
       "   ('rankmerging', 0.011659892091309746),\n",
       "   ('dan grand', 0.009447817590041605),\n",
       "   ('réseau', 0.008174720778324757),\n",
       "   ('article décrire rankmerging', 0.005829946045654873)],\n",
       "  [('biclustering', 0.005860154431271946),\n",
       "   ('the', 0.005083398710585078),\n",
       "   ('of', 0.004224086193799653),\n",
       "   ('dataset', 0.0040082305451585604),\n",
       "   ('we', 0.0037602719870549017)],\n",
       "  [('motif', 0.013820762250162186),\n",
       "   ('complexité évaluation', 0.011466473527830779),\n",
       "   ('motif lier', 0.011466473527830779),\n",
       "   ('méthode découvrir motif', 0.010830215724299779),\n",
       "   ('complexité', 0.010673570895665846)],\n",
       "  [('lien', 0.012801876116566904),\n",
       "   ('rankmerging', 0.011551446026168437),\n",
       "   ('dan grand', 0.009359945538243073),\n",
       "   ('réseau', 0.008098689517049281),\n",
       "   ('article décrire rankmerging', 0.005775723013084219)],\n",
       "  [('lien', 0.007650079074354304),\n",
       "   ('rankmerging', 0.0069028535129288925),\n",
       "   ('dan grand', 0.005593267959103755),\n",
       "   ('réseau', 0.004839573093813533),\n",
       "   ('article décrire rankmerging', 0.0034514267564644463)],\n",
       "  [('lien', 0.006436506107518807),\n",
       "   ('rankmerging', 0.0058078169341046395),\n",
       "   ('dan grand', 0.0047059779421690755),\n",
       "   ('réseau', 0.0040718457251690135),\n",
       "   ('article décrire rankmerging', 0.0029039084670523197)],\n",
       "  [('lien', 0.005810159595095118),\n",
       "   ('rankmerging', 0.005242649151971675),\n",
       "   ('dan grand', 0.004248031841849648),\n",
       "   ('réseau', 0.0036756080262553615),\n",
       "   ('article décrire rankmerging', 0.0026213245759858375)]],\n",
       " [[('mapreduce', 0.05545519408264755),\n",
       "   ('algorithme baser mapreduce', 0.022309937766993384),\n",
       "   ('appliquer dautre', 0.022309937766993384),\n",
       "   ('appliquer dautre casdétuder', 0.022309937766993384),\n",
       "   ('architecturer maîtreesclaveil', 0.022309937766993384)],\n",
       "  [('chaîne structurer', 0.010769440613187424),\n",
       "   ('chaîne structurer traitementspour', 0.010769440613187424),\n",
       "   ('conception chaîne', 0.010769440613187424),\n",
       "   ('conception chaîne structurer', 0.010769440613187424),\n",
       "   ('connaissance dan don', 0.010769440613187424)],\n",
       "  [('chaîne structurer', 0.004729610482096658),\n",
       "   ('chaîne structurer traitementspour', 0.004729610482096658),\n",
       "   ('conception chaîne', 0.004729610482096658),\n",
       "   ('conception chaîne structurer', 0.004729610482096658),\n",
       "   ('connaissance dan don', 0.004729610482096658)],\n",
       "  [('chaîne structurer', 0.006733860736029905),\n",
       "   ('chaîne structurer traitementspour', 0.006733860736029905),\n",
       "   ('conception chaîne', 0.006733860736029905),\n",
       "   ('conception chaîne structurer', 0.006733860736029905),\n",
       "   ('connaissance dan don', 0.006733860736029905)],\n",
       "  [('dexplorer', 0.012442360950076342),\n",
       "   ('collection', 0.009464491370152206),\n",
       "   ('visualisation', 0.00785443142963239),\n",
       "   ('analyser sémantique latent', 0.007677782972436574),\n",
       "   ('approchecombiner', 0.007677782972436574)],\n",
       "  [('chaîne structurer', 0.0044918355545061975),\n",
       "   ('chaîne structurer traitementspour', 0.0044918355545061975),\n",
       "   ('conception chaîne', 0.0044918355545061975),\n",
       "   ('conception chaîne structurer', 0.0044918355545061975),\n",
       "   ('connaissance dan don', 0.0044918355545061975)],\n",
       "  [('algorithme permettre prédictiondinformater', 0.01445690896172372),\n",
       "   ('biais lidentification', 0.01445690896172372),\n",
       "   ('biais lidentification derègl', 0.01445690896172372),\n",
       "   ('boire prédiction', 0.01445690896172372),\n",
       "   ('boire prédiction dinformation', 0.01445690896172372)],\n",
       "  [('chaîne structurer', 0.00476298629967088),\n",
       "   ('chaîne structurer traitementspour', 0.00476298629967088),\n",
       "   ('conception chaîne', 0.00476298629967088),\n",
       "   ('conception chaîne structurer', 0.00476298629967088),\n",
       "   ('connaissance dan don', 0.00476298629967088)],\n",
       "  [('algorithme permettre prédictiondinformater', 0.004333597654633002),\n",
       "   ('biais lidentification', 0.004333597654633002),\n",
       "   ('biais lidentification derègl', 0.004333597654633002),\n",
       "   ('boire prédiction', 0.004333597654633002),\n",
       "   ('boire prédiction dinformation', 0.004333597654633002)],\n",
       "  [('motif minimal', 0.009419751467024876),\n",
       "   ('motif', 0.007569205717601084),\n",
       "   ('minimal', 0.006739127341349763),\n",
       "   ('densemble', 0.005684140061199893),\n",
       "   ('15 an', 0.003139917155674959)],\n",
       "  [('algorithme permettre prédictiondinformater', 0.006232837028034581),\n",
       "   ('biais lidentification', 0.006232837028034581),\n",
       "   ('biais lidentification derègl', 0.006232837028034581),\n",
       "   ('boire prédiction', 0.006232837028034581),\n",
       "   ('boire prédiction dinformation', 0.006232837028034581)],\n",
       "  [('motif minimal', 0.008198454428119235),\n",
       "   ('motif', 0.006587837094221305),\n",
       "   ('minimal', 0.005865380693615965),\n",
       "   ('densemble', 0.0049471754555232075),\n",
       "   ('15 an', 0.002732818142706412)],\n",
       "  [('motif minimal', 0.010238462435570249),\n",
       "   ('motif', 0.008227077824510674),\n",
       "   ('minimal', 0.007324853779260728),\n",
       "   ('densemble', 0.006178173033422495),\n",
       "   ('15 an', 0.00341282081185675)],\n",
       "  [('chaîne structurer', 0.006980220868498007),\n",
       "   ('chaîne structurer traitementspour', 0.006980220868498007),\n",
       "   ('conception chaîne', 0.006980220868498007),\n",
       "   ('conception chaîne structurer', 0.006980220868498007),\n",
       "   ('connaissance dan don', 0.006980220868498007)],\n",
       "  [('motif minimal', 0.007148908815405117),\n",
       "   ('motif', 0.00574447868073782),\n",
       "   ('minimal', 0.005114509340014389),\n",
       "   ('densemble', 0.004313850437961028),\n",
       "   ('15 an', 0.0023829696051350393)],\n",
       "  [('motif minimal', 0.008443074692008648),\n",
       "   ('motif', 0.0067844007834603),\n",
       "   ('minimal', 0.00604038818870713),\n",
       "   ('densemble', 0.005094786127272095),\n",
       "   ('15 an', 0.0028143582306695497)],\n",
       "  [('thème', 0.011375743271082072),\n",
       "   ('classification', 0.005765408975380599),\n",
       "   ('analysehiérarchique', 0.0052355896784190515),\n",
       "   ('analysehiérarchique donnée', 0.0052355896784190515),\n",
       "   ('analysehiérarchique donnée existant', 0.0052355896784190515)],\n",
       "  [('algorithme permettre prédictiondinformater', 0.0053941490236593965),\n",
       "   ('biais lidentification', 0.0053941490236593965),\n",
       "   ('biais lidentification derègl', 0.0053941490236593965),\n",
       "   ('boire prédiction', 0.0053941490236593965),\n",
       "   ('boire prédiction dinformation', 0.0053941490236593965)],\n",
       "  [('algorithme permettre prédictiondinformater', 0.005285570432606859),\n",
       "   ('biais lidentification', 0.005285570432606859),\n",
       "   ('biais lidentification derègl', 0.005285570432606859),\n",
       "   ('boire prédiction', 0.005285570432606859),\n",
       "   ('boire prédiction dinformation', 0.005285570432606859)],\n",
       "  [('chaîne structurer', 0.007994380094122337),\n",
       "   ('chaîne structurer traitementspour', 0.007994380094122337),\n",
       "   ('conception chaîne', 0.007994380094122337),\n",
       "   ('conception chaîne structurer', 0.007994380094122337),\n",
       "   ('connaissance dan don', 0.007994380094122337)],\n",
       "  [('chaîne structurer', 0.012616176346855754),\n",
       "   ('chaîne structurer traitementspour', 0.012616176346855754),\n",
       "   ('conception chaîne', 0.012616176346855754),\n",
       "   ('conception chaîne structurer', 0.012616176346855754),\n",
       "   ('connaissance dan don', 0.012616176346855754)],\n",
       "  [('chaîne structurer', 0.005448624220379597),\n",
       "   ('chaîne structurer traitementspour', 0.005448624220379597),\n",
       "   ('conception chaîne', 0.005448624220379597),\n",
       "   ('conception chaîne structurer', 0.005448624220379597),\n",
       "   ('connaissance dan don', 0.005448624220379597)],\n",
       "  [('chaîne structurer', 0.005652389553760536),\n",
       "   ('chaîne structurer traitementspour', 0.005652389553760536),\n",
       "   ('conception chaîne', 0.005652389553760536),\n",
       "   ('conception chaîne structurer', 0.005652389553760536),\n",
       "   ('connaissance dan don', 0.005652389553760536)],\n",
       "  [('chaîne structurer', 0.004618894299350209),\n",
       "   ('chaîne structurer traitementspour', 0.004618894299350209),\n",
       "   ('conception chaîne', 0.004618894299350209),\n",
       "   ('conception chaîne structurer', 0.004618894299350209),\n",
       "   ('connaissance dan don', 0.004618894299350209)],\n",
       "  [('motif minimal', 0.012899059407125962),\n",
       "   ('motif', 0.010364990473249822),\n",
       "   ('minimal', 0.009228311833126612),\n",
       "   ('densemble', 0.007783651254973009),\n",
       "   ('15 an', 0.004299686469041988)],\n",
       "  [('algorithme permettre prédictiondinformater', 0.0038894463516531658),\n",
       "   ('biais lidentification', 0.0038894463516531658),\n",
       "   ('biais lidentification derègl', 0.0038894463516531658),\n",
       "   ('boire prédiction', 0.0038894463516531658),\n",
       "   ('boire prédiction dinformation', 0.0038894463516531658)],\n",
       "  [('motif minimal', 0.007391413910571125),\n",
       "   ('motif', 0.005939342734137064),\n",
       "   ('minimal', 0.005288003590151546),\n",
       "   ('densemble', 0.004460184758065288),\n",
       "   ('15 an', 0.002463804636857042)],\n",
       "  [('adapter pallier', 0.00377610085827174),\n",
       "   ('adapter pallier confusionnative', 0.00377610085827174),\n",
       "   ('baser modélisation ontologique', 0.00377610085827174),\n",
       "   ('charger dinformation', 0.00377610085827174),\n",
       "   ('charger dinformation important', 0.00377610085827174)],\n",
       "  [('algorithme permettre prédictiondinformater', 0.009862164247007045),\n",
       "   ('biais lidentification', 0.009862164247007045),\n",
       "   ('biais lidentification derègl', 0.009862164247007045),\n",
       "   ('boire prédiction', 0.009862164247007045),\n",
       "   ('boire prédiction dinformation', 0.009862164247007045)],\n",
       "  [('chaîne structurer', 0.005918162886188969),\n",
       "   ('chaîne structurer traitementspour', 0.005918162886188969),\n",
       "   ('conception chaîne', 0.005918162886188969),\n",
       "   ('conception chaîne structurer', 0.005918162886188969),\n",
       "   ('connaissance dan don', 0.005918162886188969)]]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display labels\n",
    "# labels is composed by an array for each partition\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diachronic analysis\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=fstw, use_idf=True, ngram_range=(min_gram, max_gram))\n",
    "tfidf = vectorizer.fit_transform(usable)\n",
    "\n",
    "# Feature Recall\n",
    "# label_f: num of the label\n",
    "def FR(cluster_c, label_f, partition_C):\n",
    "    total_singledoc = 0\n",
    "    for doc in cluster_c:\n",
    "        total_singledoc += tfidf.toarray().item(doc, label_f)\n",
    "    total_everydoc = 0\n",
    "    for cluster in partition_C:\n",
    "        for doc in cluster:\n",
    "            total_everydoc += tfidf.toarray().item(doc, label_f)\n",
    "    return total_singledoc / total_everydoc\n",
    "\n",
    "# Feature Precision\n",
    "def FP(cluster_c, label_f):\n",
    "    total_singledoc = 0\n",
    "    for doc in cluster_c:\n",
    "        total_singledoc += tfidf.toarray().item(doc, label_f)\n",
    "    total_everydoc = 0\n",
    "    for doc in cluster_c:\n",
    "        total_everydoc += sum(tfidf.toarray()[doc])\n",
    "    return total_singledoc / total_everydoc\n",
    "    \n",
    "# Feature F-measure\n",
    "def FF(cluster_c, label_f, partition_C):\n",
    "    res = 2*FR(cluster_c, label_f, partition_C)*FP(cluster_c, label_f)\n",
    "    res /= (FR(cluster_c, label_f, partition_C) + FP(cluster_c, label_f))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-a2090f32e5fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mffmeanF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    945\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Vamos a implementar una nueva tecnicà para labelisar: FF-measure\n",
    "\n",
    "# list of labels of cluster C from partition P = labels[P][C]\n",
    "labels = []\n",
    "\n",
    "\n",
    "# first, we want the mean ffmeasure of each f\n",
    "ffmeanF = []\n",
    "\n",
    "for f in range(0, len(tfidf.toarray()[0])):\n",
    "    mean = 0\n",
    "    nb = 0\n",
    "    for clusters in partitions:\n",
    "        for clus in clusters:\n",
    "            mean += FF(clus, f, clusters)\n",
    "            nb += 1\n",
    "    mean /= nb\n",
    "    ffmeanF.append(mean)\n",
    "    \n",
    "\n",
    "#TODO: question: on prend on compte les zeros (genre quand le mot n'apparait pas)?\n",
    "# second, we want the mean of every ffmeasure\n",
    "ffmean_total\n",
    "\n",
    "mean = 0\n",
    "nb = 0\n",
    "for f in range(0, len(tfidf.toarray()[0])):\n",
    "    for clusters in partitions:\n",
    "        for clus in clusters:\n",
    "            mean += FF(clus, f, clusters)\n",
    "            nb += 1\n",
    "fflean_total = mean / nb\n",
    "\n",
    "# Now we fill labels[]\n",
    "for clusters in partitions:\n",
    "    labels_for_clusters = []\n",
    "    for clus in clusters:\n",
    "        labels_for_clus = []\n",
    "        for arti in clus:\n",
    "            for num_word in range(0, len(tfidf.toarray()[arti])):\n",
    "                if tfidf.toarray().item(arti, num_word) != 0:\n",
    "                    ffmes = FF(clus, num_word, partitions)\n",
    "                    if ffmes > ffmeanF[num_word] and ffmes > ffmean_total:\n",
    "                        labels_for_clus.append(num_word)\n",
    "        labels_for_clusters[clus] = labels_for_clus\n",
    "    labels[clusters] = labels_for_clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: sigma are ecart-type :)\n",
    "\n",
    "def inter(listA, listB):\n",
    "    return np.intersect1d(listA, listB)\n",
    "    \n",
    "# cluster_t and cluster_s must be in two different partitions\n",
    "def proba(cluster_t, cluster_s, partition_T, partition_S):\n",
    "    total_inter = 0\n",
    "    for f in inter(labels[partition_T][cluster_t], labels[partition_S][cluster_s]):\n",
    "        # Beware! I know I am passing the whole tfidf, but hang on, that's just to understand now\n",
    "        total_inter += FF(cluster_t, f, partition_T)\n",
    "    total_t = 0\n",
    "    for f in labels[partition_T][cluster_t]:\n",
    "        total_t += FF(cluster_t, f, partition_T)\n",
    "    return total_inter / total_t\n",
    "    \n",
    "\n",
    "def P_A(cluster_s, partition_T, partition_S):\n",
    "    # first, we have to know what are the cluster which got the label\n",
    "    total = 0\n",
    "    nb_computation = 0\n",
    "    for label_s in labels[partition_S][num_cluster_s]:\n",
    "        for cluster_t in partition_T:\n",
    "            if label_s in labels[partition_T][cluster_t]:\n",
    "                total += proba(cluster_t, cluster_s, partition_T, partition_S)\n",
    "                nb_computation += 1\n",
    "    return total / nb_computation\n",
    "\n",
    "# Define a coeficient for the activity \n",
    "def activity(partition_S, partition_T):\n",
    "    res = 0\n",
    "    for cluster_s in partition_S:\n",
    "        res += P_A(cluster_s, partition_T, partition_S)\n",
    "    return res / len(partition_S)\n",
    "\n",
    "# Standard deviation\n",
    "# Nothing have been find in the paper, so I put those random values ¯\\_(ツ)_/¯\n",
    "sigma_t = 0.01\n",
    "sigma_s = 0.01\n",
    "\n",
    "# Our Graal\n",
    "# Does cluster_t is similar to cluster_s?\n",
    "def similar(cluster_t, cluster_s, partition_T, partition_S):\n",
    "    cond1 = proba(cluster_t, cluster_s, partition_T, partition_S) > P_A(cluster_s, partition_T, partition_S)\n",
    "    cond2 = proba(cluster_t, cluster_s, partition_T, partition_S) > activity(partition_S, partition_T) + sigma_s\n",
    "    \n",
    "    cond1 = proba(cluster_t, cluster_s, partition_T, partition_S) > P_A(cluster_s, partition_T, partition_S)\n",
    "    cond2 = proba(cluster_t, cluster_s, partition_T, partition_S) > activity(partition_T, partition_S) + sigma_t\n",
    "    return cond1 and cond2 and cond3 and cond4\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
