{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy\n",
    "import string\n",
    "import math\n",
    "\n",
    "from numpy import array\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from gensim.test.utils import common_dictionary, common_corpus\n",
    "from gensim.models import LsiModel\n",
    "from gensim import corpora, models, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Lemmatize correctly, this lemmatizer is really basic, it doesn't recognize verbs and nouns\n",
    "sw = [x.replace('\\n', '') for x in open('stopwords-fr.txt', mode=\"r\", encoding=\"utf-8\").readlines()] + stopwords.words('french') + stopwords.words('english')\n",
    "lemmatizer = FrenchLefffLemmatizer()\n",
    "stemmer = nltk.stem.snowball.FrenchStemmer()\n",
    "\n",
    "#stem, lemmatize, remove punctuation, and return a string\n",
    "def process(article):\n",
    "    res = \"\"\n",
    "    for word in word_tokenize(article):\n",
    "        if not word in sw+string.punctuation.split():\n",
    "            if res == \"\": #if first word\n",
    "                res += stemmer.stem(lemmatizer.lemmatize(word.lower()))\n",
    "            else:\n",
    "                res += \" \" + stemmer.stem(lemmatizer.lemmatize(word.lower()))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('export_articles_EGC_2004_2018.csv', sep='\\t', header=0)\n",
    "\n",
    "#Let's process our corpus, and determine a limit to split it in partitions\n",
    "limit = 0\n",
    "usable = []\n",
    "for i in range(0, len(data['abstract']), 1):\n",
    "    if not isinstance(data['abstract'][i], float): #if not null, empty, or whatever (so if there is a abstract)\n",
    "        usable.append(process(data['abstract'][i]))\n",
    "        if data['year'][i] <= 2010:\n",
    "            limit += 1\n",
    "    \n",
    "print(\"nombre d'articles =\", len(usable))\n",
    "print(\"limit =\", limit)\n",
    "\n",
    "usable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfIdf\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=sw, use_idf=True, ngram_range=(1,3))\n",
    "tfidf = vectorizer.fit_transform(usable)\n",
    "\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This was a try to use gensim. However, I don't success to use it (not yet! >:)c )\n",
    "\n",
    "nb_concepts = 30\n",
    "lsi = LsiModel(tfidf, num_topics=nb_concepts)\n",
    "\n",
    "lsi.print_topics(num_topics=2, num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nous obtenons ici x clusters, représentés par des concepts grâce au LSA\n",
    "#### Nous allons seuiller et étiquetter ces clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, I determine a tresh, to remove documents from clusters\n",
    "#Thus, I tried to compute the X² of the values for each cluster. This X² was my tresh\n",
    "#Yeah yeah yeah. I know, X² is a law, what I compute is just a value to compare to this law\n",
    "#But I find this more pictured to talk about it this way\n",
    "#PS: (don't trust this snippet, I think it's wrong)\n",
    "\n",
    "partitions = []\n",
    "tresh = 0\n",
    "\n",
    "for lsa_part in [lsa1, lsa2]:\n",
    "    # X² pour ne garder que les articles au dessus d'un certain seuil\n",
    "    temp = 0\n",
    "    sumConcept = 0\n",
    "    for concept in transpose(lsa):\n",
    "        for article in concept:\n",
    "            sumConcept += article\n",
    "            \n",
    "            \n",
    "            tresh += pow(article, 2)\n",
    "    tresh /= (lsa_part.shape[0]*lsa_part.shape[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexOf(matrixSrc, listSearched):\n",
    "    if (len(matrixSrc[1]) != len(listSearched)):\n",
    "        return -1\n",
    "    pos = 0\n",
    "    for l in matrixSrc:\n",
    "        thesame = True\n",
    "        for i in range(0, len(listSearched), 1):\n",
    "            if l[i] != listSearched[i]:\n",
    "                thesame = False\n",
    "                break\n",
    "        if thesame:\n",
    "            return pos\n",
    "        pos+=1\n",
    "    return -2\n",
    "a = [[1, 2, 3], [4, 2, 1], [2, 3, 5], [3, 2, 1]]\n",
    "b = [3,2,1]\n",
    "indexOf(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I know, transpose exists in numpy\n",
    "#However, it didn't work with me because I got sparse matrix, so I was deseperate and I wrote my own function\n",
    "def transpose(matrix):\n",
    "    transp = []\n",
    "    for j in range(0, matrix.shape[1], 1):\n",
    "        l = []\n",
    "        for i in range(0, matrix.shape[0], 1):\n",
    "            l.append(matrix[i][j])\n",
    "        transp.append(l)\n",
    "    return transp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostCommons(dico, tresh):\n",
    "    res = []\n",
    "    for name in dico:\n",
    "        if dico[name] >= tresh:\n",
    "            res.append(name)\n",
    "    return res\n",
    "\n",
    "test = {\"abc\": 3, \"trucmuch\":5, \"doge\":0}\n",
    "mostCommons(test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT!! The good way to get an element from document-term matrix\n",
    "tfidf.toarray().item(0, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
