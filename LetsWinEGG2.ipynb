{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from numpy import array\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from gensim.test.utils import common_dictionary, common_corpus\n",
    "from gensim.models import LsiModel\n",
    "from gensim import corpora, models, utils\n",
    "from gensim.test.utils import common_corpus, common_dictionary, get_tmpfile\n",
    "from gensim.models import LsiModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spacy lib\n",
    "# On https://spacy.io/\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Parameters #\n",
    "##############\n",
    "\n",
    "min_gram = 1\n",
    "max_gram = 3\n",
    "\n",
    "# To create ours partitions, we must first know the years which will be the limits\n",
    "limit_years = [2007, 2010, 2013, 2016]\n",
    "\n",
    "# Ignore words that appear at a frequency more than max_frequ in the corpus\n",
    "max_frequ = 0.8\n",
    "\n",
    "# Ignore words appearing less than min_appear in the whole corpus\n",
    "min_appear = 20\n",
    "\n",
    "# Range fo cluster number you want to test\n",
    "cluster_ranges = range(2, 3) #range(2, 100) # Warning, long to compute (but nice)\n",
    "\n",
    "# Number of trial you want to do for each test\n",
    "nb_trial_by_test = 3\n",
    "\n",
    "# Number of cluster you finally choose\n",
    "nb_cluster = 5\n",
    "\n",
    "# Max iteration for each kmeans (default: 300)\n",
    "max_iter = 300\n",
    "\n",
    "# Number of labels by cluster\n",
    "nb_labels = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datas preprocessing methods.\n",
    "\n",
    "# Lemmatisation without poncutations\n",
    "\n",
    "stemmer = nltk.stem.snowball.FrenchStemmer()\n",
    "fstw = stopwords.words('french')\n",
    "\n",
    "# French Stop Words, extraits depuis le fichier stopwords-fr.txt + stopwords french de nltk\n",
    "sourceFST = [x.replace('\\n', '') for x in open('stopwords-fr.txt', mode=\"r\", encoding=\"utf-8\").readlines()]+fstw\n",
    "sourceFST += [x.replace('\\n', '') for x in open('perso_words-fr.txt', mode=\"r\", encoding=\"utf-8\").readlines()]\n",
    "\n",
    "# Based on ration of french and english stopwords\n",
    "def isEnglish(article):\n",
    "    total_fsw = len([x for x in article.split() if x in sourceFST])\n",
    "    total_esw = len([x for x in article.split() if x in stopwords.words('english')])\n",
    "    ratio = 100\n",
    "    if total_fsw != 0:\n",
    "        ratio = total_esw/total_fsw\n",
    "    return ratio > 1 and total_esw > 3\n",
    "\n",
    "def lemmatize(article):\n",
    "    arti_lower = article.lower()\n",
    "    arti_2words = re.sub(\" [0-z][0-z] \", \" \", arti_lower) # word of length < 2\n",
    "    arti_e = re.sub(\"(é|è|ê)\", \"e\", arti_2words)\n",
    "    arti_o = re.sub(\"à\", \"a\", arti_e)\n",
    "    arti_i = re.sub(\"ô\", \"o\", arti_o)\n",
    "    artiregex = re.sub(\"î\", \"i\", arti_i)\n",
    "    output = []\n",
    "    outPonc = artiregex.translate(artiregex.maketrans(\"\",\"\", string.punctuation))\n",
    "    outLem = nlp(outPonc)\n",
    "    for token in outLem:\n",
    "        if token.lemma_ not in sourceFST and [x for x in token.lemma_ if x not in \"0123456789\"] != []:\n",
    "            output.append(token.lemma_)\n",
    "    res = ' '.join(output)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Reading\n",
    "data = pd.read_csv('export_articles_EGC_2004_2018.csv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's process our corpus, and determine a limit to split it in partitions\n",
    "\n",
    "# limits[] let us know when to delimit partitions\n",
    "temp_usable = []\n",
    "\n",
    "prev_year = data['year'][0]\n",
    "numArti = 0\n",
    "for i in range(0, len(data['abstract']), 1):\n",
    "    #if not null, empty, or whatever (so if there is a abstract):\n",
    "    if isinstance(data['abstract'][i], float) or isEnglish(data['abstract'][i]):\n",
    "        temp_usable.append([])\n",
    "    else:\n",
    "        text = data['abstract'][i]\n",
    "        if not isinstance(data['title'][i], float):\n",
    "            text += \" \"+data['title'][i]\n",
    "\n",
    "        numArti+=1\n",
    "        temp_usable.append(re.sub(\" [0-z][0-z] \", \" \", stemmer.stem(lemmatize(text))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-process word removal\n",
    "post_words = [x.replace('\\n', '') for x in open('post_process_words-fr.txt', mode=\"r\", encoding=\"utf-8\").readlines()]\n",
    "\n",
    "for i in range(0, len(temp_usable)):\n",
    "    if len(temp_usable[i]) != 0:\n",
    "        arti = temp_usable[i].split()\n",
    "        res = []\n",
    "        for word in arti:\n",
    "            if word not in post_words:\n",
    "                res.append(word)\n",
    "        temp_usable[i] = ' '.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit computing\n",
    "\n",
    "# limits[] let us know when to delimit partitions\n",
    "# the number into limits represent the document numero that is not included in the previous bound\n",
    "limits = []\n",
    "usable = []\n",
    "\n",
    "prev_year = data['year'][0]\n",
    "numArti = 0\n",
    "for i in range(0, len(temp_usable)):\n",
    "    #if not null, empty, or whatever (so if there is a abstract):\n",
    "    if not isinstance(temp_usable[i], float) and not len(temp_usable[i]) == 0 and not isEnglish(temp_usable[i]):\n",
    "        usable.append(temp_usable[i])\n",
    "        numArti += 1\n",
    "        year = data['year'][i]\n",
    "        if year != prev_year:\n",
    "            prev_year = year\n",
    "            if year in limit_years:\n",
    "                limits.append(numArti)\n",
    "limits.append(numArti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display pre-processed datas\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=sourceFST, use_idf=True, ngram_range=(min_gram, max_gram), max_df=max_frequ, min_df=min_appear)\n",
    "tfidf = vectorizer.fit_transform(usable)\n",
    "\n",
    "print(\"nombre d'articles =\", len(usable))\n",
    "print(\"nombre de mots =\", len(tfidf.toarray()[0]))\n",
    "print(\"limits =\", limits)\n",
    "\n",
    "usable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of partitions_tfidf[], which give us the TFIDF of each cluster of each partition\n",
    "# partitions_tfidf[num_partition][num_doc][num_word]\n",
    "# Beware, num_doc can't be equals to 1091 (max). You have partitions, so every doc aren't in every partitions\n",
    "# num_word can be found via vectorizer.get_feature_name()\n",
    "partitions_tfidf = []\n",
    "beg = 0\n",
    "for l in limits:\n",
    "    last = l\n",
    "    partitions_tfidf.append([list(x) for x in list(tfidf.toarray())[beg:last]])\n",
    "    beg = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans & Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying KMeans on tfidf\n",
    "# the labels_ give assignment of doc to the cluster number \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE :\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "\n",
    "\n",
    "# Silhouette analysis can be used to study the separation distance between the resulting clusters.\n",
    "# The silhouette plot displays a measure of how close each point in one cluster is to points in \n",
    "# the neighboring clusters and thus provides a way to assess parameters like number of clusters visually.\n",
    "# This measure has a range of [-1, 1].\n",
    "\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def kmeans_silhouette(samples, k):\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(random_state = 1 ,n_clusters=k, max_iter=800, init='k-means++', n_init=50, n_jobs=-1)\n",
    "    cluster_labels = clusterer.fit_predict(samples)\n",
    "\n",
    "    silhouette_avg = silhouette_score(samples, cluster_labels)\n",
    "    sample_silhouette_values = silhouette_samples(samples, cluster_labels)\n",
    "\n",
    "    return {'k':k, 'cluster':clusterer, 'labels':cluster_labels ,'silhouette_avg':silhouette_avg, 'sample_silhouette_values':sample_silhouette_values }\n",
    "\n",
    "\n",
    "def kmeans_silhouette_range(samples, k_min, k_max):\n",
    "    kmeans_silhouette_range_value = {}\n",
    "    for k in range(k_min, k_max):\n",
    "        kmeans_silhouette_range_value[k] = kmeans_silhouette(samples, k)\n",
    "    return kmeans_silhouette_range_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_kmeans_silhouette(kmeans_silhouette_value):\n",
    "    clusterer = kmeans_silhouette_value['cluster']\n",
    "    cluster_labels = kmeans_silhouette_value['labels']\n",
    "    silhouette_avg = kmeans_silhouette_value['silhouette_avg']\n",
    "    sample_silhouette_values = kmeans_silhouette_value['sample_silhouette_values']\n",
    "    n_cluster = kmeans_silhouette_value['k']\n",
    "\n",
    "    print(\"For n_cluster =\", n_cluster,\n",
    "              \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1) = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(cluster_labels) + (n_cluster + 1) * 10])\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_cluster):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_cluster)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for cluster = %d\" % n_cluster),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf.toarray()\n",
    "k_s_r = kmeans_silhouette_range(X, 4, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "k_s= kmeans_silhouette(X, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_kmeans_silhouette(k_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def display_time_distribution_cluster(labels, labels_string):\n",
    "    label_limit = ['2004-2007', '2007-2010','2010-2013','2013-2016', '2016-2018']\n",
    "    limit_index = 0\n",
    "\n",
    "    zeros = np.zeros((10, 5))\n",
    "    df = pd.DataFrame(zeros, columns=label_limit)\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        if i <= limits[limit_index]:\n",
    "            df.at[labels[i],label_limit[limit_index]] += 1\n",
    "        else:\n",
    "            limit_index += 1\n",
    "            df.at[labels[i],label_limit[limit_index]] += 1\n",
    "\n",
    "    # To percent\n",
    "    df = df.apply(lambda x: x / x.sum(), axis=1)\n",
    "\n",
    "    # Plot\n",
    "    df.plot.barh(figsize=(20, 30), width=0.9)\n",
    "    plt.title('Time distribution by cluster', fontsize= 24, color='gray')\n",
    "\n",
    "    ## PlotSwagg ##\n",
    "    plt.yticks(fontsize=14, rotation=0, color='gray')\n",
    "    plt.xticks(fontsize=14, rotation=0, color='gray')\n",
    "\n",
    "    # Cleanest Percent\n",
    "    plt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x, _: '{:.0%}'.format(x))) \n",
    "    plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{}'.format(\" \".join(labels_string[y]))))\n",
    "\n",
    "    # Less border\n",
    "    plt.gca().xaxis.grid(True)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['bottom'].set_visible(False)\n",
    "    plt.gca().spines['left'].set_edgecolor('gray')\n",
    "    plt.gca().spines['right'].set_edgecolor('gray')\n",
    "\n",
    "    # Percent line under the barH\n",
    "    plt.gca().set_axisbelow(True)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# it looks like -> { doc_number : [partition_number, cluster_number] }\n",
    "doc_clustering = {}\n",
    "\n",
    "cluster_labels = k_s['labels']\n",
    "\n",
    "numDoc = 0\n",
    "for i in range(0, len(limits)):\n",
    "\n",
    "    previousBound = 0\n",
    "    if i > 0:\n",
    "        previousBound = limits[i-1]\n",
    "    clusterer = kmeans_silhouette(partitions_tfidf[i], nb_cluster)[\"labels\"]\n",
    "    for numDocItern in range(0, limits[i]-previousBound):\n",
    "        doc_clustering[numDoc] = [i, clusterer[numDocItern]]\n",
    "        numDoc+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows to get list of documents number\n",
    "# return [dou numbers]\n",
    "# params : partition_number , cluster number\n",
    "def get_doc(part, clust):\n",
    "    docs = []\n",
    "    for i in range(0,len(doc_clustering)):\n",
    "        if doc_clustering[i][0] == part and doc_clustering[i][1] == clust:\n",
    "            docs.append(i)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the partitions variable\n",
    "# Here partitions[part][cluster] = list of docs numbe\n",
    "partitions = []\n",
    "for i in range(0, len(limits)):\n",
    "    clusters = []\n",
    "    for j in range(0, nb_cluster):\n",
    "        clusters.append(get_doc(i,j))\n",
    "    partitions.append(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Khi²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_of_your_word = tf[numDoc][strWord]\n",
    "tf = []\n",
    "for doc in usable:\n",
    "    tf_doc = {}\n",
    "    for word in vectorizer.get_feature_names():\n",
    "        tf_doc[word] = doc.count(word)\n",
    "    tf.append(tf_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number total of words\n",
    "# nb_total_word[numPartition]\n",
    "nb_total_word = []\n",
    "nb = 0\n",
    "\n",
    "for numDoc in range(0, len(usable)):\n",
    "    for word in vectorizer.get_feature_names():\n",
    "        nb += tf[numDoc][word]\n",
    "    if numDoc+1 in limits:\n",
    "        nb_total_word.append(nb)\n",
    "        nb=0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_total_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_word[num_partition][word]\n",
    "nb_word = []\n",
    "\n",
    "word_in_this_parti = {}\n",
    "for word in vectorizer.get_feature_names():\n",
    "    word_in_this_parti[word] = 0\n",
    "\n",
    "for numDoc in range(0, len(usable)):\n",
    "    for word in vectorizer.get_feature_names():\n",
    "        word_in_this_parti[word] += tf[numDoc][word]\n",
    "    if numDoc+1 in limits:\n",
    "        nb_word.append(word_in_this_parti)\n",
    "        word_in_this_parti = {}\n",
    "        for word in vectorizer.get_feature_names():\n",
    "            word_in_this_parti[word] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nb_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_word_by_cluster[numPartition][numCluster]\n",
    "nb_word_by_cluster = []\n",
    "for parti in partitions:\n",
    "    nb_word_clus = []\n",
    "    for cluster in parti:\n",
    "        nb = 0\n",
    "        for numDoc in cluster:\n",
    "            for word in vectorizer.get_feature_names():\n",
    "                nb += tf[numDoc][word]\n",
    "        nb_word_clus.append(nb)\n",
    "    nb_word_by_cluster.append(nb_word_clus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_of_khi2 = khi2[numPartition][numCluster][word]\n",
    "khi2 = []\n",
    "\n",
    "for numParti in range(0, len(partitions)):\n",
    "    khi2parti = []\n",
    "    for numCluster in range(0, len(partitions[numParti])):\n",
    "        khi2cluster = {}\n",
    "        \n",
    "        for word in vectorizer.get_feature_names():\n",
    "            if nb_word_by_cluster[numParti][numCluster] == 0:\n",
    "                khi2cluster[word] = 0\n",
    "            else:\n",
    "                word_in_this_parti[word] = 0\n",
    "                E = nb_word[numParti][word]\n",
    "                E =+ nb_word_by_cluster[numParti][numCluster]\n",
    "                E = E/ nb_total_word[numParti]\n",
    "                N = 0\n",
    "                for numDoc in partitions[numParti][numCluster]:\n",
    "                    N += tf[numDoc][word]\n",
    "                khi2cluster[word] = (pow(N - E, 2)/E)        \n",
    "        khi2parti.append(khi2cluster)\n",
    "    khi2.append(khi2parti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of your labels = labels[numPartition][numCluster]\n",
    "labels = []\n",
    "\n",
    "for numPartition in range(0, len(nb_word_by_cluster)):\n",
    "    label_clus = []\n",
    "    for numCluster in range(0, len(nb_word_by_cluster[numPartition])):\n",
    "        label_clus.append(Counter(khi2[numPartition][numCluster]).most_common(nb_labels))\n",
    "    labels.append(label_clus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some clusters can be empty, so they have a score of 0 on each labels\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diachronic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_cluster = []\n",
    "\n",
    "for i in range(0, len(limits)):\n",
    "    labels_cluster.append('')\n",
    "    s = []\n",
    "    for j in range(0, nb_cluster):\n",
    "        s.append(' '.join([t[0] for t in labels[i][j]]))\n",
    "    labels_cluster.append(s)\n",
    "labels_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low level analysis\n",
    "\n",
    "display_time_distribution_cluster(k_s['labels'], labels_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inter(listA, listB):\n",
    "    return np.intersect1d(listA, listB)\n",
    "    \n",
    "# cluster_t and cluster_s must be in two different partitions\n",
    "def proba(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S):\n",
    "    total_inter = 0\n",
    "    total_t = 0\n",
    "    for f in range(0, len(labels[num_partition_T][num_cluster_t])):\n",
    "        for f_s in labels[num_partition_S][num_cluster_s]:\n",
    "            if labels[num_partition_T][num_cluster_t][f][0] == f_s[0]:\n",
    "                total_inter += labels[num_partition_T][num_cluster_t][f][1]\n",
    "                break\n",
    "        total_t += labels[num_partition_T][num_cluster_t][f][1]\n",
    "    if total_t == 0:\n",
    "        return 0\n",
    "    return total_inter / total_t\n",
    "    \n",
    "\n",
    "def P_A(num_cluster_s, num_partition_T, num_partition_S):\n",
    "    # first, we have to know what are the cluster which got the label\n",
    "    total = 0\n",
    "    nb_computation = 0\n",
    "    for label_s in labels[num_partition_S][num_cluster_s]:\n",
    "        for num_cluster_t in range(0, len(partitions[num_partition_T])):\n",
    "            if label_s in labels[num_partition_T][num_cluster_t]:\n",
    "                total += proba(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S)\n",
    "                nb_computation += 1\n",
    "    if nb_computation == 0:\n",
    "        return 0\n",
    "    return total / nb_computation\n",
    "\n",
    "# Define a coeficient for the activity \n",
    "def activity(num_partition_S, num_partition_T):\n",
    "    res = 0\n",
    "    for num_cluster_s in range(0, len(partitions[num_partition_S])):\n",
    "        res += P_A(num_cluster_s, num_partition_T, num_partition_S)\n",
    "    return res / len(partitions[num_partition_S])\n",
    "\n",
    "# Ecart-type, but it isn't very usefull xD\n",
    "sigma_t = 0.01\n",
    "sigma_s = 0.01\n",
    "\n",
    "# Our Graal\n",
    "def similar(num_cluster_t, num_partition_T, num_cluster_s, num_partition_S):\n",
    "    cond1 = proba(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S) > P_A(num_cluster_s, num_partition_T, num_partition_S)\n",
    "    cond2 = proba(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S) > activity(num_partition_S, num_partition_T) + sigma_s\n",
    "    \n",
    "    cond3 = proba(num_cluster_s, num_cluster_t, num_partition_S, num_partition_T) > P_A(num_cluster_t, num_partition_S, num_partition_T)\n",
    "    cond4 = proba(num_cluster_s, num_cluster_t, num_partition_S, num_partition_T) > activity(num_partition_T, num_partition_S) + sigma_t\n",
    "    return cond1 and cond2 and cond3 and cond4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the diachronic analysis results\n",
    "\n",
    "# Node coloring source: https://stackoverflow.com/questions/13517614/draw-different-color-for-nodes-in-networkx-based-on-their-node-value#13517947\n",
    "\n",
    "# The more the partition is high, the more the partition is recent\n",
    "\n",
    "val_map = {}\n",
    "values = []\n",
    "\n",
    "g = nx.Graph() #nx.DiGraph(directed=True)\n",
    "for n_part in range(0, len(limits)):\n",
    "    for n_clus in range(0, nb_cluster):\n",
    "        node_str = '('+str(n_clus)+','+str(n_part)+')'\n",
    "        g.add_node(node_str, posxy=(n_part, n_clus), partition = n_part)\n",
    "        val_map[node_str] = n_part/len(limits)\n",
    "\n",
    "values = [val_map.get(node, 0.25) for node in g.nodes()]\n",
    "\n",
    "for numParti in range(0, len(partitions)-1):\n",
    "    for num_cluster_t in range(0, nb_cluster):\n",
    "        for num_cluster_s in range(0, nb_cluster):\n",
    "            if not similar(num_cluster_t, numParti, num_cluster_s, numParti+1):\n",
    "                #print(\"(\"+str(num_cluster_t)+\",\"+str(numParti)+\") est similaire à (\"+str(num_cluster_s)+\",\"+str(numParti+1)+\")\")\n",
    "                g.add_edges_from([(\"(\"+str(num_cluster_t)+\",\"+str(numParti)+\")\", \"(\"+str(num_cluster_s)+\",\"+str(numParti+1)+\")\")])\n",
    "                \n",
    "positions = nx.get_node_attributes(g,'posxy')\n",
    "options = {\n",
    "    'node_color': 'blue',\n",
    "    'node_size': 100,\n",
    "    'width': 3,\n",
    "    'arrowstyle': '-|>',\n",
    "    'arrowsize': 12,\n",
    "}\n",
    "nx.draw(g, positions, node_size=3000/nb_cluster, cmap=plt.get_cmap('jet'), node_color=values, arrowsize=20, arrows=True, arrowstyle=\"-|>\", arraowsize=12)\n",
    "pos = nx.circular_layout(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for labels_clus in labels:\n",
    "    for clus in labels_clus:\n",
    "        for w in clus:\n",
    "            print(w[0])\n",
    "        print('')\n",
    "    print('___')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
