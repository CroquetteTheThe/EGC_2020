{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from numpy import array\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from gensim.test.utils import common_dictionary, common_corpus\n",
    "from gensim.models import LsiModel\n",
    "from gensim import corpora, models, utils\n",
    "from gensim.test.utils import common_corpus, common_dictionary, get_tmpfile\n",
    "from gensim.models import LsiModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spacy lib\n",
    "# On https://spacy.io/\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datas preprocessing methods.\n",
    "# Lemmatisation without poncutations\n",
    "\n",
    "stemmer = nltk.stem.snowball.FrenchStemmer()\n",
    "fstw = stopwords.words('french')\n",
    "\n",
    "# French Stop Words, extraits depuis le fichier stopwords-fr.txt + stopwords french de nltk\n",
    "sourceFST = [x.replace('\\n', '') for x in open('stopwords-fr.txt', mode=\"r\", encoding=\"utf-8\").readlines()]+fstw\n",
    "\n",
    "# Based on ration of french and english stopwords\n",
    "def isEnglish(article):\n",
    "    total_fsw = len([x for x in article.split() if x in sourceFST])\n",
    "    total_esw = len([x for x in article.split() if x in stopwords.words('english')])\n",
    "    ratio = 100\n",
    "    if total_fsw != 0:\n",
    "        ratio = total_esw/total_fsw\n",
    "    return ratio > 1 and total_esw > 3\n",
    "\n",
    "def lemmatize(article):\n",
    "    artiregex = article.replace(\" [A-z][A-z] \", \" \") # word of length < 2\n",
    "    output = []\n",
    "    outPonc = artiregex.translate(article.maketrans(\"\",\"\", string.punctuation))\n",
    "    outLem = nlp(outPonc)\n",
    "    for token in outLem:\n",
    "        if token.lemma_ not in sourceFST and [x for x in token.lemma_ if x not in \"0123456789\"] != []:\n",
    "            output.append(token.lemma_)\n",
    "    res = ' '.join(output)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Reading\n",
    "data = pd.read_csv('export_articles_EGC_2004_2018.csv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's process our corpus, and determine a limit to split it in partitions\n",
    "\n",
    "# usable[] correspond to our corpus processed\n",
    "# limits[] let us know when to delimit partitions\n",
    "limits = []\n",
    "usable = []\n",
    "\n",
    "# To create ours delimiters, we must first know the years which will be the limits\n",
    "limit_years = [2007, 2010, 2014]\n",
    "\n",
    "prev_year = data['year'][0]\n",
    "numArti = 0\n",
    "for i in range(0, len(data['abstract']), 1):\n",
    "    if not isinstance(data['abstract'][i], float) and not isEnglish(data['abstract'][i]): #if not null, empty, or whatever (so if there is a abstract)\n",
    "        year = data['year'][i]\n",
    "        if year != prev_year:\n",
    "            prev_year = year\n",
    "            if year in limit_years:\n",
    "                limits.append(numArti)\n",
    "        numArti+=1\n",
    "        usable.append(stemmer.stem(lemmatize(data['abstract'][i])))\n",
    "limits.append(numArti)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TfidfTransformer' object has no attribute '_idf_diag'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-c23d13656641>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msourceFST\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_idf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_gram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_gram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midf_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36midf_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1345\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0midf_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1347\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midf_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36midf_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m         \u001b[1;31m# if _idf_diag is not set, this will raise an attribute error,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;31m# which means hasattr(self, \"idf_\") is False\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_idf_diag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TfidfTransformer' object has no attribute '_idf_diag'"
     ]
    }
   ],
   "source": [
    "# Display pre-processed datas\n",
    "min_gram = 1\n",
    "max_gram = 3\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=sourceFST, use_idf=True, ngram_range=(min_gram, max_gram))\n",
    "print (vectorizer.idf_)\n",
    "tfidf = vectorizer.fit_transform(usable)\n",
    "\n",
    "print(\"nombre d'articles =\", len(usable))\n",
    "#print(\"nombre de mots =\", len(tfidf.toarray()[0]))\n",
    "print(\"limits =\", limits)\n",
    "\n",
    "usable[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params\n",
    "nb_concepts = 30\n",
    "min_gram = 1\n",
    "max_gram = 3\n",
    "\n",
    "# Creation of cleandocs, which is usable[] with ngrams\n",
    "cleandocs = []\n",
    "for t in usable:\n",
    "    doc = []\n",
    "    for n in range(min_gram, max_gram+1):\n",
    "        for gram in ngrams(t.split(), n):\n",
    "            doc.append(\" \".join(gram))\n",
    "    cleandocs.append(doc)\n",
    "\n",
    "# Creation of tfidf model, a tool to create ours tfidf\n",
    "corpus = []\n",
    "dictionary = corpora.Dictionary(cleandocs)\n",
    "for doc in cleandocs:\n",
    "    newVec = dictionary.doc2bow(doc)\n",
    "    corpus.append(newVec)\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "# Creation of partitions_lsa[], which give us the LSA of each partition\n",
    "partitions_lsa = []\n",
    "beg = 0\n",
    "for l in limits:\n",
    "    last = l\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    lsi = models.LsiModel(corpus_tfidf, num_topics=nb_concepts, id2word=dictionary)\n",
    "    corpus_lsi = lsi[corpus_tfidf[beg:last]]\n",
    "    partitions_lsa.append(corpus_lsi)\n",
    "    beg = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partition = 0\n",
    "for lsa in partitions_lsa:\n",
    "    print(\"Partition numéro:\",num_partition)\n",
    "    num_partition+=1\n",
    "    i=0\n",
    "    for doc in lsa:\n",
    "        if (i<3):\n",
    "            print(\"document number \", i)\n",
    "            i+=1\n",
    "            print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create ours partitions\n",
    "partitions = []\n",
    "\n",
    "# You must specify a treshold, to know what are the doc you keep, and what are the doc you drop\n",
    "tresh = 0.03\n",
    "\n",
    "for corpus_lsi in partitions_lsa:\n",
    "    # Let's create ours clusters\n",
    "    clusters = []\n",
    "\n",
    "    for i in range(0, nb_concepts):\n",
    "        dic = {}\n",
    "        num_doc = 0\n",
    "        for doc in corpus_lsi:\n",
    "            if abs(doc[i][1]) > tresh:\n",
    "                dic[num_doc] = doc[i][1]\n",
    "            num_doc+=1\n",
    "        clusters.append(dic)\n",
    "    partitions.append(clusters)\n",
    "    \n",
    "# TODO: it would be nice to know how many articles are in no cluster anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display clusters 3 of partition 0 \n",
    "partitions[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_labels_by_cluster = 5\n",
    "\n",
    "# Let's labelize our clusters\n",
    "# For this, we will use the tfidf matrix\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=sourceFST, use_idf=True, ngram_range=(min_gram, max_gram))\n",
    "tfidf = vectorizer.fit_transform(usable)\n",
    "\n",
    "# We can access the value in the tfidf using:\n",
    "#tfidf.toarray().item(num_doc, num_word)\n",
    "# To know the number of the word searched, we will use:\n",
    "#vectorizer.vocabulary_[word]\n",
    "\n",
    "# take less than 8h to compute x)\n",
    "labels = []\n",
    "for clusters in partitions:\n",
    "    l = []\n",
    "    for clus in clusters:\n",
    "        first_arti = True\n",
    "        for article in clus:\n",
    "            link = abs(clus[article])\n",
    "            if first_arti:\n",
    "                coef_list = (tfidf.toarray()[article] * link)\n",
    "                first = False\n",
    "            else:\n",
    "                # the more an article have a high coeficient, the more he is implied in the labeling step\n",
    "                coef_list += (tfidf.toarray()[article] * link)\n",
    "        # Now we have coef_list filled by every coeficient in the multiple tfidf\n",
    "        # Let's find the best ones, to finally get the labels\n",
    "        res = dict(zip(vectorizer.get_feature_names(), coef_list))\n",
    "\n",
    "        l.append(Counter(res).most_common(nb_labels_by_cluster))\n",
    "    labels.append(l)\n",
    "\n",
    "# TODO: on observe beaucoup de labels identiques entre deux clusters\n",
    "# Je pense que c'est parce que l'on a trop de clusters, mais j'aimerais en être sûr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display labels\n",
    "# labels is composed by an array for each partition\n",
    "labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
