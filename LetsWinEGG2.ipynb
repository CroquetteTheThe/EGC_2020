{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from numpy import array\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from gensim.test.utils import common_dictionary, common_corpus\n",
    "from gensim.models import LsiModel\n",
    "from gensim import corpora, models, utils\n",
    "from gensim.test.utils import common_corpus, common_dictionary, get_tmpfile\n",
    "from gensim.models import LsiModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spacy lib\n",
    "# On https://spacy.io/\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Parameters #\n",
    "##############\n",
    "\n",
    "min_gram = 1\n",
    "max_gram = 3\n",
    "\n",
    "# To create ours partitions, we must first know the years which will be the limits\n",
    "limit_years = [2007, 2010, 2014]\n",
    "\n",
    "# Ignore words that appear at a frequency less than tresh_freq in the corpus\n",
    "tresh_freq = 0.8\n",
    "\n",
    "# Number of clusters by partitions\n",
    "nb_clusters = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datas preprocessing methods.\n",
    "\n",
    "# Lemmatisation without poncutations\n",
    "\n",
    "stemmer = nltk.stem.snowball.FrenchStemmer()\n",
    "fstw = stopwords.words('french')\n",
    "\n",
    "# French Stop Words, extraits depuis le fichier stopwords-fr.txt + stopwords french de nltk\n",
    "sourceFST = [x.replace('\\n', '') for x in open('stopwords-fr.txt', mode=\"r\", encoding=\"utf-8\").readlines()]+fstw\n",
    "\n",
    "# Based on ration of french and english stopwords\n",
    "def isEnglish(article):\n",
    "    total_fsw = len([x for x in article.split() if x in sourceFST])\n",
    "    total_esw = len([x for x in article.split() if x in stopwords.words('english')])\n",
    "    ratio = 100\n",
    "    if total_fsw != 0:\n",
    "        ratio = total_esw/total_fsw\n",
    "    return ratio > 1 and total_esw > 3\n",
    "\n",
    "def lemmatize(article):\n",
    "    artiregex = re.sub(\" [A-z][A-z] \", \" \", article) # word of length < 2\n",
    "    artiregex = artiregex.lower()\n",
    "    artiregex = re.sub(\"(é|è|ê)\", \"e\", artiregex)\n",
    "    output = []\n",
    "    outPonc = artiregex.translate(artiregex.maketrans(\"\",\"\", string.punctuation))\n",
    "    outLem = nlp(outPonc)\n",
    "    for token in outLem:\n",
    "        if token.lemma_ not in sourceFST and [x for x in token.lemma_ if x not in \"0123456789\"] != []:\n",
    "            output.append(token.lemma_)\n",
    "    res = ' '.join(output)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Reading\n",
    "data = pd.read_csv('export_articles_EGC_2004_2018.csv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's process our corpus, and determine a limit to split it in partitions\n",
    "\n",
    "# usable[] correspond to our corpus processed\n",
    "# limits[] let us know when to delimit partitions\n",
    "limits = []\n",
    "usable = []\n",
    "\n",
    "prev_year = data['year'][0]\n",
    "numArti = 0\n",
    "for i in range(0, len(data['abstract']), 1):\n",
    "    #if not null, empty, or whatever (so if there is a abstract):\n",
    "    if not isinstance(data['abstract'][i], float) and not isEnglish(data['abstract'][i]):\n",
    "        text = data['abstract'][i]\n",
    "        if not isinstance(data['title'][i], float):\n",
    "            text += \" \"+data['title'][i]\n",
    "\n",
    "        numArti+=1\n",
    "        usable.append(stemmer.stem(lemmatize(text)))\n",
    "        year = data['year'][i]\n",
    "        if year != prev_year:\n",
    "            prev_year = year\n",
    "            if year in limit_years:\n",
    "                limits.append(numArti)\n",
    "limits.append(numArti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'articles = 991\n",
      "nombre de mots = 133088\n",
      "limits = [223, 468, 694, 991]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'plateforme objectif permettre citoyen danalyserpar euxmemer tweet politique devenement specifiqu francepour cas lelection presidentiell ideo2017 analyser quasitemps reel message candidat fournir principal caracteristiqueslusage lexiqu politique comparaison entrer candidat ideo2017   plateforme citoyen dediee lanalyse tweet evenement polit'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display pre-processed datas\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=sourceFST, use_idf=True, ngram_range=(min_gram, max_gram), max_df=tresh_freq)\n",
    "tfidf = vectorizer.fit_transform(usable)\n",
    "\n",
    "print(\"nombre d'articles =\", len(usable))\n",
    "print(\"nombre de mots =\", len(tfidf.toarray()[0]))\n",
    "print(\"limits =\", limits)\n",
    "\n",
    "usable[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of partitions_tfidf[], which give us the TFIDF of each cluster of each partition\n",
    "# partitions_tfidf[num_partition][num_doc][num_word]\n",
    "# Beware, num_doc can't be equals to 1091 (max). You have partitions, so every doc aren't in every partitions\n",
    "# num_word can be found via vectorizer.get_feature_name()\n",
    "partitions_tfidf = []\n",
    "beg = 0\n",
    "for l in limits:\n",
    "    last = l\n",
    "    partitions_tfidf.append([list(x) for x in list(tfidf.toarray())[beg:last]])\n",
    "    beg = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[223, 468, 694, 991]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying KMeans on tfidf\n",
    "# the labels_ give assignment of doc to the cluster number \n",
    "km = KMeans(n_clusters=nb_clusters)\n",
    "km.fit(tfidf)\n",
    "cluster = km.labels_\n",
    "\n",
    "cluster_partition = [cluster[:limits[0]],cluster[limits[0]:limits[1]],cluster[limits[1]:limits[2]],cluster[limits[2]:limits[3]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_clustering is a dictionnary \n",
    "# it looks like -> { doc_number : [partition_number, cluster_number] }\n",
    "# This is used to reassign doc number to their respective partition and and cluster\n",
    "doc_clustering = {}\n",
    "for i in range(0,len(usable)):\n",
    "    if i < limits[0]:\n",
    "        doc_clustering[i] = [0, cluster[i]]\n",
    "    elif i >= limits[0] and i < limits[1]:\n",
    "        doc_clustering[i] = [1, cluster[i]]\n",
    "    elif i >= limits[1] and i < limits[2]:\n",
    "        doc_clustering[i] = [2, cluster[i]]\n",
    "    else:\n",
    "        doc_clustering[i] = [3, cluster[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]\n",
      "[0, 2]\n",
      "\n",
      "Counter({0: 69, 4: 59, 1: 40, 2: 31, 3: 24})\n",
      "Counter({4: 62, 0: 58, 2: 54, 1: 41, 3: 30})\n",
      "Counter({4: 61, 0: 51, 2: 49, 3: 34, 1: 31})\n",
      "Counter({4: 85, 2: 77, 0: 66, 3: 42, 1: 27})\n"
     ]
    }
   ],
   "source": [
    "# example of output for doc_clustering\n",
    "# doc 465 is in cluster 1 of the partition 1\n",
    "# doc 154 is in cluster 2 of the partition 0\n",
    "\n",
    "print(doc_clustering[465])\n",
    "print(doc_clustering[154])\n",
    "print()\n",
    "\n",
    "# Here, just count docs number by cluster.\n",
    "print(Counter(cluster_partition[0]))\n",
    "print(Counter(cluster_partition[1]))\n",
    "print(Counter(cluster_partition[2]))\n",
    "print(Counter(cluster_partition[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### A quoi sert le reste en dessous là ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_partition = 0\n",
    "for partition in partitions_tfidf:\n",
    "    if (num_partition > 2):\n",
    "        break\n",
    "    print(\"Partition numéro:\",num_partition)\n",
    "    num_partition+=1\n",
    "    i=0\n",
    "    for cluster_tfidf in partition:\n",
    "        if (i<3):\n",
    "            print(\"cluster number \", i)\n",
    "            i+=1\n",
    "            print(cluster_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create ours partitions\n",
    "partitions = []\n",
    "\n",
    "for part in partitions_tfidf:\n",
    "    # Let's create ours clusters\n",
    "    clusters = []\n",
    "    kmeans = KMeans(n_clusters=nb_clusters, random_state=0).fit(part)\n",
    "    partitions.append(clusters)\n",
    "    \n",
    "# TODO: it would be nice to know how many articles are in no cluster anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'partitions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-413a7d0c7d55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Display clusters 3 of partition 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'partitions' is not defined"
     ]
    }
   ],
   "source": [
    "# Display clusters 3 of partition 0 \n",
    "len(partitions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_labels_by_cluster = 5\n",
    "\n",
    "# Let's labelize our clusters\n",
    "# For this, we will use the tfidf matrix\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=sourceFST, use_idf=True, ngram_range=(min_gram, max_gram))\n",
    "tfidf = vectorizer.fit_transform(usable)\n",
    "\n",
    "# We can access the value in the tfidf using:\n",
    "#tfidf.toarray().item(num_doc, num_word)\n",
    "# To know the number of the word searched, we will use:\n",
    "#vectorizer.vocabulary_[word]\n",
    "\n",
    "# take less than 8h to compute x)\n",
    "labels = []\n",
    "for clusters in partitions:\n",
    "    l = []\n",
    "    for clus in clusters:\n",
    "        first_arti = True\n",
    "        for article in clus:\n",
    "            link = abs(clus[article])\n",
    "            if first_arti:\n",
    "                coef_list = (tfidf.toarray()[article] * link)\n",
    "                first = False\n",
    "            else:\n",
    "                # the more an article have a high coeficient, the more he is implied in the labeling step\n",
    "                coef_list += (tfidf.toarray()[article] * link)\n",
    "        # Now we have coef_list filled by every coeficient in the multiple tfidf\n",
    "        # Let's find the best ones, to finally get the labels\n",
    "        res = dict(zip(vectorizer.get_feature_names(), coef_list))\n",
    "\n",
    "        l.append(Counter(res).most_common(nb_labels_by_cluster))\n",
    "    labels.append(l)\n",
    "\n",
    "# TODO: on observe beaucoup de labels identiques entre deux clusters\n",
    "# Je pense que c'est parce que l'on a trop de clusters, mais j'aimerais en être sûr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display labels\n",
    "# labels is composed by an array for each partition\n",
    "labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
