{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from numpy import array\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from gensim.test.utils import common_dictionary, common_corpus\n",
    "from gensim.models import LsiModel\n",
    "from gensim import corpora, models, utils\n",
    "from gensim.test.utils import common_corpus, common_dictionary, get_tmpfile\n",
    "from gensim.models import LsiModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spacy lib\n",
    "# On https://spacy.io/\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Parameters #\n",
    "##############\n",
    "\n",
    "min_gram = 1\n",
    "max_gram = 3\n",
    "\n",
    "# To create ours partitions, we must first know the years which will be the limits\n",
    "limit_years = [2007, 2010, 2013, 2016]\n",
    "\n",
    "# Ignore words that appear at a frequency less than max_frequ in the corpus\n",
    "max_frequ = 0.8\n",
    "\n",
    "# Ignore words appearing less than min_appear in the whole corpus\n",
    "min_appear = 5\n",
    "\n",
    "# Range fo cluster number you want to test\n",
    "cluster_ranges = range(2, 30)\n",
    "\n",
    "# Number of trial you want to do for each test\n",
    "nb_trial_by_test = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datas preprocessing methods.\n",
    "\n",
    "# Lemmatisation without poncutations\n",
    "\n",
    "stemmer = nltk.stem.snowball.FrenchStemmer()\n",
    "fstw = stopwords.words('french')\n",
    "\n",
    "# French Stop Words, extraits depuis le fichier stopwords-fr.txt + stopwords french de nltk\n",
    "sourceFST = [x.replace('\\n', '') for x in open('stopwords-fr.txt', mode=\"r\", encoding=\"utf-8\").readlines()]+fstw\n",
    "sourceFST += [x.replace('\\n', '') for x in open('perso_words-fr.txt', mode=\"r\", encoding=\"utf-8\").readlines()]\n",
    "\n",
    "# Based on ration of french and english stopwords\n",
    "def isEnglish(article):\n",
    "    total_fsw = len([x for x in article.split() if x in sourceFST])\n",
    "    total_esw = len([x for x in article.split() if x in stopwords.words('english')])\n",
    "    ratio = 100\n",
    "    if total_fsw != 0:\n",
    "        ratio = total_esw/total_fsw\n",
    "    return ratio > 1 and total_esw > 3\n",
    "\n",
    "def lemmatize(article):\n",
    "    arti_lower = article.lower()\n",
    "    arti_2words = re.sub(\" [0-z][0-z] \", \" \", arti_lower) # word of length < 2\n",
    "    arti_e = re.sub(\"(é|è|ê)\", \"e\", arti_2words)\n",
    "    arti_o = re.sub(\"à\", \"a\", arti_e)\n",
    "    arti_i = re.sub(\"ô\", \"o\", arti_o)\n",
    "    artiregex = re.sub(\"î\", \"i\", arti_i)\n",
    "    output = []\n",
    "    outPonc = artiregex.translate(artiregex.maketrans(\"\",\"\", string.punctuation))\n",
    "    outLem = nlp(outPonc)\n",
    "    for token in outLem:\n",
    "        if token.lemma_ not in sourceFST and [x for x in token.lemma_ if x not in \"0123456789\"] != []:\n",
    "            output.append(token.lemma_)\n",
    "    res = ' '.join(output)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Reading\n",
    "data = pd.read_csv('export_articles_EGC_2004_2018.csv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's process our corpus, and determine a limit to split it in partitions\n",
    "\n",
    "# usable[] correspond to our corpus processed\n",
    "# limits[] let us know when to delimit partitions\n",
    "limits = []\n",
    "usable = []\n",
    "\n",
    "prev_year = data['year'][0]\n",
    "numArti = 0\n",
    "for i in range(0, len(data['abstract']), 1):\n",
    "    #if not null, empty, or whatever (so if there is a abstract):\n",
    "    if not isinstance(data['abstract'][i], float) and not isEnglish(data['abstract'][i]):\n",
    "        text = data['abstract'][i]\n",
    "        if not isinstance(data['title'][i], float):\n",
    "            text += \" \"+data['title'][i]\n",
    "\n",
    "        numArti+=1\n",
    "        usable.append(re.sub(\" [0-z][0-z] \", \" \", stemmer.stem(lemmatize(text))))\n",
    "        year = data['year'][i]\n",
    "        if year != prev_year:\n",
    "            prev_year = year\n",
    "            if year in limit_years:\n",
    "                limits.append(numArti)\n",
    "limits.append(numArti)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-process word removal\n",
    "post_words = [x.replace('\\n', '') for x in open('post_process_words-fr.txt', mode=\"r\", encoding=\"utf-8\").readlines()]\n",
    "\n",
    "for i in range(0, len(usable)):\n",
    "    arti = usable[i].split()\n",
    "    res = []\n",
    "    for word in arti:\n",
    "        if word not in post_words:\n",
    "            res.append(word)\n",
    "    usable[i] = ' '.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/info/etu/m2/i140302/venv/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['quelqu'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'articles = 991\n",
      "nombre de mots = 2353\n",
      "limits = [114, 301, 468, 694, 991]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'plateforme objectif permettre citoyen euxmemer tweet politique devenement specifiqu francepour cas lelection presidentiell ideo2017 analyser quasitemps reel message candidat fournir principal caracteristiqueslusage lexiqu politique comparaison entrer candidat ideo2017 plateforme citoyen dediee lanalyse tweet evenement polit'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display pre-processed datas\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=sourceFST, use_idf=True, ngram_range=(min_gram, max_gram), max_df=max_frequ, min_df=min_appear)\n",
    "tfidf = vectorizer.fit_transform(usable)\n",
    "\n",
    "print(\"nombre d'articles =\", len(usable))\n",
    "print(\"nombre de mots =\", len(tfidf.toarray()[0]))\n",
    "print(\"limits =\", limits)\n",
    "\n",
    "usable[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of partitions_tfidf[], which give us the TFIDF of each cluster of each partition\n",
    "# partitions_tfidf[num_partition][num_doc][num_word]\n",
    "# Beware, num_doc can't be equals to 1091 (max). You have partitions, so every doc aren't in every partitions\n",
    "# num_word can be found via vectorizer.get_feature_name()\n",
    "partitions_tfidf = []\n",
    "beg = 0\n",
    "for l in limits:\n",
    "    last = l\n",
    "    partitions_tfidf.append([list(x) for x in list(tfidf.toarray())[beg:last]])\n",
    "    beg = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acce',\n",
       " 'accessible',\n",
       " 'achat',\n",
       " 'acquisition',\n",
       " 'acquérir',\n",
       " 'acteur',\n",
       " 'actif',\n",
       " 'action',\n",
       " 'activite',\n",
       " 'actuel',\n",
       " 'actuellement',\n",
       " 'adapt',\n",
       " 'adaptatif',\n",
       " 'adaptation',\n",
       " 'adapte',\n",
       " 'adaptee',\n",
       " 'adapter',\n",
       " 'adequat',\n",
       " 'adn',\n",
       " 'adopter',\n",
       " 'afc',\n",
       " 'affiner',\n",
       " 'agent',\n",
       " 'agregation',\n",
       " 'aid',\n",
       " 'aider',\n",
       " 'ainsiqu',\n",
       " 'ajoutee',\n",
       " 'ajouter',\n",
       " 'ala',\n",
       " 'aleatoir',\n",
       " 'algebriqu',\n",
       " 'algorithm',\n",
       " 'algorithm classification',\n",
       " 'algorithm dapprentissage',\n",
       " 'algorithm dextraction',\n",
       " 'algorithm efficace',\n",
       " 'algorithme',\n",
       " 'algorithme dapprentissage',\n",
       " 'algorithme dextraction',\n",
       " 'algorithme fouiller',\n",
       " 'algorithme incremental',\n",
       " 'algorithmique',\n",
       " 'alignement',\n",
       " 'alternatif',\n",
       " 'amelior',\n",
       " 'amelioration',\n",
       " 'ameliore',\n",
       " 'amelioree',\n",
       " 'ameliorer',\n",
       " 'ameliorer qualite',\n",
       " 'amene',\n",
       " 'amont',\n",
       " 'analys',\n",
       " 'analyse',\n",
       " 'analyser',\n",
       " 'analyser donnee',\n",
       " 'analyser factoriel',\n",
       " 'analyser semantiqu',\n",
       " 'analytique',\n",
       " 'anne',\n",
       " 'annees',\n",
       " 'annot',\n",
       " 'annotation',\n",
       " 'annotation semantiqu',\n",
       " 'annoter',\n",
       " 'anormal',\n",
       " 'anr',\n",
       " 'apartir',\n",
       " 'apparer',\n",
       " 'appartenir',\n",
       " 'appel',\n",
       " 'appele',\n",
       " 'appelee',\n",
       " 'appeler',\n",
       " 'applicable',\n",
       " 'applicatif',\n",
       " 'application',\n",
       " 'application donnee',\n",
       " 'application reell',\n",
       " 'appliqu',\n",
       " 'applique',\n",
       " 'appliquee',\n",
       " 'appliquees',\n",
       " 'appliquer',\n",
       " 'apport',\n",
       " 'apporter',\n",
       " 'apprehender',\n",
       " 'apprendre',\n",
       " 'apprentissage',\n",
       " 'apprentissage automatique',\n",
       " 'apprentissage supervis',\n",
       " 'approche',\n",
       " 'approche existant',\n",
       " 'appropriee',\n",
       " 'approprier',\n",
       " 'approximatif',\n",
       " 'approximation',\n",
       " 'apr',\n",
       " 'aprer',\n",
       " 'arbitraire',\n",
       " 'arborescent',\n",
       " 'arbre',\n",
       " 'arbre decision',\n",
       " 'architecturer',\n",
       " 'article',\n",
       " 'article decrit',\n",
       " 'article etuder',\n",
       " 'article etudie',\n",
       " 'article interesser',\n",
       " 'article introduire',\n",
       " 'article method',\n",
       " 'article methode',\n",
       " 'article methode original',\n",
       " 'article methodologie',\n",
       " 'article montrer',\n",
       " 'article nouvel',\n",
       " 'article present',\n",
       " 'article present method',\n",
       " 'article present methode',\n",
       " 'article presenter',\n",
       " 'article presenter method',\n",
       " 'article presenton',\n",
       " 'article traire',\n",
       " 'articlenou',\n",
       " 'artificiel',\n",
       " 'ascendant',\n",
       " 'ascendant hierarchiqu',\n",
       " 'aspect',\n",
       " 'assister',\n",
       " 'associ',\n",
       " 'association',\n",
       " 'associe',\n",
       " 'associee',\n",
       " 'associeer',\n",
       " 'associees',\n",
       " 'associer',\n",
       " 'assurer',\n",
       " 'attaquer',\n",
       " 'atteindre',\n",
       " 'attention',\n",
       " 'attribu',\n",
       " 'attribut',\n",
       " 'atypique',\n",
       " 'augmenter',\n",
       " 'automat',\n",
       " 'automate',\n",
       " 'automatique',\n",
       " 'automatiquement',\n",
       " 'automatiser',\n",
       " 'autoorganisatrice',\n",
       " 'autour',\n",
       " 'avancee',\n",
       " 'avantage',\n",
       " 'axe',\n",
       " 'axer',\n",
       " 'axiome',\n",
       " 'baptis',\n",
       " 'base',\n",
       " 'base donnee',\n",
       " 'base donnee relationnel',\n",
       " 'base donneer',\n",
       " 'baseesur',\n",
       " 'baser',\n",
       " 'baser connaissance',\n",
       " 'baser dapprentissage',\n",
       " 'baser donne',\n",
       " 'baser donneer',\n",
       " 'baser modele',\n",
       " 'baser technique',\n",
       " 'batch',\n",
       " 'bayesien',\n",
       " 'bayesien naïf',\n",
       " 'bayesienn',\n",
       " 'benchmark',\n",
       " 'beneficier',\n",
       " 'besoin',\n",
       " 'biais',\n",
       " 'bibliographique',\n",
       " 'biclustering',\n",
       " 'binaire',\n",
       " 'bioinformatique',\n",
       " 'biologique',\n",
       " 'biomedical',\n",
       " 'bloc',\n",
       " 'block',\n",
       " 'booleenn',\n",
       " 'boosting',\n",
       " 'born',\n",
       " 'bruitees',\n",
       " 'brut',\n",
       " 'cadrer',\n",
       " 'cadrer article',\n",
       " 'cadrer classification',\n",
       " 'cadrer general',\n",
       " 'cadrer lanalyse',\n",
       " 'cadrer projet',\n",
       " 'cadrer rechercher',\n",
       " 'cadrer theoriqu',\n",
       " 'cadrer travail',\n",
       " 'calcul',\n",
       " 'calcul similarite',\n",
       " 'calculee',\n",
       " 'calculer',\n",
       " 'campagne',\n",
       " 'cancer',\n",
       " 'candidat',\n",
       " 'capable',\n",
       " 'capacite',\n",
       " 'capteur',\n",
       " 'capturer',\n",
       " 'caracter',\n",
       " 'caractere',\n",
       " 'caracterisation',\n",
       " 'caracterise',\n",
       " 'caracteriseer',\n",
       " 'caracteriser',\n",
       " 'caracteristiqu',\n",
       " 'caracteristique',\n",
       " 'caracteristiquer',\n",
       " 'cart',\n",
       " 'carte',\n",
       " 'carte autoorganisatrice',\n",
       " 'carte topologique',\n",
       " 'cartographie',\n",
       " 'cartographique',\n",
       " 'cas',\n",
       " 'cas donnee',\n",
       " 'categorie',\n",
       " 'categoriel',\n",
       " 'categoriell',\n",
       " 'categorisation',\n",
       " 'causer',\n",
       " 'celer',\n",
       " 'cell',\n",
       " 'celleci',\n",
       " 'cellesci',\n",
       " 'cellule',\n",
       " 'celuici',\n",
       " 'centaine',\n",
       " 'central',\n",
       " 'centree',\n",
       " 'centrer',\n",
       " 'cesser',\n",
       " 'cestadir',\n",
       " 'chain',\n",
       " 'chaine',\n",
       " 'chaine markov',\n",
       " 'challenge',\n",
       " 'champ',\n",
       " 'changement',\n",
       " 'changer',\n",
       " 'charger',\n",
       " 'chemin',\n",
       " 'chercher',\n",
       " 'chercheur',\n",
       " 'choisir',\n",
       " 'choix',\n",
       " 'choix mesurer',\n",
       " 'chronique',\n",
       " 'cibl',\n",
       " 'cibler',\n",
       " 'clairement',\n",
       " 'classe',\n",
       " 'classement',\n",
       " 'classer',\n",
       " 'classificateur',\n",
       " 'classification',\n",
       " 'classification automatique',\n",
       " 'classification croise',\n",
       " 'classification document',\n",
       " 'classification donnee',\n",
       " 'classification hierarchiqu',\n",
       " 'classification multilabel',\n",
       " 'classification supervise',\n",
       " 'classification supervisee',\n",
       " 'classifier',\n",
       " 'classifieur',\n",
       " 'classifieur bayesien',\n",
       " 'classifieur bayesien naïf',\n",
       " 'classique',\n",
       " 'cle',\n",
       " 'clef',\n",
       " 'client',\n",
       " 'cluster',\n",
       " 'clustering',\n",
       " 'coclustering',\n",
       " 'codage',\n",
       " 'coefficient',\n",
       " 'coeur',\n",
       " 'cognitif',\n",
       " 'coherence',\n",
       " 'collaboratif',\n",
       " 'collaboration',\n",
       " 'collecter',\n",
       " 'collectif',\n",
       " 'collection',\n",
       " 'collection document',\n",
       " 'colonne',\n",
       " 'combin',\n",
       " 'combinaison',\n",
       " 'combinatoire',\n",
       " 'combiner',\n",
       " 'commencer',\n",
       " 'commercial',\n",
       " 'commun',\n",
       " 'communaut',\n",
       " 'communautair',\n",
       " 'communautaire',\n",
       " 'communaute',\n",
       " 'communication',\n",
       " 'compact',\n",
       " 'compacter',\n",
       " 'comparaison',\n",
       " 'comparaison entrer',\n",
       " 'comparatif',\n",
       " 'comparee',\n",
       " 'comparer',\n",
       " 'comparer resultat',\n",
       " 'compatible',\n",
       " 'competence',\n",
       " 'complementair',\n",
       " 'complementarite',\n",
       " 'complet',\n",
       " 'completer',\n",
       " 'complex',\n",
       " 'complexe',\n",
       " 'complexit',\n",
       " 'complexite',\n",
       " 'comportement',\n",
       " 'comportemer',\n",
       " 'comporter',\n",
       " 'composant',\n",
       " 'compose',\n",
       " 'composer',\n",
       " 'comprehension',\n",
       " 'comprendre',\n",
       " 'compromis',\n",
       " 'compromis entrer',\n",
       " 'compter',\n",
       " 'concentrer',\n",
       " 'concept',\n",
       " 'concept treillis',\n",
       " 'concepteur',\n",
       " 'conception',\n",
       " 'conceptuel',\n",
       " 'concerner',\n",
       " 'concevoir',\n",
       " 'concis',\n",
       " 'conclure',\n",
       " 'conclusion',\n",
       " 'concret',\n",
       " 'condensee',\n",
       " 'condensees',\n",
       " 'condition',\n",
       " 'conditionnel',\n",
       " 'conduire',\n",
       " 'conferenc',\n",
       " 'conference',\n",
       " 'confiance',\n",
       " 'configuration',\n",
       " 'confirmer',\n",
       " 'conflit',\n",
       " 'confronter',\n",
       " 'conjoint',\n",
       " 'conjointement',\n",
       " 'connaiss',\n",
       " 'connaissance',\n",
       " 'connaissance baser',\n",
       " 'connaissance domaine',\n",
       " 'connaissance expert',\n",
       " 'connaissance partir',\n",
       " 'connaissance partir donnee',\n",
       " 'connaissance priori',\n",
       " 'connaissancer',\n",
       " 'connaître',\n",
       " 'connexion',\n",
       " 'connu',\n",
       " 'consequent',\n",
       " 'conserver',\n",
       " 'consider',\n",
       " 'considere',\n",
       " 'consideree',\n",
       " 'considerer',\n",
       " 'consist',\n",
       " 'consister',\n",
       " 'consommation',\n",
       " 'constant',\n",
       " 'constater',\n",
       " 'constituer',\n",
       " 'construction',\n",
       " 'construction automatique',\n",
       " 'construction darbr',\n",
       " 'construction dontologie',\n",
       " 'construir',\n",
       " 'construire',\n",
       " 'construire partir',\n",
       " 'construit',\n",
       " 'construit partir',\n",
       " 'contenir',\n",
       " 'contenir document',\n",
       " 'contenu',\n",
       " 'context',\n",
       " 'contexte',\n",
       " 'contextuel',\n",
       " 'contingence',\n",
       " 'continu',\n",
       " 'continuer',\n",
       " 'contraindre',\n",
       " 'contraint',\n",
       " 'contrainte',\n",
       " 'contrairement',\n",
       " 'contribuer',\n",
       " 'contribution',\n",
       " 'controle',\n",
       " 'controler',\n",
       " 'convergence',\n",
       " 'convier',\n",
       " 'conçu',\n",
       " 'cooperativ',\n",
       " 'corpu',\n",
       " 'corpus',\n",
       " 'correct',\n",
       " 'correctement',\n",
       " 'correlation',\n",
       " 'correlation entrer',\n",
       " 'correspondance',\n",
       " 'correspondance afc',\n",
       " 'correspondance entrer',\n",
       " 'correspondant',\n",
       " 'correspondre',\n",
       " 'couleur',\n",
       " 'coupl',\n",
       " 'couple',\n",
       " 'coupler',\n",
       " 'courir',\n",
       " 'courir temps',\n",
       " 'cours',\n",
       " 'court',\n",
       " 'couverture',\n",
       " 'couvrir',\n",
       " 'coût',\n",
       " 'coûteux',\n",
       " 'creation',\n",
       " 'cree',\n",
       " 'creer',\n",
       " 'criter',\n",
       " 'critere',\n",
       " 'critere devaluation',\n",
       " 'croise',\n",
       " 'croisee',\n",
       " 'croiser',\n",
       " 'croissance',\n",
       " 'croyance',\n",
       " 'croître',\n",
       " 'cube',\n",
       " 'cuber',\n",
       " 'culturel',\n",
       " 'cycle',\n",
       " 'dabord',\n",
       " 'daccelerer',\n",
       " 'dacquisition',\n",
       " 'dadapter',\n",
       " 'dagregation',\n",
       " 'daid',\n",
       " 'daid decision',\n",
       " 'daider',\n",
       " 'dalgorithme',\n",
       " 'dalignemer',\n",
       " 'dameliorer',\n",
       " 'dameliorer performance',\n",
       " 'danalyse',\n",
       " 'danalyser',\n",
       " 'dannotation',\n",
       " 'danscet',\n",
       " 'danscet article',\n",
       " 'dansl',\n",
       " 'dansun',\n",
       " 'dappariemer',\n",
       " 'dapparition',\n",
       " 'dappartenance',\n",
       " 'dapplication',\n",
       " 'dapporter',\n",
       " 'dapprendre',\n",
       " 'dapprentissage',\n",
       " 'dapprentissage automatique',\n",
       " 'dapprentissage supervis',\n",
       " 'darbr',\n",
       " 'darbr decision',\n",
       " 'darticl',\n",
       " 'dassoci',\n",
       " 'dassociation',\n",
       " 'dater',\n",
       " 'dater mining',\n",
       " 'dater stream',\n",
       " 'dattribut',\n",
       " 'dattributs',\n",
       " 'daugmenter',\n",
       " 'dautre',\n",
       " 'dautre partir',\n",
       " 'debut',\n",
       " 'decd',\n",
       " 'decennie',\n",
       " 'dechantillon',\n",
       " 'dechantillonnage',\n",
       " 'decider',\n",
       " 'decideur',\n",
       " 'decis',\n",
       " 'decision',\n",
       " 'decisionnel',\n",
       " 'decisionnell',\n",
       " 'decomposition',\n",
       " 'decoupage',\n",
       " 'decouvert',\n",
       " 'decouverte',\n",
       " 'decouverte motif',\n",
       " 'decouvrir',\n",
       " 'decouvrir motif',\n",
       " 'decrir',\n",
       " 'decrire',\n",
       " 'decrit',\n",
       " 'decrite',\n",
       " 'decrivant',\n",
       " 'decriver',\n",
       " 'decrivons',\n",
       " 'dedie',\n",
       " 'dediee',\n",
       " 'dedition',\n",
       " 'dedonnee',\n",
       " 'deduire',\n",
       " 'defaut',\n",
       " 'deffectuer',\n",
       " 'defi',\n",
       " 'defi egc',\n",
       " 'defini',\n",
       " 'definie',\n",
       " 'definier',\n",
       " 'definir',\n",
       " 'definis',\n",
       " 'definisser',\n",
       " 'definissons',\n",
       " 'definit',\n",
       " 'definition',\n",
       " 'degager',\n",
       " 'degr',\n",
       " 'degre',\n",
       " 'delagage',\n",
       " 'delement',\n",
       " 'demander',\n",
       " 'demarch',\n",
       " 'demonstration',\n",
       " 'denrichir',\n",
       " 'dense',\n",
       " 'densembl',\n",
       " 'densite',\n",
       " 'dentiter',\n",
       " 'dentre',\n",
       " 'dentrepris',\n",
       " 'depart',\n",
       " 'dependanc',\n",
       " 'dependance',\n",
       " 'dependance fonctionnel',\n",
       " 'depender',\n",
       " 'deper',\n",
       " 'dequivalence',\n",
       " 'derive',\n",
       " 'derreur',\n",
       " 'descripteur',\n",
       " 'descriptif',\n",
       " 'description',\n",
       " 'desdonnee',\n",
       " 'desequilibre',\n",
       " 'destimer',\n",
       " 'detablir',\n",
       " 'detailler',\n",
       " 'detat',\n",
       " 'detecter',\n",
       " 'detection',\n",
       " 'detection changement',\n",
       " 'detectiond',\n",
       " 'detendre',\n",
       " 'determination',\n",
       " 'determine',\n",
       " 'determiner',\n",
       " 'detr',\n",
       " 'detre',\n",
       " 'detude',\n",
       " 'detudier',\n",
       " 'deuxieme',\n",
       " 'devaluation',\n",
       " 'devaluer',\n",
       " 'devaluer qualite',\n",
       " 'developp',\n",
       " 'developpe',\n",
       " 'developpee',\n",
       " 'developpement',\n",
       " 'developpemer',\n",
       " 'developper',\n",
       " 'devenement',\n",
       " 'devenir',\n",
       " 'devenir plaire',\n",
       " 'devolution',\n",
       " 'dexecution',\n",
       " 'dexempl',\n",
       " 'dexperience',\n",
       " 'dexperimentation',\n",
       " 'dexploiter',\n",
       " 'dexploration',\n",
       " 'dexplorer',\n",
       " 'dexpression',\n",
       " 'dexprimer',\n",
       " 'dextraction',\n",
       " 'dextraction connaissance',\n",
       " 'dextraction connaissance partir',\n",
       " 'dextraction motif',\n",
       " 'dextrair',\n",
       " 'dextraire',\n",
       " 'dheuristiqu',\n",
       " 'diagnostic',\n",
       " 'dictionnair',\n",
       " 'didentification',\n",
       " 'didentifier',\n",
       " 'difference',\n",
       " 'differenter',\n",
       " 'difficile',\n",
       " 'difficilement',\n",
       " 'difficult',\n",
       " 'difficulte',\n",
       " 'diffusion',\n",
       " 'dimag',\n",
       " 'dimage',\n",
       " 'dimager',\n",
       " 'dimension',\n",
       " 'dincertitude',\n",
       " 'dindexation',\n",
       " 'dindice',\n",
       " 'dindividus',\n",
       " 'dinduction',\n",
       " 'dinference',\n",
       " 'dinferer',\n",
       " 'dinfluence',\n",
       " 'dinform',\n",
       " 'dinformation',\n",
       " 'dintegration',\n",
       " 'dintegrer',\n",
       " 'dinteraction',\n",
       " 'dinteraction entrer',\n",
       " 'dinteret',\n",
       " 'dinternet',\n",
       " 'dinterpretation',\n",
       " 'dinterrogation',\n",
       " 'dintroduir',\n",
       " 'direct',\n",
       " 'direction',\n",
       " 'discret',\n",
       " 'discretisation',\n",
       " 'discriminant',\n",
       " 'discriminanter',\n",
       " 'discrimination',\n",
       " 'discriminer',\n",
       " 'discussion',\n",
       " 'discuter',\n",
       " 'disponible',\n",
       " 'disposer',\n",
       " 'dispositif',\n",
       " 'disposition',\n",
       " 'dissimilarit',\n",
       " 'dissimilarite',\n",
       " 'dissimilarite entrer',\n",
       " 'distance',\n",
       " 'distancer',\n",
       " 'distancer entrer',\n",
       " 'distinct',\n",
       " 'distinguer',\n",
       " 'distribu',\n",
       " 'distribuer',\n",
       " 'distribution',\n",
       " 'ditems',\n",
       " 'ditemset',\n",
       " 'ditemset frequent',\n",
       " 'diversite',\n",
       " 'diviser',\n",
       " 'dizaine',\n",
       " 'dobjet',\n",
       " 'dobjets',\n",
       " 'dobtenir',\n",
       " 'docu',\n",
       " 'document',\n",
       " 'document textuel',\n",
       " 'document xml',\n",
       " 'documentaire',\n",
       " 'doffr',\n",
       " 'domaine',\n",
       " 'domaine dapplication',\n",
       " 'domaine fouiller',\n",
       " 'domaine lapprentissage',\n",
       " 'donne',\n",
       " 'donnee',\n",
       " 'donnee article',\n",
       " 'donnee artificiel',\n",
       " 'donnee baser',\n",
       " 'donnee cluster',\n",
       " 'donnee disponible',\n",
       " 'donnee geographiqu',\n",
       " 'donnee grand',\n",
       " 'donnee issu',\n",
       " 'donnee massif',\n",
       " 'donnee multidimensionnel',\n",
       " 'donnee numeriqu',\n",
       " 'donnee permettre',\n",
       " 'donnee plaire',\n",
       " 'donnee pouvoir',\n",
       " 'donnee presenter',\n",
       " 'donnee qualitatif',\n",
       " 'donnee reel',\n",
       " 'donnee reell',\n",
       " 'donnee reell issu',\n",
       " 'donnee reell montrer',\n",
       " 'donnee relationnel',\n",
       " 'donnee sequentiell',\n",
       " 'donnee simuleer',\n",
       " 'donnee symbolique',\n",
       " 'donnee synthetiqu',\n",
       " 'donnee textuel',\n",
       " 'donnee visualisation',\n",
       " 'donneer',\n",
       " 'donneer article',\n",
       " 'donneer luci',\n",
       " 'donneer plaire',\n",
       " 'donneer reell',\n",
       " 'donneer textuel',\n",
       " 'donnees',\n",
       " 'donner',\n",
       " 'donner resultat',\n",
       " 'dontologie',\n",
       " 'dontologie partir',\n",
       " 'dontologier',\n",
       " 'dopinion',\n",
       " 'doptimisation',\n",
       " 'dordre',\n",
       " 'dorigine',\n",
       " 'doter',\n",
       " 'doutil',\n",
       " 'doutils',\n",
       " 'duree',\n",
       " 'durer',\n",
       " 'durer processus',\n",
       " 'dusage',\n",
       " 'dutilisateur',\n",
       " 'dutilisation',\n",
       " 'dutiliser',\n",
       " 'dynam',\n",
       " 'dynamique',\n",
       " 'ecd',\n",
       " 'echantillon',\n",
       " 'echantillonnage',\n",
       " 'echell',\n",
       " 'echelle',\n",
       " 'eclairage',\n",
       " 'economiqu',\n",
       " 'effectu',\n",
       " 'efficace',\n",
       " 'efficacement',\n",
       " 'efficacite',\n",
       " 'effort',\n",
       " 'egc',\n",
       " 'element',\n",
       " 'elev',\n",
       " 'eleve',\n",
       " 'elevee',\n",
       " 'emergent',\n",
       " 'empirique',\n",
       " 'empiriquement',\n",
       " 'employer',\n",
       " 'encompte',\n",
       " 'encourageant',\n",
       " 'encourageant prometteur',\n",
       " 'engendrer',\n",
       " 'enjeu',\n",
       " 'enjeu majeur',\n",
       " 'enregistrement',\n",
       " 'enrichir',\n",
       " 'enrichissement',\n",
       " 'ensembl',\n",
       " 'ensembl donnee',\n",
       " 'ensembl donneer',\n",
       " 'ensembl mesure',\n",
       " 'ensemble',\n",
       " 'ensemble donnee',\n",
       " 'ensemble donneer',\n",
       " 'ensemble regl',\n",
       " 'ensembliste',\n",
       " 'entite',\n",
       " 'entrainer',\n",
       " 'entrepot',\n",
       " 'entrepot donnee',\n",
       " 'entreprendre',\n",
       " 'entreprise',\n",
       " 'entrer',\n",
       " 'entrer attribut',\n",
       " 'entrer classe',\n",
       " 'entrer concept',\n",
       " 'entrer donnee',\n",
       " 'entrer ensemble',\n",
       " 'entrer groupe',\n",
       " 'entrer individu',\n",
       " 'entrer objet',\n",
       " 'entrer ontologie',\n",
       " 'entrer variable',\n",
       " 'environnement',\n",
       " 'envisageable',\n",
       " 'equivalent',\n",
       " 'erreur',\n",
       " 'espac',\n",
       " 'espace',\n",
       " 'espacer',\n",
       " 'essentiel',\n",
       " 'essentiellement',\n",
       " 'estim',\n",
       " 'estimateur',\n",
       " 'estimation',\n",
       " 'estimation densite',\n",
       " 'estimee',\n",
       " 'estimer',\n",
       " 'etablie',\n",
       " 'etablir',\n",
       " 'etablisser',\n",
       " 'etape',\n",
       " 'etat',\n",
       " 'etde',\n",
       " 'etendu',\n",
       " 'eter',\n",
       " 'etiquetage',\n",
       " 'etiquete',\n",
       " 'etl',\n",
       " 'etle',\n",
       " 'etroitement',\n",
       " 'etude',\n",
       " 'etude comparatif',\n",
       " 'etude experimental',\n",
       " 'etuder',\n",
       " 'etudie',\n",
       " 'etudiee',\n",
       " 'etudier',\n",
       " 'euclidien',\n",
       " 'evalu',\n",
       " 'evaluation',\n",
       " 'evalue',\n",
       " 'evaluee',\n",
       " 'evaluees',\n",
       " 'evaluer',\n",
       " 'evenement',\n",
       " 'evidence',\n",
       " 'eviter',\n",
       " 'evoluer',\n",
       " 'evolution',\n",
       " 'evolutiv',\n",
       " 'exact',\n",
       " 'examiner',\n",
       " 'exemple',\n",
       " 'exhaustif',\n",
       " 'exhiber',\n",
       " 'existant',\n",
       " 'exister',\n",
       " 'exister entrer',\n",
       " 'experience',\n",
       " 'experience meneer',\n",
       " 'experience montrer',\n",
       " 'experiment',\n",
       " 'experimental',\n",
       " 'experimentalement',\n",
       " 'experimentation',\n",
       " 'experimentation jeu',\n",
       " 'experimentation jeu donnee',\n",
       " 'experimentation menee',\n",
       " 'experimentation montrer',\n",
       " 'experimentaux',\n",
       " 'experimentaux montrer',\n",
       " 'expert',\n",
       " 'expert domaine',\n",
       " 'explicatif',\n",
       " 'expliciter',\n",
       " 'expliquer',\n",
       " 'exploitable',\n",
       " 'exploitation',\n",
       " 'exploiter',\n",
       " 'exploration',\n",
       " 'exploratoire',\n",
       " 'explorer',\n",
       " 'exponentiel',\n",
       " 'exposer',\n",
       " 'expression',\n",
       " 'exprimeer',\n",
       " 'exprimer',\n",
       " 'extension',\n",
       " 'extraction',\n",
       " 'extraction connaissance',\n",
       " 'extraction motif',\n",
       " 'extraction regl',\n",
       " 'extraire',\n",
       " 'extraire connaissance',\n",
       " 'extraire motif',\n",
       " 'extrait',\n",
       " 'extraite',\n",
       " 'face',\n",
       " 'facile',\n",
       " 'facilement',\n",
       " 'faciliter',\n",
       " 'facteur',\n",
       " 'factoriel',\n",
       " 'factoriel correspondance',\n",
       " 'factoriel correspondance afc',\n",
       " 'factorisation',\n",
       " 'faible',\n",
       " 'faire',\n",
       " 'faire appel',\n",
       " 'faire face',\n",
       " 'faire lobjet',\n",
       " 'faire partir',\n",
       " 'faisabilite',\n",
       " 'famille',\n",
       " 'favoriser',\n",
       " 'feature',\n",
       " 'ferme',\n",
       " 'ferme frequent',\n",
       " 'fiabilite',\n",
       " 'fiable',\n",
       " 'fichier',\n",
       " 'filtrage',\n",
       " 'filtrer',\n",
       " 'fin',\n",
       " 'final',\n",
       " 'finalement',\n",
       " 'financier',\n",
       " 'fixe',\n",
       " 'fixer',\n",
       " 'flexible',\n",
       " 'flot',\n",
       " 'flot donneer',\n",
       " 'flou',\n",
       " 'flouer',\n",
       " 'flux',\n",
       " 'flux donne',\n",
       " 'flux donnee',\n",
       " 'flux donneer',\n",
       " 'focaliser',\n",
       " 'fonction',\n",
       " 'fonction croyance',\n",
       " 'fonctionnalite',\n",
       " 'fonctionnel',\n",
       " 'fonctionnement',\n",
       " 'fonctionner',\n",
       " 'fond',\n",
       " 'fondamental',\n",
       " 'fondee',\n",
       " 'fondre',\n",
       " 'foret',\n",
       " 'foret aleatoir',\n",
       " 'formalisation',\n",
       " 'formaliser',\n",
       " 'formalisme',\n",
       " 'format',\n",
       " 'formation',\n",
       " 'forme',\n",
       " 'formel',\n",
       " 'formel concept',\n",
       " 'former',\n",
       " 'former regl',\n",
       " 'formulation',\n",
       " 'formuler',\n",
       " 'fort',\n",
       " 'fortement',\n",
       " 'fouill',\n",
       " 'fouill donneer',\n",
       " 'fouiller',\n",
       " 'fouiller donne',\n",
       " 'fouiller donnee',\n",
       " 'fouiller donneer',\n",
       " 'fouiller grand',\n",
       " 'fouiller texter',\n",
       " 'fournir',\n",
       " 'framework',\n",
       " 'france',\n",
       " 'français',\n",
       " 'frequence',\n",
       " 'frequent',\n",
       " 'frequente',\n",
       " 'fusion',\n",
       " 'futur',\n",
       " 'gain',\n",
       " 'galoi',\n",
       " 'garantir',\n",
       " 'gen',\n",
       " 'gene',\n",
       " 'gener',\n",
       " 'general',\n",
       " 'generalement',\n",
       " 'generalisation',\n",
       " 'generalise',\n",
       " 'generaliser',\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans & Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying KMeans on tfidf\n",
    "# the labels_ give assignment of doc to the cluster number \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_clustering is a dictionnary \n",
    "# it looks like -> { doc_number : [partition_number, cluster_number] }\n",
    "# This is used to reassign doc number to their respective partition and and cluster\n",
    "\n",
    "def kmeans(nb_clusters):\n",
    "    doc_clustering = {}\n",
    "    \n",
    "    km = KMeans(n_clusters=nb_clusters)\n",
    "\n",
    "    # Silhouette score mean\n",
    "    silhouette_mean = 0\n",
    "\n",
    "    numDoc = 0\n",
    "    for i in range(0, len(limits)):\n",
    "        dash = km.fit(partitions_tfidf[i])\n",
    "\n",
    "        # Silhouette\n",
    "        silhouette_mean += silhouette_score(partitions_tfidf[i], dash.labels_)\n",
    "\n",
    "        previousBound = 0\n",
    "        if i > 0:\n",
    "            previousBound = limits[i-1]\n",
    "        for numDocItern in range(0, limits[i]-previousBound):\n",
    "            doc_clustering[numDoc] = [i, dash.labels_[numDocItern]]\n",
    "            numDoc+=1\n",
    "\n",
    "    silhouette_mean = silhouette_mean / len(limits)\n",
    "    res = {}\n",
    "    res[\"silhouette\"] = silhouette_mean\n",
    "    res[\"clustering\"] = doc_clustering\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Silhouette Score for each number of cluster\n",
    "\n",
    "silhouette_by_cluster_nb = {}\n",
    "\n",
    "for nb_clusters in cluster_ranges:\n",
    "    silhouette_avg = 0\n",
    "    for trial in range(0, nb_trial_by_test):\n",
    "        km = kmeans(nb_clusters)\n",
    "        silhouette_avg += km[\"silhouette\"]\n",
    "    silhouette_avg = silhouette_avg / nb_trial_by_test\n",
    "    silhouette_by_cluster_nb[nb_clusters] = silhouette_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 0.00578205545072553,\n",
       " 3: 0.005859485301478853,\n",
       " 4: 0.006947141674082224,\n",
       " 5: 0.0076449044031812975,\n",
       " 6: 0.00863541884545567,\n",
       " 7: 0.009097191680325507,\n",
       " 8: 0.010283301950888975,\n",
       " 9: 0.011125581347600952,\n",
       " 10: 0.012199983527272367,\n",
       " 11: 0.013407125870923258,\n",
       " 12: 0.013401377450969637,\n",
       " 13: 0.013806295770944067,\n",
       " 14: 0.014532242413086002,\n",
       " 15: 0.015638030584180243,\n",
       " 16: 0.016143565582583584,\n",
       " 17: 0.017349385658343926,\n",
       " 18: 0.017327238367165192,\n",
       " 19: 0.018444712635895956,\n",
       " 20: 0.018258440418140218,\n",
       " 21: 0.020663557779260495,\n",
       " 22: 0.02085535316646132,\n",
       " 23: 0.020957775875229307,\n",
       " 24: 0.021521382784714438,\n",
       " 25: 0.022189173848375404,\n",
       " 26: 0.02322660672229924,\n",
       " 27: 0.023700977824183723,\n",
       " 28: 0.022771922014560894,\n",
       " 29: 0.024773691200101925}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want silhouette scores to be high\n",
    "silhouette_by_cluster_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clustering = kmeans(5)[\"clustering\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows to get list of documents number\n",
    "# return [dou numbers]\n",
    "# params : partition_number , cluster number\n",
    "def get_doc(part, clust):\n",
    "    docs = []\n",
    "    for i in range(0,len(doc_clustering)):\n",
    "        if doc_clustering[i][0] == part and doc_clustering[i][1] == clust:\n",
    "            docs.append(i)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the partitions variable\n",
    "# Here partitions[part][cluster] = list of docs numbe\n",
    "partitions = []\n",
    "for i in range(0, len(limits)):\n",
    "    clusters = []\n",
    "    for j in range(0, nb_clusters):\n",
    "        clusters.append(get_doc(i,j))\n",
    "    partitions.append(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[8,\n",
       "   11,\n",
       "   24,\n",
       "   26,\n",
       "   30,\n",
       "   31,\n",
       "   38,\n",
       "   49,\n",
       "   55,\n",
       "   57,\n",
       "   74,\n",
       "   78,\n",
       "   80,\n",
       "   81,\n",
       "   94,\n",
       "   98,\n",
       "   103,\n",
       "   109,\n",
       "   110,\n",
       "   111],\n",
       "  [0,\n",
       "   3,\n",
       "   4,\n",
       "   10,\n",
       "   12,\n",
       "   22,\n",
       "   27,\n",
       "   41,\n",
       "   42,\n",
       "   44,\n",
       "   51,\n",
       "   53,\n",
       "   56,\n",
       "   58,\n",
       "   65,\n",
       "   75,\n",
       "   85,\n",
       "   92,\n",
       "   95,\n",
       "   108,\n",
       "   113],\n",
       "  [2,\n",
       "   6,\n",
       "   7,\n",
       "   9,\n",
       "   21,\n",
       "   23,\n",
       "   28,\n",
       "   33,\n",
       "   37,\n",
       "   39,\n",
       "   50,\n",
       "   61,\n",
       "   62,\n",
       "   64,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   72,\n",
       "   88,\n",
       "   90,\n",
       "   93,\n",
       "   96,\n",
       "   97,\n",
       "   99,\n",
       "   102,\n",
       "   104,\n",
       "   112],\n",
       "  [15, 36, 63, 70, 79, 87],\n",
       "  [1,\n",
       "   5,\n",
       "   13,\n",
       "   14,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   25,\n",
       "   29,\n",
       "   32,\n",
       "   34,\n",
       "   35,\n",
       "   40,\n",
       "   43,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   52,\n",
       "   54,\n",
       "   59,\n",
       "   60,\n",
       "   69,\n",
       "   71,\n",
       "   73,\n",
       "   76,\n",
       "   77,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   86,\n",
       "   89,\n",
       "   91,\n",
       "   100,\n",
       "   101,\n",
       "   105,\n",
       "   106,\n",
       "   107],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  []],\n",
       " [[114,\n",
       "   120,\n",
       "   121,\n",
       "   123,\n",
       "   132,\n",
       "   137,\n",
       "   140,\n",
       "   149,\n",
       "   150,\n",
       "   152,\n",
       "   154,\n",
       "   163,\n",
       "   167,\n",
       "   168,\n",
       "   172,\n",
       "   173,\n",
       "   174,\n",
       "   180,\n",
       "   181,\n",
       "   182,\n",
       "   183,\n",
       "   188,\n",
       "   193,\n",
       "   196,\n",
       "   199,\n",
       "   203,\n",
       "   212,\n",
       "   216,\n",
       "   217,\n",
       "   219,\n",
       "   224,\n",
       "   226,\n",
       "   232,\n",
       "   235,\n",
       "   236,\n",
       "   237,\n",
       "   239,\n",
       "   241,\n",
       "   246,\n",
       "   250,\n",
       "   253,\n",
       "   260,\n",
       "   261,\n",
       "   267,\n",
       "   268,\n",
       "   275,\n",
       "   285,\n",
       "   287,\n",
       "   290,\n",
       "   291,\n",
       "   296,\n",
       "   297,\n",
       "   298],\n",
       "  [134,\n",
       "   139,\n",
       "   141,\n",
       "   148,\n",
       "   151,\n",
       "   153,\n",
       "   157,\n",
       "   165,\n",
       "   169,\n",
       "   170,\n",
       "   175,\n",
       "   176,\n",
       "   177,\n",
       "   185,\n",
       "   186,\n",
       "   191,\n",
       "   194,\n",
       "   197,\n",
       "   200,\n",
       "   208,\n",
       "   221,\n",
       "   222,\n",
       "   223,\n",
       "   225,\n",
       "   227,\n",
       "   230,\n",
       "   238,\n",
       "   240,\n",
       "   242,\n",
       "   252,\n",
       "   256,\n",
       "   266,\n",
       "   270,\n",
       "   273,\n",
       "   280],\n",
       "  [118,\n",
       "   122,\n",
       "   124,\n",
       "   125,\n",
       "   128,\n",
       "   135,\n",
       "   136,\n",
       "   138,\n",
       "   143,\n",
       "   146,\n",
       "   147,\n",
       "   162,\n",
       "   171,\n",
       "   190,\n",
       "   192,\n",
       "   204,\n",
       "   231,\n",
       "   243,\n",
       "   244,\n",
       "   248,\n",
       "   249,\n",
       "   255,\n",
       "   257,\n",
       "   258,\n",
       "   259,\n",
       "   262,\n",
       "   269,\n",
       "   271,\n",
       "   274,\n",
       "   278,\n",
       "   279,\n",
       "   286,\n",
       "   289,\n",
       "   293,\n",
       "   294,\n",
       "   300],\n",
       "  [115,\n",
       "   129,\n",
       "   130,\n",
       "   133,\n",
       "   142,\n",
       "   155,\n",
       "   158,\n",
       "   159,\n",
       "   160,\n",
       "   178,\n",
       "   179,\n",
       "   184,\n",
       "   187,\n",
       "   189,\n",
       "   198,\n",
       "   201,\n",
       "   202,\n",
       "   205,\n",
       "   206,\n",
       "   207,\n",
       "   209,\n",
       "   210,\n",
       "   211,\n",
       "   213,\n",
       "   214,\n",
       "   215,\n",
       "   220,\n",
       "   228,\n",
       "   229,\n",
       "   233,\n",
       "   234,\n",
       "   251,\n",
       "   264,\n",
       "   265,\n",
       "   272,\n",
       "   276,\n",
       "   277,\n",
       "   281,\n",
       "   282,\n",
       "   283,\n",
       "   284,\n",
       "   288,\n",
       "   292,\n",
       "   295],\n",
       "  [116,\n",
       "   117,\n",
       "   119,\n",
       "   126,\n",
       "   127,\n",
       "   131,\n",
       "   144,\n",
       "   145,\n",
       "   156,\n",
       "   161,\n",
       "   164,\n",
       "   166,\n",
       "   195,\n",
       "   218,\n",
       "   245,\n",
       "   247,\n",
       "   254,\n",
       "   263,\n",
       "   299],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  []],\n",
       " [[302,\n",
       "   314,\n",
       "   316,\n",
       "   318,\n",
       "   324,\n",
       "   344,\n",
       "   351,\n",
       "   365,\n",
       "   375,\n",
       "   385,\n",
       "   391,\n",
       "   404,\n",
       "   409,\n",
       "   416,\n",
       "   420,\n",
       "   421,\n",
       "   435,\n",
       "   449,\n",
       "   462],\n",
       "  [306,\n",
       "   307,\n",
       "   309,\n",
       "   311,\n",
       "   321,\n",
       "   330,\n",
       "   332,\n",
       "   336,\n",
       "   337,\n",
       "   341,\n",
       "   343,\n",
       "   345,\n",
       "   346,\n",
       "   348,\n",
       "   350,\n",
       "   354,\n",
       "   355,\n",
       "   356,\n",
       "   357,\n",
       "   358,\n",
       "   359,\n",
       "   360,\n",
       "   362,\n",
       "   364,\n",
       "   376,\n",
       "   380,\n",
       "   390,\n",
       "   394,\n",
       "   397,\n",
       "   403,\n",
       "   412,\n",
       "   414,\n",
       "   424,\n",
       "   434,\n",
       "   436,\n",
       "   441,\n",
       "   446,\n",
       "   447,\n",
       "   448,\n",
       "   451,\n",
       "   452,\n",
       "   453,\n",
       "   456,\n",
       "   457,\n",
       "   460,\n",
       "   463,\n",
       "   466],\n",
       "  [303,\n",
       "   305,\n",
       "   310,\n",
       "   315,\n",
       "   317,\n",
       "   322,\n",
       "   325,\n",
       "   331,\n",
       "   333,\n",
       "   334,\n",
       "   338,\n",
       "   339,\n",
       "   342,\n",
       "   347,\n",
       "   352,\n",
       "   361,\n",
       "   363,\n",
       "   368,\n",
       "   370,\n",
       "   373,\n",
       "   374,\n",
       "   378,\n",
       "   379,\n",
       "   381,\n",
       "   382,\n",
       "   383,\n",
       "   384,\n",
       "   386,\n",
       "   389,\n",
       "   395,\n",
       "   399,\n",
       "   400,\n",
       "   401,\n",
       "   402,\n",
       "   406,\n",
       "   407,\n",
       "   408,\n",
       "   410,\n",
       "   411,\n",
       "   426,\n",
       "   437,\n",
       "   438,\n",
       "   442,\n",
       "   444,\n",
       "   450,\n",
       "   454,\n",
       "   455,\n",
       "   458,\n",
       "   459,\n",
       "   464,\n",
       "   465],\n",
       "  [301,\n",
       "   304,\n",
       "   326,\n",
       "   327,\n",
       "   328,\n",
       "   329,\n",
       "   335,\n",
       "   340,\n",
       "   349,\n",
       "   366,\n",
       "   371,\n",
       "   387,\n",
       "   388,\n",
       "   393,\n",
       "   396,\n",
       "   405,\n",
       "   413,\n",
       "   415,\n",
       "   418,\n",
       "   422,\n",
       "   425,\n",
       "   429,\n",
       "   430,\n",
       "   432,\n",
       "   433,\n",
       "   439,\n",
       "   445,\n",
       "   467],\n",
       "  [308,\n",
       "   312,\n",
       "   313,\n",
       "   319,\n",
       "   320,\n",
       "   323,\n",
       "   353,\n",
       "   367,\n",
       "   369,\n",
       "   372,\n",
       "   377,\n",
       "   392,\n",
       "   398,\n",
       "   417,\n",
       "   419,\n",
       "   423,\n",
       "   427,\n",
       "   428,\n",
       "   431,\n",
       "   440,\n",
       "   443,\n",
       "   461],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  []],\n",
       " [[472,\n",
       "   475,\n",
       "   479,\n",
       "   481,\n",
       "   487,\n",
       "   489,\n",
       "   490,\n",
       "   497,\n",
       "   501,\n",
       "   505,\n",
       "   507,\n",
       "   508,\n",
       "   510,\n",
       "   519,\n",
       "   521,\n",
       "   522,\n",
       "   524,\n",
       "   533,\n",
       "   541,\n",
       "   543,\n",
       "   545,\n",
       "   548,\n",
       "   550,\n",
       "   570,\n",
       "   574,\n",
       "   575,\n",
       "   577,\n",
       "   579,\n",
       "   580,\n",
       "   581,\n",
       "   582,\n",
       "   586,\n",
       "   587,\n",
       "   588,\n",
       "   599,\n",
       "   606,\n",
       "   609,\n",
       "   613,\n",
       "   621,\n",
       "   626,\n",
       "   637,\n",
       "   638,\n",
       "   639,\n",
       "   641,\n",
       "   642,\n",
       "   648,\n",
       "   655,\n",
       "   656,\n",
       "   661,\n",
       "   673,\n",
       "   681,\n",
       "   691],\n",
       "  [478,\n",
       "   482,\n",
       "   485,\n",
       "   486,\n",
       "   494,\n",
       "   495,\n",
       "   502,\n",
       "   509,\n",
       "   520,\n",
       "   529,\n",
       "   530,\n",
       "   538,\n",
       "   540,\n",
       "   544,\n",
       "   546,\n",
       "   549,\n",
       "   551,\n",
       "   552,\n",
       "   561,\n",
       "   585,\n",
       "   595,\n",
       "   598,\n",
       "   600,\n",
       "   607,\n",
       "   614,\n",
       "   623,\n",
       "   627,\n",
       "   629,\n",
       "   633,\n",
       "   634,\n",
       "   646,\n",
       "   649,\n",
       "   651,\n",
       "   659,\n",
       "   664,\n",
       "   666,\n",
       "   667,\n",
       "   674,\n",
       "   675,\n",
       "   679,\n",
       "   682,\n",
       "   686,\n",
       "   689,\n",
       "   693],\n",
       "  [470,\n",
       "   474,\n",
       "   484,\n",
       "   491,\n",
       "   492,\n",
       "   498,\n",
       "   500,\n",
       "   526,\n",
       "   528,\n",
       "   535,\n",
       "   536,\n",
       "   542,\n",
       "   554,\n",
       "   556,\n",
       "   563,\n",
       "   571,\n",
       "   572,\n",
       "   590,\n",
       "   615,\n",
       "   620,\n",
       "   628,\n",
       "   663,\n",
       "   685,\n",
       "   690],\n",
       "  [471,\n",
       "   473,\n",
       "   477,\n",
       "   493,\n",
       "   496,\n",
       "   499,\n",
       "   503,\n",
       "   511,\n",
       "   512,\n",
       "   517,\n",
       "   518,\n",
       "   525,\n",
       "   531,\n",
       "   532,\n",
       "   537,\n",
       "   539,\n",
       "   559,\n",
       "   560,\n",
       "   562,\n",
       "   564,\n",
       "   567,\n",
       "   568,\n",
       "   573,\n",
       "   578,\n",
       "   583,\n",
       "   602,\n",
       "   610,\n",
       "   616,\n",
       "   618,\n",
       "   619,\n",
       "   624,\n",
       "   632,\n",
       "   635,\n",
       "   640,\n",
       "   643,\n",
       "   644,\n",
       "   647,\n",
       "   650,\n",
       "   657,\n",
       "   660,\n",
       "   662,\n",
       "   671,\n",
       "   677,\n",
       "   687,\n",
       "   688],\n",
       "  [468,\n",
       "   469,\n",
       "   476,\n",
       "   480,\n",
       "   483,\n",
       "   488,\n",
       "   504,\n",
       "   506,\n",
       "   513,\n",
       "   514,\n",
       "   515,\n",
       "   516,\n",
       "   523,\n",
       "   527,\n",
       "   534,\n",
       "   547,\n",
       "   553,\n",
       "   555,\n",
       "   557,\n",
       "   558,\n",
       "   565,\n",
       "   566,\n",
       "   569,\n",
       "   576,\n",
       "   584,\n",
       "   589,\n",
       "   591,\n",
       "   592,\n",
       "   593,\n",
       "   594,\n",
       "   596,\n",
       "   597,\n",
       "   601,\n",
       "   603,\n",
       "   604,\n",
       "   605,\n",
       "   608,\n",
       "   611,\n",
       "   612,\n",
       "   617,\n",
       "   622,\n",
       "   625,\n",
       "   630,\n",
       "   631,\n",
       "   636,\n",
       "   645,\n",
       "   652,\n",
       "   653,\n",
       "   654,\n",
       "   658,\n",
       "   665,\n",
       "   668,\n",
       "   669,\n",
       "   670,\n",
       "   672,\n",
       "   676,\n",
       "   678,\n",
       "   680,\n",
       "   683,\n",
       "   684,\n",
       "   692],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  []],\n",
       " [[704,\n",
       "   705,\n",
       "   713,\n",
       "   723,\n",
       "   726,\n",
       "   734,\n",
       "   735,\n",
       "   736,\n",
       "   746,\n",
       "   758,\n",
       "   765,\n",
       "   776,\n",
       "   777,\n",
       "   791,\n",
       "   807,\n",
       "   809,\n",
       "   811,\n",
       "   818,\n",
       "   820,\n",
       "   829,\n",
       "   835,\n",
       "   837,\n",
       "   842,\n",
       "   846,\n",
       "   847,\n",
       "   851,\n",
       "   855,\n",
       "   857,\n",
       "   859,\n",
       "   860,\n",
       "   861,\n",
       "   863,\n",
       "   885,\n",
       "   889,\n",
       "   896,\n",
       "   899,\n",
       "   902,\n",
       "   903,\n",
       "   904,\n",
       "   909,\n",
       "   910,\n",
       "   932,\n",
       "   937,\n",
       "   938,\n",
       "   943,\n",
       "   951,\n",
       "   952,\n",
       "   958,\n",
       "   961,\n",
       "   966,\n",
       "   977,\n",
       "   984,\n",
       "   988,\n",
       "   989,\n",
       "   990],\n",
       "  [697,\n",
       "   707,\n",
       "   724,\n",
       "   725,\n",
       "   727,\n",
       "   732,\n",
       "   748,\n",
       "   749,\n",
       "   753,\n",
       "   756,\n",
       "   767,\n",
       "   773,\n",
       "   782,\n",
       "   783,\n",
       "   789,\n",
       "   798,\n",
       "   808,\n",
       "   815,\n",
       "   823,\n",
       "   824,\n",
       "   834,\n",
       "   865,\n",
       "   876,\n",
       "   877,\n",
       "   879,\n",
       "   881,\n",
       "   882,\n",
       "   887,\n",
       "   891,\n",
       "   895,\n",
       "   912,\n",
       "   913,\n",
       "   914,\n",
       "   915,\n",
       "   927,\n",
       "   929,\n",
       "   939,\n",
       "   942,\n",
       "   949,\n",
       "   955,\n",
       "   962,\n",
       "   968,\n",
       "   973],\n",
       "  [694,\n",
       "   698,\n",
       "   709,\n",
       "   715,\n",
       "   718,\n",
       "   730,\n",
       "   738,\n",
       "   742,\n",
       "   743,\n",
       "   757,\n",
       "   759,\n",
       "   760,\n",
       "   763,\n",
       "   766,\n",
       "   780,\n",
       "   788,\n",
       "   803,\n",
       "   804,\n",
       "   821,\n",
       "   825,\n",
       "   830,\n",
       "   833,\n",
       "   840,\n",
       "   841,\n",
       "   848,\n",
       "   852,\n",
       "   856,\n",
       "   858,\n",
       "   884,\n",
       "   886,\n",
       "   892,\n",
       "   894,\n",
       "   898,\n",
       "   901,\n",
       "   905,\n",
       "   918,\n",
       "   921,\n",
       "   925,\n",
       "   931,\n",
       "   934,\n",
       "   947,\n",
       "   953,\n",
       "   956,\n",
       "   959,\n",
       "   965,\n",
       "   969,\n",
       "   970,\n",
       "   971,\n",
       "   985],\n",
       "  [695,\n",
       "   696,\n",
       "   699,\n",
       "   700,\n",
       "   701,\n",
       "   703,\n",
       "   708,\n",
       "   710,\n",
       "   711,\n",
       "   712,\n",
       "   714,\n",
       "   716,\n",
       "   717,\n",
       "   719,\n",
       "   720,\n",
       "   721,\n",
       "   722,\n",
       "   728,\n",
       "   729,\n",
       "   731,\n",
       "   733,\n",
       "   739,\n",
       "   740,\n",
       "   741,\n",
       "   744,\n",
       "   745,\n",
       "   750,\n",
       "   751,\n",
       "   752,\n",
       "   755,\n",
       "   762,\n",
       "   764,\n",
       "   768,\n",
       "   769,\n",
       "   770,\n",
       "   771,\n",
       "   774,\n",
       "   775,\n",
       "   778,\n",
       "   781,\n",
       "   784,\n",
       "   785,\n",
       "   786,\n",
       "   787,\n",
       "   790,\n",
       "   792,\n",
       "   793,\n",
       "   794,\n",
       "   795,\n",
       "   796,\n",
       "   797,\n",
       "   799,\n",
       "   800,\n",
       "   801,\n",
       "   802,\n",
       "   810,\n",
       "   812,\n",
       "   813,\n",
       "   814,\n",
       "   816,\n",
       "   817,\n",
       "   822,\n",
       "   827,\n",
       "   831,\n",
       "   832,\n",
       "   836,\n",
       "   844,\n",
       "   845,\n",
       "   850,\n",
       "   854,\n",
       "   864,\n",
       "   866,\n",
       "   867,\n",
       "   868,\n",
       "   869,\n",
       "   871,\n",
       "   872,\n",
       "   874,\n",
       "   875,\n",
       "   878,\n",
       "   880,\n",
       "   900,\n",
       "   906,\n",
       "   908,\n",
       "   916,\n",
       "   917,\n",
       "   919,\n",
       "   920,\n",
       "   922,\n",
       "   924,\n",
       "   926,\n",
       "   928,\n",
       "   930,\n",
       "   935,\n",
       "   936,\n",
       "   940,\n",
       "   944,\n",
       "   945,\n",
       "   946,\n",
       "   950,\n",
       "   954,\n",
       "   957,\n",
       "   960,\n",
       "   964,\n",
       "   967,\n",
       "   972,\n",
       "   978,\n",
       "   979,\n",
       "   980,\n",
       "   981,\n",
       "   982,\n",
       "   983,\n",
       "   986],\n",
       "  [702,\n",
       "   706,\n",
       "   737,\n",
       "   747,\n",
       "   754,\n",
       "   761,\n",
       "   772,\n",
       "   779,\n",
       "   805,\n",
       "   806,\n",
       "   819,\n",
       "   826,\n",
       "   828,\n",
       "   838,\n",
       "   839,\n",
       "   843,\n",
       "   849,\n",
       "   853,\n",
       "   862,\n",
       "   870,\n",
       "   873,\n",
       "   883,\n",
       "   888,\n",
       "   890,\n",
       "   893,\n",
       "   897,\n",
       "   907,\n",
       "   911,\n",
       "   923,\n",
       "   933,\n",
       "   941,\n",
       "   948,\n",
       "   963,\n",
       "   974,\n",
       "   975,\n",
       "   976,\n",
       "   987],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  []]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Khi²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_of_your_word = tf[numDoc][strWord]\n",
    "tf = []\n",
    "for doc in usable:\n",
    "    tf_doc = {}\n",
    "    for word in vectorizer.get_feature_names():\n",
    "        tf_doc[word] = doc.count(word)\n",
    "    tf.append(tf_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number total of words\n",
    "# nb_total_word[numPartition]\n",
    "nb_total_word = []\n",
    "nb = 0\n",
    "\n",
    "for numDoc in range(0, len(usable)):\n",
    "    for word in vectorizer.get_feature_names():\n",
    "        nb += tf[numDoc][word]\n",
    "    if numDoc+1 in limits:\n",
    "        nb_total_word.append(nb)\n",
    "        nb=0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12667, 20096, 19095, 24614, 33564]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_total_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experimentalement': 0,\n",
       " 'theorie': 0,\n",
       " 'iteratif': 0,\n",
       " 'tâche': 0,\n",
       " 'attaquer': 0,\n",
       " 'apprentissage supervis': 0,\n",
       " 'collection document': 0,\n",
       " 'metadonnee': 0,\n",
       " 'lexploitation connaissance': 0,\n",
       " 'papier': 0,\n",
       " 'mis': 0,\n",
       " 'classement': 0,\n",
       " 'reference': 0,\n",
       " 'dautre partir': 0,\n",
       " 'limiter': 0,\n",
       " 'cell': 0,\n",
       " 'jeu': 0,\n",
       " 'fouiller grand': 0,\n",
       " 'cube': 0,\n",
       " 'volum': 0,\n",
       " 'introduire notion': 0,\n",
       " 'lusage': 1,\n",
       " 'latent': 0,\n",
       " 'cluster': 0,\n",
       " 'darbr': 0,\n",
       " 'lespace representation': 0,\n",
       " 'levaluation': 0,\n",
       " 'extraction motif': 0,\n",
       " 'fournir': 1,\n",
       " 'montrer linteret': 0,\n",
       " 'detr': 0,\n",
       " 'mesurer similarite entrer': 0,\n",
       " 'extraire connaissance': 0,\n",
       " 'utilisateur': 0,\n",
       " 'axiome': 0,\n",
       " 'ensembliste': 0,\n",
       " 'variabilite': 0,\n",
       " 'methodologie': 0,\n",
       " 'realisation': 0,\n",
       " 'plaire pertinent': 0,\n",
       " 'resumer': 0,\n",
       " 'voir': 0,\n",
       " 'dedonnee': 0,\n",
       " 'ecd': 0,\n",
       " 'dependanc': 0,\n",
       " 'decrivons': 0,\n",
       " 'lavantage': 0,\n",
       " 'recent': 0,\n",
       " 'annotation semantiqu': 0,\n",
       " 'methode fouiller': 0,\n",
       " 'ajoutee': 0,\n",
       " 'complexite': 0,\n",
       " 'porter': 0,\n",
       " 'ontologie domaine': 0,\n",
       " 'technique fouiller': 0,\n",
       " 'resultat obtenir montrer': 0,\n",
       " 'adequat': 0,\n",
       " 'prediction': 0,\n",
       " 'utilisation': 0,\n",
       " 'croisee': 0,\n",
       " 'priser': 0,\n",
       " 'utilisee': 0,\n",
       " 'fusion': 0,\n",
       " 'regle dassoci': 0,\n",
       " 'partir corpus': 0,\n",
       " 'difficult': 0,\n",
       " 'contextuel': 0,\n",
       " 'classification automatique': 0,\n",
       " 'developp': 0,\n",
       " 'itemset frequent': 0,\n",
       " 'reside': 1,\n",
       " 'temps reponse': 0,\n",
       " 'evenement': 2,\n",
       " 'expert domaine': 0,\n",
       " 'notion': 0,\n",
       " 'multiagent': 0,\n",
       " 'permettre visualiser': 0,\n",
       " 'grand base donneer': 0,\n",
       " 'presenter nouvel': 0,\n",
       " 'etiquete': 0,\n",
       " 'economiqu': 0,\n",
       " 'patrimoine': 0,\n",
       " 'propose': 0,\n",
       " 'amelioree': 0,\n",
       " 'poisson': 0,\n",
       " 'couple': 0,\n",
       " 'systematiqu': 0,\n",
       " 'combiner': 0,\n",
       " 'adapt': 0,\n",
       " 'depart': 0,\n",
       " 'liste': 0,\n",
       " 'distance': 0,\n",
       " 'interessant': 0,\n",
       " 'classification donnee': 0,\n",
       " 'nonsupervise': 0,\n",
       " 'lon': 0,\n",
       " 'mettre placer': 0,\n",
       " 'connaissance partir donnee': 0,\n",
       " 'baser modele': 0,\n",
       " 'technique fouill': 0,\n",
       " 'expert': 0,\n",
       " 'lapproch': 0,\n",
       " 'tester': 0,\n",
       " 'plaire specifiquement': 0,\n",
       " 'linterpretation': 0,\n",
       " 'miser placer': 0,\n",
       " 'ressourc': 0,\n",
       " 'integrer': 0,\n",
       " 'cuber': 0,\n",
       " 'test': 0,\n",
       " 'relation entrer': 0,\n",
       " 'connaissance domaine': 0,\n",
       " 'memoir': 0,\n",
       " 'distribution': 0,\n",
       " 'modification': 0,\n",
       " 'vouloir': 0,\n",
       " 'dynam': 0,\n",
       " 'robuste': 0,\n",
       " 'propre': 0,\n",
       " 'lentrepot': 0,\n",
       " 'regle dassociation': 0,\n",
       " 'dappariemer': 0,\n",
       " 'them': 0,\n",
       " 'algorithme': 0,\n",
       " 'guid': 0,\n",
       " 'passage lechelle': 0,\n",
       " 'regl dassoci': 0,\n",
       " 'plaire particulierement': 0,\n",
       " 'logique': 0,\n",
       " 'frequence': 0,\n",
       " 'conception': 0,\n",
       " 'expliciter': 0,\n",
       " 'flouer': 0,\n",
       " 'typique': 0,\n",
       " 'formaliser': 0,\n",
       " 'connaissance baser': 0,\n",
       " 'concepteur': 0,\n",
       " 'proposee': 0,\n",
       " 'puissance': 0,\n",
       " 'dexploiter': 0,\n",
       " 'coeur': 0,\n",
       " 'extension': 0,\n",
       " 'enrichir': 0,\n",
       " 'approximation': 0,\n",
       " 'dapprendre': 0,\n",
       " 'reduir': 0,\n",
       " 'incremental': 0,\n",
       " 'sousensembl': 0,\n",
       " 'methode permettre': 0,\n",
       " 'mene': 0,\n",
       " 'topographique': 0,\n",
       " 'analyser donnee': 0,\n",
       " 'encourageant prometteur': 0,\n",
       " 'selection variable': 0,\n",
       " 'nombre': 0,\n",
       " 'caracteristiqu': 1,\n",
       " 'entrer': 1,\n",
       " 'applique': 0,\n",
       " 'tâch': 0,\n",
       " 'structurer donneer': 0,\n",
       " 'resultat rechercher': 0,\n",
       " 'implicitement': 0,\n",
       " 'experimentation montrer': 0,\n",
       " 'jusqua': 0,\n",
       " 'exposer': 0,\n",
       " 'discussion': 0,\n",
       " 'treillis galoi': 0,\n",
       " 'mettre evidence': 0,\n",
       " 'plaire complexe': 0,\n",
       " 'permettre didentifier': 0,\n",
       " 'dexempl': 0,\n",
       " 'fouiller donne': 0,\n",
       " 'classifieur bayesien naïf': 0,\n",
       " 'conference': 0,\n",
       " 'comprehension': 0,\n",
       " 'grâce': 0,\n",
       " 'oeuvrer': 0,\n",
       " 'presenter resultat': 0,\n",
       " 'structuration': 0,\n",
       " 'obtenir': 0,\n",
       " 'dintegrer': 0,\n",
       " 'naïf': 0,\n",
       " 'page': 0,\n",
       " 'lexperimentation': 0,\n",
       " 'proteine': 0,\n",
       " 'couverture': 0,\n",
       " 'algorithme dapprentissage': 0,\n",
       " 'transform': 0,\n",
       " 'ensembl donneer': 0,\n",
       " 'objectif': 1,\n",
       " 'discriminer': 0,\n",
       " 'phenomene': 0,\n",
       " 'perspectif': 0,\n",
       " 'ala': 0,\n",
       " 'tabl': 0,\n",
       " 'dextraction connaissance': 0,\n",
       " 'rechercheer': 0,\n",
       " 'parallele': 0,\n",
       " 'semantiqu entrer': 0,\n",
       " 'regler': 0,\n",
       " 'sappui': 0,\n",
       " 'classificateur': 0,\n",
       " 'localisation': 0,\n",
       " 'extraction regl': 0,\n",
       " 'synthetiqu': 0,\n",
       " 'symetriqu': 0,\n",
       " 'dabord': 0,\n",
       " 'devenir plaire': 0,\n",
       " 'jeu donnee': 0,\n",
       " 'assurer': 0,\n",
       " 'reseaux dynamique': 0,\n",
       " 'cadrer': 0,\n",
       " 'expliquer': 0,\n",
       " 'efficacite': 0,\n",
       " 'stochastique': 0,\n",
       " 'jeu donneer': 0,\n",
       " 'mesure similarite': 0,\n",
       " 'technologie': 0,\n",
       " 'inclure': 0,\n",
       " 'item': 1,\n",
       " 'concret': 0,\n",
       " 'neuron': 0,\n",
       " 'amene': 0,\n",
       " 'faible': 0,\n",
       " 'donnee reell': 0,\n",
       " 'statique': 0,\n",
       " 'methode original': 0,\n",
       " 'acquisition': 0,\n",
       " 'mesurer similarit': 0,\n",
       " 'reduiser': 0,\n",
       " 'linfluence': 0,\n",
       " 'approche existant': 0,\n",
       " 'lieu': 0,\n",
       " 'valide': 0,\n",
       " 'conduire': 0,\n",
       " 'experimentation jeu': 0,\n",
       " 'lextraction motif': 0,\n",
       " 'adaptatif': 0,\n",
       " 'maladie': 0,\n",
       " 'appliquer': 0,\n",
       " 'realise': 0,\n",
       " 'documentaire': 0,\n",
       " 'precedent': 0,\n",
       " 'connaissance partir': 0,\n",
       " 'lapproch proposee': 0,\n",
       " 'refleter': 0,\n",
       " 'system recommandation': 0,\n",
       " 'poser': 0,\n",
       " 'experimentation jeu donnee': 0,\n",
       " 'processus': 0,\n",
       " 'web permettre': 0,\n",
       " 'classifieur bayesien': 0,\n",
       " 'espace': 0,\n",
       " 'methode': 0,\n",
       " 'acquérir': 0,\n",
       " 'lapproch propose': 0,\n",
       " 'applicatif': 0,\n",
       " 'mettre jour': 0,\n",
       " 'thematiqu': 0,\n",
       " 'tot': 0,\n",
       " 'primordial': 0,\n",
       " 'article': 0,\n",
       " 'france': 1,\n",
       " 'densite': 0,\n",
       " 'extrait': 0,\n",
       " 'face': 0,\n",
       " 'discret': 0,\n",
       " 'communaute': 0,\n",
       " 'lextraction regl': 0,\n",
       " 'erreur': 0,\n",
       " 'bayesien': 0,\n",
       " 'difference': 0,\n",
       " 'fixer': 0,\n",
       " 'etde': 0,\n",
       " 'quantite': 0,\n",
       " 'laugmentation': 0,\n",
       " 'loutil': 0,\n",
       " 'etat': 0,\n",
       " 'version': 0,\n",
       " 'estimer': 0,\n",
       " 'convergence': 0,\n",
       " 'etablir': 0,\n",
       " 'generiqu': 0,\n",
       " 'decoupage': 0,\n",
       " 'difficile': 0,\n",
       " 'methode propose': 0,\n",
       " 'comparaison entrer': 1,\n",
       " 'source': 0,\n",
       " 'dextrair': 0,\n",
       " 'relever': 0,\n",
       " 'danscet article': 0,\n",
       " 'present method': 0,\n",
       " 'matrice': 0,\n",
       " 'parametre': 0,\n",
       " 'organisation': 0,\n",
       " 'precision': 0,\n",
       " 'dinferer': 0,\n",
       " 'protocole': 0,\n",
       " 'minimiser': 0,\n",
       " 'rester': 0,\n",
       " 'lapport': 0,\n",
       " 'experience montrer': 0,\n",
       " 'regl': 0,\n",
       " 'condition': 0,\n",
       " 'agregation': 0,\n",
       " 'derive': 0,\n",
       " 'favoriser': 0,\n",
       " 'petit': 0,\n",
       " 'comporter': 0,\n",
       " 'colonne': 0,\n",
       " 'impliqu': 0,\n",
       " 'estimation': 0,\n",
       " 'direction': 0,\n",
       " 'apprentissage automatique': 0,\n",
       " 'larbr': 0,\n",
       " 'successif': 0,\n",
       " 'lefficacite': 0,\n",
       " 'metriqu': 0,\n",
       " 'avancee': 0,\n",
       " 'typologie': 0,\n",
       " 'text': 0,\n",
       " 'specialite': 0,\n",
       " 'adaptee': 0,\n",
       " 'ladequation': 0,\n",
       " 'benchmark': 0,\n",
       " 'dindexation': 0,\n",
       " 'graphe voisinage': 0,\n",
       " 'dalignemer': 0,\n",
       " 'mem': 1,\n",
       " 'qualit': 0,\n",
       " 'type': 0,\n",
       " 'fixe': 0,\n",
       " 'donnee reell montrer': 0,\n",
       " 'dextraire': 0,\n",
       " 'amont': 0,\n",
       " 'ameliorer': 0,\n",
       " 'mal': 0,\n",
       " 'employer': 0,\n",
       " 'tendance': 0,\n",
       " 'actuellement': 0,\n",
       " 'donne': 0,\n",
       " 'prometteur': 0,\n",
       " 'simuleer': 0,\n",
       " 'engendrer': 0,\n",
       " 'genere': 0,\n",
       " 'synonyme': 0,\n",
       " 'genre': 0,\n",
       " 'requet': 0,\n",
       " 'arbre': 0,\n",
       " 'induire': 0,\n",
       " 'operateur': 0,\n",
       " 'dimage': 0,\n",
       " 'lheure actuel': 0,\n",
       " 'daid': 0,\n",
       " 'web': 0,\n",
       " 'identifier': 0,\n",
       " 'donnee reell issu': 0,\n",
       " 'profiter': 0,\n",
       " 'composer': 0,\n",
       " 'aspect': 0,\n",
       " 'dopinion': 0,\n",
       " 'doutils': 0,\n",
       " 'dimension': 0,\n",
       " 'rappel': 0,\n",
       " 'importance': 0,\n",
       " 'question': 0,\n",
       " 'reduction': 0,\n",
       " 'linguistique': 0,\n",
       " 'profil utilisateur': 0,\n",
       " 'lanalys': 1,\n",
       " 'visualis': 0,\n",
       " 'devaluer': 0,\n",
       " 'volumetrie': 0,\n",
       " 'facile': 0,\n",
       " 'volee': 0,\n",
       " 'systeme dapprentissage': 0,\n",
       " 'caracteristique': 1,\n",
       " 'defaut': 0,\n",
       " 'logiciel': 0,\n",
       " 'article methode': 0,\n",
       " 'analys': 2,\n",
       " 'ligne colonne': 0,\n",
       " 'compacter': 0,\n",
       " 'decisionnel': 0,\n",
       " 'presence': 0,\n",
       " 'differenter': 0,\n",
       " 'maintenance': 0,\n",
       " 'developpe': 0,\n",
       " 'table': 0,\n",
       " 'afc': 0,\n",
       " 'resume': 0,\n",
       " 'lexploitation': 0,\n",
       " 'recommandation': 0,\n",
       " 'problem classification': 0,\n",
       " 'ignorer': 0,\n",
       " 'meilleur resultat': 0,\n",
       " 'recente': 0,\n",
       " 'framework': 0,\n",
       " 'voisinage': 0,\n",
       " 'donnees': 0,\n",
       " 'online': 0,\n",
       " 'demander': 0,\n",
       " 'caracter': 1,\n",
       " 'donnee geographiqu': 0,\n",
       " 'probabilit': 0,\n",
       " 'laid': 0,\n",
       " 'utiliser modele': 0,\n",
       " 'procedur': 0,\n",
       " 'croître': 0,\n",
       " 'resolu': 0,\n",
       " 'millier': 0,\n",
       " 'proportion': 0,\n",
       " 'nousproposon': 0,\n",
       " 'transformation': 0,\n",
       " 'permettre mettre': 0,\n",
       " 'dusage': 0,\n",
       " 'partitionnement': 0,\n",
       " 'domaine fouiller': 0,\n",
       " 'exploratoire': 0,\n",
       " 'ontolog': 0,\n",
       " 'pouvoir': 0,\n",
       " 'verification': 0,\n",
       " 'lannotation': 0,\n",
       " 'poser probleme': 0,\n",
       " 'classification document': 0,\n",
       " 'construction dontologie': 0,\n",
       " 'hybride': 0,\n",
       " 'flexible': 0,\n",
       " 'automatiquement': 0,\n",
       " 'qualite regle': 0,\n",
       " 'letiquetage': 0,\n",
       " 'sequentiell': 0,\n",
       " 'grammaire': 0,\n",
       " 'utile': 0,\n",
       " 'savere': 0,\n",
       " 'methode donnee': 0,\n",
       " 'factoriel': 0,\n",
       " 'dinform': 0,\n",
       " 'party': 0,\n",
       " 'flux donnee': 0,\n",
       " 'capacite': 0,\n",
       " 'daider': 0,\n",
       " 'negativ': 0,\n",
       " 'sou contraint': 0,\n",
       " 'fouiller donnee': 0,\n",
       " 'suite': 0,\n",
       " 'evidence': 0,\n",
       " 'anne': 0,\n",
       " 'dagregation': 0,\n",
       " 'longueur': 0,\n",
       " 'nombre classe': 0,\n",
       " 'laclassification': 0,\n",
       " 'article method': 0,\n",
       " 'bayesien naïf': 0,\n",
       " 'propriete': 0,\n",
       " 'annoter': 0,\n",
       " 'linterrogation': 0,\n",
       " 'presenton algorithm': 0,\n",
       " 'exprimer': 0,\n",
       " 'algorithme dextraction': 0,\n",
       " 'consister': 0,\n",
       " 'compact': 0,\n",
       " 'correctement': 0,\n",
       " 'etude comparatif': 0,\n",
       " 'coupl': 0,\n",
       " 'estimation densite': 0,\n",
       " 'stockeer': 0,\n",
       " 'method propose': 0,\n",
       " 'simple': 0,\n",
       " 'anr': 0,\n",
       " 'kohonen': 0,\n",
       " 'ascendant': 0,\n",
       " 'enjeu': 0,\n",
       " 'lanalys formel concept': 0,\n",
       " 'challenge': 0,\n",
       " 'construir': 0,\n",
       " 'developper': 0,\n",
       " 'dappartenance': 0,\n",
       " 'scenario': 0,\n",
       " 'appartenir': 0,\n",
       " 'minimal': 0,\n",
       " 'parametriqu': 0,\n",
       " 'contingence': 0,\n",
       " 'seri': 0,\n",
       " 'qualite': 0,\n",
       " 'interroger': 0,\n",
       " 'concentrer': 0,\n",
       " 'travail': 0,\n",
       " 'quune': 0,\n",
       " 'montrer': 0,\n",
       " 'decouverte': 0,\n",
       " 'sparql': 0,\n",
       " 'preparation donnee': 0,\n",
       " 'algorithm dextraction': 0,\n",
       " 'donnee permettre': 0,\n",
       " 'former regl': 0,\n",
       " 'document': 0,\n",
       " 'lalignement': 0,\n",
       " 'grand nombre': 0,\n",
       " 'treillis': 0,\n",
       " 'article montrer': 0,\n",
       " 'diagnostic': 0,\n",
       " 'reponse requete': 0,\n",
       " 'vis vis': 0,\n",
       " 'probabiliste': 0,\n",
       " 'miser jour': 0,\n",
       " 'donnee multidimensionnel': 0,\n",
       " 'hierarchie': 0,\n",
       " 'naturellement': 0,\n",
       " 'collaboratif': 0,\n",
       " 'pratique': 0,\n",
       " 'decision': 0,\n",
       " 'posseder': 0,\n",
       " 'consist': 0,\n",
       " 'tableau': 0,\n",
       " 'construit partir': 0,\n",
       " 'compose': 0,\n",
       " 'consider': 0,\n",
       " 'devaluation': 0,\n",
       " 'vie': 0,\n",
       " 'mouvement': 0,\n",
       " 'factorisation': 0,\n",
       " 'dapparition': 0,\n",
       " 'raisonnement': 0,\n",
       " 'opinion': 0,\n",
       " 'coûteux': 0,\n",
       " 'serie': 0,\n",
       " 'action': 0,\n",
       " 'annees': 0,\n",
       " 'temps reel': 1,\n",
       " 'fortement': 0,\n",
       " 'essentiellement': 0,\n",
       " 'graph voisinage': 0,\n",
       " 'representation connaissance': 0,\n",
       " 'learning': 0,\n",
       " 'lier': 0,\n",
       " 'existant': 0,\n",
       " 'coefficient': 0,\n",
       " 'motif frequent': 0,\n",
       " 'entite': 0,\n",
       " 'baser technique': 0,\n",
       " 'anormal': 0,\n",
       " 'clef': 0,\n",
       " 'decrir': 0,\n",
       " 'remedier': 0,\n",
       " 'structurer donnee': 0,\n",
       " 'plaire efficace': 0,\n",
       " 'fouiller donneer': 0,\n",
       " 'resultat obtenu': 0,\n",
       " 'partition': 0,\n",
       " 'video': 0,\n",
       " 'presentent': 0,\n",
       " 'texte': 0,\n",
       " 'sagit': 0,\n",
       " 'graphe': 0,\n",
       " 'synthes': 0,\n",
       " 'multilingue': 0,\n",
       " 'numeriqu': 0,\n",
       " 'approximatif': 0,\n",
       " 'doptimisation': 0,\n",
       " 'communaut': 0,\n",
       " 'situer': 0,\n",
       " 'simplifier': 0,\n",
       " 'langue': 0,\n",
       " 'dater': 0,\n",
       " 'ajouter': 0,\n",
       " 'dater stream': 0,\n",
       " 'bioinformatique': 0,\n",
       " 'jeu dedonnee': 0,\n",
       " 'skyline': 0,\n",
       " 'article methodologie': 0,\n",
       " 'domaine dapplication': 0,\n",
       " 'geographiqu': 0,\n",
       " 'descripteur': 0,\n",
       " 'topologique': 0,\n",
       " 'public': 0,\n",
       " 'faire appel': 0,\n",
       " 'referent': 0,\n",
       " 'capteur': 0,\n",
       " 'cestadir': 0,\n",
       " 'cesser': 0,\n",
       " 'presenter method': 0,\n",
       " 'fin': 0,\n",
       " 'lapprentissage automatique': 0,\n",
       " 'appele': 0,\n",
       " 'variable': 0,\n",
       " 'cooperativ': 0,\n",
       " 'generatif': 0,\n",
       " 'method fouiller': 0,\n",
       " 'recemment': 0,\n",
       " 'decrire': 0,\n",
       " 'generaliser': 0,\n",
       " 'critere devaluation': 0,\n",
       " 'intelligible': 0,\n",
       " 'experience': 0,\n",
       " 'jouer': 0,\n",
       " 'cartographique': 0,\n",
       " 'remedier probleme': 0,\n",
       " 'initial': 0,\n",
       " 'partir baser': 0,\n",
       " 'donnee textuel': 0,\n",
       " 'interaction entrer': 0,\n",
       " 'system rechercher': 0,\n",
       " 'ver': 0,\n",
       " 'point dinteret': 0,\n",
       " 'necessit': 0,\n",
       " 'precision rappel': 0,\n",
       " 'ponder': 0,\n",
       " 'cibler': 0,\n",
       " 'factoriel correspondance afc': 0,\n",
       " 'collectif': 0,\n",
       " 'lanalyst': 0,\n",
       " 'representation': 0,\n",
       " 'terrain': 0,\n",
       " 'risque': 0,\n",
       " 'lensembl dapprentissage': 0,\n",
       " 'utiliser methode': 0,\n",
       " 'paire': 0,\n",
       " 'tirer': 0,\n",
       " 'tableau donnee': 0,\n",
       " 'devenement': 1,\n",
       " 'motif ensembliste': 0,\n",
       " 'langage': 0,\n",
       " 'spectral': 0,\n",
       " 'extraire motif': 0,\n",
       " 'lapprentissage': 0,\n",
       " 'lacces': 0,\n",
       " 'enregistrement': 0,\n",
       " 'statistique implicatif': 0,\n",
       " 'potentiellement': 0,\n",
       " 'performance': 0,\n",
       " 'selectionde': 0,\n",
       " 'linteret lapproch': 0,\n",
       " 'deduire': 0,\n",
       " 'loriginalite': 0,\n",
       " 'syntaxe': 0,\n",
       " 'donnee relationnel': 0,\n",
       " 'plan': 0,\n",
       " 'vecteur support': 0,\n",
       " 'lanalyse exploratoire': 0,\n",
       " 'resultatsobtenu': 0,\n",
       " 'decouverte motif': 0,\n",
       " 'xquery': 0,\n",
       " 'international': 0,\n",
       " 'deper': 0,\n",
       " 'veritabl': 0,\n",
       " 'contenir': 0,\n",
       " 'melange': 0,\n",
       " 'introduire': 0,\n",
       " 'lapprentissage supervis': 0,\n",
       " 'surla': 0,\n",
       " 'distribu': 0,\n",
       " 'pretraitement': 0,\n",
       " 'linteret methode': 0,\n",
       " 'page web': 0,\n",
       " 'generalise': 0,\n",
       " 'methode classique': 0,\n",
       " 'decrit': 0,\n",
       " 'choix mesurer': 0,\n",
       " 'partiel': 0,\n",
       " 'superviser': 0,\n",
       " 'lemergence': 0,\n",
       " 'continu': 0,\n",
       " 'implemente': 0,\n",
       " 'condensees': 0,\n",
       " 'comparee': 0,\n",
       " 'classique': 0,\n",
       " 'olap': 0,\n",
       " 'extraction connaissance': 0,\n",
       " 'securite': 0,\n",
       " 'uniquement': 0,\n",
       " 'commercial': 0,\n",
       " 'respecter': 0,\n",
       " 'sousgraph': 0,\n",
       " 'method selection': 0,\n",
       " 'traitement': 0,\n",
       " 'dintegration': 0,\n",
       " 'clustering': 0,\n",
       " 'evaluer': 0,\n",
       " 'dinterpretation': 0,\n",
       " 'comparaison': 1,\n",
       " 'inferieur': 0,\n",
       " 'activite': 0,\n",
       " 'meilleur qualite': 0,\n",
       " 'partir corpu': 0,\n",
       " 'finalement': 0,\n",
       " 'variable explicatif': 0,\n",
       " 'projet': 0,\n",
       " 'arbitraire': 0,\n",
       " 'exprimeer': 0,\n",
       " 'vectoriel': 0,\n",
       " 'terminologie': 0,\n",
       " 'grand': 0,\n",
       " 'fort': 0,\n",
       " 'modifier': 0,\n",
       " 'gener': 0,\n",
       " 'alternatif': 0,\n",
       " 'travailler': 0,\n",
       " 'associee': 0,\n",
       " 'conditionnel': 0,\n",
       " 'sou former regl': 0,\n",
       " 'dincertitude': 0,\n",
       " 'proposonsune': 0,\n",
       " 'ferme frequent': 0,\n",
       " 'restitution': 0,\n",
       " 'entrer variable': 0,\n",
       " 'autoorganisatrice': 0,\n",
       " 'consommation': 0,\n",
       " 'donneer textuel': 0,\n",
       " 'format': 0,\n",
       " 'debut': 0,\n",
       " 'classification hierarchiqu': 0,\n",
       " 'mecanisme': 0,\n",
       " 'pendre': 0,\n",
       " 'pallier probleme': 0,\n",
       " 'loi': 0,\n",
       " 'necessite': 0,\n",
       " 'griller': 0,\n",
       " 'processus dextraction': 0,\n",
       " 'vecteur': 0,\n",
       " 'partir ensembl': 0,\n",
       " 'coherence': 0,\n",
       " 'saverer': 0,\n",
       " 'modelise': 0,\n",
       " 'cibl': 0,\n",
       " 'representent': 0,\n",
       " 'connexion': 0,\n",
       " 'dimager': 0,\n",
       " 'algorithm classification': 0,\n",
       " 'positionner': 0,\n",
       " 'regroupement': 0,\n",
       " 'lobjectif': 0,\n",
       " 'visualiser': 0,\n",
       " 'lextension': 0,\n",
       " 'dinternet': 0,\n",
       " 'lacquisition': 0,\n",
       " 'danscet': 0,\n",
       " 'intervall': 0,\n",
       " 'attribut': 0,\n",
       " 'temporel': 0,\n",
       " 'arborescent': 0,\n",
       " 'interesser': 0,\n",
       " 'campagne': 0,\n",
       " 'born': 0,\n",
       " 'principalement': 0,\n",
       " 'lexploration': 0,\n",
       " 'stockee': 0,\n",
       " 'partir donnee': 0,\n",
       " 'emergent': 0,\n",
       " 'permettre dutiliser': 0,\n",
       " 'lagregation': 0,\n",
       " 'permettre caracteriser': 0,\n",
       " 'semiautomatique': 0,\n",
       " 'satisfaire': 0,\n",
       " 'experimentation': 0,\n",
       " 'mesure': 0,\n",
       " 'rechercher': 0,\n",
       " 'actif': 0,\n",
       " 'permettre determiner': 0,\n",
       " 'dapprentissage supervis': 0,\n",
       " 'generalist': 0,\n",
       " 'schema': 0,\n",
       " 'flot': 0,\n",
       " 'aid': 0,\n",
       " 'resultat': 0,\n",
       " 'trer': 1,\n",
       " 'lidentification': 0,\n",
       " 'baser connaissance': 0,\n",
       " 'quantitatif': 0,\n",
       " 'reposer lutilisation': 0,\n",
       " 'resum': 0,\n",
       " 'exact': 0,\n",
       " 'laspect': 0,\n",
       " 'jouer role': 0,\n",
       " 'comparatif': 0,\n",
       " 'restreindre': 0,\n",
       " 'volumineux': 0,\n",
       " 'using': 0,\n",
       " 'positif': 0,\n",
       " 'annot': 0,\n",
       " 'caracteristiquer': 0,\n",
       " 'classification croise': 0,\n",
       " 'important': 0,\n",
       " 'exhiber': 0,\n",
       " 'caracterise': 0,\n",
       " 'application': 0,\n",
       " 'jeu donnee reel': 0,\n",
       " 'exister entrer': 0,\n",
       " 'lune': 0,\n",
       " 'modalite': 0,\n",
       " 'multilabel': 0,\n",
       " 'synthetiser': 0,\n",
       " 'donnee simuleer': 0,\n",
       " 'manier': 0,\n",
       " 'indicateur': 0,\n",
       " 'temps': 1,\n",
       " 'montron': 0,\n",
       " 'repondre besoin': 0,\n",
       " 'detendre': 0,\n",
       " 'apprentissage': 0,\n",
       " 'dinteret': 0,\n",
       " 'donnee symbolique': 0,\n",
       " 'pondere': 0,\n",
       " 'proposon': 0,\n",
       " 'topolog': 0,\n",
       " 'reell montrer': 0,\n",
       " 'construction automatique': 0,\n",
       " 'combinaison': 0,\n",
       " 'serie temporel': 0,\n",
       " 'optimisation': 0,\n",
       " 'variation': 0,\n",
       " 'prevention': 0,\n",
       " 'redondant': 0,\n",
       " 'caracteriser': 0,\n",
       " 'specifiqu': 1,\n",
       " 'qualitativement': 0,\n",
       " 'sousgraphe': 0,\n",
       " 'lheure': 0,\n",
       " 'croiser': 0,\n",
       " 'presenter nouvel algorithme': 0,\n",
       " 'dobjet': 0,\n",
       " 'lien': 0,\n",
       " 'semantiqu partir': 0,\n",
       " 'context': 0,\n",
       " 'presenton': 0,\n",
       " 'partir': 0,\n",
       " 'duree': 0,\n",
       " 'elevee': 0,\n",
       " 'particularite': 0,\n",
       " 'consequent': 0,\n",
       " 'grand volume donnee': 0,\n",
       " 'travail recent': 0,\n",
       " 'bibliographique': 0,\n",
       " 'taill': 0,\n",
       " 'decrite': 0,\n",
       " 'graphe conceptuel': 0,\n",
       " 'algorithm': 0,\n",
       " 'reseaux neuron': 0,\n",
       " 'caracteriseer': 0,\n",
       " 'deuxieme': 0,\n",
       " 'eviter': 0,\n",
       " 'quen': 0,\n",
       " 'conflit': 0,\n",
       " 'priser decision': 0,\n",
       " 'imposer': 0,\n",
       " 'treillis concept': 0,\n",
       " 'extraite': 0,\n",
       " 'pouvoir aider': 0,\n",
       " 'necessitent': 0,\n",
       " 'structurer document': 0,\n",
       " 'elev': 0,\n",
       " 'autour': 0,\n",
       " 'visualisation grand': 0,\n",
       " 'poindre': 0,\n",
       " 'reduit': 0,\n",
       " 'discrimination': 0,\n",
       " 'experimental': 0,\n",
       " 'etuder': 0,\n",
       " 'lorsquil': 0,\n",
       " 'devaluer qualite': 0,\n",
       " 'mettre disposition': 0,\n",
       " 'strategiqu': 0,\n",
       " 'apartir': 0,\n",
       " 'entrepot donnee': 0,\n",
       " 'denrichir': 0,\n",
       " 'social': 0,\n",
       " 'identification': 0,\n",
       " 'pouvoir prendre': 0,\n",
       " 'instance': 0,\n",
       " 'gestion': 0,\n",
       " 'minoritaire': 0,\n",
       " 'repartition': 0,\n",
       " 'foret aleatoir': 0,\n",
       " 'region': 0,\n",
       " 'problematiqu': 0,\n",
       " 'dadapter': 0,\n",
       " 'defi egc': 0,\n",
       " 'demarch': 0,\n",
       " 'eleve': 0,\n",
       " 'quantite donnee': 0,\n",
       " 'français': 0,\n",
       " 'mass': 0,\n",
       " 'presenter papier': 0,\n",
       " 'generees': 0,\n",
       " 'donnee': 0,\n",
       " 'referentiel': 0,\n",
       " 'codage': 0,\n",
       " 'informationnel': 0,\n",
       " 'nettement': 0,\n",
       " 'logique inductif': 0,\n",
       " 'outil': 0,\n",
       " 'completer': 0,\n",
       " 'appeler': 0,\n",
       " 'limplementation': 0,\n",
       " 'loin': 0,\n",
       " 'partir ensemble': 0,\n",
       " 'quil': 0,\n",
       " 'indice': 0,\n",
       " 'apprendre': 0,\n",
       " 'presentons': 0,\n",
       " 'mapping': 0,\n",
       " 'milieu': 0,\n",
       " 'permettre lextraction': 0,\n",
       " 'granularite': 0,\n",
       " 'lindice': 0,\n",
       " 'fouiller texter': 0,\n",
       " 'monder reel': 0,\n",
       " 'multidimensionnel': 0,\n",
       " 'dependance': 0,\n",
       " 'lextraction motif sequentiel': 0,\n",
       " 'protein': 0,\n",
       " 'selection modele': 0,\n",
       " 'concis': 0,\n",
       " 'concept treillis': 0,\n",
       " 'reconstruction': 0,\n",
       " 'lexemple': 0,\n",
       " 'lintegration': 0,\n",
       " 'chercheur': 0,\n",
       " 'sappuyer': 0,\n",
       " 'reconnaitre': 0,\n",
       " 'issu': 0,\n",
       " 'dobjets': 0,\n",
       " 'montrer method': 0,\n",
       " 'commun': 0,\n",
       " 'achat': 0,\n",
       " 'classification supervise': 0,\n",
       " 'fiable': 0,\n",
       " 'permettre dobtenir': 0,\n",
       " 'testee': 0,\n",
       " 'permettre traiter': 0,\n",
       " 'representeer': 0,\n",
       " 'etude experimental': 0,\n",
       " 'tir': 0,\n",
       " 'dense': 0,\n",
       " 'national': 0,\n",
       " 'lapproche': 0,\n",
       " 'mesurer dissimilarite': 0,\n",
       " 'reduite': 0,\n",
       " 'sommet': 0,\n",
       " 'simulation': 0,\n",
       " 'multimedia': 0,\n",
       " 'procedure': 0,\n",
       " 'etl': 0,\n",
       " 'lhypothese': 0,\n",
       " 'contraindre': 0,\n",
       " 'trajectoire': 0,\n",
       " 'letat': 0,\n",
       " 'passer': 0,\n",
       " 'darbr decision': 0,\n",
       " 'modele permettre': 0,\n",
       " 'dependance fonctionnel': 0,\n",
       " 'classification': 0,\n",
       " 'fondamental': 0,\n",
       " 'article decrit': 0,\n",
       " 'considerer': 0,\n",
       " 'problematique': 0,\n",
       " 'definit': 0,\n",
       " 'constant': 0,\n",
       " 'tâche classification': 0,\n",
       " 'plaire plaire': 0,\n",
       " 'fondre': 0,\n",
       " 'inductif': 0,\n",
       " 'lelaboration': 0,\n",
       " 'technique': 0,\n",
       " 'utiliser': 0,\n",
       " 'pixel': 0,\n",
       " 'iii': 0,\n",
       " 'lun': 0,\n",
       " 'cancer': 0,\n",
       " 'lextraction connaissance': 0,\n",
       " 'sinteresser': 0,\n",
       " 'construire partir': 0,\n",
       " 'courir': 0,\n",
       " 'independamment': 0,\n",
       " 'texter': 0,\n",
       " 'mecanism': 0,\n",
       " 'population': 0,\n",
       " 'fonction': 0,\n",
       " 'configuration': 0,\n",
       " 'connaissance expert': 0,\n",
       " 'reel': 1,\n",
       " 'deffectuer': 0,\n",
       " 'rapidement': 0,\n",
       " 'quun': 0,\n",
       " 'strategie': 0,\n",
       " 'manipuler': 0,\n",
       " 'parametr': 0,\n",
       " 'morphosyntaxique': 0,\n",
       " 'premiere': 0,\n",
       " 'programmation logique': 0,\n",
       " 'propager': 0,\n",
       " 'lalgorithm': 0,\n",
       " 'criter': 0,\n",
       " 'tailler': 0,\n",
       " 'pouvoir faire': 0,\n",
       " 'model': 0,\n",
       " 'controle': 0,\n",
       " 'stockage': 0,\n",
       " 'sift': 0,\n",
       " 'dansun': 0,\n",
       " ...}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_word[num_partition][word]\n",
    "nb_word = []\n",
    "\n",
    "word_in_this_parti = {}\n",
    "for word in vectorizer.get_feature_names():\n",
    "    word_in_this_parti[word] = 0\n",
    "\n",
    "for numDoc in range(0, len(usable)):\n",
    "    for word in vectorizer.get_feature_names():\n",
    "        word_in_this_parti[word] += tf[numDoc][word]\n",
    "    if numDoc+1 in limits:\n",
    "        nb_word.append(word_in_this_parti)\n",
    "        word_in_this_parti = {}\n",
    "        for word in vectorizer.get_feature_names():\n",
    "            word_in_this_parti[word] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nb_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_word_by_cluster[numPartition][numCluster]\n",
    "nb_word_by_cluster = []\n",
    "for parti in partitions:\n",
    "    nb_word_clus = []\n",
    "    for cluster in parti:\n",
    "        nb = 0\n",
    "        for numDoc in cluster:\n",
    "            for word in vectorizer.get_feature_names():\n",
    "                nb += tf[numDoc][word]\n",
    "        nb_word_clus.append(nb)\n",
    "    nb_word_by_cluster.append(nb_word_clus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_of_khi2 = khi2[numPartition][numCluster][word]\n",
    "khi2 = []\n",
    "\n",
    "for numParti in range(0, len(partitions)):\n",
    "    khi2parti = []\n",
    "    for numCluster in range(0, len(partitions[numParti])):\n",
    "        khi2cluster = {}\n",
    "        \n",
    "        for word in vectorizer.get_feature_names():\n",
    "            if nb_word_by_cluster[numParti][numCluster] == 0:\n",
    "                khi2cluster[word] = 0\n",
    "            else:\n",
    "                word_in_this_parti[word] = 0\n",
    "                E = nb_word[numParti][word]\n",
    "                E =+ nb_word_by_cluster[numParti][numCluster]\n",
    "                E = E/ nb_total_word[numParti]\n",
    "                N = 0\n",
    "                for numDoc in partitions[numParti][numCluster]:\n",
    "                    N += tf[numDoc][word]\n",
    "                khi2cluster[word] = (pow(N - E, 2)/E)        \n",
    "        khi2parti.append(khi2cluster)\n",
    "    khi2.append(khi2parti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of your labels = labels[numPartition][numCluster]\n",
    "labels = []\n",
    "\n",
    "for numPartition in range(0, len(nb_word_by_cluster)):\n",
    "    label_clus = []\n",
    "    for numCluster in range(0, len(nb_word_by_cluster[numPartition])):\n",
    "        label_clus.append(Counter(khi2[numPartition][numCluster]).most_common(5))\n",
    "    labels.append(label_clus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some clusters can be empty\n",
    "\"donne\" in labels[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diachronic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: sigma are ecart-type :)\n",
    "\n",
    "def inter(listA, listB):\n",
    "    return np.intersect1d(listA, listB)\n",
    "    \n",
    "# cluster_t and cluster_s must be in two different partitions\n",
    "def proba(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S):\n",
    "    total_inter = 0\n",
    "    total_t = 0\n",
    "    for f in range(0, len(labels[num_partition_T][num_cluster_t])):\n",
    "        for f_s in labels[num_partition_S][num_cluster_s]:\n",
    "            if labels[num_partition_T][num_cluster_t][f][0] == f_s[0]:\n",
    "                total_inter += labels[num_partition_T][num_cluster_t][f][1]\n",
    "                break\n",
    "        total_t += labels[num_partition_T][num_cluster_t][f][1]\n",
    "    if total_t == 0:\n",
    "        return 0\n",
    "    return total_inter / total_t\n",
    "    \n",
    "\n",
    "def P_A(num_cluster_s, num_partition_T, num_partition_S):\n",
    "    # first, we have to know what are the cluster which got the label\n",
    "    total = 0\n",
    "    nb_computation = 0\n",
    "    for label_s in labels[num_partition_S][num_cluster_s]:\n",
    "        for num_cluster_t in range(0, len(partitions[num_partition_T])):\n",
    "            if label_s in labels[num_partition_T][num_cluster_t]:\n",
    "                total += proba(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S)\n",
    "                nb_computation += 1\n",
    "    if nb_computation == 0:\n",
    "        return 0\n",
    "    return total / nb_computation\n",
    "\n",
    "# Define a coeficient for the activity \n",
    "def activity(num_partition_S, num_partition_T):\n",
    "    res = 0\n",
    "    for num_cluster_s in range(0, len(partitions[num_partition_S])):\n",
    "        res += P_A(num_cluster_s, num_partition_T, num_partition_S)\n",
    "    return res / len(partitions[num_partition_S])\n",
    "\n",
    "# Ecart-type, but it isn't very usefull xD\n",
    "sigma_t = 0.01\n",
    "sigma_s = 0.01\n",
    "\n",
    "# Our Graal\n",
    "# Does cluster_t is similar to cluster_s?\n",
    "def similar(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S):\n",
    "    cond1 = proba(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S) > P_A(num_cluster_s, num_partition_T, num_partition_S)\n",
    "    cond2 = proba(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S) > activity(num_partition_S, num_partition_T) + sigma_s\n",
    "    \n",
    "    cond1 = proba(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S) > P_A(num_cluster_s, num_partition_T, num_partition_S)\n",
    "    cond2 = proba(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S) > activity(num_partition_T, num_partition_S) + sigma_t\n",
    "    return cond1 and cond2 and cond3 and cond4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar(3, 3, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('donne', 4130.919130068158), ('utilis', 3839.2070727325768), ('donnee', 3287.8242734071123), ('text', 3028.1535314172306), ('method', 2779.1632278759153)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4130.919130068158"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(labels[0][1])\n",
    "labels[0][1][0][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
