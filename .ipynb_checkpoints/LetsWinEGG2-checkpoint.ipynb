{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from numpy import array\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from gensim.test.utils import common_dictionary, common_corpus\n",
    "from gensim.models import LsiModel\n",
    "from gensim import corpora, models, utils\n",
    "from gensim.test.utils import common_corpus, common_dictionary, get_tmpfile\n",
    "from gensim.models import LsiModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spacy lib\n",
    "# On https://spacy.io/\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Parameters #\n",
    "##############\n",
    "\n",
    "min_gram = 1\n",
    "max_gram = 3\n",
    "\n",
    "# To create ours partitions, we must first know the years which will be the limits\n",
    "limit_years = [2007, 2010, 2014]\n",
    "\n",
    "# Ignore words that appear at a frequency less than tresh_freq in the corpus\n",
    "tresh_freq = 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datas preprocessing methods.\n",
    "\n",
    "# Lemmatisation without poncutations\n",
    "\n",
    "stemmer = nltk.stem.snowball.FrenchStemmer()\n",
    "fstw = stopwords.words('french')\n",
    "\n",
    "# French Stop Words, extraits depuis le fichier stopwords-fr.txt + stopwords french de nltk\n",
    "sourceFST = [x.replace('\\n', '') for x in open('stopwords-fr.txt', mode=\"r\", encoding=\"utf-8\").readlines()]+fstw\n",
    "\n",
    "# Based on ration of french and english stopwords\n",
    "def isEnglish(article):\n",
    "    total_fsw = len([x for x in article.split() if x in sourceFST])\n",
    "    total_esw = len([x for x in article.split() if x in stopwords.words('english')])\n",
    "    ratio = 100\n",
    "    if total_fsw != 0:\n",
    "        ratio = total_esw/total_fsw\n",
    "    return ratio > 1 and total_esw > 3\n",
    "\n",
    "def lemmatize(article):\n",
    "    artiregex = re.sub(\" [A-z][A-z] \", \" \", article) # word of length < 2\n",
    "    artiregex = artiregex.lower()\n",
    "    artiregex = re.sub(\"(é|è|ê)\", \"e\", artiregex)\n",
    "    output = []\n",
    "    outPonc = artiregex.translate(artiregex.maketrans(\"\",\"\", string.punctuation))\n",
    "    outLem = nlp(outPonc)\n",
    "    for token in outLem:\n",
    "        if token.lemma_ not in sourceFST and [x for x in token.lemma_ if x not in \"0123456789\"] != []:\n",
    "            output.append(token.lemma_)\n",
    "    res = ' '.join(output)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Reading\n",
    "data = pd.read_csv('export_articles_EGC_2004_2018.csv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's process our corpus, and determine a limit to split it in partitions\n",
    "\n",
    "# usable[] correspond to our corpus processed\n",
    "# limits[] let us know when to delimit partitions\n",
    "limits = []\n",
    "usable = []\n",
    "\n",
    "prev_year = data['year'][0]\n",
    "numArti = 0\n",
    "for i in range(0, len(data['abstract']), 1):\n",
    "    #if not null, empty, or whatever (so if there is a abstract):\n",
    "    if not isinstance(data['abstract'][i], float) and not isEnglish(data['abstract'][i]):\n",
    "        text = data['abstract'][i]\n",
    "        if not isinstance(data['title'][i], float):\n",
    "            text += \" \"+data['title'][i]\n",
    "\n",
    "        numArti+=1\n",
    "        usable.append(stemmer.stem(lemmatize(text)))\n",
    "        year = data['year'][i]\n",
    "        if year != prev_year:\n",
    "            prev_year = year\n",
    "            if year in limit_years:\n",
    "                limits.append(numArti)\n",
    "limits.append(numArti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/info/etu/m2/i140302/venv/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['quelqu'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'articles = 991\n",
      "nombre de mots = 133091\n",
      "limits = [223, 468, 694, 991]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'plateforme objectif permettre citoyen danalyserpar euxmemer tweet politique devenement specifiqu francepour cas lelection presidentiell ideo2017 analyser quasitemps reel message candidat fournir principal caracteristiqueslusage lexiqu politique comparaison entrer candidat ideo2017   plateforme citoyen dediee lanalyse tweet evenement polit'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display pre-processed datas\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=sourceFST, use_idf=True, ngram_range=(min_gram, max_gram), max_df=tresh_freq)\n",
    "tfidf = vectorizer.fit_transform(usable)\n",
    "\n",
    "print(\"nombre d'articles =\", len(usable))\n",
    "print(\"nombre de mots =\", len(tfidf.toarray()[0]))\n",
    "print(\"limits =\", limits)\n",
    "\n",
    "usable[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of partitions_tfidf[], which give us the TFIDF of each cluster of each partition\n",
    "# partitions_tfidf[num_partition][num_doc][num_word]\n",
    "# Beware, num_doc can't be equals to 1091 (max). You have partitions, so every doc aren't in every partitions\n",
    "# num_word can be found via vectorizer.get_feature_name()\n",
    "partitions_tfidf = []\n",
    "beg = 0\n",
    "for l in limits:\n",
    "    last = l\n",
    "    partitions_tfidf.append([list(x) for x in list(tfidf.toarray())[beg:last]])\n",
    "    beg = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133091"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(partitions_tfidf[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From here, the end use LSA :P Then the code below isn't important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params\n",
    "nb_concepts = 30\n",
    "min_gram = 1\n",
    "max_gram = 3\n",
    "\n",
    "# Creation of cleandocs, which is usable[] with ngrams\n",
    "cleandocs = []\n",
    "for t in usable:\n",
    "    doc = []\n",
    "    for n in range(min_gram, max_gram+1):\n",
    "        for gram in ngrams(t.split(), n):\n",
    "            doc.append(\" \".join(gram))\n",
    "    cleandocs.append(doc)\n",
    "\n",
    "# Creation of tfidf model, a tool to create ours tfidf\n",
    "corpus = []\n",
    "dictionary = corpora.Dictionary(cleandocs)\n",
    "for doc in cleandocs:\n",
    "    newVec = dictionary.doc2bow(doc)\n",
    "    corpus.append(newVec)\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "# Creation of partitions_lsa[], which give us the LSA of each partition\n",
    "partitions_lsa = []\n",
    "beg = 0\n",
    "for l in limits:\n",
    "    last = l\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    lsi = models.LsiModel(corpus_tfidf, num_topics=nb_concepts, id2word=dictionary)\n",
    "    corpus_lsi = lsi[corpus_tfidf[beg:last]]\n",
    "    partitions_lsa.append(corpus_lsi)\n",
    "    beg = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition numéro: 0\n",
      "document number  0\n",
      "[(0, 0.02982208116887981), (1, -0.001906143223681982), (2, 0.010527762486456876), (3, 0.00512208054114747), (4, -0.0240719681584426), (5, -0.005643712332486874), (6, -0.009955091977091294), (7, -0.009954155383911407), (8, -0.017674406998599592), (9, -0.0024169278588186544), (10, -0.0001852330263841159), (11, -0.05189196475192622), (12, 0.001499112528902667), (13, 0.008017471966816216), (14, -0.034872470177392735), (15, -0.030681999944283774), (16, 0.044520907777872515), (17, 0.006840765395550134), (18, 0.0009695079689956844), (19, -0.04243464814555533), (20, 0.00746589186631983), (21, -0.01899344996867067), (22, 0.024389782182991832), (23, 0.030600463480148228), (24, -0.007809583555913306), (25, -0.018998973180313038), (26, 0.01497127187502871), (27, -0.03010393486014037), (28, 0.016232599468182927), (29, 0.01620951523807301)]\n",
      "document number  1\n",
      "[(0, 0.08978169498851843), (1, 0.009278949774664891), (2, -0.07193406882349739), (3, -0.049958924950165406), (4, 0.0018188240341508821), (5, 0.034287488153797833), (6, -0.07931077792494476), (7, 0.009048459004719), (8, -0.013771571734637332), (9, -0.03582223094178656), (10, -0.013786995708751611), (11, -0.040572256950245585), (12, -0.03853675353566395), (13, -0.058822949836927395), (14, -0.0021867234847296794), (15, 0.07625537741111604), (16, 0.03625845166618659), (17, 0.012080565458675087), (18, -0.018069479977765014), (19, 0.036715457316352375), (20, 0.0937272081014348), (21, -0.057353269092663375), (22, -0.025299545565953433), (23, 0.04666421010772104), (24, -0.09157805985077686), (25, -0.016141967698065536), (26, 0.008743356474670918), (27, 0.0112302414602738), (28, 0.030181529169727737), (29, 0.004561779005325219)]\n",
      "document number  2\n",
      "[(0, 0.07105528766150307), (1, 0.010839159883371686), (2, 0.00518416270275765), (3, 0.08744356153210764), (4, -0.003998883354287811), (5, 0.0396087973717273), (6, 0.014012956817337247), (7, -0.006291826640522368), (8, 0.13400838832533035), (9, -0.09571990130761537), (10, 0.006585893301060354), (11, 0.0010557576602313263), (12, 0.07932474912133723), (13, -0.053722006393052225), (14, -0.030628635126780365), (15, 0.047213730514448424), (16, 0.0015835413858868547), (17, 0.04917038230352815), (18, 0.01177756214283021), (19, -0.0015179644228913406), (20, 0.07503183464796498), (21, 0.054843479868589264), (22, -0.025628754488043342), (23, 0.007136481612851997), (24, -0.035664195522138516), (25, -0.01323818069098864), (26, 0.02402051666667037), (27, -0.05276250392090671), (28, 0.023356607047097907), (29, -0.01434917910468766)]\n",
      "Partition numéro: 1\n",
      "document number  0\n",
      "[(0, 0.0625443585699367), (1, 0.00849587028564912), (2, 0.026430279900447), (3, -0.03580539168082096), (4, 0.013824116756826547), (5, -0.009094746605461647), (6, 0.03317631698640013), (7, -0.07658208440660137), (8, -0.03906586692169087), (9, 0.02566343907025969), (10, -0.004982230431051058), (11, 0.02010414406093591), (12, -0.003988162578443492), (13, 0.010247742912043016), (14, -0.03185227136797016), (15, -0.015853778457653272), (16, 0.026222440419148185), (17, -0.004289051965270928), (18, -0.06117447945936967), (19, -0.01643968533546465), (20, 0.04862812641031697), (21, -0.07620633175874117), (22, 0.0317013054450652), (23, 0.003620453028959332), (24, -0.0032891807044364784), (25, 0.038565831998480356), (26, -0.024372504297506807), (27, 0.0589657085043573), (28, -0.028722023725941466), (29, 0.040076792803496346)]\n",
      "document number  1\n",
      "[(0, 0.04739926541070791), (1, 0.0034299169925382537), (2, 0.03691406566630476), (3, 0.011356241405512956), (4, 0.0135587720346319), (5, 0.02565645248850119), (6, -0.052535877998335555), (7, 0.007432798050627931), (8, -0.018260453464594813), (9, 0.005117679115931652), (10, -0.0017177852620642018), (11, -0.04701942583186806), (12, -0.025021390808219038), (13, 0.026710851034474167), (14, 0.003596101497060261), (15, -0.02118973747642986), (16, 0.0344325137440139), (17, -0.043700312930641874), (18, 0.05440540225425151), (19, 0.05530854552709661), (20, 0.0023515625068264907), (21, 0.06565322302227701), (22, -0.007231256136807861), (23, -0.02135964636379073), (24, 0.012607987970850817), (25, 0.010985760267903856), (26, -0.06691386756810504), (27, 0.014131575696426762), (28, 0.017889460587342264), (29, 0.012912818330563208)]\n",
      "document number  2\n",
      "[(0, 0.07714870746491787), (1, 0.042927428739031784), (2, 0.11253361327946865), (3, -0.1740024522171437), (4, 0.0023298682288330204), (5, -0.017712346908068118), (6, 0.0651986213210715), (7, -0.028954831212029002), (8, -0.017721634578300394), (9, 0.03880117210013234), (10, 0.021065517369431828), (11, 0.034286916727062626), (12, -0.010704862717506144), (13, -0.02895132677134189), (14, 0.021474748831771563), (15, -0.031554969780155), (16, 0.017179693693385616), (17, 0.02385516689422295), (18, 0.0181281160752587), (19, -0.04055877709473199), (20, -0.01340087384385731), (21, 0.009848536872206156), (22, 0.049445748294840185), (23, -0.02680848406952026), (24, 0.010533955902265905), (25, -0.023960742174650565), (26, 0.005008588345419368), (27, 0.02524737494396091), (28, -0.029413787116995865), (29, -0.009466679775200987)]\n",
      "Partition numéro: 2\n",
      "document number  0\n",
      "[(0, 0.05268868549291227), (1, 0.0042813429387050655), (2, -0.012799773781014647), (3, -0.01247167374201545), (4, -0.020747076721805768), (5, -0.004425310037338708), (6, 0.012847135100873137), (7, 0.023160408892314445), (8, 0.018159181658891625), (9, -0.012978282519424407), (10, 0.03732104757113337), (11, -0.021043114705068686), (12, -0.0027166979504263194), (13, 0.002807317940601496), (14, -0.012208851108601817), (15, -0.020146837016043796), (16, -0.001198159968993019), (17, 0.015815193274865014), (18, -0.004784001045652035), (19, 0.015351850419295744), (20, -0.058953953307541176), (21, 0.018725794801782904), (22, -0.015494310915061066), (23, 0.005471422125145188), (24, 0.04234579048291613), (25, -0.01884372511416803), (26, 0.02082216428698392), (27, 0.01099637299518696), (28, 0.0065364268343773395), (29, -0.0060944644640791076)]\n",
      "document number  1\n",
      "[(0, 0.03363067940698081), (1, 0.011516981176527614), (2, 0.013348170728744498), (3, -0.009806316409301567), (4, -0.03266635652117793), (5, 0.04353446069873226), (6, -0.04906601494964848), (7, -0.006932540207995242), (8, 0.014397419550903632), (9, 0.034233335751862146), (10, -0.04368871357564983), (11, 0.016005622050156855), (12, -0.048253439752364256), (13, 0.01952171257916325), (14, -0.01837724519373616), (15, -0.005169466822695366), (16, 0.013679235197385214), (17, 0.0019221462341212385), (18, -0.051205466920995264), (19, -0.017972353580074537), (20, -0.009891520115938673), (21, 0.003766136226749901), (22, -0.058222467718743134), (23, -0.004475078824875831), (24, -0.028489751027531887), (25, -0.011574299358312827), (26, -0.017233105373728835), (27, 0.006707840704607119), (28, 0.020762540867841323), (29, 0.02540656576062053)]\n",
      "document number  2\n",
      "[(0, 0.09164934974193689), (1, -0.1694730767503561), (2, 0.013833828534287792), (3, 0.026640836460027118), (4, -0.0029556249336133026), (5, -0.03601163442066592), (6, -0.017388773037890416), (7, -0.0017679215673154192), (8, 0.012839014096589812), (9, 0.0010468647405437027), (10, -0.03851547106507636), (11, -0.04753301538976407), (12, -0.08623942771941853), (13, -0.014610683905921552), (14, -0.08191528693671123), (15, -0.02987410253535882), (16, -0.04608623312184881), (17, 0.04778994842422652), (18, 0.027426820321613362), (19, 0.008673570226479266), (20, -0.004907237639869926), (21, -0.015208146620311227), (22, -0.02717496697268553), (23, -0.04806367639789188), (24, 0.03375791961065467), (25, -0.03765822041558687), (26, -0.017600225589835296), (27, -0.018855932184835382), (28, -0.00982470539137849), (29, -0.0004924314589995394)]\n",
      "Partition numéro: 3\n",
      "document number  0\n",
      "[(0, 0.06894076321019578), (1, -0.010967085908737009), (2, 0.019208188963434365), (3, -0.020545850632547564), (4, 0.00877869461874207), (5, -0.01630185474732368), (6, 0.0020503647210168606), (7, -0.012097751587352608), (8, 0.011801795993482175), (9, -0.0318436180365467), (10, 0.005877010128132687), (11, -0.04457284422918198), (12, 0.02895897490538549), (13, -0.007897650989471584), (14, -0.0026355328687129127), (15, -0.06724899184749099), (16, 0.011586311896211468), (17, 0.03687538800203487), (18, 0.0002578333470368103), (19, -0.009025549012398661), (20, -0.030869050224475017), (21, -0.013695735612956247), (22, 0.0009846523152628967), (23, 0.07666019131889515), (24, -0.0036950063046078192), (25, -0.007918367887326356), (26, 0.009359609457267628), (27, 0.003684206675690558), (28, 0.041638875742925865), (29, -0.021426825029692384)]\n",
      "document number  1\n",
      "[(0, 0.07179029217877402), (1, -0.03887095622073477), (2, 0.014484111999688406), (3, -0.03555078882581077), (4, 0.005787851514710675), (5, -0.029014310763245482), (6, 0.007056807885046423), (7, -0.024854762509481612), (8, -0.011266148817146139), (9, 0.03895453299112836), (10, -0.04492367734632278), (11, -0.08640394260131246), (12, 0.05015356171169068), (13, 0.045301092980265106), (14, 0.06014262075141352), (15, -0.07925607366022291), (16, -0.05315254287488818), (17, -0.02038913937642785), (18, -0.022580071861583822), (19, 0.020356404633111676), (20, -0.02551656508584379), (21, 0.01853586259076179), (22, -0.12240819172020961), (23, -0.06223816097588169), (24, -0.018386792003624013), (25, -0.08345054165039699), (26, -0.07794559070055619), (27, 0.05946253811166186), (28, 0.028976773230451072), (29, -0.006671819580665496)]\n",
      "document number  2\n",
      "[(0, 0.07129590463174698), (1, -0.0027705357322645066), (2, -0.05712575412900709), (3, -0.010030185185478942), (4, 0.010961650379954687), (5, 0.03116792663756646), (6, -0.035490892110970826), (7, 0.051252046988467075), (8, -0.02900980037821969), (9, -0.04172741245542847), (10, -0.037326867240657105), (11, 0.018443515538209155), (12, 0.022390862938886084), (13, -0.03885240521448884), (14, -0.009468016280332352), (15, 0.009078768048858288), (16, 0.05354379143156372), (17, -0.020784369581407773), (18, -0.04064041851506803), (19, 0.00978342311634892), (20, 0.07029119704544201), (21, 0.03435741170617292), (22, -0.042183688677466774), (23, 0.0817508180013652), (24, -0.0657070752307794), (25, 0.06624286576327883), (26, 0.10170506986520444), (27, -0.03503914896699468), (28, -0.029354297170201892), (29, 0.006446705756866941)]\n"
     ]
    }
   ],
   "source": [
    "num_partition = 0\n",
    "for lsa in partitions_lsa:\n",
    "    print(\"Partition numéro:\",num_partition)\n",
    "    num_partition+=1\n",
    "    i=0\n",
    "    for doc in lsa:\n",
    "        if (i<3):\n",
    "            print(\"document number \", i)\n",
    "            i+=1\n",
    "            print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create ours partitions\n",
    "partitions = []\n",
    "\n",
    "# You must specify a treshold, to know what are the doc you keep, and what are the doc you drop\n",
    "tresh = 0.03\n",
    "\n",
    "for corpus_lsi in partitions_lsa:\n",
    "    # Let's create ours clusters\n",
    "    clusters = []\n",
    "\n",
    "    for i in range(0, nb_concepts):\n",
    "        dic = {}\n",
    "        num_doc = 0\n",
    "        for doc in corpus_lsi:\n",
    "            if abs(doc[i][1]) > tresh:\n",
    "                dic[num_doc] = doc[i][1]\n",
    "            num_doc+=1\n",
    "        clusters.append(dic)\n",
    "    partitions.append(clusters)\n",
    "    \n",
    "# TODO: it would be nice to know how many articles are in no cluster anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: -0.049958924950165406,\n",
       " 2: 0.08744356153210764,\n",
       " 3: -0.03321621232189094,\n",
       " 11: 0.07050055958917299,\n",
       " 13: 0.03422501551162143,\n",
       " 15: 0.03230912163574654,\n",
       " 21: 0.11285664431396818,\n",
       " 27: -0.10550485491711832,\n",
       " 29: 0.044595159264449326,\n",
       " 30: -0.0306190621928106,\n",
       " 35: -0.056128342241752865,\n",
       " 36: 0.0957441200392091,\n",
       " 41: 0.04519098986834995,\n",
       " 44: -0.03445520091494497,\n",
       " 45: 0.04356654060428543,\n",
       " 46: 0.05346084279427925,\n",
       " 51: 0.059784304490782744,\n",
       " 55: 0.03644473963900893,\n",
       " 56: 0.0986950896418553,\n",
       " 60: -0.03162306028701053,\n",
       " 66: -0.032069014959174594,\n",
       " 70: 0.03399078536635222,\n",
       " 80: 0.036483227620393244,\n",
       " 82: -0.04452735144575009,\n",
       " 83: 0.03444596871834685,\n",
       " 84: -0.03627525441889498,\n",
       " 89: -0.048165782102142535,\n",
       " 92: 0.042760666958574084,\n",
       " 93: 0.08176969284388126,\n",
       " 95: 0.04521500397173245,\n",
       " 98: 0.038611903229600925,\n",
       " 99: 0.21473160161328592,\n",
       " 100: -0.04437905694905508,\n",
       " 103: 0.0844815488506724,\n",
       " 104: 0.07832763891585205,\n",
       " 106: -0.055187865477333654,\n",
       " 108: 0.03183604490658634,\n",
       " 109: 0.07442991967020039,\n",
       " 114: 0.054047654754957146,\n",
       " 118: 0.03184597113484077,\n",
       " 119: 0.11606773905612494,\n",
       " 121: 0.030381686476352695,\n",
       " 122: 0.07436935843880442,\n",
       " 127: 0.07626968729009222,\n",
       " 130: 0.033663078564643345,\n",
       " 136: 0.04397541283541928,\n",
       " 141: 0.07656308904122139,\n",
       " 142: -0.06419612513149231,\n",
       " 150: 0.07916826602468285,\n",
       " 152: 0.0357451493436433,\n",
       " 154: -0.03776900608324175,\n",
       " 158: 0.056930890352467754,\n",
       " 166: 0.04347042145832956,\n",
       " 167: 0.04270966529818326,\n",
       " 168: -0.03691699262334722,\n",
       " 169: 0.0340244750045009,\n",
       " 173: 0.04099216082528118,\n",
       " 178: 0.03305034552188388,\n",
       " 182: 0.039683196888734985,\n",
       " 189: -0.03416626553821977,\n",
       " 195: 0.04078517349736215,\n",
       " 196: 0.03714206373482275,\n",
       " 197: -0.03132566841088004,\n",
       " 198: 0.09917288482706736,\n",
       " 205: 0.1155114151117455,\n",
       " 206: 0.031616421752329434,\n",
       " 207: -0.031838507844295844,\n",
       " 208: -0.030534039637207546,\n",
       " 212: -0.038208718761759665,\n",
       " 214: 0.1116130586712106,\n",
       " 215: 0.1065520980403484,\n",
       " 219: 0.13703353364925797,\n",
       " 221: 0.05233419487419184}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display clusters 3 of partition 0 \n",
    "partitions[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_labels_by_cluster = 5\n",
    "\n",
    "# Let's labelize our clusters\n",
    "# For this, we will use the tfidf matrix\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=sourceFST, use_idf=True, ngram_range=(min_gram, max_gram))\n",
    "tfidf = vectorizer.fit_transform(usable)\n",
    "\n",
    "# We can access the value in the tfidf using:\n",
    "#tfidf.toarray().item(num_doc, num_word)\n",
    "# To know the number of the word searched, we will use:\n",
    "#vectorizer.vocabulary_[word]\n",
    "\n",
    "# take less than 8h to compute x)\n",
    "labels = []\n",
    "for clusters in partitions:\n",
    "    l = []\n",
    "    for clus in clusters:\n",
    "        first_arti = True\n",
    "        for article in clus:\n",
    "            link = abs(clus[article])\n",
    "            if first_arti:\n",
    "                coef_list = (tfidf.toarray()[article] * link)\n",
    "                first = False\n",
    "            else:\n",
    "                # the more an article have a high coeficient, the more he is implied in the labeling step\n",
    "                coef_list += (tfidf.toarray()[article] * link)\n",
    "        # Now we have coef_list filled by every coeficient in the multiple tfidf\n",
    "        # Let's find the best ones, to finally get the labels\n",
    "        res = dict(zip(vectorizer.get_feature_names(), coef_list))\n",
    "\n",
    "        l.append(Counter(res).most_common(nb_labels_by_cluster))\n",
    "    labels.append(l)\n",
    "\n",
    "# TODO: on observe beaucoup de labels identiques entre deux clusters\n",
    "# Je pense que c'est parce que l'on a trop de clusters, mais j'aimerais en être sûr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display labels\n",
    "# labels is composed by an array for each partition\n",
    "labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
