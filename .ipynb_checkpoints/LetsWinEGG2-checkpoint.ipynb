{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from numpy import array\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from gensim.test.utils import common_dictionary, common_corpus\n",
    "from gensim.models import LsiModel\n",
    "from gensim import corpora, models, utils\n",
    "from gensim.test.utils import common_corpus, common_dictionary, get_tmpfile\n",
    "from gensim.models import LsiModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spacy lib\n",
    "# On https://spacy.io/\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Parameters #\n",
    "##############\n",
    "\n",
    "min_gram = 1\n",
    "max_gram = 3\n",
    "\n",
    "# To create ours partitions, we must first know the years which will be the limits\n",
    "limit_years = [2007, 2010, 2013, 2016]\n",
    "\n",
    "# Ignore words that appear at a frequency less than max_frequ in the corpus\n",
    "max_frequ = 0.8\n",
    "\n",
    "# Ignore words appearing less than min_appear in the whole corpus\n",
    "min_appear = 5\n",
    "\n",
    "# Range fo cluster number you want to test\n",
    "cluster_ranges = range(20, 114)\n",
    "\n",
    "# Number of trial you want to do for each test\n",
    "nb_trial_by_test = 10\n",
    "\n",
    "# Number of cluster you finally choose\n",
    "nb_cluster = 20\n",
    "\n",
    "# Max iteration for each kmeans (default: 300)\n",
    "max_iter = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datas preprocessing methods.\n",
    "\n",
    "# Lemmatisation without poncutations\n",
    "\n",
    "stemmer = nltk.stem.snowball.FrenchStemmer()\n",
    "fstw = stopwords.words('french')\n",
    "\n",
    "# French Stop Words, extraits depuis le fichier stopwords-fr.txt + stopwords french de nltk\n",
    "sourceFST = [x.replace('\\n', '') for x in open('stopwords-fr.txt', mode=\"r\", encoding=\"utf-8\").readlines()]+fstw\n",
    "sourceFST += [x.replace('\\n', '') for x in open('perso_words-fr.txt', mode=\"r\", encoding=\"utf-8\").readlines()]\n",
    "\n",
    "# Based on ration of french and english stopwords\n",
    "def isEnglish(article):\n",
    "    total_fsw = len([x for x in article.split() if x in sourceFST])\n",
    "    total_esw = len([x for x in article.split() if x in stopwords.words('english')])\n",
    "    ratio = 100\n",
    "    if total_fsw != 0:\n",
    "        ratio = total_esw/total_fsw\n",
    "    return ratio > 1 and total_esw > 3\n",
    "\n",
    "def lemmatize(article):\n",
    "    arti_lower = article.lower()\n",
    "    arti_2words = re.sub(\" [0-z][0-z] \", \" \", arti_lower) # word of length < 2\n",
    "    arti_e = re.sub(\"(é|è|ê)\", \"e\", arti_2words)\n",
    "    arti_o = re.sub(\"à\", \"a\", arti_e)\n",
    "    arti_i = re.sub(\"ô\", \"o\", arti_o)\n",
    "    artiregex = re.sub(\"î\", \"i\", arti_i)\n",
    "    output = []\n",
    "    outPonc = artiregex.translate(artiregex.maketrans(\"\",\"\", string.punctuation))\n",
    "    outLem = nlp(outPonc)\n",
    "    for token in outLem:\n",
    "        if token.lemma_ not in sourceFST and [x for x in token.lemma_ if x not in \"0123456789\"] != []:\n",
    "            output.append(token.lemma_)\n",
    "    res = ' '.join(output)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Reading\n",
    "data = pd.read_csv('export_articles_EGC_2004_2018.csv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's process our corpus, and determine a limit to split it in partitions\n",
    "\n",
    "# usable[] correspond to our corpus processed\n",
    "# limits[] let us know when to delimit partitions\n",
    "limits = []\n",
    "usable = []\n",
    "\n",
    "prev_year = data['year'][0]\n",
    "numArti = 0\n",
    "for i in range(0, len(data['abstract']), 1):\n",
    "    #if not null, empty, or whatever (so if there is a abstract):\n",
    "    if not isinstance(data['abstract'][i], float) and not isEnglish(data['abstract'][i]):\n",
    "        text = data['abstract'][i]\n",
    "        if not isinstance(data['title'][i], float):\n",
    "            text += \" \"+data['title'][i]\n",
    "\n",
    "        numArti+=1\n",
    "        usable.append(re.sub(\" [0-z][0-z] \", \" \", stemmer.stem(lemmatize(text))))\n",
    "        year = data['year'][i]\n",
    "        if year != prev_year:\n",
    "            prev_year = year\n",
    "            if year in limit_years:\n",
    "                limits.append(numArti)\n",
    "limits.append(numArti)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-process word removal\n",
    "post_words = [x.replace('\\n', '') for x in open('post_process_words-fr.txt', mode=\"r\", encoding=\"utf-8\").readlines()]\n",
    "\n",
    "for i in range(0, len(usable)):\n",
    "    arti = usable[i].split()\n",
    "    res = []\n",
    "    for word in arti:\n",
    "        if word not in post_words:\n",
    "            res.append(word)\n",
    "    usable[i] = ' '.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'articles = 991\n",
      "nombre de mots = 2179\n",
      "limits = [114, 301, 468, 694, 991]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'plateforme objectif permettre citoyen euxmemer tweet politique devenement specifiqu francepour lelection presidentiell ideo2017 analyser quasitemps reel message candidat fournir principal caracteristiqueslusage lexiqu politique comparaison candidat ideo2017 plateforme citoyen dediee lanalyse tweet evenement polit'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display pre-processed datas\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=sourceFST, use_idf=True, ngram_range=(min_gram, max_gram), max_df=max_frequ, min_df=min_appear)\n",
    "tfidf = vectorizer.fit_transform(usable)\n",
    "\n",
    "print(\"nombre d'articles =\", len(usable))\n",
    "print(\"nombre de mots =\", len(tfidf.toarray()[0]))\n",
    "print(\"limits =\", limits)\n",
    "\n",
    "usable[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of partitions_tfidf[], which give us the TFIDF of each cluster of each partition\n",
    "# partitions_tfidf[num_partition][num_doc][num_word]\n",
    "# Beware, num_doc can't be equals to 1091 (max). You have partitions, so every doc aren't in every partitions\n",
    "# num_word can be found via vectorizer.get_feature_name()\n",
    "partitions_tfidf = []\n",
    "beg = 0\n",
    "for l in limits:\n",
    "    last = l\n",
    "    partitions_tfidf.append([list(x) for x in list(tfidf.toarray())[beg:last]])\n",
    "    beg = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acce',\n",
       " 'accessible',\n",
       " 'achat',\n",
       " 'acquisition',\n",
       " 'acquérir',\n",
       " 'acteur',\n",
       " 'actif',\n",
       " 'action',\n",
       " 'activite',\n",
       " 'actuel',\n",
       " 'actuellement',\n",
       " 'adapt',\n",
       " 'adaptatif',\n",
       " 'adaptation',\n",
       " 'adapte',\n",
       " 'adaptee',\n",
       " 'adapter',\n",
       " 'adequat',\n",
       " 'adn',\n",
       " 'adopter',\n",
       " 'afc',\n",
       " 'affiner',\n",
       " 'agent',\n",
       " 'agregation',\n",
       " 'aid',\n",
       " 'aider',\n",
       " 'ainsiqu',\n",
       " 'ajoutee',\n",
       " 'ajouter',\n",
       " 'ala',\n",
       " 'aleatoir',\n",
       " 'algebriqu',\n",
       " 'algorithme',\n",
       " 'algorithme dapprentissage',\n",
       " 'algorithme dextraction',\n",
       " 'algorithme efficace',\n",
       " 'algorithme fouiller',\n",
       " 'algorithme incremental',\n",
       " 'algorithmique',\n",
       " 'alignement',\n",
       " 'alternatif',\n",
       " 'amelior',\n",
       " 'amelioration',\n",
       " 'ameliore',\n",
       " 'amelioree',\n",
       " 'ameliorer',\n",
       " 'ameliorer qualite',\n",
       " 'amene',\n",
       " 'amont',\n",
       " 'analys',\n",
       " 'analyse',\n",
       " 'analyser',\n",
       " 'analyser factoriel',\n",
       " 'analyser semantiqu',\n",
       " 'analytique',\n",
       " 'anne',\n",
       " 'annees',\n",
       " 'annot',\n",
       " 'annotation',\n",
       " 'annotation semantiqu',\n",
       " 'annoter',\n",
       " 'anormal',\n",
       " 'anr',\n",
       " 'apartir',\n",
       " 'apparer',\n",
       " 'appartenir',\n",
       " 'appel',\n",
       " 'appele',\n",
       " 'appelee',\n",
       " 'appeler',\n",
       " 'applicable',\n",
       " 'applicatif',\n",
       " 'application',\n",
       " 'application reell',\n",
       " 'appliqu',\n",
       " 'applique',\n",
       " 'appliquee',\n",
       " 'appliquees',\n",
       " 'appliquer',\n",
       " 'apport',\n",
       " 'apporter',\n",
       " 'apprehender',\n",
       " 'apprendre',\n",
       " 'apprentissage',\n",
       " 'apprentissage automatique',\n",
       " 'apprentissage supervis',\n",
       " 'approche',\n",
       " 'approche existant',\n",
       " 'appropriee',\n",
       " 'approprier',\n",
       " 'approximatif',\n",
       " 'approximation',\n",
       " 'arbitraire',\n",
       " 'arborescent',\n",
       " 'arbre',\n",
       " 'arbre decision',\n",
       " 'architecturer',\n",
       " 'article',\n",
       " 'article classification',\n",
       " 'article decrit',\n",
       " 'article etuder',\n",
       " 'article etudie',\n",
       " 'article interesser',\n",
       " 'article introduire',\n",
       " 'article methode',\n",
       " 'article methode original',\n",
       " 'article methodologie',\n",
       " 'article montrer',\n",
       " 'article nouvel',\n",
       " 'article permettre',\n",
       " 'article present',\n",
       " 'article present methode',\n",
       " 'article presenter',\n",
       " 'article presenton',\n",
       " 'article traire',\n",
       " 'artificiel',\n",
       " 'ascendant',\n",
       " 'ascendant hierarchiqu',\n",
       " 'aspect',\n",
       " 'assister',\n",
       " 'associ',\n",
       " 'association',\n",
       " 'associe',\n",
       " 'associee',\n",
       " 'associeer',\n",
       " 'associees',\n",
       " 'associer',\n",
       " 'assurer',\n",
       " 'attaquer',\n",
       " 'atteindre',\n",
       " 'attention',\n",
       " 'attribu',\n",
       " 'attribut',\n",
       " 'atypique',\n",
       " 'augmenter',\n",
       " 'automat',\n",
       " 'automate',\n",
       " 'automatique',\n",
       " 'automatiquement',\n",
       " 'automatiser',\n",
       " 'autoorganisatrice',\n",
       " 'autour',\n",
       " 'avancee',\n",
       " 'avantage',\n",
       " 'axe',\n",
       " 'axer',\n",
       " 'axiome',\n",
       " 'baptis',\n",
       " 'base',\n",
       " 'base donneer',\n",
       " 'base relationnel',\n",
       " 'baser',\n",
       " 'baser dapprentissage',\n",
       " 'baser donneer',\n",
       " 'baser modele',\n",
       " 'baser relationnel',\n",
       " 'baser technique',\n",
       " 'batch',\n",
       " 'bayesien',\n",
       " 'bayesien naïf',\n",
       " 'bayesienn',\n",
       " 'benchmark',\n",
       " 'beneficier',\n",
       " 'besoin',\n",
       " 'biais',\n",
       " 'bibliographique',\n",
       " 'biclustering',\n",
       " 'binaire',\n",
       " 'bioinformatique',\n",
       " 'biologique',\n",
       " 'biomedical',\n",
       " 'bloc',\n",
       " 'block',\n",
       " 'booleenn',\n",
       " 'boosting',\n",
       " 'born',\n",
       " 'bruitees',\n",
       " 'brut',\n",
       " 'cadrer',\n",
       " 'cadrer article',\n",
       " 'cadrer classification',\n",
       " 'cadrer general',\n",
       " 'cadrer lanalyse',\n",
       " 'cadrer projet',\n",
       " 'cadrer rechercher',\n",
       " 'cadrer theoriqu',\n",
       " 'cadrer travail',\n",
       " 'calcul',\n",
       " 'calcul similarite',\n",
       " 'calculee',\n",
       " 'calculer',\n",
       " 'campagne',\n",
       " 'cancer',\n",
       " 'candidat',\n",
       " 'capable',\n",
       " 'capacite',\n",
       " 'capteur',\n",
       " 'capturer',\n",
       " 'caracter',\n",
       " 'caractere',\n",
       " 'caracterisation',\n",
       " 'caracterise',\n",
       " 'caracteriseer',\n",
       " 'caracteriser',\n",
       " 'caracteristiqu',\n",
       " 'caracteristique',\n",
       " 'caracteristiquer',\n",
       " 'cart',\n",
       " 'carte',\n",
       " 'carte autoorganisatrice',\n",
       " 'carte topologique',\n",
       " 'cartographie',\n",
       " 'cartographique',\n",
       " 'categorie',\n",
       " 'categoriel',\n",
       " 'categoriell',\n",
       " 'categorisation',\n",
       " 'causer',\n",
       " 'celer',\n",
       " 'cell',\n",
       " 'cellule',\n",
       " 'centaine',\n",
       " 'central',\n",
       " 'centree',\n",
       " 'centrer',\n",
       " 'cesser',\n",
       " 'cestadir',\n",
       " 'chain',\n",
       " 'chaine',\n",
       " 'chaine markov',\n",
       " 'challenge',\n",
       " 'champ',\n",
       " 'changement',\n",
       " 'charger',\n",
       " 'chemin',\n",
       " 'chercheur',\n",
       " 'choisir',\n",
       " 'choix',\n",
       " 'choix mesurer',\n",
       " 'chronique',\n",
       " 'cibl',\n",
       " 'cibler',\n",
       " 'classe',\n",
       " 'classement',\n",
       " 'classer',\n",
       " 'classificateur',\n",
       " 'classification',\n",
       " 'classification automatique',\n",
       " 'classification document',\n",
       " 'classification hierarchiqu',\n",
       " 'classification multilabel',\n",
       " 'classification supervise',\n",
       " 'classification supervisee',\n",
       " 'classifier',\n",
       " 'classifieur',\n",
       " 'classifieur bayesien',\n",
       " 'classifieur bayesien naïf',\n",
       " 'classique',\n",
       " 'cle',\n",
       " 'clef',\n",
       " 'client',\n",
       " 'cluster',\n",
       " 'clustering',\n",
       " 'coclustering',\n",
       " 'codage',\n",
       " 'coefficient',\n",
       " 'coeur',\n",
       " 'cognitif',\n",
       " 'coherence',\n",
       " 'collaboratif',\n",
       " 'collaboration',\n",
       " 'collecter',\n",
       " 'collectif',\n",
       " 'collection',\n",
       " 'collection document',\n",
       " 'colonne',\n",
       " 'combin',\n",
       " 'combinaison',\n",
       " 'combinatoire',\n",
       " 'combiner',\n",
       " 'commencer',\n",
       " 'commercial',\n",
       " 'commun',\n",
       " 'communaut',\n",
       " 'communautair',\n",
       " 'communautaire',\n",
       " 'communaute',\n",
       " 'communication',\n",
       " 'compact',\n",
       " 'compacter',\n",
       " 'comparaison',\n",
       " 'comparatif',\n",
       " 'comparee',\n",
       " 'comparer',\n",
       " 'comparer resultat',\n",
       " 'compatible',\n",
       " 'competence',\n",
       " 'complementair',\n",
       " 'complementarite',\n",
       " 'complet',\n",
       " 'completer',\n",
       " 'complex',\n",
       " 'complexe',\n",
       " 'complexit',\n",
       " 'complexite',\n",
       " 'comportement',\n",
       " 'comportemer',\n",
       " 'comporter',\n",
       " 'composant',\n",
       " 'compose',\n",
       " 'composer',\n",
       " 'comprehension',\n",
       " 'comprendre',\n",
       " 'compromis',\n",
       " 'compter',\n",
       " 'concentrer',\n",
       " 'concept',\n",
       " 'concept treillis',\n",
       " 'concepteur',\n",
       " 'conception',\n",
       " 'conceptuel',\n",
       " 'concerner',\n",
       " 'concevoir',\n",
       " 'concret',\n",
       " 'condensee',\n",
       " 'condensees',\n",
       " 'condition',\n",
       " 'conditionnel',\n",
       " 'conduire',\n",
       " 'conferenc',\n",
       " 'conference',\n",
       " 'confiance',\n",
       " 'configuration',\n",
       " 'confirmer',\n",
       " 'conflit',\n",
       " 'confronter',\n",
       " 'conjoint',\n",
       " 'conjointement',\n",
       " 'connaissancer',\n",
       " 'connaître',\n",
       " 'connexion',\n",
       " 'connu',\n",
       " 'consequent',\n",
       " 'conserver',\n",
       " 'consider',\n",
       " 'considere',\n",
       " 'consideree',\n",
       " 'considerer',\n",
       " 'consist',\n",
       " 'consister',\n",
       " 'consommation',\n",
       " 'constant',\n",
       " 'constater',\n",
       " 'constituer',\n",
       " 'construction',\n",
       " 'construction automatique',\n",
       " 'construction darbr',\n",
       " 'construction dontologie',\n",
       " 'construir',\n",
       " 'construire',\n",
       " 'construire partir',\n",
       " 'construit',\n",
       " 'construit partir',\n",
       " 'contenir',\n",
       " 'contenir document',\n",
       " 'contenu',\n",
       " 'context',\n",
       " 'contexte',\n",
       " 'contextuel',\n",
       " 'contingence',\n",
       " 'continu',\n",
       " 'continuer',\n",
       " 'contraindre',\n",
       " 'contraint',\n",
       " 'contrainte',\n",
       " 'contrairement',\n",
       " 'contribuer',\n",
       " 'contribution',\n",
       " 'controle',\n",
       " 'controler',\n",
       " 'convergence',\n",
       " 'convier',\n",
       " 'conçu',\n",
       " 'cooperativ',\n",
       " 'corpu',\n",
       " 'corpus',\n",
       " 'correct',\n",
       " 'correctement',\n",
       " 'correlation',\n",
       " 'correspondance',\n",
       " 'correspondance afc',\n",
       " 'correspondant',\n",
       " 'correspondre',\n",
       " 'couleur',\n",
       " 'coupl',\n",
       " 'couple',\n",
       " 'coupler',\n",
       " 'courir',\n",
       " 'courir temps',\n",
       " 'cours',\n",
       " 'court',\n",
       " 'couverture',\n",
       " 'couvrir',\n",
       " 'coût',\n",
       " 'coûteux',\n",
       " 'creation',\n",
       " 'cree',\n",
       " 'creer',\n",
       " 'criter',\n",
       " 'critere',\n",
       " 'critere devaluation',\n",
       " 'croisee',\n",
       " 'croiser',\n",
       " 'croissance',\n",
       " 'croyance',\n",
       " 'croître',\n",
       " 'cube',\n",
       " 'cuber',\n",
       " 'culturel',\n",
       " 'cycle',\n",
       " 'dabord',\n",
       " 'daccelerer',\n",
       " 'dacquisition',\n",
       " 'dadapter',\n",
       " 'dagregation',\n",
       " 'daid',\n",
       " 'daid decision',\n",
       " 'daider',\n",
       " 'dalgorithme',\n",
       " 'dalignemer',\n",
       " 'dameliorer',\n",
       " 'dameliorer performance',\n",
       " 'danalyse',\n",
       " 'danalyser',\n",
       " 'dannotation',\n",
       " 'danscet',\n",
       " 'danscet article',\n",
       " 'dansl',\n",
       " 'dansun',\n",
       " 'dappariemer',\n",
       " 'dapparition',\n",
       " 'dappartenance',\n",
       " 'dapplication',\n",
       " 'dapporter',\n",
       " 'dapprendre',\n",
       " 'dapprentissage',\n",
       " 'dapprentissage automatique',\n",
       " 'dapprentissage supervis',\n",
       " 'darbr',\n",
       " 'darbr decision',\n",
       " 'darticl',\n",
       " 'dassoci',\n",
       " 'dassociation',\n",
       " 'dater',\n",
       " 'dater mining',\n",
       " 'dater stream',\n",
       " 'dattribut',\n",
       " 'dattributs',\n",
       " 'daugmenter',\n",
       " 'dautre',\n",
       " 'dautre partir',\n",
       " 'debut',\n",
       " 'decd',\n",
       " 'decennie',\n",
       " 'dechantillon',\n",
       " 'dechantillonnage',\n",
       " 'decider',\n",
       " 'decideur',\n",
       " 'decis',\n",
       " 'decision',\n",
       " 'decisionnel',\n",
       " 'decisionnell',\n",
       " 'decomposition',\n",
       " 'decoupage',\n",
       " 'decouvert',\n",
       " 'decouverte',\n",
       " 'decouverte motif',\n",
       " 'decouvrir',\n",
       " 'decouvrir motif',\n",
       " 'decrir',\n",
       " 'decrire',\n",
       " 'decrit',\n",
       " 'decrite',\n",
       " 'decrivant',\n",
       " 'decriver',\n",
       " 'decrivons',\n",
       " 'dedie',\n",
       " 'dediee',\n",
       " 'dedition',\n",
       " 'dedonnee',\n",
       " 'deduire',\n",
       " 'defaut',\n",
       " 'deffectuer',\n",
       " 'defi',\n",
       " 'defi egc',\n",
       " 'defini',\n",
       " 'definie',\n",
       " 'definier',\n",
       " 'definir',\n",
       " 'definis',\n",
       " 'definisser',\n",
       " 'definit',\n",
       " 'definition',\n",
       " 'degager',\n",
       " 'degr',\n",
       " 'degre',\n",
       " 'delagage',\n",
       " 'delement',\n",
       " 'demander',\n",
       " 'demarch',\n",
       " 'demonstration',\n",
       " 'denrichir',\n",
       " 'dense',\n",
       " 'densembl',\n",
       " 'densite',\n",
       " 'dentiter',\n",
       " 'dentre',\n",
       " 'dentrepris',\n",
       " 'depart',\n",
       " 'dependanc',\n",
       " 'dependance',\n",
       " 'dependance fonctionnel',\n",
       " 'depender',\n",
       " 'deper',\n",
       " 'dequivalence',\n",
       " 'derive',\n",
       " 'derreur',\n",
       " 'descripteur',\n",
       " 'descriptif',\n",
       " 'description',\n",
       " 'desdonnee',\n",
       " 'desequilibre',\n",
       " 'destimer',\n",
       " 'detablir',\n",
       " 'detailler',\n",
       " 'detat',\n",
       " 'detecter',\n",
       " 'detection',\n",
       " 'detection changement',\n",
       " 'detectiond',\n",
       " 'detendre',\n",
       " 'determination',\n",
       " 'determine',\n",
       " 'determiner',\n",
       " 'detr',\n",
       " 'detre',\n",
       " 'detude',\n",
       " 'detudier',\n",
       " 'deuxieme',\n",
       " 'devaluation',\n",
       " 'devaluer',\n",
       " 'devaluer qualite',\n",
       " 'developp',\n",
       " 'developpe',\n",
       " 'developpee',\n",
       " 'developpement',\n",
       " 'developpemer',\n",
       " 'developper',\n",
       " 'devenement',\n",
       " 'devenir',\n",
       " 'devolution',\n",
       " 'dexecution',\n",
       " 'dexempl',\n",
       " 'dexperience',\n",
       " 'dexperimentation',\n",
       " 'dexploiter',\n",
       " 'dexploration',\n",
       " 'dexplorer',\n",
       " 'dexpression',\n",
       " 'dexprimer',\n",
       " 'dextraction',\n",
       " 'dextraction motif',\n",
       " 'dextraction partir',\n",
       " 'dextrair',\n",
       " 'dextraire',\n",
       " 'dextraire motif',\n",
       " 'dheuristiqu',\n",
       " 'diagnostic',\n",
       " 'dictionnair',\n",
       " 'didentification',\n",
       " 'didentifier',\n",
       " 'difference',\n",
       " 'differenter',\n",
       " 'difficile',\n",
       " 'difficilement',\n",
       " 'difficult',\n",
       " 'difficulte',\n",
       " 'diffusion',\n",
       " 'dimag',\n",
       " 'dimage',\n",
       " 'dimager',\n",
       " 'dimension',\n",
       " 'dincertitude',\n",
       " 'dindexation',\n",
       " 'dindice',\n",
       " 'dindividus',\n",
       " 'dinduction',\n",
       " 'dinference',\n",
       " 'dinferer',\n",
       " 'dinfluence',\n",
       " 'dinform',\n",
       " 'dinformation',\n",
       " 'dintegration',\n",
       " 'dintegrer',\n",
       " 'dinteraction',\n",
       " 'dinteret',\n",
       " 'dinternet',\n",
       " 'dinterpretation',\n",
       " 'dinterrogation',\n",
       " 'dintroduir',\n",
       " 'direct',\n",
       " 'direction',\n",
       " 'discret',\n",
       " 'discretisation',\n",
       " 'discriminant',\n",
       " 'discriminanter',\n",
       " 'discrimination',\n",
       " 'discriminer',\n",
       " 'discussion',\n",
       " 'discuter',\n",
       " 'disponible',\n",
       " 'disposer',\n",
       " 'dispositif',\n",
       " 'disposition',\n",
       " 'dissimilarit',\n",
       " 'dissimilarite',\n",
       " 'distance',\n",
       " 'distancer',\n",
       " 'distinct',\n",
       " 'distinguer',\n",
       " 'distribu',\n",
       " 'distribuer',\n",
       " 'distribution',\n",
       " 'ditems',\n",
       " 'ditemset',\n",
       " 'ditemset frequent',\n",
       " 'diversite',\n",
       " 'diviser',\n",
       " 'dizaine',\n",
       " 'dobjet',\n",
       " 'dobjets',\n",
       " 'dobtenir',\n",
       " 'docu',\n",
       " 'document',\n",
       " 'document textuel',\n",
       " 'document xml',\n",
       " 'documentaire',\n",
       " 'doffr',\n",
       " 'domaine',\n",
       " 'domaine dapplication',\n",
       " 'domaine fouiller',\n",
       " 'domaine lapprentissage',\n",
       " 'donneer',\n",
       " 'donneer article',\n",
       " 'donneer luci',\n",
       " 'donneer reell',\n",
       " 'donneer textuel',\n",
       " 'donnees',\n",
       " 'donner',\n",
       " 'donner resultat',\n",
       " 'dontologie',\n",
       " 'dontologie partir',\n",
       " 'dontologier',\n",
       " 'dopinion',\n",
       " 'doptimisation',\n",
       " 'dordre',\n",
       " 'dorigine',\n",
       " 'doter',\n",
       " 'doutil',\n",
       " 'doutils',\n",
       " 'duree',\n",
       " 'durer',\n",
       " 'durer processus',\n",
       " 'dusage',\n",
       " 'dutilisateur',\n",
       " 'dutilisation',\n",
       " 'dutiliser',\n",
       " 'dynam',\n",
       " 'dynamique',\n",
       " 'ecd',\n",
       " 'echantillon',\n",
       " 'echantillonnage',\n",
       " 'echell',\n",
       " 'echelle',\n",
       " 'eclairage',\n",
       " 'economiqu',\n",
       " 'effectu',\n",
       " 'efficace',\n",
       " 'efficacement',\n",
       " 'efficacite',\n",
       " 'effort',\n",
       " 'egc',\n",
       " 'element',\n",
       " 'elev',\n",
       " 'eleve',\n",
       " 'elevee',\n",
       " 'emergent',\n",
       " 'empirique',\n",
       " 'empiriquement',\n",
       " 'employer',\n",
       " 'encompte',\n",
       " 'encourageant',\n",
       " 'encourageant prometteur',\n",
       " 'engendrer',\n",
       " 'enjeu',\n",
       " 'enjeu majeur',\n",
       " 'enregistrement',\n",
       " 'enrichir',\n",
       " 'enrichissement',\n",
       " 'ensembl',\n",
       " 'ensembl donneer',\n",
       " 'ensembl mesure',\n",
       " 'ensemble',\n",
       " 'ensemble donneer',\n",
       " 'ensemble regl',\n",
       " 'ensembliste',\n",
       " 'entite',\n",
       " 'entrainer',\n",
       " 'entrepot',\n",
       " 'entreprendre',\n",
       " 'entreprise',\n",
       " 'environnement',\n",
       " 'envisageable',\n",
       " 'equivalent',\n",
       " 'erreur',\n",
       " 'espac',\n",
       " 'espace',\n",
       " 'espacer',\n",
       " 'essentiel',\n",
       " 'essentiellement',\n",
       " 'estim',\n",
       " 'estimateur',\n",
       " 'estimation',\n",
       " 'estimation densite',\n",
       " 'estimee',\n",
       " 'estimer',\n",
       " 'etablie',\n",
       " 'etablir',\n",
       " 'etablisser',\n",
       " 'etape',\n",
       " 'etat',\n",
       " 'etde',\n",
       " 'etendu',\n",
       " 'eter',\n",
       " 'etiquetage',\n",
       " 'etiquete',\n",
       " 'etl',\n",
       " 'etle',\n",
       " 'etroitement',\n",
       " 'etude',\n",
       " 'etude comparatif',\n",
       " 'etude experimental',\n",
       " 'etuder',\n",
       " 'etudie',\n",
       " 'etudiee',\n",
       " 'etudier',\n",
       " 'euclidien',\n",
       " 'evalu',\n",
       " 'evaluation',\n",
       " 'evalue',\n",
       " 'evaluee',\n",
       " 'evaluees',\n",
       " 'evaluer',\n",
       " 'evenement',\n",
       " 'evidence',\n",
       " 'eviter',\n",
       " 'evoluer',\n",
       " 'evolution',\n",
       " 'evolutiv',\n",
       " 'exact',\n",
       " 'examiner',\n",
       " 'exemple',\n",
       " 'exhaustif',\n",
       " 'exhiber',\n",
       " 'existant',\n",
       " 'exister',\n",
       " 'experience',\n",
       " 'experience meneer',\n",
       " 'experience montrer',\n",
       " 'experiment',\n",
       " 'experimental',\n",
       " 'experimentalement',\n",
       " 'experimentation',\n",
       " 'experimentation jeu',\n",
       " 'experimentation menee',\n",
       " 'experimentation montrer',\n",
       " 'experimentaux',\n",
       " 'experimentaux montrer',\n",
       " 'expert',\n",
       " 'expert domaine',\n",
       " 'explicatif',\n",
       " 'expliciter',\n",
       " 'expliquer',\n",
       " 'exploitable',\n",
       " 'exploitation',\n",
       " 'exploiter',\n",
       " 'exploration',\n",
       " 'exploratoire',\n",
       " 'explorer',\n",
       " 'exponentiel',\n",
       " 'exposer',\n",
       " 'expression',\n",
       " 'exprimeer',\n",
       " 'exprimer',\n",
       " 'extension',\n",
       " 'extraction',\n",
       " 'extraction motif',\n",
       " 'extraction regl',\n",
       " 'extraire',\n",
       " 'extraire motif',\n",
       " 'extrait',\n",
       " 'extraite',\n",
       " 'face',\n",
       " 'facile',\n",
       " 'facilement',\n",
       " 'faciliter',\n",
       " 'facteur',\n",
       " 'factoriel',\n",
       " 'factoriel correspondance',\n",
       " 'factoriel correspondance afc',\n",
       " 'factorisation',\n",
       " 'faible',\n",
       " 'faire',\n",
       " 'faire appel',\n",
       " 'faire face',\n",
       " 'faire lobjet',\n",
       " 'faire partir',\n",
       " 'faisabilite',\n",
       " 'famille',\n",
       " 'favoriser',\n",
       " 'feature',\n",
       " 'ferme',\n",
       " 'ferme frequent',\n",
       " 'fiabilite',\n",
       " 'fiable',\n",
       " 'fichier',\n",
       " 'filtrage',\n",
       " 'filtrer',\n",
       " 'fin',\n",
       " 'final',\n",
       " 'finalement',\n",
       " 'financier',\n",
       " 'fixe',\n",
       " 'fixer',\n",
       " 'flexible',\n",
       " 'flot',\n",
       " 'flot donneer',\n",
       " 'flou',\n",
       " 'flouer',\n",
       " 'flux',\n",
       " 'flux donneer',\n",
       " 'focaliser',\n",
       " 'fonction',\n",
       " 'fonction croyance',\n",
       " 'fonctionnalite',\n",
       " 'fonctionnel',\n",
       " 'fonctionnement',\n",
       " 'fonctionner',\n",
       " 'fond',\n",
       " 'fondamental',\n",
       " 'fondee',\n",
       " 'fondre',\n",
       " 'foret',\n",
       " 'foret aleatoir',\n",
       " 'formalisation',\n",
       " 'formaliser',\n",
       " 'formalisme',\n",
       " 'format',\n",
       " 'formation',\n",
       " 'forme',\n",
       " 'formel',\n",
       " 'formel concept',\n",
       " 'former',\n",
       " 'former regl',\n",
       " 'formulation',\n",
       " 'formuler',\n",
       " 'fort',\n",
       " 'fortement',\n",
       " 'fouill',\n",
       " 'fouill donneer',\n",
       " 'fouiller',\n",
       " 'fouiller donneer',\n",
       " 'fouiller grand',\n",
       " 'fouiller texter',\n",
       " 'fournir',\n",
       " 'framework',\n",
       " 'france',\n",
       " 'français',\n",
       " 'frequence',\n",
       " 'frequent',\n",
       " 'frequente',\n",
       " 'fusion',\n",
       " 'futur',\n",
       " 'gain',\n",
       " 'galoi',\n",
       " 'garantir',\n",
       " 'gene',\n",
       " 'gener',\n",
       " 'general',\n",
       " 'generalement',\n",
       " 'generalisation',\n",
       " 'generalise',\n",
       " 'generaliser',\n",
       " 'generalist',\n",
       " 'generateur',\n",
       " 'generatif',\n",
       " 'generation',\n",
       " 'genere',\n",
       " 'generee',\n",
       " 'generees',\n",
       " 'generer',\n",
       " 'generiqu',\n",
       " 'genetiqu',\n",
       " 'genre',\n",
       " 'geographiqu',\n",
       " 'gerer',\n",
       " 'gestion',\n",
       " 'gestion flux',\n",
       " 'gestion flux donneer',\n",
       " 'global',\n",
       " 'graduel',\n",
       " 'grammaire',\n",
       " 'grand',\n",
       " 'grand base',\n",
       " 'grand base donneer',\n",
       " 'grand echelle',\n",
       " 'grand ensembl',\n",
       " 'grand graphe',\n",
       " 'grand nombre',\n",
       " 'grand quantite',\n",
       " 'grand tailler',\n",
       " 'grand volume',\n",
       " 'granularite',\n",
       " 'graph',\n",
       " 'graph voisinage',\n",
       " 'graphe',\n",
       " 'graphe conceptuel',\n",
       " 'graphe voisinage',\n",
       " 'grapher',\n",
       " 'graphique',\n",
       " 'griller',\n",
       " 'gros',\n",
       " 'groupe',\n",
       " 'groupement',\n",
       " 'grouper',\n",
       " 'grâce',\n",
       " 'grâce algorithme',\n",
       " 'guid',\n",
       " 'guidee',\n",
       " 'guider',\n",
       " 'habituellement',\n",
       " 'heterogen',\n",
       " 'heterogener',\n",
       " 'heuristique',\n",
       " 'hierarchi',\n",
       " 'hierarchie',\n",
       " 'hierarchiqu',\n",
       " 'historique',\n",
       " 'homogen',\n",
       " 'humain',\n",
       " 'hybride',\n",
       " 'hypothese',\n",
       " 'idee',\n",
       " 'identification',\n",
       " 'identifier',\n",
       " 'ignorer',\n",
       " 'iii',\n",
       " 'illustree',\n",
       " 'illustrer',\n",
       " 'image',\n",
       " 'imager',\n",
       " 'impact',\n",
       " 'implement',\n",
       " 'implementation',\n",
       " 'implemente',\n",
       " 'implicatif',\n",
       " 'implicitement',\n",
       " 'impliqu',\n",
       " 'impliquer',\n",
       " 'importance',\n",
       " 'important',\n",
       " 'importer',\n",
       " 'imposer',\n",
       " 'impossible',\n",
       " 'inclure',\n",
       " 'inconvenient',\n",
       " 'incremental',\n",
       " 'independamment',\n",
       " 'independant',\n",
       " 'index',\n",
       " 'indicateur',\n",
       " 'indice',\n",
       " 'indiquer',\n",
       " 'indispensable',\n",
       " 'individu',\n",
       " 'individuel',\n",
       " 'inductif',\n",
       " 'induire',\n",
       " 'induit',\n",
       " 'industriel',\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans & Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying KMeans on tfidf\n",
    "# the labels_ give assignment of doc to the cluster number \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_clustering is a dictionnary \n",
    "# it looks like -> { doc_number : [partition_number, cluster_number] }\n",
    "# This is used to reassign doc number to their respective partition and and cluster\n",
    "\n",
    "def kmeans(nb_clusters):\n",
    "    doc_clustering = {}\n",
    "    \n",
    "    km = KMeans(n_clusters=nb_clusters, max_iter=max_iter)\n",
    "\n",
    "    # Silhouette score mean\n",
    "    silhouette_mean = 0\n",
    "\n",
    "    numDoc = 0\n",
    "    for i in range(0, len(limits)):\n",
    "        dash = km.fit(partitions_tfidf[i])\n",
    "\n",
    "        # Silhouette\n",
    "        silhouette_mean += silhouette_score(partitions_tfidf[i], dash.labels_)\n",
    "\n",
    "        previousBound = 0\n",
    "        if i > 0:\n",
    "            previousBound = limits[i-1]\n",
    "        for numDocItern in range(0, limits[i]-previousBound):\n",
    "            doc_clustering[numDoc] = [i, dash.labels_[numDocItern]]\n",
    "            numDoc+=1\n",
    "\n",
    "    silhouette_mean = silhouette_mean / len(limits)\n",
    "    res = {}\n",
    "    res[\"silhouette\"] = silhouette_mean\n",
    "    res[\"clustering\"] = doc_clustering\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing for 100 clusters...\n",
      "Computing for 101 clusters...\n",
      "Computing for 102 clusters...\n",
      "Computing for 103 clusters...\n",
      "Computing for 104 clusters...\n",
      "Computing for 105 clusters...\n",
      "Computing for 106 clusters...\n",
      "Computing for 107 clusters...\n",
      "Computing for 108 clusters...\n",
      "Computing for 109 clusters...\n",
      "Computing for 110 clusters...\n",
      "Computing for 111 clusters...\n",
      "Computing for 112 clusters...\n",
      "Computing for 113 clusters...\n"
     ]
    }
   ],
   "source": [
    "# Compute Silhouette Score for each number of cluster\n",
    "\n",
    "silhouette_by_cluster_nb = {}\n",
    "\n",
    "for nbClusters in cluster_ranges:\n",
    "    print(\"Computing for\", nbClusters, \"clusters...\")\n",
    "    silhouette_avg = 0\n",
    "    for trial in range(0, nb_trial_by_test):\n",
    "        km = kmeans(nbClusters)\n",
    "        silhouette_avg += km[\"silhouette\"]\n",
    "    silhouette_avg = silhouette_avg / nb_trial_by_test\n",
    "    silhouette_by_cluster_nb[nbClusters] = silhouette_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{100: 0.036557359276791525,\n",
       " 101: 0.03629278409992143,\n",
       " 102: 0.035843330147088424,\n",
       " 103: 0.035373240939817224,\n",
       " 104: 0.035125518215555394,\n",
       " 105: 0.03438170976646685,\n",
       " 106: 0.0345749372907675,\n",
       " 107: 0.03462626744026736,\n",
       " 108: 0.03512047932538543,\n",
       " 109: 0.034223712282743075,\n",
       " 110: 0.03357531726203595,\n",
       " 111: 0.03325973554656931,\n",
       " 112: 0.03144209209710567,\n",
       " 113: 0.031394276760518695}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want silhouette scores to be high\n",
    "silhouette_by_cluster_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clustering = kmeans(nb_cluster)[\"clustering\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows to get list of documents number\n",
    "# return [dou numbers]\n",
    "# params : partition_number , cluster number\n",
    "def get_doc(part, clust):\n",
    "    docs = []\n",
    "    for i in range(0,len(doc_clustering)):\n",
    "        if doc_clustering[i][0] == part and doc_clustering[i][1] == clust:\n",
    "            docs.append(i)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the partitions variable\n",
    "# Here partitions[part][cluster] = list of docs numbe\n",
    "partitions = []\n",
    "for i in range(0, len(limits)):\n",
    "    clusters = []\n",
    "    for j in range(0, nb_clusters):\n",
    "        clusters.append(get_doc(i,j))\n",
    "    partitions.append(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Khi²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_of_your_word = tf[numDoc][strWord]\n",
    "tf = []\n",
    "for doc in usable:\n",
    "    tf_doc = {}\n",
    "    for word in vectorizer.get_feature_names():\n",
    "        tf_doc[word] = doc.count(word)\n",
    "    tf.append(tf_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number total of words\n",
    "# nb_total_word[numPartition]\n",
    "nb_total_word = []\n",
    "nb = 0\n",
    "\n",
    "for numDoc in range(0, len(usable)):\n",
    "    for word in vectorizer.get_feature_names():\n",
    "        nb += tf[numDoc][word]\n",
    "    if numDoc+1 in limits:\n",
    "        nb_total_word.append(nb)\n",
    "        nb=0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_total_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_word[num_partition][word]\n",
    "nb_word = []\n",
    "\n",
    "word_in_this_parti = {}\n",
    "for word in vectorizer.get_feature_names():\n",
    "    word_in_this_parti[word] = 0\n",
    "\n",
    "for numDoc in range(0, len(usable)):\n",
    "    for word in vectorizer.get_feature_names():\n",
    "        word_in_this_parti[word] += tf[numDoc][word]\n",
    "    if numDoc+1 in limits:\n",
    "        nb_word.append(word_in_this_parti)\n",
    "        word_in_this_parti = {}\n",
    "        for word in vectorizer.get_feature_names():\n",
    "            word_in_this_parti[word] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nb_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_word_by_cluster[numPartition][numCluster]\n",
    "nb_word_by_cluster = []\n",
    "for parti in partitions:\n",
    "    nb_word_clus = []\n",
    "    for cluster in parti:\n",
    "        nb = 0\n",
    "        for numDoc in cluster:\n",
    "            for word in vectorizer.get_feature_names():\n",
    "                nb += tf[numDoc][word]\n",
    "        nb_word_clus.append(nb)\n",
    "    nb_word_by_cluster.append(nb_word_clus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_of_khi2 = khi2[numPartition][numCluster][word]\n",
    "khi2 = []\n",
    "\n",
    "for numParti in range(0, len(partitions)):\n",
    "    khi2parti = []\n",
    "    for numCluster in range(0, len(partitions[numParti])):\n",
    "        khi2cluster = {}\n",
    "        \n",
    "        for word in vectorizer.get_feature_names():\n",
    "            if nb_word_by_cluster[numParti][numCluster] == 0:\n",
    "                khi2cluster[word] = 0\n",
    "            else:\n",
    "                word_in_this_parti[word] = 0\n",
    "                E = nb_word[numParti][word]\n",
    "                E =+ nb_word_by_cluster[numParti][numCluster]\n",
    "                E = E/ nb_total_word[numParti]\n",
    "                N = 0\n",
    "                for numDoc in partitions[numParti][numCluster]:\n",
    "                    N += tf[numDoc][word]\n",
    "                khi2cluster[word] = (pow(N - E, 2)/E)        \n",
    "        khi2parti.append(khi2cluster)\n",
    "    khi2.append(khi2parti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of your labels = labels[numPartition][numCluster]\n",
    "labels = []\n",
    "\n",
    "for numPartition in range(0, len(nb_word_by_cluster)):\n",
    "    label_clus = []\n",
    "    for numCluster in range(0, len(nb_word_by_cluster[numPartition])):\n",
    "        label_clus.append(Counter(khi2[numPartition][numCluster]).most_common(5))\n",
    "    labels.append(label_clus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some clusters can be empty\n",
    "len(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diachronic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inter(listA, listB):\n",
    "    return np.intersect1d(listA, listB)\n",
    "    \n",
    "# cluster_t and cluster_s must be in two different partitions\n",
    "def proba(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S):\n",
    "    total_inter = 0\n",
    "    total_t = 0\n",
    "    for f in range(0, len(labels[num_partition_T][num_cluster_t])):\n",
    "        for f_s in labels[num_partition_S][num_cluster_s]:\n",
    "            if labels[num_partition_T][num_cluster_t][f][0] == f_s[0]:\n",
    "                total_inter += labels[num_partition_T][num_cluster_t][f][1]\n",
    "                break\n",
    "        total_t += labels[num_partition_T][num_cluster_t][f][1]\n",
    "    if total_t == 0:\n",
    "        return 0\n",
    "    return total_inter / total_t\n",
    "    \n",
    "\n",
    "def P_A(num_cluster_s, num_partition_T, num_partition_S):\n",
    "    # first, we have to know what are the cluster which got the label\n",
    "    total = 0\n",
    "    nb_computation = 0\n",
    "    for label_s in labels[num_partition_S][num_cluster_s]:\n",
    "        for num_cluster_t in range(0, len(partitions[num_partition_T])):\n",
    "            if label_s in labels[num_partition_T][num_cluster_t]:\n",
    "                total += proba(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S)\n",
    "                nb_computation += 1\n",
    "    if nb_computation == 0:\n",
    "        return 0\n",
    "    return total / nb_computation\n",
    "\n",
    "# Define a coeficient for the activity \n",
    "def activity(num_partition_S, num_partition_T):\n",
    "    res = 0\n",
    "    for num_cluster_s in range(0, len(partitions[num_partition_S])):\n",
    "        res += P_A(num_cluster_s, num_partition_T, num_partition_S)\n",
    "    return res / len(partitions[num_partition_S])\n",
    "\n",
    "# Ecart-type, but it isn't very usefull xD\n",
    "sigma_t = 0.01\n",
    "sigma_s = 0.01\n",
    "\n",
    "# Our Graal\n",
    "def similar(num_cluster_t, num_partition_T, num_cluster_s, num_partition_S):\n",
    "    cond1 = proba(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S) > P_A(num_cluster_s, num_partition_T, num_partition_S)\n",
    "    cond2 = proba(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S) > activity(num_partition_S, num_partition_T) + sigma_s\n",
    "    \n",
    "    cond3 = proba(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S) > P_A(num_cluster_s, num_partition_T, num_partition_S)\n",
    "    cond4 = proba(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S) > activity(num_partition_T, num_partition_S) + sigma_t\n",
    "    return cond1 and cond2 and cond3 and cond4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for numParti in range(0, len(partitions)-1):\n",
    "    for num_cluster_t in range(0, nb_cluster):\n",
    "        for num_cluster_s in range(0, nb_cluster):\n",
    "            if similar(num_cluster_t, numParti, num_cluster_s, numParti+1):\n",
    "                print(\"(\"+str(num_cluster_t)+\",\"+str(numParti)+\") est similaire à (\"+str(num_cluster_s)+\",\"+str(numParti+1)+\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[0][1])\n",
    "labels[0][1][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.0000000e+00, -5.0000000e-01],\n",
       "        [ 1.0000000e+00,  0.0000000e+00],\n",
       "        [-1.0000000e+00,  1.2246468e-16],\n",
       "        [ 2.0000000e+00,  5.0000000e-01],\n",
       "        [ 0.0000000e+00,  5.0000000e-01]]), array([1, 0, 0, 1, 1]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
