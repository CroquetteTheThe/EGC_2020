{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from numpy import array\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from gensim.test.utils import common_dictionary, common_corpus\n",
    "from gensim.models import LsiModel\n",
    "from gensim import corpora, models, utils\n",
    "from gensim.test.utils import common_corpus, common_dictionary, get_tmpfile\n",
    "from gensim.models import LsiModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spacy lib\n",
    "# On https://spacy.io/\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datas preprocessing methods.\n",
    "# Lemmatisation without poncutations\n",
    "\n",
    "stemmer = nltk.stem.snowball.FrenchStemmer()\n",
    "fstw = stopwords.words('french')\n",
    "\n",
    "# French Stop Words, extraits depuis le fichier stopwords-fr.txt + stopwords french de nltk\n",
    "sourceFST = [x.replace('\\n', '') for x in open('stopwords-fr.txt', mode=\"r\", encoding=\"utf-8\").readlines()]+fstw\n",
    "\n",
    "def lemmatize(article):\n",
    "    output = []\n",
    "    outPonc = article.translate(article.maketrans(\"\",\"\", string.punctuation))\n",
    "    outLem = nlp(outPonc)\n",
    "    for token in outLem:\n",
    "        if token.lemma_ not in sourceFST:\n",
    "            output.append(token.lemma_)\n",
    "    res = ' '.join(output)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Reading\n",
    "data = pd.read_csv('export_articles_EGC_2004_2018.csv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's process our corpus, and determine a limit to split it in partitions\n",
    "\n",
    "# usable[] correspond to our corpus processed\n",
    "# limits[] let us know when to delimit partitions\n",
    "limits = []\n",
    "usable = []\n",
    "\n",
    "# To create ours delimiters, we must first know the years which will be the limits\n",
    "limit_years = [2007, 2010, 2014]\n",
    "\n",
    "prev_year = data['year'][0]\n",
    "numArti = 0\n",
    "for i in range(0, len(data['abstract']), 1):\n",
    "    if not isinstance(data['abstract'][i], float): #if not null, empty, or whatever (so if there is a abstract)\n",
    "        year = data['year'][i]\n",
    "        if year != prev_year:\n",
    "            prev_year = year\n",
    "            if year in limit_years:\n",
    "                limits.append(numArti)\n",
    "        numArti+=1\n",
    "        usable.append(stemmer.stem(lemmatize(data['abstract'][i])))\n",
    "limits.append(numArti)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre d'articles = 1096\n",
      "limits = [267, 543, 790, 1096]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'plateforme objectif permettre citoyen danalyserpar euxmême tweet politique dévénement spécifique francepour cas lélection présidentiel 2017 idéo2017 analyser quasitemp réel message candidat fournir principal caractéristiqueslusage lexiqu politique comparaison entrer candidat'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display pre-processed datas\n",
    "print(\"nombre d'articles =\", len(usable))\n",
    "print(\"limits =\", limits)\n",
    "\n",
    "usable[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params\n",
    "nb_concepts = 30\n",
    "min_gram = 1\n",
    "max_gram = 3\n",
    "\n",
    "# Creation of cleandocs, which is usable[] with ngrams\n",
    "cleandocs = []\n",
    "for t in usable:\n",
    "    doc = []\n",
    "    for n in range(min_gram, max_gram+1):\n",
    "        for gram in ngrams(t.split(), n):\n",
    "            doc.append(\" \".join(gram))\n",
    "    cleandocs.append(doc)\n",
    "\n",
    "# Creation of tfidf model, a tool to create ours tfidf\n",
    "corpus = []\n",
    "dictionary = corpora.Dictionary(cleandocs)\n",
    "for doc in cleandocs:\n",
    "    newVec = dictionary.doc2bow(doc)\n",
    "    corpus.append(newVec)\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "# Creation of partitions_lsa[], which give us the LSA of each partition\n",
    "partitions_lsa = []\n",
    "beg = 0\n",
    "for l in limits:\n",
    "    last = l\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    lsi = models.LsiModel(corpus_tfidf, num_topics=nb_concepts, id2word=dictionary)\n",
    "    corpus_lsi = lsi[corpus_tfidf[beg:last]]\n",
    "    partitions_lsa.append(corpus_lsi)\n",
    "    beg = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition numéro: 0\n",
      "document number  0\n",
      "[(0, -0.0024738714432981676), (1, 0.025725858959848486), (2, 0.010897726235135094), (3, 0.002088915119678054), (4, -0.005944901364661566), (5, 0.007568059563352835), (6, 0.008793288625785241), (7, 0.0031626869060070945), (8, 0.047708279742918004), (9, -0.027463326654428303), (10, 0.02620071902818843), (11, 0.0009731866249140437), (12, -0.04105947650342904), (13, 0.010784925393505836), (14, -0.03636026730466142), (15, -0.011689967761567499), (16, 0.013060364670370908), (17, -0.03557253576370232), (18, -0.03137421860215876), (19, -0.03637000443815013), (20, -0.02230409015631147), (21, 0.006799142267632275), (22, 0.03076702211417731), (23, -0.005632811842326894), (24, -0.005758752464520116), (25, -0.04667623602450392), (26, 0.02370002234286569), (27, 0.011377821251532841), (28, -0.02304024343921165), (29, 0.026354590088884567)]\n",
      "document number  1\n",
      "[(0, -0.011798800815419088), (1, 0.09810500889688491), (2, 0.039856291176258773), (3, 0.09294078473146268), (4, -0.04242514244395343), (5, 0.008918347754533394), (6, -0.08215918844086788), (7, 0.0011575525649740797), (8, 0.008367227479417195), (9, 0.019950459159537835), (10, 0.021742899020011586), (11, -0.013447066257230035), (12, -0.12817828120080502), (13, 0.06426831581639089), (14, -0.018023857025604443), (15, 0.02396390039123999), (16, 0.01854692025449098), (17, -0.00846375751162013), (18, 0.05996129613743984), (19, 0.10359059712417239), (20, -0.0261335917439761), (21, 0.03414038829365783), (22, -0.007446311059720647), (23, 0.05259558449847082), (24, -0.016763565569517535), (25, -0.04176041729837368), (26, -0.0273702409972692), (27, -0.04358660547310487), (28, -0.016960949037646667), (29, -0.0012129030220662074)]\n",
      "document number  2\n",
      "[(0, -0.010149119590112377), (1, 0.06664921715329752), (2, 0.02113877652148029), (3, -0.020192894181721655), (4, -0.035928291862056545), (5, 0.06431049852515475), (6, -0.012562395157314967), (7, 0.08046310136603359), (8, 0.0020209175214926725), (9, -0.04956220608712084), (10, 0.03998295459179287), (11, 0.030844057418487024), (12, 0.06369713752017689), (13, 0.09933535311762476), (14, 0.008433879100574889), (15, 0.031184729312477014), (16, -0.04953590641473934), (17, -0.028736038270280675), (18, 0.0022733769850542106), (19, 0.013786682826555095), (20, 0.031252617850681064), (21, 0.011331506864138623), (22, -0.029309468206299386), (23, -0.043413057185573506), (24, 0.0008981795667758586), (25, 0.029224343761317488), (26, -0.03353266342021686), (27, 0.010309741330139456), (28, 0.06632210488378022), (29, 0.017556015399975318)]\n",
      "Partition numéro: 1\n",
      "document number  0\n",
      "[(0, -0.005227054179681984), (1, 0.04170393753908848), (2, -0.008877621180013525), (3, 0.01508540740225567), (4, 0.03145310532866909), (5, -0.03330606081571231), (6, 0.016644197686399537), (7, -0.0049233684531807785), (8, -0.004933592873933248), (9, -0.020046568356513663), (10, 0.05843375186798835), (11, 0.021828199372327453), (12, -0.03209701837837643), (13, -0.05321913764113509), (14, 0.011050295542742533), (15, -0.019543120396155623), (16, -0.05289717397514863), (17, -0.03717986258768315), (18, -0.055550648101469896), (19, 0.006171711661411015), (20, -0.02841928300516109), (21, 0.0029144539597529794), (22, 0.026667552272575945), (23, 0.021571041890997605), (24, -0.051310746767614955), (25, -0.04878745709315877), (26, 0.012215709668504783), (27, -0.039468188033620205), (28, -0.05692383528159578), (29, 0.014230756221449167)]\n",
      "document number  1\n",
      "[(0, -0.006832319723358891), (1, 0.0607405461452109), (2, 0.0039308319382931976), (3, 0.004724464465267251), (4, 0.010730786417870387), (5, -0.026261402183095982), (6, -0.023900564059419167), (7, 0.008125196736006075), (8, -0.04257233246782086), (9, 0.008815821397420032), (10, 0.02170279862301654), (11, -0.02330209477927728), (12, -0.008790330828955379), (13, -0.0299126429282417), (14, 0.0013720427075635975), (15, 0.0015697794795149617), (16, -0.004353225455627755), (17, -0.045909280080504516), (18, -0.07832461169320187), (19, 0.013298631566254505), (20, 0.018851218780079334), (21, 0.03803169744881444), (22, 0.017638539953361183), (23, 0.01504732013862009), (24, 0.0025371043367661576), (25, -0.019521096313769193), (26, -0.006518554892876205), (27, 0.027226685362902146), (28, -0.025066257609528787), (29, 0.012654023366458872)]\n",
      "document number  2\n",
      "[(0, -0.0038263949672169236), (1, 0.049134635439082364), (2, -0.015640189697234273), (3, 0.04637593867242039), (4, 0.014853156964535303), (5, 0.021184549366058923), (6, 0.010594072861071804), (7, -0.0013949268063646475), (8, -0.00835151547334111), (9, 0.006965101215785778), (10, -0.001856614917557771), (11, 0.026554670904066727), (12, -0.016409530196864135), (13, -0.008007661244427219), (14, -0.025330238311589075), (15, 0.001257769511203796), (16, 0.005023363831169908), (17, 0.010795042998930249), (18, 0.0059053186540600315), (19, -0.001094181685228034), (20, -0.03757852945572093), (21, 0.019218085296935566), (22, 0.05449297845834472), (23, 0.014912745131161111), (24, 0.05644682526468815), (25, 0.0015018112728080297), (26, 0.012635074461297855), (27, -0.027593962961027528), (28, 0.03898659506276407), (29, 0.03090495072468144)]\n",
      "Partition numéro: 2\n",
      "document number  0\n",
      "[(0, -0.00609537624109891), (1, 0.02863780234232002), (2, -0.01572304615714957), (3, 0.013189743811980662), (4, -0.005986013223365533), (5, 0.019917393576671074), (6, 0.020428427395605638), (7, -0.027717490738832413), (8, 0.014936257936049944), (9, -0.08441395918433109), (10, -0.0035255421146940602), (11, -0.007326205361660898), (12, -0.05782011699949759), (13, -0.04255151526138143), (14, -0.07061601786729203), (15, -0.036037907611380635), (16, -0.03623509234325245), (17, 0.024999414256262047), (18, 0.0024186282181776096), (19, 0.005248701708337892), (20, -0.0012191269693192662), (21, 0.018104467950002066), (22, 0.014610895170215823), (23, -0.014947039713537221), (24, 0.012298749289657518), (25, -0.014862891572963438), (26, 0.021737254475130845), (27, 0.037192309050344), (28, -0.027300606190344634), (29, -0.005306923216176595)]\n",
      "document number  1\n",
      "[(0, -0.008762547273960765), (1, 0.05598043025498), (2, 0.01250705499062365), (3, 0.02028464682845503), (4, -0.010719435736683453), (5, -0.01847482266698113), (6, -0.024881707659884192), (7, -0.015586739547351023), (8, 0.02330987097385226), (9, -0.05408966712769151), (10, -0.01842069723485285), (11, -0.015371059438146638), (12, 0.002400192336946231), (13, 0.0007402267234739017), (14, -0.04034120066990032), (15, -0.019043555106093405), (16, 0.006900114647610245), (17, 0.04633917337712239), (18, -0.019174509313974675), (19, 0.03227206690833987), (20, -0.03813010639653993), (21, 0.0030170420991301903), (22, 0.01141611183329811), (23, 0.012591528973583132), (24, -0.03762501491550853), (25, 0.05110077965704462), (26, 0.029580719452379285), (27, 0.05882008374263852), (28, -0.013824458492678271), (29, 0.017970549423250912)]\n",
      "document number  2\n",
      "[(0, -0.005599077121845375), (1, 0.03853574972635039), (2, 0.007666716779560493), (3, 0.010347142870637767), (4, -0.035294873442711344), (5, 0.028448423593212424), (6, 0.024797696180257033), (7, -0.022514810470932626), (8, 0.002563029724274776), (9, -0.04529643171527689), (10, 5.608498200560677e-05), (11, -0.026838895757220656), (12, 0.010002025482037775), (13, 0.017632178247210164), (14, 0.0015078580862012828), (15, -0.01856648456130971), (16, -0.0023820739138299316), (17, 0.001572446689886204), (18, -0.009763312931818714), (19, 0.007365275562310696), (20, 0.012730915926412595), (21, 0.03903690264601651), (22, 0.012054471125290721), (23, -0.04385051800186267), (24, -0.018322850527863775), (25, 0.04554487007542841), (26, 0.006293099636983586), (27, -0.0039675130545578545), (28, -0.04703431846622588), (29, -0.02541903871862625)]\n",
      "Partition numéro: 3\n",
      "document number  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, -0.00883326551114477), (1, -0.09255659273569147), (2, 0.014287639216764361), (3, -0.046420605465063755), (4, 0.09267097849447672), (5, 0.015405572692952023), (6, -0.03190702601682441), (7, -0.03421792359381664), (8, -0.022030197577816744), (9, 0.04055678947203388), (10, 0.02133720847965256), (11, -0.017535057095893977), (12, -0.007300457435961515), (13, 0.03861250341160007), (14, 0.036442066021075595), (15, 0.04585926990288329), (16, -0.000536848492156182), (17, -0.03594774843741222), (18, 0.011112852714998194), (19, 0.050708550962722136), (20, -0.031087756088032284), (21, 0.006951733966797193), (22, -0.018422063755440823), (23, -0.06457645435890354), (24, -0.0457051336661303), (25, 0.008730804029211435), (26, 0.03966877366012255), (27, 0.016366095473602994), (28, -0.01268171513660413), (29, 0.01037252148161957)]\n",
      "document number  1\n",
      "[(0, -0.005954548281800257), (1, -0.07428989477475446), (2, 0.003941653109995601), (3, -0.006012277917508063), (4, 0.021136537134105744), (5, -0.0295755927093568), (6, -0.02520730426391121), (7, -0.03011519141589615), (8, -0.0496908233422799), (9, -0.013534184831053757), (10, 0.01592957769916471), (11, -0.0026217038576884406), (12, 0.04749852094247455), (13, -0.019734464140749492), (14, -0.005269692471976831), (15, -0.022726025652280595), (16, -0.0168486655134144), (17, -0.01613163447954471), (18, 0.020889713539824353), (19, -0.02288329625495354), (20, 0.02736062025718602), (21, -0.01799486347376112), (22, 0.04573998795034094), (23, -0.004014934064920058), (24, 0.03718139382856261), (25, -0.03347385942709155), (26, 0.020390821881130138), (27, 0.004357764857748109), (28, 0.033994828392782075), (29, -0.0009186656560046399)]\n",
      "document number  2\n",
      "[(0, -0.007835086806220358), (1, -0.07353384914639988), (2, 0.025507839225334346), (3, -0.01344578234037578), (4, -0.0041906950788075156), (5, -0.0677897334794657), (6, 0.030552658286639993), (7, -0.030233356696546608), (8, 0.044622030069415244), (9, -0.04160448831340575), (10, 0.08330200464285328), (11, -0.004651221513218178), (12, -0.0372409764748298), (13, -0.0022104119057156767), (14, -0.011462419302727515), (15, -0.09777444831380297), (16, 0.010105511220716358), (17, 0.0042341306958163525), (18, 0.012845713068768954), (19, 0.0007120201740236896), (20, 0.04429429766720452), (21, 0.03324199125395744), (22, -0.0010674777443063868), (23, -0.042722128571420055), (24, 0.004262064030998052), (25, -0.05845648941193782), (26, -0.01013485683162227), (27, 0.041657793137555105), (28, -0.020498777598955974), (29, 0.04184191211489)]\n"
     ]
    }
   ],
   "source": [
    "num_partition = 0\n",
    "for lsa in partitions_lsa:\n",
    "    print(\"Partition numéro:\",num_partition)\n",
    "    num_partition+=1\n",
    "    i=0\n",
    "    for doc in lsa:\n",
    "        if (i<3):\n",
    "            print(\"document number \", i)\n",
    "            i+=1\n",
    "            print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create ours partitions\n",
    "partitions = []\n",
    "\n",
    "# You must specify a treshold, to know what are the doc you keep, and what are the doc you drop\n",
    "tresh = 0.03\n",
    "\n",
    "for corpus_lsi in partitions_lsa:\n",
    "    # Let's create ours clusters\n",
    "    clusters = []\n",
    "\n",
    "    for i in range(0, nb_concepts):\n",
    "        dic = {}\n",
    "        num_doc = 0\n",
    "        for doc in corpus_lsi:\n",
    "            if abs(doc[i][1]) > tresh:\n",
    "                dic[num_doc] = doc[i][1]\n",
    "            num_doc+=1\n",
    "        clusters.append(dic)\n",
    "    partitions.append(clusters)\n",
    "    \n",
    "# TODO: it would be nice to know how many articles are in no cluster anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.09294078473146268,\n",
       " 3: 0.03633034707949508,\n",
       " 4: -0.033597979538271906,\n",
       " 5: 0.030765634349993413,\n",
       " 6: 0.04511204152660263,\n",
       " 8: 0.031418011626709146,\n",
       " 10: 0.03342599432471933,\n",
       " 12: -0.06736343299005722,\n",
       " 14: -0.0311220007757374,\n",
       " 15: -0.031654142275983795,\n",
       " 17: -0.03046065533635964,\n",
       " 18: 0.030365562232329644,\n",
       " 19: -0.05111297047644732,\n",
       " 23: 0.033231655979149524,\n",
       " 26: -0.034406123885720445,\n",
       " 27: -0.08033971196171126,\n",
       " 30: -0.03129488625685242,\n",
       " 34: 0.037195101416340526,\n",
       " 41: 0.03841495745162948,\n",
       " 43: -0.046974534621905754,\n",
       " 44: -0.050109069456024544,\n",
       " 45: 0.05685036713129945,\n",
       " 50: 0.06722140330478575,\n",
       " 53: -0.0450644945235948,\n",
       " 54: -0.050575752302505425,\n",
       " 56: -0.04715046177240906,\n",
       " 60: 0.08940531521159073,\n",
       " 61: -0.03154192453606096,\n",
       " 63: -0.055722088088707225,\n",
       " 69: 0.09949425444196348,\n",
       " 74: 0.03226583513992607,\n",
       " 76: -0.0417604537786582,\n",
       " 78: -0.07336368074270433,\n",
       " 83: -0.06861860674985723,\n",
       " 85: -0.0531494843493874,\n",
       " 86: 0.03529134079166965,\n",
       " 90: -0.03117028216984973,\n",
       " 92: -0.030471152665575214,\n",
       " 94: 0.04801171294121091,\n",
       " 97: -0.03631549997804391,\n",
       " 99: 0.07250223690398809,\n",
       " 100: 0.05262260080638467,\n",
       " 104: 0.0356845066689063,\n",
       " 106: -0.03444395916976763,\n",
       " 110: 0.04213007886831942,\n",
       " 111: 0.034797237999915305,\n",
       " 112: -0.07421082228706642,\n",
       " 114: -0.05772118422335443,\n",
       " 117: -0.03230819998717658,\n",
       " 119: 0.07854488447689045,\n",
       " 121: -0.03114006532999342,\n",
       " 128: 0.049462673425831535,\n",
       " 142: -0.04517993406819909,\n",
       " 150: 0.03504079036627821,\n",
       " 157: 0.16843102030984552,\n",
       " 158: 0.03661041166736384,\n",
       " 164: 0.04415868048052124,\n",
       " 168: 0.0313496439900454,\n",
       " 169: -0.07622970295499733,\n",
       " 173: 0.05250174978432926,\n",
       " 179: 0.09185076484021017,\n",
       " 190: -0.037190450925767546,\n",
       " 192: -0.0759832606559432,\n",
       " 201: -0.04576178984321814,\n",
       " 203: -0.03944571702887377,\n",
       " 204: -0.03389344846730012,\n",
       " 205: -0.03805656630030488,\n",
       " 206: -0.03688923677044062,\n",
       " 211: 0.08026601995394941,\n",
       " 213: -0.04830096495121033,\n",
       " 214: 0.06449936273728338,\n",
       " 215: -0.05312236599772884,\n",
       " 219: -0.039883984308149635,\n",
       " 223: -0.038159176206384454,\n",
       " 227: -0.08143540165429163,\n",
       " 229: 0.04480848286042859,\n",
       " 231: -0.06425863968776986,\n",
       " 234: -0.03466893511820598,\n",
       " 244: 0.045126408119602834,\n",
       " 245: 0.05594888774056782,\n",
       " 247: 0.0505997389386943,\n",
       " 254: 0.03682312380377627,\n",
       " 256: -0.034568616573782886,\n",
       " 259: -0.03610259161477857}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display clusters 3 of partition 0 \n",
    "partitions[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_labels_by_cluster = 5\n",
    "\n",
    "# Let's labelize our clusters\n",
    "# For this, we will use the tfidf matrix\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=sourceFST, use_idf=True, ngram_range=(min_gram, max_gram))\n",
    "tfidf = vectorizer.fit_transform(usable)\n",
    "\n",
    "# We can access the value in the tfidf using:\n",
    "#tfidf.toarray().item(num_doc, num_word)\n",
    "# To know the number of the word searched, we will use:\n",
    "#vectorizer.vocabulary_[word]\n",
    "\n",
    "# take less than 8h to compute x)\n",
    "labels = []\n",
    "for clusters in partitions:\n",
    "    l = []\n",
    "    for clus in clusters:\n",
    "        first_arti = True\n",
    "        for article in clus:\n",
    "            link = abs(clus[article])\n",
    "            if first_arti:\n",
    "                coef_list = (tfidf.toarray()[article] * link)\n",
    "                first = False\n",
    "            else:\n",
    "                # the more an article have a high coeficient, the more he is implied in the labeling step\n",
    "                coef_list += (tfidf.toarray()[article] * link)\n",
    "        # Now we have coef_list filled by every coeficient in the multiple tfidf\n",
    "        # Let's find the best ones, to finally get the labels\n",
    "        res = dict(zip(vectorizer.get_feature_names(), coef_list))\n",
    "\n",
    "        l.append(Counter(res).most_common(nb_labels_by_cluster))\n",
    "    labels.append(l)\n",
    "\n",
    "# TODO: on observe beaucoup de labels identiques entre deux clusters\n",
    "# Je pense que c'est parce que l'on a trop de clusters, mais j'aimerais en être sûr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[('xplor', 0.04005849687206974),\n",
       "   ('xplor everywhere', 0.04005849687206974),\n",
       "   ('everywhere', 0.03783570961575117),\n",
       "   ('and', 0.028254465076511328),\n",
       "   ('dater', 0.020189613761603598)],\n",
       "  [('hotspot', 0.02679405983234753),\n",
       "   ('relation', 0.006417877842461409),\n",
       "   ('photographie', 0.006326824184650093),\n",
       "   ('détecter', 0.0038242003822581004),\n",
       "   ('approcher extraire relation', 0.003349257479043441)],\n",
       "  [('sémiotique', 0.005892694383508915),\n",
       "   ('style', 0.005565717412234285),\n",
       "   ('indicateur', 0.00467976961655377),\n",
       "   ('action suggéré', 0.0029463471917544576),\n",
       "   ('action suggéré proposer', 0.0029463471917544576)],\n",
       "  [('changement', 0.008396827799076783),\n",
       "   ('composer complexe', 0.00667051147145592),\n",
       "   ('formalisation', 0.006289212590426272),\n",
       "   ('composer', 0.00603201563636381),\n",
       "   ('élémentaire', 0.005834057223851985)],\n",
       "  [('olfactif', 0.00669056655565228),\n",
       "   ('qualité olfactif', 0.00669056655565228),\n",
       "   ('neuroscientifique', 0.004460377703768188),\n",
       "   ('découvrir sousgroupe', 0.00421287788494158),\n",
       "   ('sousgroupe', 0.004037273968876664)],\n",
       "  [('olfactif', 0.00761219677452357),\n",
       "   ('qualité olfactif', 0.00761219677452357),\n",
       "   ('neuroscientifique', 0.0050747978496823816),\n",
       "   ('découvrir sousgroupe', 0.004793204757842435),\n",
       "   ('sousgroupe', 0.0045934112796154275)],\n",
       "  [('hotspot', 0.02782652429219517),\n",
       "   ('relation', 0.006665180073681532),\n",
       "   ('photographie', 0.006570617814851277),\n",
       "   ('détecter', 0.003971559573314715),\n",
       "   ('approcher extraire relation', 0.003478315536524396)],\n",
       "  [('xplor', 0.011209216486017587),\n",
       "   ('xplor everywhere', 0.011209216486017587),\n",
       "   ('everywhere', 0.010587233498537891),\n",
       "   ('and', 0.007906198196868967),\n",
       "   ('dater', 0.005649481860131554)],\n",
       "  [('réseau social', 0.009156752825842622),\n",
       "   ('social', 0.00825528032298448),\n",
       "   ('baser interaction', 0.007621754172386075),\n",
       "   ('baser interaction utilisateur', 0.007621754172386075),\n",
       "   ('baser lanalyse lenrichissement', 0.007621754172386075)],\n",
       "  [('xplor', 0.007485853104808894),\n",
       "   ('xplor everywhere', 0.007485853104808894),\n",
       "   ('everywhere', 0.007070474092032115),\n",
       "   ('and', 0.005279997794055735),\n",
       "   ('dater', 0.003772894508370098)],\n",
       "  [('réseau social', 0.009576725362409656),\n",
       "   ('social', 0.008633907013390603),\n",
       "   ('baser interaction', 0.007971324319549328),\n",
       "   ('baser interaction utilisateur', 0.007971324319549328),\n",
       "   ('baser lanalyse lenrichissement', 0.007971324319549328)],\n",
       "  [('réécriture graphe', 0.005866623995138518),\n",
       "   ('modèle', 0.0058413361159271515),\n",
       "   ('propagation', 0.0049327780572986915),\n",
       "   ('réécriture', 0.0049327780572986915),\n",
       "   ('formalisme', 0.004498564423773173)],\n",
       "  [('sémiotique', 0.006368654609214013),\n",
       "   ('style', 0.006015267302883864),\n",
       "   ('indicateur', 0.005057760406162047),\n",
       "   ('action suggéré', 0.0031843273046070065),\n",
       "   ('action suggéré proposer', 0.0031843273046070065)],\n",
       "  [('réécriture graphe', 0.006940383526744503),\n",
       "   ('modèle', 0.006910467244322055),\n",
       "   ('propagation', 0.005835617145113158),\n",
       "   ('réécriture', 0.005835617145113158),\n",
       "   ('formalisme', 0.005321930031075227)],\n",
       "  [('the', 0.008674132155424627),\n",
       "   ('codicum', 0.006295647010948049),\n",
       "   ('codicum stemma', 0.006295647010948049),\n",
       "   ('stemma', 0.006295647010948049),\n",
       "   ('triplet', 0.005349116573642264)],\n",
       "  [('hotspot', 0.022893316330336484),\n",
       "   ('relation', 0.005483547791423053),\n",
       "   ('photographie', 0.005405749943528719),\n",
       "   ('détecter', 0.0032674641173986636),\n",
       "   ('approcher extraire relation', 0.0028616645412920605)],\n",
       "  [('olfactif', 0.008681419286261401),\n",
       "   ('qualité olfactif', 0.008681419286261401),\n",
       "   ('neuroscientifique', 0.0057876128575076015),\n",
       "   ('découvrir sousgroupe', 0.005466466706036723),\n",
       "   ('sousgroupe', 0.00523860988539407)],\n",
       "  [('xplor', 0.006181853673876703),\n",
       "   ('xplor everywhere', 0.006181853673876703),\n",
       "   ('everywhere', 0.00583883167755462),\n",
       "   ('and', 0.004360247697123039),\n",
       "   ('dater', 0.0031156745198131102)],\n",
       "  [('the', 0.007567048502347521),\n",
       "   ('codicum', 0.005492130559217999),\n",
       "   ('codicum stemma', 0.005492130559217999),\n",
       "   ('stemma', 0.005492130559217999),\n",
       "   ('triplet', 0.004666406256232617)],\n",
       "  [('xplor', 0.00650485799684718),\n",
       "   ('xplor everywhere', 0.00650485799684718),\n",
       "   ('everywhere', 0.006143912964243244),\n",
       "   ('and', 0.00458807238041251),\n",
       "   ('dater', 0.0032784697576106895)],\n",
       "  [('hotspot', 0.01804517020284823),\n",
       "   ('relation', 0.004322290042380553),\n",
       "   ('photographie', 0.004260967541681595),\n",
       "   ('détecter', 0.0025755091695485927),\n",
       "   ('approcher extraire relation', 0.002255646275356029)],\n",
       "  [('olfactif', 0.00748747751049348),\n",
       "   ('qualité olfactif', 0.00748747751049348),\n",
       "   ('neuroscientifique', 0.004991651673662321),\n",
       "   ('découvrir sousgroupe', 0.004714672241218015),\n",
       "   ('sousgroupe', 0.004518152206426662)],\n",
       "  [('xplor', 0.009825824940261971),\n",
       "   ('xplor everywhere', 0.009825824940261971),\n",
       "   ('everywhere', 0.00928060432128114),\n",
       "   ('and', 0.006930450448731518),\n",
       "   ('dater', 0.004952247985404028)],\n",
       "  [('réseau social', 0.009089417411806132),\n",
       "   ('social', 0.008194574008299798),\n",
       "   ('baser interaction', 0.007565706577497028),\n",
       "   ('baser interaction utilisateur', 0.007565706577497028),\n",
       "   ('baser lanalyse lenrichissement', 0.007565706577497028)],\n",
       "  [('changement', 0.007609934249992788),\n",
       "   ('composer complexe', 0.00604539653858132),\n",
       "   ('formalisation', 0.005699830393405582),\n",
       "   ('composer', 0.0054667361872901546),\n",
       "   ('élémentaire', 0.00528732908981299)],\n",
       "  [('hotspot', 0.04179311854496056),\n",
       "   ('relation', 0.01001054454440118),\n",
       "   ('photographie', 0.009868519918843448),\n",
       "   ('détecter', 0.005964951221107778),\n",
       "   ('approcher extraire relation', 0.00522413981812007)],\n",
       "  [('hotspot', 0.019917666899509225),\n",
       "   ('relation', 0.0047708019563934695),\n",
       "   ('photographie', 0.004703116191801789),\n",
       "   ('détecter', 0.0028427625319712227),\n",
       "   ('approcher extraire relation', 0.002489708362438653)],\n",
       "  [('xplor', 0.007727616200447073),\n",
       "   ('xplor everywhere', 0.007727616200447073),\n",
       "   ('everywhere', 0.0072988221079744965),\n",
       "   ('and', 0.005450520591361715),\n",
       "   ('dater', 0.0038947439012300624)],\n",
       "  [('sémiotique', 0.009309414810067935),\n",
       "   ('style', 0.00879284903203371),\n",
       "   ('indicateur', 0.007393208223724216),\n",
       "   ('action suggéré', 0.004654707405033967),\n",
       "   ('action suggéré proposer', 0.004654707405033967)],\n",
       "  [('olfactif', 0.007187790103238117),\n",
       "   ('qualité olfactif', 0.007187790103238117),\n",
       "   ('neuroscientifique', 0.004791860068825413),\n",
       "   ('découvrir sousgroupe', 0.004525966779592349),\n",
       "   ('sousgroupe', 0.004337312488586913)]],\n",
       " [[('olfactif', 0.06862195147040816),\n",
       "   ('qualité olfactif', 0.06862195147040816),\n",
       "   ('neuroscientifique', 0.04574796764693878),\n",
       "   ('découvrir sousgroupe', 0.04320947999941572),\n",
       "   ('sousgroupe', 0.04140839435054192)],\n",
       "  [('fusion', 0.011907188046366804),\n",
       "   ('formel', 0.009286585741388631),\n",
       "   ('agg', 0.00800385358017674),\n",
       "   ('algébrique spo', 0.00800385358017674),\n",
       "   ('algébrique spo simple', 0.00800385358017674)],\n",
       "  [('phraser', 0.005190157228881583),\n",
       "   ('syntaxique', 0.004683448342040255),\n",
       "   ('traduire', 0.004370217380340153),\n",
       "   ('analyser presque', 0.003054279841774859),\n",
       "   ('analyser presque instantan', 0.003054279841774859)],\n",
       "  [('fusion', 0.005301164271799502),\n",
       "   ('formel', 0.0041344536046252645),\n",
       "   ('agg', 0.003563372180797455),\n",
       "   ('algébrique spo', 0.003563372180797455),\n",
       "   ('algébrique spo simple', 0.003563372180797455)],\n",
       "  [('document', 0.011525922790177749),\n",
       "   ('document administratif', 0.005656117526076141),\n",
       "   ('administratif', 0.005342267857743528),\n",
       "   ('lhomme', 0.005119587974274627),\n",
       "   ('nest', 0.0033181621020866104)],\n",
       "  [('document', 0.009286507422556553),\n",
       "   ('document administratif', 0.004557168943862782),\n",
       "   ('administratif', 0.004304298321041892),\n",
       "   ('lhomme', 0.004124883758899395),\n",
       "   ('nest', 0.002673463769558876)],\n",
       "  [('document', 0.011434531495573759),\n",
       "   ('document administratif', 0.005611269064694731),\n",
       "   ('administratif', 0.005299907971725851),\n",
       "   ('lhomme', 0.0050789937605769005),\n",
       "   ('nest', 0.00329185174622738)],\n",
       "  [('protéinearn', 0.00562959791868569),\n",
       "   ('rosettadock', 0.00562959791868569),\n",
       "   ('scor', 0.005095583971095289),\n",
       "   ('fonction', 0.004303178789878757),\n",
       "   ('poids', 0.004027556075914487)],\n",
       "  [('1dsax', 0.014183757127620671),\n",
       "   ('sax', 0.014183757127620671),\n",
       "   ('série', 0.01282609963903124),\n",
       "   ('symbolisation', 0.009455838085080447),\n",
       "   ('série temporel', 0.007137218515076719)],\n",
       "  [('document', 0.017976050150088962),\n",
       "   ('document administratif', 0.008821389328600028),\n",
       "   ('administratif', 0.008331903368973114),\n",
       "   ('lhomme', 0.007984607553659815),\n",
       "   ('nest', 0.005175069227781427)],\n",
       "  [('bayésien naïf', 0.008184090631373046),\n",
       "   ('naïf', 0.007174403422717772),\n",
       "   ('bayésien', 0.006543745085864878),\n",
       "   ('logvraisemblance', 0.006238319617071332),\n",
       "   ('variablesexplicative', 0.00589216440835364)],\n",
       "  [('fusion', 0.005795568026999162),\n",
       "   ('formel', 0.004520046142985074),\n",
       "   ('agg', 0.003895703815327662),\n",
       "   ('algébrique spo', 0.003895703815327662),\n",
       "   ('algébrique spo simple', 0.003895703815327662)],\n",
       "  [('phraser', 0.006363119589013214),\n",
       "   ('syntaxique', 0.0057418957798682415),\n",
       "   ('traduire', 0.005357875415863032),\n",
       "   ('analyser presque', 0.0037445393336790573),\n",
       "   ('analyser presque instantan', 0.0037445393336790573)],\n",
       "  [('phraser', 0.006166475997043822),\n",
       "   ('syntaxique', 0.005564450268264666),\n",
       "   ('traduire', 0.005192297533448411),\n",
       "   ('analyser presque', 0.003628819417599417),\n",
       "   ('analyser presque instantan', 0.003628819417599417)],\n",
       "  [('phraser', 0.0052119479191405785),\n",
       "   ('syntaxique', 0.004703111633086141),\n",
       "   ('traduire', 0.0043885655823502085),\n",
       "   ('analyser presque', 0.0030671031269011847),\n",
       "   ('analyser presque instantan', 0.0030671031269011847)],\n",
       "  [('protéinearn', 0.004608139070975979),\n",
       "   ('rosettadock', 0.004608139070975979),\n",
       "   ('scor', 0.004171018947677377),\n",
       "   ('fonction', 0.0035223912253514815),\n",
       "   ('poids', 0.003296778701080172)],\n",
       "  [('mapreduce', 0.007167385066937171),\n",
       "   ('algorithme baser mapreduce', 0.0028834794907963623),\n",
       "   ('appliquer dautre', 0.0028834794907963623),\n",
       "   ('appliquer dautre casdétuder', 0.0028834794907963623),\n",
       "   ('architecturer maîtreesclaveil', 0.0028834794907963623)],\n",
       "  [('fusion', 0.005412538678414273),\n",
       "   ('formel', 0.004221316092426483),\n",
       "   ('agg', 0.0036382365769632398),\n",
       "   ('algébrique spo', 0.0036382365769632398),\n",
       "   ('algébrique spo simple', 0.0036382365769632398)],\n",
       "  [('fusion', 0.009979540691927954),\n",
       "   ('formel', 0.007783186083429966),\n",
       "   ('agg', 0.006708114643404739),\n",
       "   ('algébrique spo', 0.006708114643404739),\n",
       "   ('algébrique spo simple', 0.006708114643404739)],\n",
       "  [('article méthode dalignemer', 0.004382680261092201),\n",
       "   ('avecagrovoc', 0.004382680261092201),\n",
       "   ('avecagrovoc nalt', 0.004382680261092201),\n",
       "   ('cibl déjà', 0.004382680261092201),\n",
       "   ('cibl déjà publier', 0.004382680261092201)],\n",
       "  [('phraser', 0.006635062644873368),\n",
       "   ('syntaxique', 0.005987289358122527),\n",
       "   ('traduire', 0.00558685697013455),\n",
       "   ('analyser presque', 0.003904571131752987),\n",
       "   ('analyser presque instantan', 0.003904571131752987)],\n",
       "  [('fusion', 0.006648275505198998),\n",
       "   ('formel', 0.005185084863948439),\n",
       "   ('agg', 0.004468882451262256),\n",
       "   ('algébrique spo', 0.004468882451262256),\n",
       "   ('algébrique spo simple', 0.004468882451262256)],\n",
       "  [('phraser', 0.00525494482574816),\n",
       "   ('syntaxique', 0.004741910802761302),\n",
       "   ('traduire', 0.004424769847514215),\n",
       "   ('analyser presque', 0.003092405748636695),\n",
       "   ('analyser presque instantan', 0.003092405748636695)],\n",
       "  [('fusion', 0.00446993243554144),\n",
       "   ('formel', 0.0034861640430324443),\n",
       "   ('agg', 0.0030046291860044067),\n",
       "   ('algébrique spo', 0.0030046291860044067),\n",
       "   ('algébrique spo simple', 0.0030046291860044067)],\n",
       "  [('phraser', 0.008644064898799259),\n",
       "   ('syntaxique', 0.007800155107727553),\n",
       "   ('traduire', 0.007278477508794302),\n",
       "   ('analyser presque', 0.005086819533034725),\n",
       "   ('analyser presque instantan', 0.005086819533034725)],\n",
       "  [('mapreduce', 0.012833158072903686),\n",
       "   ('algorithme baser mapreduce', 0.005162851968992714),\n",
       "   ('appliquer dautre', 0.005162851968992714),\n",
       "   ('appliquer dautre casdétuder', 0.005162851968992714),\n",
       "   ('architecturer maîtreesclaveil', 0.005162851968992714)],\n",
       "  [('fusion', 0.006305084877154041),\n",
       "   ('formel', 0.00491742559959736),\n",
       "   ('agg', 0.0042381942714615805),\n",
       "   ('algébrique spo', 0.0042381942714615805),\n",
       "   ('algébrique spo simple', 0.0042381942714615805)],\n",
       "  [('fusion', 0.004821524954139719),\n",
       "   ('formel', 0.0037603760616282166),\n",
       "   ('agg', 0.0032409650049893806),\n",
       "   ('algébrique spo', 0.0032409650049893806),\n",
       "   ('algébrique spo simple', 0.0032409650049893806)],\n",
       "  [('bayésien naïf', 0.012964804443025819),\n",
       "   ('naïf', 0.01136531125576085),\n",
       "   ('bayésien', 0.01036625560303894),\n",
       "   ('logvraisemblance', 0.009882416694944704),\n",
       "   ('variablesexplicative', 0.009334055882473257)],\n",
       "  [('bayésien naïf', 0.00810638268796421),\n",
       "   ('naïf', 0.0071062824597084495),\n",
       "   ('bayésien', 0.006481612223984667),\n",
       "   ('logvraisemblance', 0.006179086770124197),\n",
       "   ('variablesexplicative', 0.005836218305234412)]],\n",
       " [[('risquer', 0.03994779392889273),\n",
       "   ('chimique', 0.033283209792385256),\n",
       "   ('alimentaire', 0.027578451700127123),\n",
       "   ('risquer chimique', 0.027578451700127123),\n",
       "   ('alimentaire entrer', 0.018385634466751417)],\n",
       "  [('lien', 0.011616303208987985),\n",
       "   ('rankmerging', 0.010481674585850979),\n",
       "   ('dan grand', 0.008493127444901686),\n",
       "   ('réseau', 0.007348675472945104),\n",
       "   ('article décrire rankmerging', 0.005240837292925489)],\n",
       "  [('lien', 0.01126228852713843),\n",
       "   ('rankmerging', 0.01016223847721966),\n",
       "   ('dan grand', 0.00823429365275444),\n",
       "   ('réseau', 0.007124719627202598),\n",
       "   ('article décrire rankmerging', 0.00508111923860983)],\n",
       "  [('lien', 0.007202168326313195),\n",
       "   ('rankmerging', 0.006498692686544867),\n",
       "   ('dan grand', 0.0052657831303579835),\n",
       "   ('réseau', 0.0045562169632976405),\n",
       "   ('article décrire rankmerging', 0.0032493463432724337)],\n",
       "  [('2d', 0.004894593714476836),\n",
       "   ('projection', 0.004081586729011379),\n",
       "   ('loutil', 0.0037815300405375156),\n",
       "   ('changement', 0.0034035033201373728),\n",
       "   ('lutilisateur', 0.0028544467964988465)],\n",
       "  [('lien', 0.007196929283383795),\n",
       "   ('rankmerging', 0.006493965369933092),\n",
       "   ('dan grand', 0.005261952663944686),\n",
       "   ('réseau', 0.004552902653608548),\n",
       "   ('article décrire rankmerging', 0.003246982684966546)],\n",
       "  [('lien', 0.011306088507907888),\n",
       "   ('rankmerging', 0.010201760271462839),\n",
       "   ('dan grand', 0.008266317508542865),\n",
       "   ('réseau', 0.0071524282569280125),\n",
       "   ('article décrire rankmerging', 0.0051008801357314195)],\n",
       "  [('item', 0.006053041027447072),\n",
       "   ('meilleur', 0.0041009237261733465),\n",
       "   ('gestion', 0.004069144094485725),\n",
       "   ('affinité', 0.003947450647766506),\n",
       "   ('affinité entrer', 0.003947450647766506)],\n",
       "  [('lien', 0.012455526938660226),\n",
       "   ('rankmerging', 0.011238926689286296),\n",
       "   ('dan grand', 0.009106716291771413),\n",
       "   ('réseau', 0.007879583002440729),\n",
       "   ('article décrire rankmerging', 0.005619463344643148)],\n",
       "  [('item', 0.003409496679056985),\n",
       "   ('meilleur', 0.0023099274830706215),\n",
       "   ('gestion', 0.0022920269685673686),\n",
       "   ('affinité', 0.0022234806956161726),\n",
       "   ('affinité entrer', 0.0022234806956161726)],\n",
       "  [('2d', 0.00550139264461792),\n",
       "   ('projection', 0.004587594501038862),\n",
       "   ('loutil', 0.0042503387950021945),\n",
       "   ('changement', 0.0038254468549567336),\n",
       "   ('lutilisateur', 0.0032083219827348662)],\n",
       "  [('biclustering', 0.008269524072352647),\n",
       "   ('the', 0.007173409591771716),\n",
       "   ('of', 0.005960795551208176),\n",
       "   ('dataset', 0.005656192062763354),\n",
       "   ('we', 0.0053062867336065266)],\n",
       "  [('lien', 0.007227244774593433),\n",
       "   ('rankmerging', 0.006521319779339674),\n",
       "   ('dan grand', 0.005284117489170584),\n",
       "   ('réseau', 0.004572080760678805),\n",
       "   ('article décrire rankmerging', 0.003260659889669837)],\n",
       "  [('2d', 0.009796592353009701),\n",
       "   ('projection', 0.008169348401546064),\n",
       "   ('loutil', 0.007568781075380023),\n",
       "   ('changement', 0.006812155726201023),\n",
       "   ('lutilisateur', 0.005713212023286918)],\n",
       "  [('biclustering', 0.009704941230875494),\n",
       "   ('the', 0.00841856410405698),\n",
       "   ('of', 0.00699546551985326),\n",
       "   ('dataset', 0.00663798920946196),\n",
       "   ('we', 0.00622734760226351)],\n",
       "  [('lien', 0.00776237469686544),\n",
       "   ('rankmerging', 0.007004180600505822),\n",
       "   ('dan grand', 0.005675371621201151),\n",
       "   ('réseau', 0.004910613257970766),\n",
       "   ('article décrire rankmerging', 0.003502090300252911)],\n",
       "  [('lien', 0.01464362435408332),\n",
       "   ('rankmerging', 0.013213300520442908),\n",
       "   ('dan grand', 0.010706518731214396),\n",
       "   ('réseau', 0.00926381148889181),\n",
       "   ('article décrire rankmerging', 0.006606650260221454)],\n",
       "  [('item', 0.008354906120054142),\n",
       "   ('meilleur', 0.005660432926576678),\n",
       "   ('gestion', 0.005616568059631946),\n",
       "   ('affinité', 0.005448596734449353),\n",
       "   ('affinité entrer', 0.005448596734449353)],\n",
       "  [('new', 0.005911660398290484),\n",
       "   ('classe', 0.003252573279625825),\n",
       "   ('we', 0.003190897523183224),\n",
       "   ('algorithm to obtain', 0.0031308488908540775),\n",
       "   ('and min', 0.0031308488908540775)],\n",
       "  [('motif', 0.014258655870394547),\n",
       "   ('complexité évaluation', 0.011829774445212628),\n",
       "   ('motif lier', 0.011829774445212628),\n",
       "   ('méthode découvrir motif', 0.011173357606460108),\n",
       "   ('complexité', 0.011011749681734974)],\n",
       "  [('lien', 0.012467995275988302),\n",
       "   ('rankmerging', 0.01125017717510337),\n",
       "   ('dan grand', 0.009115832374233126),\n",
       "   ('réseau', 0.00788747068951835),\n",
       "   ('article décrire rankmerging', 0.005625088587551685)],\n",
       "  [('lien', 0.006807930667669333),\n",
       "   ('rankmerging', 0.006142962401870871),\n",
       "   ('dan grand', 0.004977540768033002),\n",
       "   ('réseau', 0.004306815362765568),\n",
       "   ('article décrire rankmerging', 0.0030714812009354354)],\n",
       "  [('motif', 0.009009522974011154),\n",
       "   ('complexité évaluation', 0.007474801665057933),\n",
       "   ('motif lier', 0.007474801665057933),\n",
       "   ('méthode découvrir motif', 0.007060035880468943),\n",
       "   ('complexité', 0.006957921745460151)],\n",
       "  [('lien', 0.01724521331635725),\n",
       "   ('rankmerging', 0.015560777890661518),\n",
       "   ('dan grand', 0.012608640793485101),\n",
       "   ('réseau', 0.010909621920471681),\n",
       "   ('article décrire rankmerging', 0.007780388945330759)],\n",
       "  [('lien', 0.008948925911284992),\n",
       "   ('rankmerging', 0.008074834791020495),\n",
       "   ('dan grand', 0.006542904992417799),\n",
       "   ('réseau', 0.005661246195999751),\n",
       "   ('article décrire rankmerging', 0.0040374173955102476)],\n",
       "  [('lien', 0.01670787841220193),\n",
       "   ('rankmerging', 0.015075927460395743),\n",
       "   ('dan grand', 0.012215774514129196),\n",
       "   ('réseau', 0.01056969451328518),\n",
       "   ('article décrire rankmerging', 0.007537963730197872)],\n",
       "  [('lien', 0.00578015045614816),\n",
       "   ('rankmerging', 0.005215571171706815),\n",
       "   ('dan grand', 0.004226091002582378),\n",
       "   ('réseau', 0.0036566237229554025),\n",
       "   ('article décrire rankmerging', 0.0026077855858534075)],\n",
       "  [('tweet', 0.008562429429413128),\n",
       "   ('the tweet', 0.006889427432885095),\n",
       "   ('vocabulary', 0.006889427432885095),\n",
       "   ('this', 0.005285953206344487),\n",
       "   ('to', 0.004874129759712488)],\n",
       "  [('biclustering', 0.0080232453362512),\n",
       "   ('the', 0.006959774776473705),\n",
       "   ('of', 0.005783274187020041),\n",
       "   ('dataset', 0.00548774224386488),\n",
       "   ('we', 0.005148257616246125)],\n",
       "  [('lien', 0.01314610840031299),\n",
       "   ('rankmerging', 0.011862055237658289),\n",
       "   ('dan grand', 0.00961162704172199),\n",
       "   ('réseau', 0.008316456847588954),\n",
       "   ('article décrire rankmerging', 0.005931027618829144)]],\n",
       " [[('mapreduce', 0.05548842017470997),\n",
       "   ('algorithme baser mapreduce', 0.02232330481147705),\n",
       "   ('appliquer dautre', 0.02232330481147705),\n",
       "   ('appliquer dautre casdétuder', 0.02232330481147705),\n",
       "   ('architecturer maîtreesclaveil', 0.02232330481147705)],\n",
       "  [('chaîne structurer', 0.010262283424290895),\n",
       "   ('chaîne structurer traitementspour', 0.010262283424290895),\n",
       "   ('conception chaîne', 0.010262283424290895),\n",
       "   ('conception chaîne structurer', 0.010262283424290895),\n",
       "   ('connaissance dan don', 0.010262283424290895)],\n",
       "  [('approcher permettre générer', 0.003535429816781244),\n",
       "   ('approcher sarticul', 0.003535429816781244),\n",
       "   ('approcher sarticul autour', 0.003535429816781244),\n",
       "   ('automatique lexploitation', 0.003535429816781244),\n",
       "   ('automatique lexploitation connaissance', 0.003535429816781244)],\n",
       "  [('chaîne structurer', 0.006160944263600348),\n",
       "   ('chaîne structurer traitementspour', 0.006160944263600348),\n",
       "   ('conception chaîne', 0.006160944263600348),\n",
       "   ('conception chaîne structurer', 0.006160944263600348),\n",
       "   ('connaissance dan don', 0.006160944263600348)],\n",
       "  [('dexplorer', 0.010932203954468544),\n",
       "   ('collection', 0.008315765022326935),\n",
       "   ('visualisation', 0.006901121634363324),\n",
       "   ('analyser sémantique latent', 0.006745913392932663),\n",
       "   ('approchecombiner', 0.006745913392932663)],\n",
       "  [('chaîne structurer', 0.0045149941253140495),\n",
       "   ('chaîne structurer traitementspour', 0.0045149941253140495),\n",
       "   ('conception chaîne', 0.0045149941253140495),\n",
       "   ('conception chaîne structurer', 0.0045149941253140495),\n",
       "   ('connaissance dan don', 0.0045149941253140495)],\n",
       "  [('chaîne structurer', 0.004438835967483095),\n",
       "   ('chaîne structurer traitementspour', 0.004438835967483095),\n",
       "   ('conception chaîne', 0.004438835967483095),\n",
       "   ('conception chaîne structurer', 0.004438835967483095),\n",
       "   ('connaissance dan don', 0.004438835967483095)],\n",
       "  [('motif minimal', 0.017989214771149377),\n",
       "   ('motif', 0.014455165593020009),\n",
       "   ('minimal', 0.012869937124992474),\n",
       "   ('densemble', 0.010855192592731384),\n",
       "   ('15 an', 0.0059964049237164595)],\n",
       "  [('chaîne structurer', 0.010651426330140983),\n",
       "   ('chaîne structurer traitementspour', 0.010651426330140983),\n",
       "   ('conception chaîne', 0.010651426330140983),\n",
       "   ('conception chaîne structurer', 0.010651426330140983),\n",
       "   ('connaissance dan don', 0.010651426330140983)],\n",
       "  [('motif minimal', 0.007744558802919627),\n",
       "   ('motif', 0.006223110978730692),\n",
       "   ('minimal', 0.005540652336545221),\n",
       "   ('densemble', 0.004673282209418772),\n",
       "   ('15 an', 0.002581519600973209)],\n",
       "  [('chaîne structurer', 0.00562160303031578),\n",
       "   ('chaîne structurer traitementspour', 0.00562160303031578),\n",
       "   ('conception chaîne', 0.00562160303031578),\n",
       "   ('conception chaîne structurer', 0.00562160303031578),\n",
       "   ('connaissance dan don', 0.00562160303031578)],\n",
       "  [('algorithme permettre prédictiondinformater', 0.003933935203029128),\n",
       "   ('biais lidentification', 0.003933935203029128),\n",
       "   ('biais lidentification derègl', 0.003933935203029128),\n",
       "   ('boire prédiction', 0.003933935203029128),\n",
       "   ('boire prédiction dinformation', 0.003933935203029128)],\n",
       "  [('motif minimal', 0.00896016783930104),\n",
       "   ('motif', 0.00719990902916289),\n",
       "   ('minimal', 0.006410329644077966),\n",
       "   ('densemble', 0.005406814516151122),\n",
       "   ('15 an', 0.0029867226131003468)],\n",
       "  [('motif minimal', 0.012985228461921756),\n",
       "   ('motif', 0.010434231291813223),\n",
       "   ('minimal', 0.009289959344229791),\n",
       "   ('densemble', 0.007835648059571845),\n",
       "   ('15 an', 0.004328409487307252)],\n",
       "  [('algorithme permettre prédictiondinformater', 0.007894986762815612),\n",
       "   ('biais lidentification', 0.007894986762815612),\n",
       "   ('biais lidentification derègl', 0.007894986762815612),\n",
       "   ('boire prédiction', 0.007894986762815612),\n",
       "   ('boire prédiction dinformation', 0.007894986762815612)],\n",
       "  [('motif minimal', 0.016840883183128118),\n",
       "   ('motif', 0.013532428082144075),\n",
       "   ('minimal', 0.012048391797723467),\n",
       "   ('densemble', 0.010162257369773267),\n",
       "   ('15 an', 0.005613627727709372)],\n",
       "  [('chaîne structurer', 0.004843657156768462),\n",
       "   ('chaîne structurer traitementspour', 0.004843657156768462),\n",
       "   ('conception chaîne', 0.004843657156768462),\n",
       "   ('conception chaîne structurer', 0.004843657156768462),\n",
       "   ('connaissance dan don', 0.004843657156768462)],\n",
       "  [('attribuer', 0.011062465769007913),\n",
       "   ('isomorphisme', 0.010051772762289513),\n",
       "   ('isomorphisme sousgraphe', 0.010051772762289513),\n",
       "   ('graphe', 0.009954694372737018),\n",
       "   ('graphe orienter', 0.009098278937223394)],\n",
       "  [('algorithme permettre prédictiondinformater', 0.013173805113122841),\n",
       "   ('biais lidentification', 0.013173805113122841),\n",
       "   ('biais lidentification derègl', 0.013173805113122841),\n",
       "   ('boire prédiction', 0.013173805113122841),\n",
       "   ('boire prédiction dinformation', 0.013173805113122841)],\n",
       "  [('chaîne structurer', 0.006685456572020276),\n",
       "   ('chaîne structurer traitementspour', 0.006685456572020276),\n",
       "   ('conception chaîne', 0.006685456572020276),\n",
       "   ('conception chaîne structurer', 0.006685456572020276),\n",
       "   ('connaissance dan don', 0.006685456572020276)],\n",
       "  [('attribuer', 0.006012008115145276),\n",
       "   ('isomorphisme', 0.0054627368509272516),\n",
       "   ('isomorphisme sousgraphe', 0.0054627368509272516),\n",
       "   ('graphe', 0.005409978625231321),\n",
       "   ('graphe orienter', 0.004944551056391454)],\n",
       "  [('chaîne structurer', 0.006097171404971986),\n",
       "   ('chaîne structurer traitementspour', 0.006097171404971986),\n",
       "   ('conception chaîne', 0.006097171404971986),\n",
       "   ('conception chaîne structurer', 0.006097171404971986),\n",
       "   ('connaissance dan don', 0.006097171404971986)],\n",
       "  [('adapter pallier', 0.0037057198963835465),\n",
       "   ('adapter pallier confusionnative', 0.0037057198963835465),\n",
       "   ('baser modélisation ontologique', 0.0037057198963835465),\n",
       "   ('charger dinformation', 0.0037057198963835465),\n",
       "   ('charger dinformation important', 0.0037057198963835465)],\n",
       "  [('chaîne structurer', 0.004613507296399192),\n",
       "   ('chaîne structurer traitementspour', 0.004613507296399192),\n",
       "   ('conception chaîne', 0.004613507296399192),\n",
       "   ('conception chaîne structurer', 0.004613507296399192),\n",
       "   ('connaissance dan don', 0.004613507296399192)],\n",
       "  [('chaîne structurer', 0.006402697657958354),\n",
       "   ('chaîne structurer traitementspour', 0.006402697657958354),\n",
       "   ('conception chaîne', 0.006402697657958354),\n",
       "   ('conception chaîne structurer', 0.006402697657958354),\n",
       "   ('connaissance dan don', 0.006402697657958354)],\n",
       "  [('attribuer', 0.006756951441145445),\n",
       "   ('isomorphisme', 0.006139620394803688),\n",
       "   ('isomorphisme sousgraphe', 0.006139620394803688),\n",
       "   ('graphe', 0.006080324937724988),\n",
       "   ('graphe orienter', 0.005557226594910232)],\n",
       "  [('chaîne structurer', 0.007038816556976015),\n",
       "   ('chaîne structurer traitementspour', 0.007038816556976015),\n",
       "   ('conception chaîne', 0.007038816556976015),\n",
       "   ('conception chaîne structurer', 0.007038816556976015),\n",
       "   ('connaissance dan don', 0.007038816556976015)],\n",
       "  [('attribuer', 0.007688255305043017),\n",
       "   ('isomorphisme', 0.006985838137575526),\n",
       "   ('isomorphisme sousgraphe', 0.006985838137575526),\n",
       "   ('graphe', 0.006918370046910516),\n",
       "   ('graphe orienter', 0.006323173582316271)],\n",
       "  [('chaîne structurer', 0.005282070643927451),\n",
       "   ('chaîne structurer traitementspour', 0.005282070643927451),\n",
       "   ('conception chaîne', 0.005282070643927451),\n",
       "   ('conception chaîne structurer', 0.005282070643927451),\n",
       "   ('connaissance dan don', 0.005282070643927451)],\n",
       "  [('motif minimal', 0.01018263289449918),\n",
       "   ('motif', 0.008182216207623489),\n",
       "   ('minimal', 0.007284911920072177),\n",
       "   ('densemble', 0.006144483935349008),\n",
       "   ('15 an', 0.00339421096483306)]]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display labels\n",
    "# labels is composed by an array for each partition\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.toarray()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diachronic analysis\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=fstw, use_idf=True, ngram_range=(min_gram, max_gram))\n",
    "tfidf = vectorizer.fit_transform(usable)\n",
    "\n",
    "# Feature Recall\n",
    "# label_f: num of the label\n",
    "def FR(num_cluster_c, num_label_f, num_partition_C):\n",
    "    total_singledoc = 0\n",
    "    for doc in partitions[num_partition_C][num_cluster_c]:\n",
    "        total_singledoc += tfidf.toarray().item(doc, num_label_f)\n",
    "    total_everydoc = 0\n",
    "    for cluster in partitions[num_partition_C]:\n",
    "        for doc in cluster:\n",
    "            total_everydoc += tfidf.toarray().item(doc, num_label_f)\n",
    "    return total_singledoc / total_everydoc\n",
    "\n",
    "# Feature Precision\n",
    "def FP(num_cluster_c, num_label_f, num_partition_C):\n",
    "    total_singledoc = 0\n",
    "    for doc in partitions[num_partition_C][num_cluster_c]:\n",
    "        total_singledoc += tfidf.toarray().item(doc, num_label_f)\n",
    "    total_everydoc = 0\n",
    "    for doc in partitions[num_partition_C][cluster_c]:\n",
    "        total_everydoc += sum(tfidf.toarray()[doc])\n",
    "    return total_singledoc / total_everydoc\n",
    "    \n",
    "# Feature F-measure\n",
    "def FF(num_cluster_c, num_label_f, num_partition_C):\n",
    "    fr = FR(num_cluster_c, num_label_f, num_partition_C)\n",
    "    fp = FP(num_cluster_c, num_label_f)\n",
    "    return 2*fr*fp / (fr + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-6afabf5fd1f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mnb_for_mean_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mmean_clus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mnb_for_mean_clus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    945\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Vamos a implementar una nueva tecnicà para labelisar: FF-measure\n",
    "\n",
    "# list of labels of cluster C from partition P = labels[P][C]\n",
    "labels = []\n",
    "\n",
    "\n",
    "# first, we want the mean ffmeasure of each f\n",
    "ffmeanF = []\n",
    "\n",
    "\n",
    "#TODO: question: on prend on compte les zeros (genre quand le mot n'apparait pas)? -> non\n",
    "# second, we want the mean of every ffmeasure\n",
    "ffmean_total = -1\n",
    "\n",
    "\n",
    "mean_total = 0\n",
    "nb_for_mean_total = 0\n",
    "\n",
    "for f in range(0, len(tfidf.toarray()[0])):\n",
    "    mean_clus = 0\n",
    "    nb_for_mean_clus = 0\n",
    "    for num_parti in range(0, len(partitions)):\n",
    "        for num_clus in range(0, len(partitions[num_parti])):\n",
    "            ffmesure = FF(num_clus, f, num_clusters)\n",
    "            mean_clus += ffmesure\n",
    "            nb_for_mean_clus += 1\n",
    "            if ffmesure > 0:\n",
    "                mean_total += ffmesure\n",
    "                nb_for_mean_total += 1\n",
    "    mean_clus /= nb_for_mean_clus\n",
    "    ffmeanF.append(mean_clus)\n",
    "ffmean_total = mean_total / nb_for_mean_total\n",
    "    \n",
    "\n",
    "\n",
    "# Now we fill labels[]\n",
    "for clusters in partitions:\n",
    "    labels_for_clusters = []\n",
    "    for clus in clusters:\n",
    "        labels_for_clus = []\n",
    "        for arti in clus:\n",
    "            for num_word in range(0, len(tfidf.toarray()[arti])):\n",
    "                if tfidf.toarray().item(arti, num_word) != 0:\n",
    "                    ffmes = FF(clus, num_word, partitions)\n",
    "                    if ffmes > ffmeanF[num_word] and ffmes > ffmean_total:\n",
    "                        labels_for_clus[num_word] = ffmes\n",
    "        labels_for_clusters[clus] = labels_for_clus\n",
    "    labels[clusters] = labels_for_clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: sigma are ecart-type :)\n",
    "\n",
    "def inter(listA, listB):\n",
    "    return np.intersect1d(listA, listB)\n",
    "    \n",
    "# cluster_t and cluster_s must be in two different partitions\n",
    "def proba(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S):\n",
    "    total_inter = 0\n",
    "    total_t = 0\n",
    "    for f in range(0, len(labels[num_partition_T][num_cluster_t])):\n",
    "        if labels[num_partition_T][num_cluster_t][f][0] == labels[num_partition_S][num_cluster_s][f][0]:\n",
    "            total_inter += labels[partition_T][cluster_t][f][1]\n",
    "            \n",
    "        total_t += labels[num_partition_T][num_cluster_t][f][1]\n",
    "    return total_inter / total_t\n",
    "    \n",
    "\n",
    "def P_A(num_cluster_s, num_partition_T, num_partition_S):\n",
    "    # first, we have to know what are the cluster which got the label\n",
    "    total = 0\n",
    "    nb_computation = 0\n",
    "    for label_s in labels[num_partition_S][num_cluster_s]:\n",
    "        for num_cluster_t in range(0, len(partitions[num_partition_T])):\n",
    "            if label_s in labels[num_partition_T][num_cluster_t]:\n",
    "                total += proba(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S)\n",
    "                nb_computation += 1\n",
    "    if nb_computation == 0:\n",
    "        return 0\n",
    "    return total / nb_computation\n",
    "\n",
    "# Define a coeficient for the activity \n",
    "def activity(num_partition_S, num_partition_T):\n",
    "    res = 0\n",
    "    for num_cluster_s in range(0, len(partitions[num_partition_S])):\n",
    "        res += P_A(num_cluster_s, num_partition_T, num_partition_S)\n",
    "    return res / len(partitions[num_partition_S])\n",
    "\n",
    "# Standard deviation\n",
    "# Nothing have been find in the paper, so I put those random values ¯\\_(ツ)_/¯\n",
    "sigma_t = 0.01\n",
    "sigma_s = 0.01\n",
    "\n",
    "# Our Graal\n",
    "# Does cluster_t is similar to cluster_s?\n",
    "def similar(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S):\n",
    "    cond1 = proba(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S) > P_A(num_cluster_s, num_partition_T, num_partition_S)\n",
    "    cond2 = proba(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S) > activity(num_partition_S, num_partition_T) + sigma_s\n",
    "    \n",
    "    cond1 = proba(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S) > P_A(num_cluster_s, num_partition_T, num_partition_S)\n",
    "    cond2 = proba(num_cluster_t, num_cluster_s, num_partition_T, num_partition_S) > activity(num_partition_T, num_partition_S) + sigma_t\n",
    "    return cond1 and cond2 and cond3 and cond4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar(3, 3, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hotspot', 0.02679405983234753), ('relation', 0.006417877842461409), ('photographie', 0.006326824184650093), ('détecter', 0.0038242003822581004), ('approcher extraire relation', 0.003349257479043441)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.02679405983234753"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(labels[0][1])\n",
    "labels[0][1][0][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
