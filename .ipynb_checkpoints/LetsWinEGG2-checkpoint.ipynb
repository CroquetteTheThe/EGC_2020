{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy\n",
    "import string\n",
    "import math\n",
    "\n",
    "from numpy import array\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from gensim.test.utils import common_dictionary, common_corpus\n",
    "from gensim.models import LsiModel\n",
    "from gensim import corpora, models, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Lemmatize correctly, this lemmatizer is really basic, it doesn't recognize verbs and nouns\n",
    "sw = [x.replace('\\n', '') for x in open('stopwords-fr.txt', mode=\"r\", encoding=\"utf-8\").readlines()] + stopwords.words('french') + stopwords.words('english')\n",
    "lemmatizer = FrenchLefffLemmatizer()\n",
    "stemmer = nltk.stem.snowball.FrenchStemmer()\n",
    "\n",
    "#stem, lemmatize, remove punctuation, and return a string\n",
    "def process(article):\n",
    "    res = \"\"\n",
    "    for word in word_tokenize(article):\n",
    "        if not word in sw+string.punctuation.split():\n",
    "            if res == \"\": #if first word\n",
    "                res += stemmer.stem(lemmatizer.lemmatize(word.lower()))\n",
    "            else:\n",
    "                res += \" \" + stemmer.stem(lemmatizer.lemmatize(word.lower()))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('export_articles_EGC_2004_2018.csv', sep='\\t', header=0)\n",
    "\n",
    "#Let's process our corpus, and determine a limit to split it in partitions\n",
    "limit = 0\n",
    "usable = []\n",
    "for i in range(0, len(data['abstract']), 1):\n",
    "    if not isinstance(data['abstract'][i], float): #if not null, empty, or whatever (so if there is a abstract)\n",
    "        usable.append(process(data['abstract'][i]))\n",
    "        if data['year'][i] <= 2010:\n",
    "            limit += 1\n",
    "    \n",
    "print(\"nombre d'articles =\", len(usable))\n",
    "print(\"limit =\", limit)\n",
    "\n",
    "usable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfIdf\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=sw, use_idf=True, ngram_range=(1,3))\n",
    "tfidf = vectorizer.fit_transform(usable)\n",
    "\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This was a try to use gensim. However, I don't success to use it (not yet! >:)c )\n",
    "\n",
    "nb_concepts = 30\n",
    "lsi = LsiModel(tfidf, num_topics=nb_concepts)\n",
    "\n",
    "lsi.print_topics(num_topics=2, num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then this is my previous method, I have written a method getlsa(), but it wasn't that good. Use gensim ;)\n",
    "\n",
    "nbconcepts = 10\n",
    "lsa = getlsa(nbconcepts, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before, I though it would be nice to split the document-concept matrix. But it isn't a good idea, cf roadmap\n",
    "\n",
    "lsa1 = numpy.transpose(lsa)[0:limit]\n",
    "lsa2 = numpy.transpose(lsa)[limit:]\n",
    "lsa2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nous obtenons ici x clusters, représentés par des concepts grâce au LSA\n",
    "#### Nous allons seuiller et étiquetter ces clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, I determine a tresh, to remove documents from clusters\n",
    "#Thus, I tried to compute the X² of the values for each cluster. This X² was my tresh\n",
    "#Yeah yeah yeah. I know, X² is a law, what I compute is just a value to compare to this law\n",
    "#But I find this more pictured to talk about it this way\n",
    "#PS: (don't trust this snippet, I think it's wrong)\n",
    "\n",
    "partitions = []\n",
    "tresh = 0\n",
    "\n",
    "for lsa_part in [lsa1, lsa2]:\n",
    "    # X² pour ne garder que les articles au dessus d'un certain seuil\n",
    "    temp = 0\n",
    "    sumConcept = 0\n",
    "    for concept in transpose(lsa):\n",
    "        for article in concept:\n",
    "            sumConcept += article\n",
    "            \n",
    "            \n",
    "            tresh += pow(article, 2)\n",
    "    tresh /= (lsa_part.shape[0]*lsa_part.shape[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A présent, passons au k-mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No need to comment here. Functionnal kmean, proud David xD\n",
    "def kmeans(k, composants):\n",
    "    nb_concepts = composants.shape[1]\n",
    "\n",
    "    #init\n",
    "    centroids = []\n",
    "    cluster = []\n",
    "    for _ in range(1, k+1, 1):\n",
    "        curr_centroid = []\n",
    "        for n in range(1, nb_concepts+1, 1):\n",
    "            curr_centroid.append((random.random()*2 -1)/1000)\n",
    "        centroids.append(curr_centroid)\n",
    "        cluster.append([])\n",
    "\n",
    "    #loop\n",
    "    for _ in range(0, 5, 1):\n",
    "        print(\"---\")\n",
    "        print(len(cluster[0]))\n",
    "        print(len(cluster[1]))\n",
    "        print(len(cluster[2]))\n",
    "        for i in range(0, k, 1):\n",
    "            cluster[i] = []\n",
    "        #determine clusters\n",
    "        for doc in composants:\n",
    "            distanceMin = 10000000\n",
    "            posMin = -1\n",
    "            pos = 0\n",
    "            for c in centroids:\n",
    "                #calcule de la distance\n",
    "                dist = 0\n",
    "                for i in range(0, nb_concepts, 1):\n",
    "                    x_c = c[i]\n",
    "                    x_doc = doc[i]\n",
    "                    dist += pow(x_c - x_doc, 2)\n",
    "                dist = math.sqrt(dist)\n",
    "                if dist < distanceMin:\n",
    "                    distanceMin = dist\n",
    "                    posMin = pos\n",
    "                pos += 1\n",
    "            cluster[posMin].append(doc)\n",
    "\n",
    "        #rearrange centroids\n",
    "        for i in range(0, k, 1):\n",
    "            c = centroids[i]\n",
    "            clus = cluster[i]\n",
    "            #on fait la moyenne des points\n",
    "            if len(clus) != 0:\n",
    "                for axe in range(0, nb_concepts, 1):\n",
    "                    newPosition = 0;\n",
    "                    for doc in clus:\n",
    "                        newPosition += doc[axe]\n",
    "                    newPosition/=len(clus)\n",
    "                    centroids[i][axe] = newPosition\n",
    "    return cluster\n",
    "\n",
    "\n",
    "\n",
    "len(kmeans(3, numpy.transpose(lsa))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean1 = kmeans(5, lsa1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexOf(matrixSrc, listSearched):\n",
    "    if (len(matrixSrc[1]) != len(listSearched)):\n",
    "        return -1\n",
    "    pos = 0\n",
    "    for l in matrixSrc:\n",
    "        thesame = True\n",
    "        for i in range(0, len(listSearched), 1):\n",
    "            if l[i] != listSearched[i]:\n",
    "                thesame = False\n",
    "                break\n",
    "        if thesame:\n",
    "            return pos\n",
    "        pos+=1\n",
    "    return -2\n",
    "a = [[1, 2, 3], [4, 2, 1], [2, 3, 5], [3, 2, 1]]\n",
    "b = [3,2,1]\n",
    "indexOf(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I know, transpose exists in numpy\n",
    "#However, it didn't work with me because I got sparse matrix, so I was deseperate and I wrote my own function\n",
    "def transpose(matrix):\n",
    "    transp = []\n",
    "    for j in range(0, matrix.shape[1], 1):\n",
    "        l = []\n",
    "        for i in range(0, matrix.shape[0], 1):\n",
    "            l.append(matrix[i][j])\n",
    "        transp.append(l)\n",
    "    return transp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kmean1[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostCommons(dico, tresh):\n",
    "    res = []\n",
    "    for name in dico:\n",
    "        if dico[name] >= tresh:\n",
    "            res.append(name)\n",
    "    return res\n",
    "\n",
    "test = {\"abc\": 3, \"trucmuch\":5, \"doge\":0}\n",
    "mostCommons(test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT!! The good way to get an element from document-term matrix\n",
    "tfidf.toarray().item(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to get the most important terms of each cluster\n",
    "#For this, I added each value of the document-term matrix for a term\n",
    "importanceWords = []\n",
    "for i in range(0, tfidf.shape[1], 1):\n",
    "    importanceWords.append(0.)\n",
    "\n",
    "labels = []\n",
    "for cluster in kmean1:\n",
    "    for point in cluster:\n",
    "        numDoc = indexOf(transpose(lsa), point)\n",
    "        if numDoc == -1:\n",
    "            print(\"numDoc est égal à -1\")\n",
    "        for i in range(0, tfidf[numDoc].shape[0], 1):\n",
    "            importanceWords[i] += tfidf.toarray().item(numDoc, i)\n",
    "    \n",
    "    res = dict(zip(vectorizer.get_feature_names(), importanceWords))\n",
    "    print(importanceWords[0:10])\n",
    "    labels.append(mostCommons(res, 0.2))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa2 = getlsa(42, part2)\n",
    "kmean2 = kmeans(5, lsa2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A présent, nous étiquettons nos clusters. Pour cela, nous n'avons qu'à prendre les n premiers termes du tfidf de chaque document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lol what did I try here? x)\n",
    "for lsa in kmeans1:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ignore this x)\n",
    "\n",
    "TODOlist:\n",
    "    - étiquetter les clusters\n",
    "    - faire l'analyse diachronique"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
