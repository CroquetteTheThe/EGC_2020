	series	booktitle	year	title	abstract	authors	pdf1page	pdfarticle	abstract_prepro
0	Revue des Nouvelles Technologies de l'Information	EGC	2018	#Idéo2017 : une plateforme citoyenne dédiée à l'analyse des tweets lors des événements politiques	Cette plateforme a pour objectif de permettre aux citoyens d'analyserpar eux-mêmes les tweets politiques lors d'événements spécifiques en France.Pour le cas de l'élection présidentielle de 2017, #Idéo2017 analysait en quasitemps réel les messages des candidats, et fournissait leurs principales caractéristiques,l'usage du lexique politique et des comparaisons entre les candidats.	Claudia Marinica, Julien Longhi, Nader Hassine, Abdulhafiz Alkhouli, Boris Borzic	http://editions-rnti.fr/render_pdf.php?p1&p=1002425	http://editions-rnti.fr/render_pdf.php?p=1002425	plateforme objectif permettre citoyen danalyserpar euxmême tweet politique dévénement spécifique francepour cas lélection présidentiel 2017 idéo2017 analyser quasitemp réel message candidat fournir principal caractéristiqueslusage lexiqu politique comparaison entrer candidat
1	Revue des Nouvelles Technologies de l'Information	EGC	2018	A two level co-clustering algorithm for very large data sets	La classification croisée (co-clustering) est une technique qui permet d'extraire la structuresous-jacente existante entre les lignes et les colonnes d'une table de données sous forme de blocs. Plusieurs applications utilisent cette technique, cependant de nombreux algorithmes de co-clustering actuels ne passent pas à l'échelle. Une des approches utilisées avec succès est la méthode MODL, qui optimise un critère de vraisemblance régularisée. Cependent, pour des tailles plus importante, cette méthode atteint sa limite. Dans cet article, nous présentons un nouvel algorithme de co-clustering à deux niveaux, qui compte tenu du critère MODL permet de traiter efficacement de données de très grande taille, ne pouvant pas tenir en mémoire. Nos expériences montrent que l'approche proposée gagne en temps de calcul tout en produisant des solutions de qualité.	Marius Barctus, Marc Boullé, Fabrice Clérot	http://editions-rnti.fr/render_pdf.php?p1&p=1002372	http://editions-rnti.fr/render_pdf.php?p=1002372	classification croisé coclustering technique permettre dextraire structuresousjacente existant entrer ligne colonne dune tabler donnée sou former bloc application utiliser technique algorithme coclustering actuel passer léchelle approche utiliser succès méthode modl optimiser critère vraisemblance régulariser cependent taille plaire important méthode atteindre limiter Dans article présenter nouvel algorithme coclustering niveau compter critère modl permettre traiter efficacement donnée grand tailler pouvoir mémoire expérience montrer lapproche proposer gagner temps calcul produire solution qualité
2	Revue des Nouvelles Technologies de l'Information	EGC	2018	ALGeoSPF: Un modèle de factorisation basé sur du clustering géographique pour la recommandation de POI	La recommandation de points d'intérêts est devenue une caractéristiqueessentielle des réseaux sociaux géo-localisés qui a accompagnél'émergence des échanges massifs de données digitales. Cependantles faibles densités de points d'intérêts visités par les utilisateurs rendentle problème difficile à traiter, d'autant plus que les espaces de mobilitédes utilisateurs sont très hétérogènes, allant de la ville au monde entier.Dans ce papier nous explorons l'impact d'une approche de clusteringspatial sur la qualité de la recommandation. Notre approche est baséesur un modèle de factorisation de matrices de Poisson et un réseau socialinféré des différents comportements de mobilité. Nous avons conduitune évaluation comparative des performances de notre approche sur unjeu de données réaliste. Les résultats expérimentaux montrent que notreapproche permet une précision supérieure aux techniques de recommandationalternatives.	Jean-Benoît Griesner, Talel Abdesssalem, Hubert Naacke, Pierre Dosne	http://editions-rnti.fr/render_pdf.php?p1&p=1002380	http://editions-rnti.fr/render_pdf.php?p=1002380	recommandation point dintérêts devenir caractéristiqueessentielle réseau social géolocaliser accompagnélémergence échange massif donnée digital cependantle faible densité point dintérêts visiter utilisateur rendentl problème difficile traiter dautant plaire espace mobilitéde utilisateur hétérogène aller ville monder entierDans papier explorer limpact dune approcher clusteringspatial qualité recommandation approcher baséesur modeler factorisation matrice Poisson réseau socialinféré comportement mobilité conduitune évaluation comparatif performance approcher unjeu donnée réaliste résultat expérimental montrer notreapproche permettre précision supérieur technique recommandationalternativ
3	Revue des Nouvelles Technologies de l'Information	EGC	2018	Analyse des sentiments à partir des commentaires Facebook publiés en Arabe standard ou dialectal marocain par une approche d'apprentissage automatique	L'analyse des sentiments est un processus pendant lequel la polarité(positive, négative ou neutre) d'un texte donné est déterminée. Nous nous intéressonsdans ce travail à l'analyse des sentiments à partir des commentairesFacebook, réels, partagés en arabe standard ou dialectal marocain par une approchebasée sur l'apprentissage automatique. Ce processus commence par lacollecte des commentaires et leur annotation à l'aide du crowdsourcing suivid'une phase de prétraitement du texte afin d'extraire des mots arabes réduits àleur racine. Ces mots vont être utilisés pour la construction des variables d'entréeen utilisant plusieurs combinaisons de schémas d'extraction et de pondération.Pour réduire la dimensionnalité, une méthode de sélection de variables est appliquée.Les résultats obtenus des expérimentations sont très prometteurs.	Abdeljalil Elouardighi, Mohcine Maghfour, Hafdalla Hammia, Fatima-Zahra Aazi	http://editions-rnti.fr/render_pdf.php?p1&p=1002397	http://editions-rnti.fr/render_pdf.php?p=1002397	lanalyse sentiment processus pendre polaritépositive négatif neutre dun texte donner déterminer intéressonsdans travail lanalyse sentiment partir commentairesfacebook réel partagé arabe standard dialectal marocain approchebasée lapprentissage automatique processus commencer lacollecte commentaire annotation laid crowdsourcing suividun phase prétraitement texte dextraire arabe réduire àleur racin aller utiliser construction variable dentréeen utiliser combinaison schéma dextraction pondérationpour réduire dimensionnalité méthode sélection variable appliquéeles résultat obtenir expérimentation prometteur
4	Revue des Nouvelles Technologies de l'Information	EGC	2018	Analyse en rôles sémantiques pour le résumé automatique	Cet article présente une approche visant à extraire les informations expriméesdans un corpus de textes et en produire un résumé. Plusieurs variantes deméthodes extractives de résumé de texte ont été implémentées et évaluées. Leurprincipale originalité réside dans l'exploitation de structures appelées CDS (pourClause Description Structure) issues d'un composant d'annotation en rôles sémantiqueset non directement des phrases composant les textes. Le résumé obtenuest un sous-ensemble des CDS issus du corpus d'origine ; ce format permettradans la suite la détection d'incohérences textuelles. Dans ce travail, nous retransformonsles CDS résumés en texte pour permettre la comparaison de notreapproche avec celles de la littérature. Les premiers résultats sont très encourageants: les variantes que nous proposons obtiennent généralement de meilleursscores que des implémentations de méthodes de référence.	Elyase Lassouli, Yasmine Mesbahi, Camille Pradel, Damien Sileo	http://editions-rnti.fr/render_pdf.php?p1&p=1002384	http://editions-rnti.fr/render_pdf.php?p=1002384	article présenter approcher viser extraire information expriméesdans corpus texte produire résumer variante deméthode extractif résumer texte implémenter évaluée leurprincipal originalité résider dan lexploitation structure appeler CDS pourclaus description structurer issu dun composer dannotation rôle sémantiqueset phrase composer texte résumer obtenuest sousensemble cd issu corpus dorigine   format permettradans suite détection dincohérence textuel Dans travail retransformonsl CDS résumer texte permettre comparaison notreapproche littérature résultat encourageant variante proposer obtenir généralement meilleursscore implémentation méthode référence
5	Revue des Nouvelles Technologies de l'Information	EGC	2018	Analyse Ontologique de scénario dans un contexte Big Data		Marwan Batrouni, Aurélie Bertaux, Christophe Nicolle	http://editions-rnti.fr/render_pdf.php?p1&p=1002414	http://editions-rnti.fr/render_pdf.php?p=1002414	
6	Revue des Nouvelles Technologies de l'Information	EGC	2018	Apport de la fouille de données pour la prévention du risque suicidaire	Avec plus de 800 000 décès par an dans le monde, le suicide est latroisième cause de décès évitable. Il y a 20 fois plus de tentatives, impliquant denombreuses hospitalisations, des coûts humains et sociétaux énormes. Ces dernièresannées, les modalités de collecte de données, sociologiques et cliniques,concernant les patients reçus en consultation après une tentative, ont connu deprofonds changements liés aux outils numériques. Nous présentons les principauxrésultats d'un processus complet de fouille de données sur un échantillonde suicidants de deux hôpitaux européens. Le premier objectif est d'identifierdes groupes de patients similaires et le second d'identifier des facteurs de risqueassociés au nombre de tentatives. Des méthodes non supervisées (ACM et clustering)et supervisées (arbres de régression) sont appliquées pour y répondre.Les résultats mettent en lumière l'apport de la fouille de données à des fins descriptivesou explicatives.	Romain Billot, Sofian Berrouiguet, Mark Larsen, Michel Walter, Jorge López Castroman, Enrique Baca-García, Philippe Courtet, Philippe Lenca	http://editions-rnti.fr/render_pdf.php?p1&p=1002376	http://editions-rnti.fr/render_pdf.php?p=1002376	Avec plaire 800 000 décès an dan monder suicider latroisièm causer décès évitable yu 20 plaire tentative impliquer denombreus hospitalisation coût humain sociétal énorme dernièresannée modalité collecter donnée sociologique cliniquesconcerner patient recevoir consultation tentative connaître deprofond changement lier outil numérique présenter principauxrésultat dun processus complet fouiller donnée échantillonde suicidant hôpital européen objectif didentifierder groupe patient similaire second didentifier facteur risqueassocié nombre tentative méthode superviser acm clusteringet superviser arbr régression appliquer yu répondreles résultat mettre lumière lapport fouiller donnée fin descriptivesou explicatif
7	Revue des Nouvelles Technologies de l'Information	EGC	2018	Apport des modèles locaux pour les K-moyennes prédictives	Dans le cadre du clustering prédictif, pour attribuer la classe aux groupesformés à la fin de la phase d'apprentissage, le vote majoritaire est la méthodecommunément utilisée. Cependant, cette approche comporte certaines limitationsqui influent directement sur la qualité des résultats obtenus en termes deprédiction. Pour surmonter ce problème, nous proposons d'incorporer des modèlesprédictifs localement dans les clusters formés afin d'améliorer la qualitéprédictive du modèle global. Les résultats expérimentaux montrent que cette incorporationpermet d'obtenir des résultats (en termes de prédiction) significativementmeilleurs par rapport à ceux obtenus en utilisant le vote majoritaire ainsique des résultats très compétitifs avec ceux obtenus par des algorithmes performantsd'apprentissage supervisé “similaires”. Ceci est effectué sans dégrader lepouvoir descriptif (explicatif) du modèle global.	Vincent Lemaire, Oumaima Alaoui Ismaili	http://editions-rnti.fr/render_pdf.php?p1&p=1002379	http://editions-rnti.fr/render_pdf.php?p=1002379	Dans cadrer clustering prédictif attribuer classer groupesformer fin phase dapprentissage voter majoritaire méthodecommunément utiliser approcher comporter limitationsqui influer qualité résultat obtenir terme deprédiction Pour surmonter problème proposer dincorporer modèlesprédictif localement dan cluster former daméliorer qualitéprédictive modeler global résultat expérimental montrer incorporationpermet dobtenir résultat terme prédiction significativementmeilleur rapport obtenu utiliser voter majoritaire ainsiqu résultat compétitif obtenir algorithme performantsdapprentissage superviser “ similaire ” effectuer dégrader lepouvoir descriptif explicatif modeler global
8	Revue des Nouvelles Technologies de l'Information	EGC	2018	Apprendre les relations de préférence et de co-occurrence entre les labels en classification multi-labels	En classification multi-labels, chaque instance est associée àun ou plusieurs labels. Par exemple, un morceau de musique peut êtreassocié aux labels 'heureux' et 'relaxant'. Des relations de co-occurrencepeuvent exister entre les labels : par exemple, les labels 'heureux' et 'triste'ne peuvent pas être associés au même morceau de musique. Les labelspeuvent aussi avoir des relations de préférence : par exemple, pour un morceaude musique contenant plusieurs piques, le label 'heureux' est préférépar rapport au label 'relaxant'. Les relations entre les labels peuvent aiderà mieux prédire les labels associés aux instances. Les approches existantespeuvent apprendre soit les relations de co-occurrence, soit les relationsde préférence. Ce travail introduit une approche permettant de combinerl'apprentissage des deux types de relations. Les expérimentations menéesmontrent que la nouvelle approche introduite offre les meilleurs résultatsde prédiction par rapports à cinq approches de l'état de l'art.	Khalil Laghmari, Christophe Marsala, Mohammed Ramdani	http://editions-rnti.fr/render_pdf.php?p1&p=1002381	http://editions-rnti.fr/render_pdf.php?p=1002381	En classification multilabel instance associer àun label Par exemple morceau musiquer pouvoir êtreassocié label heureux relaxer relation cooccurrencepeuvent exister entrer label   exemple label heureux tristen pouvoir associer morceau musiqu labelspeuver relation préférence   exemple morceaude musiqu contenir pique label heureux préférépar rapport label relaxer relation entrer label pouvoir aiderà mieux prédir label associé instance approche existantespeuvent relation cooccurrence relationsd préférenc travail introduire approcher permettre combinerlapprentissage type relation expérimentation menéesmontrent approcher introduire offrir meilleur résultatsd prédiction rapport approche létat lart
9	Revue des Nouvelles Technologies de l'Information	EGC	2018	Approche contextuelle par régression pour les tests A/B	Les tests A/B sont des procédures utilisées par les entreprises du webet de la santé entre autres, pour mesurer l'impact d'un changement de versiond'une variable par rapport à un objectif. Bien qu'un nombre de plus en plusimportant de données soit disponible, la mise en place concrète d'un tel testpeut impliquer un coût important relatif à l'observation et à l'évaluation d'unevariation lorsque celle-ci n'est pas optimale.Dans ce papier, nous présentons une nouvelle approche intégrant le principed'un bandit contextuel prenant en compte ces variables via une procédure destratification.	Emmanuelle Claeys, Pierre Gançarski, Myriam Maumy-Bertrand	http://editions-rnti.fr/render_pdf.php?p1&p=1002387	http://editions-rnti.fr/render_pdf.php?p=1002387	test AB procédure utiliser entreprise webet santé entrer mesurer limpact dun changement versiondune variable rapport objectif quun nombre plaire plusimportant donnée disponible miser placer concret dun testpeut impliquer coût importer relatif lobservation lévaluation dunevariation celleci nest optimaleDans papier présenter approcher intégrer principedun bandit contextuel prendre compter variable procédure destratification
10	Revue des Nouvelles Technologies de l'Information	EGC	2018	Big Data for understanding human dynamics: the power of networks		Fosca Giannotti	http://editions-rnti.fr/render_pdf.php?p1&p=1002363	http://editions-rnti.fr/render_pdf.php?p=1002363	
11	Revue des Nouvelles Technologies de l'Information	EGC	2018	Cartes Auto-Organisatrices Incrémentales appliquées au Clustering Collaboratif	Le Clustering Collaboratif (CC) vise à faire ressortir les structurescommunes présentes dans plusieurs vues indépendantes en se basant sur unepremière étape de clustering locale, effectuée dans notre cas à l'aide de CartesAuto-Organisatrices (SOM pour Self Organizing Maps en anglais). Pour faireface à la quantité toujours croissante de données disponibles, l'utilisation de méthodesde clustering incrémentales est devenue nécessaire. Ce papier présente unalgorithme de SOM incrémentales compatibles avec les contraintes du CC. Lesexpérimentations conduites sur plusieurs jeux de données démontrent la validitéde cette méthode et présentent l'influence de la taille du batch utilisé lors del'apprentissage.	Denis Maurel, Jérémie Sublime, Sylvain Lefebvre	http://editions-rnti.fr/render_pdf.php?p1&p=1002418	http://editions-rnti.fr/render_pdf.php?p=1002418	Clustering Collaboratif CC viser faire ressortir structurescommune présenter dan vue indépendant baser unepremière étape clustering local effectuer dan cas laid cartesautoorganisatrice SOM Self Organizing Maps anglais Pour faireface quantité croissant donnée disponible lutilisation méthodesde clustering incrémental devenir nécessaire papier présenter unalgorithm som incrémental compatible contrainte CC Lesexpérimentations conduire jeu donnée démontrer validitéde méthode présenter linfluence tailler batch utiliser delapprentissage
12	Revue des Nouvelles Technologies de l'Information	EGC	2018	Catégorisation d'articles scientifiques basée sur les relations sémantiques des mots-clés		Bastien Latard, Jonathan Weber, Germain Forestier, Michel Hassenforder	http://editions-rnti.fr/render_pdf.php?p1&p=1002406	http://editions-rnti.fr/render_pdf.php?p=1002406	
13	Revue des Nouvelles Technologies de l'Information	EGC	2018	Classification de Données Complexes par Globalisation de Mesures de Similarité via les Moyennes Quasi-Arithmétiques	La plupart des méthodes de classification sont conçues pour des types particuliers de données: données numériques, textuelles, catégoriques, fonctionnelles, probabilistes ou encore de type graphes. Cependant, les données générées dans notre quotidien sont en général composées de données de types mixtes. Par exemple, si nous considérons la prévention cardiaque dans le domaine de la santé, les applications vont combiner des données issues de capteurs avec d'autres données telles que l'âge, le niveau d'effort, la fréquence cardiaque maximale, des histogrammes de fréquences cardiaques moyennes lors de précédents efforts, etc. Ceci nous amène à la problématique de construire des classes en tenant compte de ces différentes données, et de définir une mesure de similarité à partir des similarités de paires d'objets sur les différents types de variables. Dans cet article nous proposons une méthode de classification basée sur la fusion des matrices de similarité à l'aide des moyennes quasi-arithmétiques qui permet de choisir les différentes “dimensions” des données à considérer, et ce quel que soit le type de données, pour autant qu'une mesure, de similarité ou de dissimilarité existe pour chacun des types de données, ce qui est très souvent le cas.	Étienne-Cuvelier, Marie-Aude-Aufaure	http://editions-rnti.fr/render_pdf.php?p1&p=1002368	http://editions-rnti.fr/render_pdf.php?p=1002368	méthode classification conçu type donnée donnée numérique textuel catégorique fonctionnel probabiliste typer graphe donnée générer dan quotidien général composer donnée type mixte Par exemple considérer prévention cardiaque dan domaine santé application aller combiner donnée issu capteur dautr donnée lâge niveau deffort fréquence cardiaque maximal histogramme fréquence cardiaque moyenne précédent effort amener problématique construire classe compter donnée définir mesurer similarité partir similarité paire dobjet type variable Dans article proposer méthode classification basé fusion matrice similarité laid moyen quasiarithmétique permettre choisir “ dimension ” donnée considérer typer donnée autant quune mesurer similarité dissimilarité exister type donnée cas
14	Revue des Nouvelles Technologies de l'Information	EGC	2018	Community structure in complex networks		Santo Fortunato	http://editions-rnti.fr/render_pdf.php?p1&p=1002362	http://editions-rnti.fr/render_pdf.php?p=1002362	
15	Revue des Nouvelles Technologies de l'Information	EGC	2018	Comparaison de mesures de centralité basées sur les plus courts chemins dans les réseaux dynamiques	Définir l'importance des noeuds dans les réseaux statiques est unequestion de recherche très étudiée depuis de nombreuses années. Dernièrement,des adaptations des métriques classiques ont été proposées pour les réseaux dynamiques.Ces méthodes reposent sur des approches très différentes dans leurfaçon d'évaluer l'importance des noeuds à un instant donné. Il est donc nécessairede pouvoir les évaluer et les comparer. Dans cet article, nous comparonstrois approches existes pour mieux comprendre ce qui les différencie. Nous montronsque la nature des jeux de données influe grandement sur le comportementdes méthodes, et que pour certains d'entre eux, la notion d'importance n'est pastoujours pertinente.	Marwan Ghanem, Clémence Magnien, Fabien Tarissan	http://editions-rnti.fr/render_pdf.php?p1&p=1002416	http://editions-rnti.fr/render_pdf.php?p=1002416	définir limportance noeud dan réseau statique unequestion rechercher étudier année dernièrementd adaptation métrique classique proposer réseau dynamiquesCes méthode reposer approche dan leurfaçon dévaluer limportance noeud instant donner nécessairede pouvoir évaluer comparer Dans article comparonstrois approch exist mieux comprendre différencier montronsque nature jeu donnée influ grandement comportementde méthode dentre notion dimportanc nest pastoujours pertinent
16	Revue des Nouvelles Technologies de l'Information	EGC	2018	Complémentarités de représentations vectorielles pour la similarité sémantique	La tâche de similarité sémantique textuelle consiste à exprimer automatiquementun nombre reflétant la similarité sémantique de deux fragmentsde texte. Chaque année depuis 2012, les campagnes de SemEval déroulent cettetâche de similarité sémantique textuelle. Cet article présente une méthode associantdifférentes représentations vectorielles de phrases dans l'objectif d'améliorerles résultats obtenus en similarité sémantique. Notre hypothèse est que différentesreprésentations permettraient de représenter différents aspects sémantiques,et par extension, d'améliorer les similarités calculées, la principale difficultéétant de sélectionner les représentations les plus complémentaires pourcette tâche. Notre système se base sur le système vainqueur de la campagne de2015 ainsi que sur notre méthode de sélection par complémentarité. Les résultatsobtenus viennent confirmer l'intérêt de cette méthode lorsqu'ils sont comparésaux résultats de la campagne de 2016.	Julien Hay, Tim Van de Cruys, Philippe Muller, Bich-Liên Doan, Fabrice Popineau, Lyes Benamsili	http://editions-rnti.fr/render_pdf.php?p1&p=1002378	http://editions-rnti.fr/render_pdf.php?p=1002378	tâcher similarité sémantique textuel consister exprimer automatiquementun nombre refléter similarité sémantique fragmentsde texte année 2012 campagne SemEval dérouler cettetâche similarité sémantique textuel article présenter méthode associantdifférent représentation vectoriel phrase dan lobjectif daméliorerl résultat obtenir similarité sémantique hypothèse différentesreprésentation permettre représenter aspect sémantiqueset extension daméliorer similarité calculer principal difficultééter sélectionner représentation plaire complémentaire pourcette tâch système baser système vainqueur campagne de2015 méthode sélection complémentarité résultatsobtenu venir confirmer lintérêt méthode lorsquils comparésaux résultat campagne 2016
17	Revue des Nouvelles Technologies de l'Information	EGC	2018	Contextualisation de Singularités en Temps-Réel par Extraction de Connaissances du Web des Données	L'émergence de l'IoT et du traitement en temps-réel oblige les entreprises à considérer la détection d'anomalies comme un élément clé de leur activité. Afin de garantir une haute précision dans le processus de détection, des métadonnées fournissant un contexte spatio-temporel sur les mesures des capteurs sont nécessaires. Dans cet article, nous présentons un système générique qui aide à capturer, analyser, qualifier et stocker les informations contextuelles d'un domaine d'application donné. L'approche proposée est basée sur des méthodes sémantiques qui exploitent des ontologies pour évaluer la pertinence de l'information contextuelle. Après une description des composants principaux de l'architecture, la performance et la pertinence du système sont démontrées par une évaluation sur des ensembles de données du monde réel.	Badre Belabbess, Jérémy Lhez, Musab Bairat, Olivier Curé	http://editions-rnti.fr/render_pdf.php?p1&p=1002369	http://editions-rnti.fr/render_pdf.php?p=1002369	lémergence liot traitement tempsréel obliger entreprise considérer détection danomalier élément cler activité Afin garantir précision dan processus détection métadonnée fournir contexte spatiotemporel mesure capteur nécessaire Dans article présenter système générique aider capturer analyser qualifier stocker information contextuel dun domaine dapplication donner Lapproche proposer baser méthode sémantique exploiter ontologie évaluer pertinence linformation contextuel Après description composant principal larchitecture performance pertinence système démontrer évaluation ensemble donnée monder réel
18	Revue des Nouvelles Technologies de l'Information	EGC	2018	Contraintes prescriptives compatibles avec OWL2-ER pour évaluer la complétude d'ontologies	L'article définit les contraintes prescriptives comme des règles permettant aux moteurs d'inférence de vérifier que certains objets formels sont réellement utilisés – pas seulement inférés – ou non, dans certaines conditions. Il montre que ces contraintes nécessitent de ne pas exploiter de mécanisme d'héritage (ou autres mécanismes ajoutant des relations à des objets) durant les tests des conclusions des règles. Il donne une méthode générale pour effectuer cela et des commandes SPARQL pour implémenter cette méthode lorsque les règles sont représentées via des relations sous-classe-de entre conditions et conclusions. L'article illustre ces commandes avec la vérification de patrons de conception d'ontologies. Plus généralement, l'approche peut être utilisée pour vérifier la complétude d'une ontologie, ou représenter dans une ontologie (plutôt que par des requêtes ou des procédures ad hoc) des contraintes permettant de calculer un degré de complétude d'ontologie. L'approche peut ainsi aider l'élicitation, la modélisation ou la validation de connaissances.	Philippe Martin, Jun Jo	http://editions-rnti.fr/render_pdf.php?p1&p=1002366	http://editions-rnti.fr/render_pdf.php?p=1002366	larticle définir contrainte prescriptiver règle permettre moteur dinférence vérifier objet formel réellement utiliser – inférer – dan condition montrer contrainte nécessiter exploiter mécanisme dhéritage mécanisme ajouter relation objet durer test conclusion règle donner méthode général effectuer celer commande sparql implémenter méthode règle représenter relation sousclassede entrer condition conclusion Larticle illustrer commande vérification patron conception dontologier plaire généralement lapproch pouvoir utiliser vérifier complétude dune ontologie représenter dan ontologie requête procédure ad hoc contrainte permettre calculer degré complétude dontologie Lapproche pouvoir aider lélicitation modélisation validation connaissance
19	Revue des Nouvelles Technologies de l'Information	EGC	2018	Contribution à l'étude de la distributivité d'un treillis de concepts	Nous nous intéressons aux treillis distributifs dans le cadre de l'analyse formelle de concepts (FCA). La motivation primitive vient de la phylogénie et des graphes médians pour représenter les dérivations biologiques et les arbres parcimonieux. La FCA propose des algorithmes efficaces de construction de treillis de concepts. Cependant, un treillis de concepts n'est pas en correspondance avec un graphe médian sauf s'il est distributif, d'où l'idée d'étudier la transformation d'un treillis de concepts en un treillis distributif. Pour ce faire, nous nous appuyons sur le théorème de représentation de Birkhoff qui nous permet de systématiser la transformation d'un contexte quelconque en un contexte de treillis de concepts distributif. Ainsi, nous pouvons bénéficier de l'algorithmique de FCA pour construire mais aussi visualiser les treillis de concepts distributifs, et enfin étudier les graphes médians associés.	Alain Gély, Miguel Couceiro, Yassine Namir, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1002373	http://editions-rnti.fr/render_pdf.php?p=1002373	intéresser treillis distributif dan cadrer lanalyse formel concept FCA motivation primitif venir phylogénie graphe médian représenter dérivation biologique arbre parcimonieux FCA proposer algorithme efficace construction treillis concept treillis concept nest correspondance graphe médian sil distributif doù lider détudier transformation dun treillis concept treillis distributif Pour faire appuyer théorème représentation Birkhoff permettre systématiser transformation dun contexte contexte treillis concept distributif pouvoir bénéficier lalgorithmique FCA construire visualiser treillis concept distributif étudier graphe médian associé
20	Revue des Nouvelles Technologies de l'Information	EGC	2018	Découverte de motifs graduels partiellement ordonnés : application aux données d'expériences scientifiques	Les données séquentielles sont aujourd'hui omniprésentes etconcernent divers domaines d'application. La fouille de données de séquencespermet d'extraire des informations et des connaissances pouvant être à forte valeurajoutée. Cependant, lorsque les données de séquences sont riches en donnéesnumériques, des méthodes de fouille de données plus fines sont nécessairespour extraire des connaissances plus expressives représentant la variabilité desvaleurs numériques ainsi que leur éventuelle interdépendance. Dans cet article,nous présentons une nouvelle méthode de découverte de séquences graduellesfréquentes représentées par des graphes à partir d'une source de données de séquencesen RDF (Resource Description Framework 1). Ces dernières sont transforméesen graphes graduels partiellement ordonnés, gpo. Nous proposons unalgorithme permettant de découvrir les sous-graphes gpo fréquents. Une expérimentationsur deux jeux de données réelles ont montré la faisabilité et la pertinencede notre approche.	Simon Ser, Fatiha Saïs, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1002382	http://editions-rnti.fr/render_pdf.php?p=1002382	donnée séquentiel aujourdhui omniprésente etconcernent domaine dapplication fouiller donnée séquencespermet dextraire information connaissance pouvoir fort valeurajouter donnée séquence richer donnéesnumérique méthode fouiller donnée plaire fin nécessairespour extraire connaissance plaire expressif représenter variabilité desvaleur numérique éventuel interdépendance Dans articlenou présenter méthode découvrir séquence graduellesfréquent représenter graphe partir dune source donnée séquencesen RDF Resource description Framework 1 dernière transforméesen graphe graduel partiellement ordonner gpo proposer unalgorithm permettre découvrir sousgraphe gpo fréquent expérimentationsur jeu donnée réel montrer faisabilité pertinencede approcher
21	Revue des Nouvelles Technologies de l'Information	EGC	2018	Définir les catégories de DBpédia avec des règles d'associations et des redescriptions	DBpédia, qui encode les connaissances de Wikipédia, est devenue unebase de référence pour le web des données. Les ressources peuvent y être répertoriéespar des catégories définies manuellement, dont la sémantique n'est pasdirectement accessible par des machines. Dans cet article, nous proposons deremédier à cette lacune au moyen de méthodes de fouille de données, à savoirla recherche de règles d'associations et de motifs apparentés. Nous présentonsune étude comparative de ces variantes sur une partie de DBpédia et discutonsle potentiel des différentes approches.	Justine Reynaud, Esther Galbrun, Mehwish Alam, Yannick Toussaint, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1002398	http://editions-rnti.fr/render_pdf.php?p=1002398	dbpédia encoder connaissance Wikipédia devenir unebase référence web donnée ressource pouvoir yu répertoriéespar catégorie définir manuellemer sémantique nest pasdirectement accessible machine Dans article proposer deremédier lacune moyen méthode fouiller donnée savoirla rechercher règle dassociation motif apparenté présentonsune étude comparatif variante partir dbpédia discutonsle potentiel approche
22	Revue des Nouvelles Technologies de l'Information	EGC	2018	Détection de Singularités en temps-réel par combinaison d'apprentissage automatique et web sémantique basés sur Spark		Badre Belabbess, Musab Bairat, Jérémy Lhez, Olivier Curé	http://editions-rnti.fr/render_pdf.php?p1&p=1002408	http://editions-rnti.fr/render_pdf.php?p=1002408	
23	Revue des Nouvelles Technologies de l'Information	EGC	2018	Echantillonnage de motifs séquentiels sous contrainte sur la norme	L'échantillonnage de motifs est une méthode non-exhaustive pour découvrir des motifs pertinents qui assure une bonne interactivité tout en offrant des garanties statistiques fortes grâce à sa nature aléatoire. Curieusement, une telle approche explorée pour les motifs ensemblistes et les sous-graphes ne l'a pas encore été pour les données séquentielles. Dans cet article, nous proposons la première méthode d'échantillonnage de motifs séquentiels. Outre le passage aux séquences, l'originalité de notre approche est d'introduire une contrainte sur la norme pour maîtriser la longueur des motifs tirés et éviter l'écueil de la « longue traîne ». Nous démontrons que notre méthode fondée sur une procédure aléatoire en deux étapes effectue un tirage exact. Malgré le recours à un échantillonnage avec rejet, les expérimentations montrent qu'elle reste performante.	Lamine Diop, Cheikh Talibouya Diop, Arnaud Giacometti, Dominique Li, Arnaud Soulet	http://editions-rnti.fr/render_pdf.php?p1&p=1002367	http://editions-rnti.fr/render_pdf.php?p=1002367	léchantillonnage motif méthode nonexhaustiv découvrir motif pertinent assurer interactivité offrir garantie statistique fort grâce nature aléatoire curieusement approcher explorer motif ensembliste sousgraphe donnée séquentiel Dans article proposer méthode déchantillonnage motif séquentiel Outre passage séquence loriginalité approcher dintroduir contraint norme maîtriser longueur motif tirer éviter lécueil « long traîner » démontrer méthode fonder procédure aléatoire étape effectuer tirage exact Malgré recours échantillonnage rejet expérimentation montrer rester performant
24	Revue des Nouvelles Technologies de l'Information	EGC	2018	eDOI : exploration itérative de grands graphes multi-couches basée sur une mesure de l'intérêt de l'utilisateur		Antoine Laumond, Norbert Feron, Guy Melançon, Bruno Pinaud	http://editions-rnti.fr/render_pdf.php?p1&p=1002410	http://editions-rnti.fr/render_pdf.php?p=1002410	
25	Revue des Nouvelles Technologies de l'Information	EGC	2018	Elaboration et utilisation d'une base de connaissances d'un domaine technique.	Ce poster rend compte d'une entreprise d'élaboration d'un système de représentation des connaissances pour le domaine géotechnique.	Nicolas Faure, René-Michel Faure	http://editions-rnti.fr/render_pdf.php?p1&p=1002404	http://editions-rnti.fr/render_pdf.php?p=1002404	poster compter dune entreprendre délaboration dun système représentation connaissance domaine géotechnique
26	Revue des Nouvelles Technologies de l'Information	EGC	2018	Élimination des liens inter-langues erronés dans Wikipédia	Un lien inter-langue dans Wikipédia est un lien qui mène d'un articleappartenant à une édition linguistique à un autre article décrivant le mêmeconcept dans une autre langue. Ces liens sont ajoutés manuellement par les utilisateursdeWikipédia et ainsi ils sont susceptibles d'être erronés. Dans ce papier,nous proposons une approche pour l'élimination automatique des liens interlangues.Le principe de base est que la présence d'un lien erroné est révélée parl'existence d'un chemin de liens inter-langues reliant deux articles appartenant àune même édition linguistique. Notre approche élimine des liens inter-langues,à partir de ceux qui ont un faible score de correction, jusqu'à ce qu'il n'y aitplus de chemins entre deux articles d'une même édition linguistique. Les résultatsde notre évaluation sur un sous-graphe deWikipédia consistant en 8 languesmontre que l'approche est prometteuse.	Nacéra Bennacer Seghouani, Francesca Bugiotti, Jorge Galicia, Mariana Patricio, Gianluca Quercini	http://editions-rnti.fr/render_pdf.php?p1&p=1002417	http://editions-rnti.fr/render_pdf.php?p=1002417	lien interlangue dan Wikipédia lien mener dun articleappartener édition linguistique article décrire mêmeconcept dan langue lien ajouter manuellemer utilisateursdewikipédia susceptible dêtre erroner Dans papiernous proposer approcher lélimination automatique lien interlanguesl principe baser présence dun lien erroné révéler parlexistence dun chemin lien interlangu relier article appartenir àune édition linguistique approcher élimin lien interlanguesà partir faible score correction jusquà quil ny aitplus chemin entrer article dune édition linguistique résultatsde évaluation sousgraphe dewikipédier consister 8 languesmontre lapproche prometteur
27	Revue des Nouvelles Technologies de l'Information	EGC	2018	Et si les réseaux sociaux pouvaient nous aider dans nos choix de carrière?	Dans cet article, nous présentons une méthode d'analyse de corpusafin de générer deux interfaces originales de visualisation dans le domaine del'e-recrutement. Notre approche s'appuie sur des millions de profils issus deplusieurs réseaux sociaux et sur des milliers d'offres d'emploi collectées surInternet. Nous décrivons dans ces travaux les étapes nécessaires pour leur réalisation.La première visualisation est une carte dynamique indiquant les métiersqui recrutent, dans quel domaine, dans quelle région tandis que la seconde meten avant les parcours professionnels et permet d'observer les perspectives ainsique les antécédents à plus ou moins long terme pour chaque métier considéré.	Rémy Kessler, Guy Lapalme, Fabrizio Gotti, Abdessamad Outerqiss, Philippe Langlais	http://editions-rnti.fr/render_pdf.php?p1&p=1002391	http://editions-rnti.fr/render_pdf.php?p=1002391	Dans article présenter méthode danalyse corpusafin générer interface original visualisation dan domaine delerecrutemer approcher sappui million profil issu deplusieur réseau social millier doffr demploi collecter surInternet décrire dan travail étape nécessaire réalisationla visualisation carte dynamique indiquer métiersqui recruter dan domaine dan région second meten parcours professionnel permettre dobserver perspective ainsiqu antécédent plaire long terme métier considérer
28	Revue des Nouvelles Technologies de l'Information	EGC	2018	Étiquetage thématique automatisé de corpus par représentation sémantique	Dans les corpus de textes scientifiques, certains articles issus de communautésde chercheurs différentes peuvent ne pas être décrits par les mêmesmots-clés alors qu'ils partagent la même thématique. Ce phénomène cause desproblèmes dans la recherche d'information, ces articles étant mal indexés, etlimite les échanges potentiellement fructueux entre disciplines scientifiques.Notre modèle permet d'attribuer automatiquement une étiquette thématique auxarticles au moyen d'un apprentissage des représentations sémantiques d'articlesdu corpus déjà étiquetés. Passant bien à l'échelle, cette méthode a pu être testéesur une bibliothèque numérique d'articles scientifiques comportant des millionsde documents. Nous utilisons un réseau sémantique de synonymes pour extrairedavantage d'articles sémantiquement similaires et nous les fusionnons avec ceuxobtenus par un modèle de classement thématique. Cette méthode combinée présentede meilleurs taux de rappel que les versions utilisant soit le réseau sémantiqueseul, soit la seule représentation sémantique des textes.	Lucie Martinet, Hussein T. Al-Natsheh, Fabien Rico, Fabrice Muhlenbach, Djamel A. Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1002396	http://editions-rnti.fr/render_pdf.php?p=1002396	Dans corpus texte scientifique article issu communautésde chercheur pouvoir décrire mêmesmotsclé quils partager thématique phénomène causer desproblème dan rechercher dinformation article mal indexé etlimite échange potentiellement fructueux entrer discipline scientifiquesNotre modeler permettre dattribuer automatiquement étiqueter thématique auxarticl moyen dun apprentissage représentation sémantique darticlesdu corpu déjà étiqueter passer léchelle méthode pouvoir testéesur bibliothèque numérique darticl scientifique comporter millionsde document utiliser réseau sémantique synonyme extrairedavantage darticl sémantiquement similaire fusionnon ceuxobtenus modeler classement thématique méthode combiner présentede meilleur taux rappel version utiliser réseau sémantiqueseul représentation sémantique texte
29	Revue des Nouvelles Technologies de l'Information	EGC	2018	Évaluation comparative d'algorithmes de centralité pour la détection d'influenceurs		Kévin Deturck, Damien Nouvel, Frédérique Segond	http://editions-rnti.fr/render_pdf.php?p1&p=1002405	http://editions-rnti.fr/render_pdf.php?p=1002405	
30	Revue des Nouvelles Technologies de l'Information	EGC	2018	Exploration et analyses multi-objectifs de séries temporelles de données météorologiques	Cet article présente les investigations menées sur les donnéesmesurées par des capteurs positionnés dans cinq villes de l'île de laRéunion. Des analyses exploratoires préalables permettent de comparer lescaractéristiques statistiques des villes considérées relativement aux différentesvariables météorologiques mesurées (flux solaires diffus et global, pressionatmosphérique, humidité, température, force et direction du vent). Nousappliquons diverses transformations sur les données avant d'analyser les sériesunivariées ou multivariées agrégées au pas de l'heure ou de la journée afin deconstruire des modèles de prédiction. Une approche classique de clusteringde séries temporelles est testée. Deux algorithmes de biclustering appliquéssuccessivement ont permis de grouper les journées d'observations partageantdes paramètres météorologiques horaires. Une caractérisation des biclusters, unevisualisation calendaire de leur succession ainsi qu'une recherche de séquencesfréquentes permettent d'exploiter les résultats et de faciliter leur interprétation.	Yelen Per, Kevin Dalleau, Malika Smail-Tabbone	http://editions-rnti.fr/render_pdf.php?p1&p=1002421	http://editions-rnti.fr/render_pdf.php?p=1002421	article présenter investigation mener donnéesmesurée capteur positionner dan ville lîle laréunion analyse exploratoire préalable permettre comparer lescaractéristiqu statistique ville considérer différentesvariabl météorologique mesuré flux solaire diffus global pressionatmosphériqu humidité température forc direction vent nousappliquon transformation donnée danalyser sériesunivariée multivarier agréger lheure journée deconstruire modèle prédiction approcher classique clusteringde série temporel tester Deux algorithme biclustering appliquéssuccessivement permettre grouper journé dobservation partageantd paramètre météorologique horaire caractérisation bicluster unevisualisation calendaire succession quune rechercher séquencesfréquente permettre dexploiter résultat faciliter interprétation
31	Revue des Nouvelles Technologies de l'Information	EGC	2018	Extraction de chaînes cohérentes en vue de reconstuire la Trajectoire de l'information	Sur Internet, l'information se propage en particulier au travers des documentstextuels. Cette propagation soulève de nombreux défis : identifier uneinformation, suivre son évolution dans le temps, comprendre les mécanismes quirégissent sa propagation, etc. Étant donné un document parmi un grand corpusdans lequel de nombreuses informations circulent, pouvons-nous retrouver leschemins empruntés par l'information pour arriver à ce document ? Nous proposonsde définir la notion de trajectoire comme l'ensemble des chemins le longdesquels de l'information s'est propagée et nous proposons une méthode pourl'estimer. Nous avons mis en oeuvre une évaluation humaine pour juger de laqualité des chemins calculés. Nous montrons que les évaluations concordent laplupart du temps et que notre algorithme est efficace pour retrouver les bonschemins.	Charles Huyghues-Despointes, Leila Khouas, Julien Velcin, Sabine Loudcher	http://editions-rnti.fr/render_pdf.php?p1&p=1002395	http://editions-rnti.fr/render_pdf.php?p=1002395	Sur Internet linformation propager travers documentstextuel propagation soulèv défi   identifier uneinformation évolution dan temps comprendre mécanisme quirégissent propagation donner document grand corpusdan information circuler pouvonsnous retrouver leschemins emprunter linformation document   proposonsde définir notion trajectoire lensembl chemin longdesquel linformation sest propager proposer méthode pourlestimer mettre oeuvrer évaluation humain juger laqualité chemin calculé montrer évaluation concorder laplupart temps algorithme efficace retrouver bonschemin
32	Revue des Nouvelles Technologies de l'Information	EGC	2018	Extraction de connaissances sur les défaillances de compteurs d'essieux	Cet article propose une méthode d'analyse pour des enregistrementsopérationnels d'un ensemble de compteurs d'essieux, qui constituent un élémentcentral à l'infrastructure ferroviaire. Notre objectif est de fournir une façon efficaced'extraire automatiquement des éléments de connaissance concernant lesdéfaillances de ces systèmes.Puisque les données fournies ne contiennent pas de vérité de terrain sur lescauses de défaillances, les informations et leurs causes doivent être extraites desrelations sous-tendant les événements enregistrés. Après une phase de prétraitement,les événements sont groupés en fonction des relations qui ont été misesen lumière entre eux. Ces regroupements peuvent ensuite être utilisés pour créerdes classes d'événements en utilisant un système de classification adapté.Au delà de cette application spécifique, cette approche est une façon nouvelled'aborder les problèmes d'analyse de fiabilité.	Iwo Doboszewski, Simon Fossier, Christophe Marsala	http://editions-rnti.fr/render_pdf.php?p1&p=1002394	http://editions-rnti.fr/render_pdf.php?p=1002394	article proposer méthode danalyse enregistrementsopérationnel dun ensemble compteur dessieux constituer élémentcentral linfrastructure ferroviaire objectif fournir efficacedextrair automatiquemer élément connaissance concerner lesdéfaillance systèmespuisque donnée fournie contenir vérité terrain lescause défaillance information cause devoir extraire desrelation soustender événement enregistrer Après phase prétraitementl événement grouper fonction relation misesen lumière entrer regroupement pouvoir ensuite utiliser créerd classe dévénement utiliser système classification adaptéau application spécifique approcher nouvelledaborder problème danalyse fiabilité
33	Revue des Nouvelles Technologies de l'Information	EGC	2018	Fouille de Motifs Graduels Fermés Fréquents Sous Contrainte de la Temporalité		Jerry Lonlac, Benjamin Négrevergne, Yannick Miras, Aude Beauger, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1002412	http://editions-rnti.fr/render_pdf.php?p=1002412	
34	Revue des Nouvelles Technologies de l'Information	EGC	2018	Fouille de motifs temporels négatifs	Dans cet article nous étudions le problème de l'extraction de motifsfréquents contenant des événements positifs, des événements négatifs spécifiantl'absence d'événement ainsi que des informations temporelles sur le délai entreces événements. Nous définissons la sémantique de tels motifs et proposons laméthode NTGSP basée sur des approches de l'état de l'art. Les performancesde la méthode sont évaluées sur des données commerciales fournies par EDF(Électricité de France).	Katerina Tsesmeli, Manel Boumghar, Thomas Guyet, Rene Quiniou, Laurent Pierre	http://editions-rnti.fr/render_pdf.php?p1&p=1002386	http://editions-rnti.fr/render_pdf.php?p=1002386	Dans article étudier problème lextraction motifsfréquent contenir événement positif événement négatif spécifiantlabsence dévénement information temporel délai entrec événement définir sémantique motif proposon laméthode ntgsp baser approche létat lart performancesde méthode évaluer donnée commercial fournie edfélectricité France
35	Revue des Nouvelles Technologies de l'Information	EGC	2018	Interrogation de données structurellement hétérogènes dans les bases de données orientées documents	Les systèmes orientés documents permettent de stocker tout docu-ment, quel que soit leur schéma. Cette flexibilité génère une potentielle hété-rogénéité des documents qui complexifie leur interrogation car une même entitépeut être décrite selon des schémas différents. Cet article présente une approched’interrogation transparente des systèmes orientés documents. Pour cela, nousproposons de générer un dictionnaire de façon automatique lors de l’insertiondes documents, et qui associe à chaque attribut tous les chemins permettant d’yaccéder. Ce dictionnaire permet de réécrire la requête utilisateur à partir de dis-jonctions de chemins afin de retrouver tous les documents quelles que soientleurs structures. Nos expérimentations montrent des coûts d’exécution de la re-quête réécrite largement acceptables comparés au coût d’une requête sur sché-mas homogènes.	Hamdi Ben Hamadou,  Faiza Ghozzi, André Péninou, Olivier Teste	http://editions-rnti.fr/render_pdf.php?p1&p=1002430	http://editions-rnti.fr/render_pdf.php?p=1002430	système orienter document permettre stocker document schéma flexibilité génèr potentiel hétérogénéité document complexifier interrogation entitépeut décrire schéma article présenter approched’ interrogation transparent système orienter document Pour celer nousproposon générer dictionnaire automatique l’ insertionde document associer attribut tou chemin permettre d’ yaccéder dictionnair permettre réécrire requête utilisateur partir disjonction chemin retrouver tou document soientleur structur expérimentation montrer coût d’ exécution requête réécrire largement acceptable comparer coût d’ requête schéma homogène
36	Revue des Nouvelles Technologies de l'Information	EGC	2018	KTI-MOOC: un système de recommandation pour la personnalisation du processus d'échange d'informations dans les MOOCs	Afin d'aider les apprenants à tirer profit du MOOC (Massive OpenOnline Course) qu'ils suivent, nous proposons un outil pour recommander àchacun d'entre eux une liste ordonnée des “Apprenants leaders” capables dele soutenir durant son processus d'apprentissage. La phase de recommandationest basée sur une approche d'aide à la décision multicritère pour la prédictionpériodique des “Apprenants leaders”. Etant donnée l'hétérogénéité des profilsdes apprenants, nous recommandons à chacun d'entre eux les leaders appropriésà son profil en utilisant la distance euclidienne et le filtrage démographique.	Sarra Bouzayane, Inès Saad	http://editions-rnti.fr/render_pdf.php?p1&p=1002428	http://editions-rnti.fr/render_pdf.php?p=1002428	Afin daider apprenant tirer profit mooc Massive openonlin courser quils proposer outil recommander àchacun dentre liste ordonner “ apprenant leader ” capable del soutenir durer processus dapprentissage phase recommandationest baser approcher daid décision multicritère prédictionpériodique “ apprenant leader ” eter donner lhétérogénéité profilsde apprenant recommander dentre leader appropriésà profil utiliser distancer euclidien filtrage démographique
37	Revue des Nouvelles Technologies de l'Information	EGC	2018	L'exploitation de données contextuelles pour la recommandation d'hôtels	Les systèmes de recommandation ont pour rôle d'aider les utilisateurssubmergés par la quantité d'information à faire de bons choix à partir de vastescatalogues de produits. Le déploiement de ces systèmes dans l'industrie hôtelièreest confronté à des contraintes spécifiques, limitant la performance des approchestraditionnelles. Les systèmes de recommandation d'hôtels souffrent enparticulier d'un problème de démarrage à froid continu à cause de la volatilitédes préférences des voyageurs et du changement de comportements en fonctiondu contexte. Dans cet article, nous présentons le problème de recommandationd'hôtels ainsi que ses caractéristiques distinctives. Nous proposons de nouvellesméthodes contextuelles qui prennent en compte les dimensions géographique ettemporelle ainsi que la raison du voyage, afin de générer les listes de recommandation.Nos expérimentations sur des jeux de données réels soulignent lacontribution des données contextuelles à l'amélioration de la qualité de recommandation.	Marie Al Ghossein, Talel Abdessalem, Anthony Barré	http://editions-rnti.fr/render_pdf.php?p1&p=1002390	http://editions-rnti.fr/render_pdf.php?p=1002390	système recommandation rôle daider utilisateurssubmergé quantité dinformation faire choix partir vastescatalogue produit déploiement système dan lindustrie hôtelièreest confronter contrainte spécifique limiter performance approchestraditionnelle système recommandation dhôtel souffrir enparticulier dun problème démarrage froid continu causer volatilitéd préférence voyageur changement comportement fonctiondu contexte Dans article présenter problème recommandationdhôtel caractéristique distinctif proposer nouvellesméthode contextuelle prendre compter dimension géographique ettemporelle raison voyager générer liste recommandationnos expérimentation jeu donnée réel souligner lacontribution donnée contextuel lamélioration qualité recommandation
38	Revue des Nouvelles Technologies de l'Information	EGC	2018	L'ontologie OntoBiotope pour l'étude de la biodiversité microbienne	L'intégration des données hétérogènes en Sciences de la Vie est unsujet de recherche majeur. L'importance et le volume considérable des informationssur les milieux de vie des microorganismes dans tous les domaines telsque la santé, l'agriculture ou l'environnement justifie le développement de traitementsautomatisés. Nous proposons ici l'ontologie OntoBiotope dont nous décrivonsles principes de construction ainsi que des exemples d'utilisation pourl'annotation et l'indexation sémantique des habitats microbiens décrits en languenaturelle dans les documents scientifiques.	Claire Nédellec, Estelle Chaix, Robert Bossy, Louise Deléger, Sandra Dérozier, Jean-Baptiste Bohuon, Valentin Loux	http://editions-rnti.fr/render_pdf.php?p1&p=1002401	http://editions-rnti.fr/render_pdf.php?p=1002401	lintégration donnée hétérogène science vie unsujet rechercher majeur Limportance volume considérable informationssur milieu vie microorganisme dan tou domaine telsqu santé lagricultur lenvironnement justifier développement traitementsautomatiser proposer lontologie OntoBiotope décrivonsl principe construction exemple dutilisation pourlannotation lindexation sémantique habitat microbien décrit languenaturelle dan document scientifique
39	Revue des Nouvelles Technologies de l'Information	EGC	2018	Long-range influences in (social) networks		Ernesto Estrada	http://editions-rnti.fr/render_pdf.php?p1&p=1002361	http://editions-rnti.fr/render_pdf.php?p=1002361	
40	Revue des Nouvelles Technologies de l'Information	EGC	2018	Mainmise sur les médias et suivi de communautés dans les graphes dynamiques	Ce court article présente le design et l'utilisation d'un tableau de bordvisuel permettant d'explorer, questionner et comprendre l'évolution des communautésd'un graphe dynamique. L'exemple ayant motivé la conception et laréalisation de ce tableau de bord est celui d'un réseau d'affiliation des personnalitésprésentes dans les médias français. Le suivi de communautés s'avère utilepour cerner le biais potentiel induit de la co-présence répétée des mêmes personnalitésdans les émissions de radio et de télévision au cours du temps.	Haolin Ren, Marie-Luce Viaud, Guy Melançon	http://editions-rnti.fr/render_pdf.php?p1&p=1002423	http://editions-rnti.fr/render_pdf.php?p=1002423	courir article présenter design lutilisation dun tableau bordvisuel permettre dexplorer questionner comprendre lévolution communautésdun graph dynamique lexemple motiver conception laréalisation tableau bord dun réseau daffiliation personnalitésprésente dan média français communauté savère utilepour cerner biais potentiel induire coprésence répéter personnalitésdan émission radio télévision cours temps
41	Revue des Nouvelles Technologies de l'Information	EGC	2018	Mean-shift : Clustering scalable et distribué	"Nous présentons dans ce papier un nouvel algorithme Mean-Shift utilisantles K-plus proches voisins pour la montée du gradient (NNMS : NearestNeighbours Mean Shift). Le coût computationnel intensif de ce dernier a longtempslimité son utilisation sur des jeux de données complexes où un partitionnementen clusters non ellipsoïdaux serait bénéfique. Or, une implémentationscalable de l'algorithme ne compense pas l'augmentation du temps d'exécutionen fonction de la taille du jeu de données en raison de sa complexité quadratique.Afin de pallier, ce problème nous avons introduit le ""Locality SensitiveHashing"" (LSH) qui est une approximation de la recherche des K-plus prochesvoisins ainsi qu'une règle empirique pour le choix du K. La combinaison de cesaméliorations au sein du NNMS offre l'opportunité d'un traitement pertinentaux problématiques du clustering appliquée aux données massives."	Gaël Beck, Hanane Azzag, Mustapha Lebbah, Tarn Duong	http://editions-rnti.fr/render_pdf.php?p1&p=1002420	http://editions-rnti.fr/render_pdf.php?p=1002420	présenter dan papier nouvel algorithme meanshift utilisantl kplus voisin monter gradient nnm   nearestneighbour Mean Shift coût computationnel intensif longtempslimiter utilisation jeu donnée complexe partitionnementen cluster ellipsoïdal bénéfique Or implémentationscalabl lalgorithm compenser laugmentation temps dexécutionen fonction tailler jeu donnée raison complexité quadratiqueafin pallier problème introduire Locality SensitiveHashing LSH approximation rechercher kplu prochesvoisin quune régler empirique choix combinaison cesamélioration NNMS offrir lopportunité dun traitement pertinentaux problématique clustering appliquer donnée massif
42	Revue des Nouvelles Technologies de l'Information	EGC	2018	Méta-analyse ordinale d'enquêtes d'opinion Application aux usages de l'Internet des objets en entreprise	La multiplicité des enquêtes d'opinion sur un même sujet nécessite la construction de synthèses qui agrègent les résultats obtenus dans des conditions indépendantes. Dans cet article, nous proposons une nouvelle approche ordinale de méta-analyse qui consiste à rechercher un ordre consensus qui rend compte « au mieux » des ordres partiels entre les modalités issus des résultats des différentes enquêtes. Nous modélisons ce problème par une variante d'une recherche d'un ordre médian sur les sommets d'un graphe orienté pondéré et nous développons un algorithme de séparation-évaluation pour le résoudre. Notre approche est appliquée sur un ensemble d'enquêtes internationales portant sur les motivations et les freins à l'intégration de l'Internet des Objets dans les entreprises.	Rostand Affogbolo, Claire Gauzente, Alain Guénoche, Pascale Kuntz	http://editions-rnti.fr/render_pdf.php?p1&p=1002365	http://editions-rnti.fr/render_pdf.php?p=1002365	multiplicité enquête dopinion nécessit construction synthèse agréger résultat obtenir dan condition indépendant Dans article proposer approcher ordinal métaanalyse consister rechercher ordre consensu compter « mieux » ordre partiel entrer modalité issu résultat enquête modéliser problème variant dune rechercher dun ordre médian sommet dun graphe orienter pondérer développer algorithme séparationévaluation résoudre approcher appliquer ensemble denquêt international porter motivation frein lintégration linternet objet dan entreprise
43	Revue des Nouvelles Technologies de l'Information	EGC	2018	Méthode basée sur les ensembles approximatifs pour l'apprentissage incrémental en présence des données déséquilibrées	Ce papier propose une méthode basée sur la théorie des ensembles approximatifset dédiée à l'apprentissage supervisé incrémental dans un contextede données déséquilibrées. Cette méthode consiste en trois phases : la constructiond'une table de décision, l'inférence d'un ensemble de règles de décisionet la classification de chaque action potentielle dans l'une des classes de décisionprédéfinies. La méthode MAI2P est validée dans le contexte des MOOCs(Massive Open Online Courses).	Sarra Bouzayane, Inès Saad	http://editions-rnti.fr/render_pdf.php?p1&p=1002393	http://editions-rnti.fr/render_pdf.php?p=1002393	papier proposer méthode baser théorie ensemble approximatifset dédier lapprentissage superviser incrémental dan contextede donner déséquilibrer méthode consister phase   constructiondune tabler décision linférence dun ensemble règle décisionet classification action potentiel dan lune classe décisionprédéfinie méthode MAI2P valider dan contexte moocsmassive Open Online Courses
44	Revue des Nouvelles Technologies de l'Information	EGC	2018	Méthode d'Apprentissage pour Extraire les Localisations dans les MicroBlogs		Thi-Bich-Ngoc Hoang, Josiane Mothe	http://editions-rnti.fr/render_pdf.php?p1&p=1002411	http://editions-rnti.fr/render_pdf.php?p=1002411	
45	Revue des Nouvelles Technologies de l'Information	EGC	2018	Modélisation des métadonnées d'un data lake en data vault	Avec l'avènement des mégadonnées, l'informatique décisionnelle adû trouver des solutions pour gérer des données de très grands volume et variété.Les lacs de données (data lakes) répondent à ces besoins du point du vuedu stockage, mais nécessitent la gestion de métadonnées adéquates pour garantirun accès efficace aux données. Sur la base d'un modèle multidimensionnelde métadonnées conçu pour un lac de données présentant un défaut d'évolutivitéde schéma, nous proposons l'utilisation d'un data vault pour traiter ceproblème. Pour montrer la faisabilité de cette approche, nous instancions notremodèle conceptuel de métadonnées en modèles logiques et physiques relationnelet orienté document. Nous comparons également les modèles physiques entermes de stockage et de temps de réponse aux requêtes sur les métadonnées.	Iuri D. Nogueira, Maram Romdhane, Jérôme Darmont	http://editions-rnti.fr/render_pdf.php?p1&p=1002385	http://editions-rnti.fr/render_pdf.php?p=1002385	Avec lavènemer mégadonnée linformatiqu décisionnel adû trouver solution gérer donnée grand volume variétéles lac donnée dater lak répondre besoin poindre vuedu stockage nécessiter gestion métadonné adéquate garantirun accès efficace donnée Sur baser dun modeler multidimensionneld métadonné concevoir lac donnée présenter défaut dévolutivitéd schéma proposer lutilisation dun dater vault traiter ceproblème Pour montrer faisabilité approcher instancier notremodèl conceptuel métadonnée modèle logique physique relationnelet orienter document comparer également modèle physique enterm stockage temps réponse requêt métadonnée
46	Revue des Nouvelles Technologies de l'Information	EGC	2018	NFB: protocole de Notarisation des Documents dans la Blockchain		Haikel Megrahi, Nouha Omrane, Rakia Jaziri	http://editions-rnti.fr/render_pdf.php?p1&p=1002415	http://editions-rnti.fr/render_pdf.php?p=1002415	
47	Revue des Nouvelles Technologies de l'Information	EGC	2018	Nouveau Modèle de Sélection de Caractéristiques basé sur la Théorie des Ensembles Approximatifs pour les Données Massives		Zaineb Chelly Dagdia, Christine Zarges, Gaël Beck, Mustapha Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1002409	http://editions-rnti.fr/render_pdf.php?p=1002409	
48	Revue des Nouvelles Technologies de l'Information	EGC	2018	PALM: Un algorithme parallèle pour extraire des clusters de liens dans les réseaux sociaux	Dans cet article, nous nous intéressons à l'optimisation du processusde recherche de clusters de liens. Nous proposons en particulier l'algorithmePALM (Stattner et al., 2017), qui vise à améliorer l'efficacité du processus d'extractionpar l'exploration conjointe de plusieurs zones de l'espace de recherche.Ainsi, nous commençons par démontrer que l'espace des solutions forme untreillis de concepts. Nous proposons ensuite une approche qui explore en parallèleles branches de ce treillis tout en réduisant l'espace de recherche en s'appuyantsur différentes propriétés. Les bonnes performances de notre algorithmesont démontrées en le comparant avec l'algorithme d'extraction d'origine.	Erick Stattner, Reynald Eugenie, Martine Collard	http://editions-rnti.fr/render_pdf.php?p1&p=1002419	http://editions-rnti.fr/render_pdf.php?p=1002419	Dans article intéresser loptimisation processusde rechercher cluster lien proposer lalgorithmePALM Stattner al 2017 viser améliorer lefficacité processus dextractionpar lexploration conjoint zone lespace rechercheAinsi commencer démontrer lespace solution former untreilli concept proposer ensuite approcher explorer parallèlele branche treillis réduire lespace rechercher sappuyantsur propriété performance algorithmesont démontrer comparer lalgorithme dextraction dorigin
49	Revue des Nouvelles Technologies de l'Information	EGC	2018	Peerus Review: Un outil de recherche d'experts scientifiques	Nous proposons un outil de recherche d'experts appliqué au mondeacadémique sur les données générées par l'entreprise DSRT dans le cadre de sonapplication Peerus 1. Un utilisateur soumet le titre, le résumé et optionnellementles auteurs et le journal de publication d'un article scientifique et se voit proposerune liste d'experts, potentiels reviewers de l'article soumis. L'algorithme derecherche est un système de votes reposant sur un modèle du langage entrainé àpartir d'un ensemble de plusieurs millions d'articles scientifiques. L'outil est accessibleà chacun sous la forme d'une application web intitulée Peerus Review 2.	Robin Brochier, Adrien Guille, Julien Velcin, Benjamin Rothan, François Di Cioccio	http://editions-rnti.fr/render_pdf.php?p1&p=1002426	http://editions-rnti.fr/render_pdf.php?p=1002426	proposer outil rechercher dexperts appliquer mondeacadémique donnée générer lentreprise dsrt dan cadrer sonapplication peeru 1 utilisateur soumettre titrer résumer optionnellementl auteur journal publication dun article scientifique voir proposerune liste dexperts potentiel reviewer larticle soumettre Lalgorithme derecherche système vote reposer modeler langage entrainer àpartir dun ensemble million darticl scientifique Loutil accessibleà sou former dune application web intituler Peerus Review 2
50	Revue des Nouvelles Technologies de l'Information	EGC	2018	PerForecast : un outil de prévision de l'évolution de séries temporelles pour le planning capacitaire	Nous présentons PerForecast, un outil qui vise à automatiser le processusde planning capacitaire en utilisant des données temporelles univariéeset des modèles prédictifs configurés automatiquement. L'objectif est d'anticiperles problèmes de dimensionnement dans les infrastructures d'Orange quiassurent la délivrance d'un service aux clients. Il s'agira par exemple de prévoirau plus « tôt » la surcharge d'un serveur, afin de commander en avance de nouvellesmachines (avant la détérioration du service considéré). Les démarches dedimensionnent et d'achat étant longues et coûteuses, plus elles sont effectuéestôt, meilleure sera la qualité de service.	Colin Leverger, Régis Marguerie, Vincent Lemaire, Thomas Guyet, Simon Malinowski	http://editions-rnti.fr/render_pdf.php?p1&p=1002424	http://editions-rnti.fr/render_pdf.php?p=1002424	présenter perforecast outil viser automatiser processusde planning capacitaire utiliser donnée temporel univariéeset modèle prédictif configurer automatiquement Lobjectif danticiperl problème dimensionnement dan infrastructure dOrange quiassurer délivrance dun service client sagirer exemple prévoirau plaire « tôt » surcharger dun serveur commander avancer nouvellesmachine détérioration service considérer démarche dedimensionnent dachat long coûteuser plaire effectuéestôt meilleur qualité service
51	Revue des Nouvelles Technologies de l'Information	EGC	2018	Prédiction du Rayonnement Solaire par Apprentissage Automatique	Cet article décrit une approche flexible pour la prédiction à courtterme de variables météorologiques. En particulier, nous nous intéressons à laprédiction du rayonnement solaire à une heure. Cette tâche est d'une grandeimportance pratique dans l'optique d'optimiser les resources énergétiques solaires.Comme le défi EGC 2018 nous fournit des données météorologiques enregistréessur cinq sites géographiques de l'île de la Réunion, nous utilisons cesdonnées historiques comme base pour créer des modèles de prédiction, et noustestons la performance de ces modèles selon le site considéré. Après avoir décritnotre méthode de nettoyage de données et de normalisation, nous combinonsune méthode de sélection de variables basée sur les modèles ARIMA (AutoRegressiveIntegrated Moving Average) à l'utilisation de méthodes de régressiongénériques, telles que les arbres de régression et les réseaux de neurones.	Pierrick Bruneau, Philippe Pinheiro, Yoann Didry	http://editions-rnti.fr/render_pdf.php?p1&p=1002422	http://editions-rnti.fr/render_pdf.php?p=1002422	article décrire approcher flexible prédiction courtterme variable météorologique En intéresser laprédiction rayonnement solaire heure tâcher dune grandeimportance pratique dan loptique doptimiser resource énergétique solairesComme défi EGC 2018 fournir donnée météorologique enregistréessur site géographique lîle réunion utiliser cesdonner historique baser créer modèle prédiction nousteston performance modèle site considérer Après décritnotre méthode nettoyage donnée normalisation combinonsune méthode sélection variable baser modèle arima AutoRegressiveIntegrated Moving average lutilisation méthode régressiongénérique arbre régression réseau neuron
52	Revue des Nouvelles Technologies de l'Information	EGC	2018	Prétraitement de données spatialement imprécises pour une classification supervisée basée sur les images satellitaires	Dans un problème de classification supervisée, les données d'apprentissageproviennent souvent d'inventaires acquis sur le terrain par des expertsdu domaine. Toutefois, la localisation de ces inventaires est approximative (enraison de la précision intrinsèque des GPS portables utilisés). Cette imprécisionspatiale est particulièrement problématique lorsque ces données sont utiliséespour entrainer un classifieur sur des images satellitaires très haute résolution(THR). En effet, la précision spatiale des inventaires peut être dans certains casbien inférieure à celles de ces images. Dans ce papier, nous proposons trois approchesvisant à améliorer la précision spatiale des données terrain via des prétraitements.Le principe est d'exploiter les images satellitaires THR disponiblespour corriger spatialement les données terrain. Nos expérimentations mettenten avant l'intérêt de ces pré-traitements sur un jeu de données constitué de 24inventaires d'habitats coralliens et une image satellitaire THR (WorldView-2).	Jannaï Tokotoko, Frédéric Flouvat, Claire Goiran, Laetitia Hédouin, Antoine Collin, Nazha Selmaoui-Folcher	http://editions-rnti.fr/render_pdf.php?p1&p=1002377	http://editions-rnti.fr/render_pdf.php?p=1002377	Dans problème classification superviser donnée dapprentissageproviennent dinventair acquérir terrain expertsdu domaine localisation inventaire approximatif enraison précision intrinsèque gp portable utiliser imprécisionspatiale problématique donnée utiliséespour entrainer classifieur image satellitaire résolutionthr En précision spatial inventaire pouvoir dan casbien inférieur image Dans papier proposer approchesviser améliorer précision spatial donnée terrain prétraitementsle principe dexploiter image satellitaire THR disponiblespour corriger spatialement donnée terrain expérimentation mettenten lintérêt prétraitement jeu donnée constituer 24inventaires dhabitat corallien imager satellitaire thr WorldView2
53	Revue des Nouvelles Technologies de l'Information	EGC	2018	Prise en compte de la structure des documents pour une indexation performante		Pascal Cuxac, Nicolas Kieffer	http://editions-rnti.fr/render_pdf.php?p1&p=1002403	http://editions-rnti.fr/render_pdf.php?p=1002403	
54	Revue des Nouvelles Technologies de l'Information	EGC	2018	Propositions pour améliorer une méthode de prédiction du succès d'une campagne de financement participatif	Le financement participatif est un mode de financement d'un projet faisant appel à un grand nombre de personnes qui a connu une forte croissance avec l'émergence d'Internet et des réseaux sociaux. Cependant plus de 60 % des projets ne sont pas financés, il est donc important de bien préparer sa campagne de financement. De plus, en cours de campagne, il est crucial d'avoir une estimation rapide de son succès afin de pouvoir réagir rapidement (restructuration, communication) : des outils de prédiction sont alors indispensables. Nous proposons dans cet article plusieurs pistes d'amélioration pour la prédiction du montant levé lors d'une campagne de financement participatif en utilisant l'algorithme k-NN. La première proposition consiste à utiliser un algorithme de clustering afin de segmenter l'ensemble d'apprentissage et faciliter le passage à l'échelle. La seconde proposition consiste à extraire des caractéristiques pertinentes depuis les séries temporelles et les informations sur les campagnes pour avoir une représentation vectorielle.	Alexandre Blansché, Xavier Mazur	http://editions-rnti.fr/render_pdf.php?p1&p=1002374	http://editions-rnti.fr/render_pdf.php?p=1002374	financement participatif mode financement dun projet faire appel grand nombre connaître fort croissance lémergence dInternet réseau social plaire 60   projet financer importer préparer campagne financement De plaire cours campagne crucial davoir estimation rapide succès pouvoir réagir rapidement restructuration communication   outil prédiction indispensable proposer dan article piste damélioration prédiction monter lever dune campagne financement participatif utiliser lalgorithm knn proposition consister utiliser algorithme clustering segmenter lensembl dapprentissage faciliter passage léchelle second proposition consister extraire caractéristique pertinent série temporel information campagne représentation vectoriel
55	Revue des Nouvelles Technologies de l'Information	EGC	2018	Qu'est-ce qu'un bon système d'apprentissage ? La réponse a évolué avec le temps. Et demain?	L'apprentissage automatique, pardon le « machine learning », a envahi la sphère médiatiquegrâce à des succès impressionnants comme la victoire d'une machine au Go, ou la promesse de véhicules autonomes arrivant très prochainement sur nos routes. De fait, tant l'exploitation des données massives que la production de code machine à partir de l'expérience de la machine plutôt que par des humains, met l'apprentissage automatique au coeur de l'intelligence artificielle. Très certainement cela signifie que nous savons répondre à la question « qu'est-ce qu'un bon système d'apprentissage ? » et qu'il ne nous reste plus qu'à en décliner la réponse pour obtenir des systèmes adaptés à chaque domaine applicatif. Pourtant, la réponse à cette question a profondément évolué au cours des 60 dernières années, au point que les publications sur l'apprentissage automatique d'il y a quelques décennies semblent venir d'une autre planète et ne sont d'ailleurs plus enseignés aux étudiants. Et ceci pas seulement parce que les connaissances passées seraient jugées obsolètes, mais parce qu'elles ne semblent pas pertinentes. Avons-nous donc raison ? Nos précurseurs avaient-ils tort ? Et nos successeurs nous citeront-ils dans leurs manuels ? Dans cette présentation, nous examinerons quelques moments clés de l'histoire de l'apprentissage automatique correspondant à des tournants dans la manière de considérer ce qu'est un bon système d'apprentissage. Et nous nous demanderons si nous vivons un autre moment charnière dans lequel changent notre perspective, la question que nous cherchons à résoudre dans nos recherches, les concepts manipulés et la manière d'écrire nos papiers.	Antoine Cornuéjols	http://editions-rnti.fr/render_pdf.php?p1&p=1002360	http://editions-rnti.fr/render_pdf.php?p=1002360	lapprentissage automatique pardon « machiner learning » envahir sphère médiatiquegrâce succès impressionnant victoire dune machiner Go promesse véhicule autonome arriver prochainement route De faire lexploitation donnée massif production coder machin partir lexpérience machiner humain mettre lapprentissage automatique coeur lintelligence artificiel certainement celer signifier savon répondre question « questce quun système dapprentissage   » quil rester plaire quà décliner réponse obtenir système adapter domaine applicatif pourtant réponse question profondément évoluer cours 60 année poindre publication lapprentissage automatique dil yu décennie sembler venir dune planèt dailleurs plaire enseigner étudiant Et connaissance passer juger obsolète sembler pertinent Avonsnous raison   précurseur avaientil tort   Et successeur citerontils dan manuel   Dans présentation examiner moment cler lhistoire lapprentissage automatique correspondre tournant dan manière considérer quest système dapprentissage Et demander vivre moment charnièr dan changer perspectif question chercher résoudre dan recherche concept manipulé manière décrir papier
56	Revue des Nouvelles Technologies de l'Information	EGC	2018	Recommendation-based Keyword Search over Relational Databases	Récemment, la recherche par mots-clés dans les bases de données relationnelles a suscité un intérêtgrandissant en raison de sa facilité d'utilisation. Bien que des recherches approfondies fussentdernièrement effectuées dans ce contexte, la plupart de ces recherches non seulement nécessitent unaccès préalable aux données, ce qui restreint leur applicabilité si cette condition n'est pas vérifiée,mais aussi renvoient des réponses très génériques. Cependant, fournir aux utilisateurs des réponsespersonnalisées est devenu plus que jamais nécessaire en raison de la surabondance de données quipeut déranger l'utilisateur. Le défi de retourner des réponses pertinentes et personnalisées qui satisfontles besoins des utilisateurs demeure. Inspiré par l'application réussie de la technique de filtragecollaboratif dans les systèmes de recommandation, nous proposons une nouvelle approche baséesur les mots-clés pour fournir aux utilisateurs des résultats personnalisés basés sur l'hypothèse queseulement une information sur le schéma de la base de données est disponible.	Haithem Ghorbel, Nouha Othman, Rim Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1002400	http://editions-rnti.fr/render_pdf.php?p=1002400	récemment rechercher motsclé dan base donnée relationnel susciter intérêtgrandissant raison faciliter dutilisation recherche approfondir fussentdernièrement effectuer dan contexte recherche nécessiter unaccè préalable donnée restreindre applicabilité condition nest vérifiéemai renvoyer réponse générique fournir utilisateur réponsespersonnalisée devenir plaire jamais nécessaire raison surabondance donnée quipeut déranger lutilisateur défi retourner réponse pertinent personnaliser satisfontl besoin utilisateur demeurer inspirer lapplication réussir technique filtragecollaboratif dan système recommandation proposer approcher baséesur motsclé fournir utilisateur résultat personnaliser baser lhypothèse queseulement information schéma baser donnée disponible
57	Revue des Nouvelles Technologies de l'Information	EGC	2018	Reconnaissance et indexation automatique des registres de la chancellerie française (1300-1483)	Les documents manuscrits sont parmi les témoins les plus importants de l'histoire européenne. Ces dernières années, d'importantes collections de manuscrits historiques ont été numérisées et mises à disposition du public et des chercheurs. Cependant, la richesse des informations qu'ils contiennent est encore largement inaccessible car seul les images et quelques méta-données sont disponibles. L'idéal pour les utilisateurs serait de pouvoir faire des recherches textuelles comme pour les livres imprimés modernes (https://books.google.fr/). Si les technologies d'analyse de documents historiques et de reconnaissance d'écriture manuscrite sont encore trop peu performantes pour permettre l'utilisation directe de la transcription brute, il est possible de mettre à la disposition des utilisateurs un moteur de recherche textuel basé sur une indexation automatique des images de documents manuscrits. Cette indexation se base sur une transcription automatique mais tire profit de la capacité de la machine à générer des hypothèses reconnaissance multiples et pondérées. Cette technologie a permis de rendre accessible pour la première fois à la recherchetextuelle les registres de la chancellerie royale française (1302 -1483), un des corpus de documents historiques les plus emblématiques pour la France, ouvrant ainsi la voie à de nouvelles méthodes de recherche en histoire : http://www.himanis.org/	Christopher Kermorvant	http://editions-rnti.fr/render_pdf.php?p1&p=1002364	http://editions-rnti.fr/render_pdf.php?p=1002364	document manuscrit témoin plaire important lhistoire européen dernière année dimportant collection manuscrit historique numériser mettre disposition public chercheur richesse information quil contenir largement inaccessible image métadonnée disponible Lidéal utilisateur pouvoir faire recherche textuel livre imprimé moderne httpsbooksgooglefr Si technologi danalyse document historique reconnaissance décriture manuscrit performant permettre lutilisation direct transcription brut mettre disposition utilisateur moteur rechercher textuel baser indexation automatique image document manuscrit indexation baser transcription automatique tir profit capacité machiner générer hypothèse reconnaissance pondérer technologie permettre accessible recherchetextuelle registre chancellerie royal français 1302 1483 corpus document historique plaire emblématique France ouvrir voir méthode rechercher histoire   httpwwwhimanisorg
58	Revue des Nouvelles Technologies de l'Information	EGC	2018	Reframing for Non-Linear Dataset Shift	Les modèles de classification discriminante supposent que les données de formation et dedéploiement ont les mêmes distributions d'attributs de données. Ces modèles donnent des performancestrès variées lorsqu'ils sont déployés dans des conditions variées avec différentesdistributions de données. Ce phénomène est appelé Dataset Shift. Dans cet article, nous avonsfourni une méthode qui détermine d'abord s'il y a un changement significatif dans les distributionsd'attributs entre les ensembles de données d'apprentissage et de déploiement. S'ilexiste un changement dans les données, la méthode proposée utilise ensuite une approche deHill climbing pour cartographier ce décalage, quelle que soit sa nature, c'est-à-dire (linéaireou non linéaire) à l'équation pour la transformation quadratique. Les résultats expérimentauxsur trois jeux de données réels montrent de forts gains de performance obtenus par la méthodeproposée par rapport aux méthodes précédemment établies telles que le reconditionnement etle recadrage linéaire.	Md Shadman Rafid, Mohammad Mazedul Islam, Md Naimul Hoque, Chowdhury Farhan Ahmed	http://editions-rnti.fr/render_pdf.php?p1&p=1002375	http://editions-rnti.fr/render_pdf.php?p=1002375	modèle classification discriminant supposer donnée formation dedéploiemer distribution dattributs donnée modèle donner performancestrè varier lorsquils déployer dan condition varier différentesdistribution donnée phénomène appeler Dataset Shift Dans article avonsfourni méthode détermin dabord sil yu changement significatif dan distributionsdattribut entrer ensemble donnée dapprentissage déploiement silexist changement dan donnée méthode proposer utiliser ensuite approcher deHill climbing cartographier décalage nature cestàdir linéaireou linéaire léquation transformation quadratique résultat expérimentauxsur jeu donnée réel montrer fort gain performance obtenir méthodeproposée rapport méthode précédemment établie reconditionnement etle recadrage linéaire
59	Revue des Nouvelles Technologies de l'Information	EGC	2018	Régression Laplacienne semi-supervisée pour la reconstitution des dates de pose des réseaux d'assainissement	La date de pose est souvent un facteur principal d'explication de la dégradationdes conduites d'assainissement. Pour les gestionnaires de ces réseaux,connaître cette information permet ainsi (par l'utilisation de modèles de détérioration)de prédire l'état de santé actuel des conduites non encore inspectées.Cette connaissance est primordiale pour prendre des décisions dans un contextede forte contrainte budgétaire. L'objectif est ainsi de reconstituer ces dates depose à partir des caractéristiques du patrimoine et de son environnement. Lesdonnées à manipuler présentent plusieurs niveaux de complexité importants.Leurs sources sont hétérogènes, leur volume est important et les informationssur leur étiquetage (dates) sont limitées : seulement 24 % du linéaire est connupour les réseaux d'assainissement de la métropole de Lyon. La base de donnéessous-jacente contient les caractéristiques connues des conduites (profil géométrique,matériau utilisé, etc.). Dans ce papier, nous proposons de mesurer l'effetet l'impact de quelques méthodes d'apprentissage statistique semi-supervisé, etde proposer ainsi une approche alternative adaptée à la reconstitution de ce typede données.	Vivien Kraus, Khalid Benabdeslem, Frederic Cherqui	http://editions-rnti.fr/render_pdf.php?p1&p=1002389	http://editions-rnti.fr/render_pdf.php?p=1002389	dater poser facteur principal dexplication dégradationde conduire dassainissemer Pour gestionnaire réseauxconnaître information permettre lutilisation modèle détériorationde prédir létat santé actuel conduite inspectéescett connaissance primordial prendre décision dan contextede fort contraint budgétaire Lobjectif reconstituer dat depose partir caractéristique patrimoine environnement lesdonner manipuler présenter niveau complexité importantsleur source hétérogèner volume importer informationssur étiquetage dat limiter   24   linéaire connupour réseau dassainissemer métropole Lyon baser donnéessousjacente contenir caractéristique connu conduite profil géométriquematériau utiliser Dans papier proposer mesurer leffetet limpact méthode dapprentissage statistique semisuperviser etde proposer approcher alternatif adapté reconstitution typede donner
60	Revue des Nouvelles Technologies de l'Information	EGC	2018	Réseau bayésien pour la gestion de l'obsolescence dans une base d'informations en vue de l'évaluation du risque de chute des personnes âgées	L'évaluation périodique du risque de chute des personnes âgéesrequiert des informations fiables et nombreuses. Comme il n'est pas possiblede recueillir régulièrement toutes ces informations, les observationssont faites au fil du temps et conservées, ce qui entraîne une problématiqueliée au vieillissement des informations. Cet article traite de la détectiondes informations obsolètes dans une base d'informations sur unepersonne âgée. Nous proposons une solution comportant un modèle deconnaissances sur les personnes âgées sous forme d'un réseau bayésien etun module de raisonnement chargé de la détection et de la gestion descontradictions et des doutes sur les informations.	Salma Chaieb, Véronique Delcroix, Ali Ben Mrad, Emmanuelle Grislin-Le Strugeon	http://editions-rnti.fr/render_pdf.php?p1&p=1002402	http://editions-rnti.fr/render_pdf.php?p=1002402	lévaluation périodique risquer chuter âgéesrequiert information fiable Comme nest possiblede recueillir régulièrement information observationssont fil temps conserver entraîner problématiqueliée vieillissemer information article traiter détectiondes information obsolète dan baser dinformation unepersonne âgé proposer solution comporter modeler deconnaissance âgé sou former dun réseau bayésien etun modul raisonnement charger détection gestion descontradiction doute information
61	Revue des Nouvelles Technologies de l'Information	EGC	2018	Savoir au dela de voir: vision artificielle et raisonnement logique		Roberto Marroquin, Julien Dubois, Christophe Nicolle	http://editions-rnti.fr/render_pdf.php?p1&p=1002413	http://editions-rnti.fr/render_pdf.php?p=1002413	
62	Revue des Nouvelles Technologies de l'Information	EGC	2018	Sémantique des données d'observation en neuro-imagerie selon un point de vue réaliste	L'objectif de ce travail est de décrire avec une approche réaliste lasignification des données d'observation en neuro-imagerie sous un format formelpour faciliter leur interprétation par les cliniciens et leur réutilisation dansd'autres contextes.	Emna Amdouni, Bernard Gibaud	http://editions-rnti.fr/render_pdf.php?p1&p=1002399	http://editions-rnti.fr/render_pdf.php?p=1002399	Lobjectif travail décrire approcher réaliste lasignification donnée dobservation neuroimagerie sou format formelpour faciliter interprétation clinicien réutilisation dansdautr contexte
63	Revue des Nouvelles Technologies de l'Information	EGC	2018	Temporal hints in the cultural heritage discourse: what can an ontology of time as it is worded reveal?	Dans le champ des sciences patrimoniales, la dimension temporelle de l'information joueun rôle à l'évidence majeur tant pour l'interpréter et l'analyser que pour relier des faits isolés. Mais la façon dont cette dimension est verbalisée pose des problèmes de formalisation non triviaux. Pourtant, cette verbalisation, que l'on associe souvent au terme-chapeau d'incertitude, peut être lue en dissociant d'une part le caractère mal connu d'un fait documenté, irréductible, et les choix faits par le producteur de l'information pour la relativiser. Dans cette contribution nous proposons un modèle formel permettant d'observer et d'analyser de façon systématique cette couche de verbalisation. L'expérience est menée sur des données fortement hétérogènes, souvent d'origine citoyenne, documentant le petit patrimoine matériel et immatériel. Ce cas d'étude est donc limité, mais il apparait néanmoins comme portant une question de fond allant au-delà du cas d'espèce. La contribution détaille d'abord la grille d'analyse d'indices temporels proposée, puis relate l'expérimentation concrète associée (ontologie OWL). Il n'est pas fait état d'une quelconque prétention à un résultat généralisable stricto sensu, mais cette expérience peut contribuer à nourrir de façcon pragmatique un débat nécessaire sur la formalisation d'indices temporels dans les sciences historiques.	Gamze Saygi, Jean-Yves Blaise, Iwona Dudek	http://editions-rnti.fr/render_pdf.php?p1&p=1002370	http://editions-rnti.fr/render_pdf.php?p=1002370	Dans champ science patrimonial dimension temporel linformation joueun rôle lévidence majeur linterpréter lanalyser relier isolé Mais dimension verbaliser poser problème formalisation trivial pourtant verbalisation lon associer termechapeau dincertitude pouvoir lire dissocier dune partir caractère mal connaître dun faire documenter irréductible choix faire producteur linformation relativiser Dans contribution proposer modeler formel permettre dobserver danalyser systématique coucher verbalisation Lexpérience mener donnée fortement hétérogèner dorigine citoyen documenter petit patrimoine matériel immatériel cas détude limité apparer porter question fondre aller audelà cas despèce contribution détailler dabord griller danalyse dindice temporel proposer pouvoir relat lexpérimentation concret associer ontologie OWL nest faire dune prétention résultat généralisable stricto sensu expérience pouvoir contribuer nourrir façcon pragmatique débattre nécessaire formalisation dindice temporel dan science historique
64	Revue des Nouvelles Technologies de l'Information	EGC	2018	Un modèle Bayésien de co-clustering de données mixtes	Nous proposons un modèle de co-clustering de données mixtes et uncritère Bayésien de sélection du meilleur modèle. Le modèle infère automatiquementles discrétisations optimales de toutes les variables et effectue un coclusteringen minimisant un critère Bayésien de sélection de modèle. Un avantagede cette approche est qu'elle ne nécessite aucun paramètre utilisateur. Deplus, le critère proposé mesure de façon exacte la qualité d'un modèle tout enétant régularisé. L'optimisation de ce critère permet donc d'améliorer continuellementles modèles trouvés sans pour autant sur-apprendre les données. Les expériencesréalisées sur des données réelles montrent l'intérêt de cette approchepour l'analyse exploratoire des grandes bases de données.	Aichetou Bouchareb, Marc Boullé, Fabrice Rossi, Fabrice Clérot	http://editions-rnti.fr/render_pdf.php?p1&p=1002388	http://editions-rnti.fr/render_pdf.php?p=1002388	proposer modeler coclustering donnée mixte uncritère Bayésien sélection meilleur modeler modeler inférer automatiquementl discrétisation optimale variable effectuer coclusteringen minimiser critère Bayésien sélection modeler avantagede approcher nécessiter paramètre utilisateur Deplus critère proposer mesurer exact qualité dun modeler enétant régulariser loptimisation critère permettre daméliorer continuellementl modèle trouver autant surapprendre donnée expériencesréalisée donnée réel montrer lintérêt approchepour lanalyse exploratoire grand base donnée
65	Revue des Nouvelles Technologies de l'Information	EGC	2018	Une approche sémantique hybride pour la recommandation des articles d'actualité à large échelle	Les portails d'actualités en ligne produisent un flux d'informationayant un volume et une vélocité importants. Dans ce contexte, il devient plusdifficile de proposer en temps réel des recommandations dynamiques adaptéesaux intérêts de chaque utilisateur. Dans cet article, nous présentons une approchehybride pour la recommandation des articles d'actualité reposant sur l'analysesémantique du contenu disponible. L'approche est basée sur l'hybridation deplusieurs approches personnalisées et non personnalisées pour remédier au problèmede démarrage à froid. L'expérimentation de notre approche dans un environnementà large échelle et à fortes contraintes temps réel dans le cadre duchallenge NEWSREEL a permis d'évaluer la qualité de ses recommandations etde confirmer l'apport de la sémantique dans le processus de recommandation.	Hemza Ficel, Mohamed Ramzi Haddad, Hajer Baazaoui Zghal	http://editions-rnti.fr/render_pdf.php?p1&p=1002392	http://editions-rnti.fr/render_pdf.php?p=1002392	portail dactualiter ligne produire flux dinformationayer volume vélocité important Dans contexte devenir plusdifficil proposer temps réel recommandation dynamique adaptéesaux intérêt utilisateur Dans article présenter approchehybrid recommandation article dactualité reposer lanalysesémantique contenir disponible Lapproche baser lhybridation deplusieur approche personnaliser personnaliser remédier problèmede démarrage froid lexpérimentation approcher dan environnementà large échelle forte contraint temps réel dan cadrer duchalleng newsreel permettre dévaluer qualité recommandation etde confirmer lapport sémantique dan processus recommandation
66	Revue des Nouvelles Technologies de l'Information	EGC	2018	Une méthode pour l'estimation désagrégée de données de population à l'aide de données ouvertes	Nous présentons dans ce travail une méthode de désagrégation pour l'estimation de population à l'échelle locale à partir de données ouvertes globales. Notre but est d'estimer notamment le nombre de personnes résidant dans chaque bâtiment de la zone d'intérêt, à partir de données à plus grande échelle. Une description fine à l'échelle résidentielle est tout d'abord effectuée à partir des données d'OpenStreetMap. Les surfaces des bâtiments d'habitation ou d'usage mixte (habitation et activités) sont notamment identifiées. Nous effectuons ensuite une désagrégation à partir de données de grille de population à grande échelle (1km2 par carreau), guidée par les surfaces des bâtiments compris dans chaque carreau de la grille. Ensuite, nous effectuons une désagrégation à partir de données de grille de population à grande échelle (1km2 par carreau), guidée par les distributions spatiales découvertes à l'étape précédente. Nous utilisons exclusivement des données ouvertes pour favoriser la réplicabilité et pour pouvoir appliquer notre méthode à toute région d'intérêt, pour peu que la qualité des données soit suffisante. L'évaluation et la validation du résultat dans le cas de plusieurs villes Françaises sont effectuées à l'aide de données de recensement INSEE.	Luciano Gervasoni, Serge Fenet, Peter Sturm	http://editions-rnti.fr/render_pdf.php?p1&p=1002371	http://editions-rnti.fr/render_pdf.php?p=1002371	présenter dan travail méthode désagrégation lestimation population léchelle local partir donnée globale boire destimer nombre résider dan bâtiment zone dintérêt partir donnée plaire grand échelle description fin léchelle résidentiel dabord effectuer partir donnée dopenstreetmap surface bâtiment dhabitation dusage mixte habitation activité identifier effectuer ensuite désagrégation partir donnée griller population grand échelle 1km2 carreau guider surface bâtiment dan carreau griller ensuite effectuer désagrégation partir donnée griller population grand échelle 1km2 carreau guider distribution spatial découvrir létape précédent utiliser exclusivement donnée favoriser réplicabilité pouvoir appliquer méthode région dintérêt qualité donnée lévaluation validation résultat dan cas ville français effectuer laid donnée recensement inse
67	Revue des Nouvelles Technologies de l'Information	EGC	2018	UNITEX/GRAMLAB: plateforme libre basée sur des lexiques et des grammaires pour le traitement des corpus textuels	L'objectif de notre recherche est de répondre aux besoins croissants etdivers d'extraction d'information pertinente exprimés par de nombreuses disciplines.Nous utilisons pour cela l'analyseur multilingue de corpus Unitex/Gram-Lab développé à l'Université Paris-Est Marne-la-Vallée. Il fait appel à une approchesymbolique et utilise des ressources linguistiques, dictionnaires électroniqueset grammaires locales. Cette présentation ne constitue qu'une prise enmain d'Unitex/GramLab et ne reflète que très partiellement les possibilités dulogiciel et son champ d'utilisation, notamment pour l'extraction d'information,qui s'étend du monde de la recherche à celui de l'industrie.	Tita Kyriacopoulou, Claude Martineau, Cristian Martinez	http://editions-rnti.fr/render_pdf.php?p1&p=1002427	http://editions-rnti.fr/render_pdf.php?p=1002427	Lobjectif rechercher répondre besoin croissant etdiver dextraction dinformation pertinent exprimer disciplinesNous utiliser celer lanalyseur multilingue corpus UnitexGramLab développer luniversité parisest marnelavallée faire appel approchesymbolique utiliser ressource linguistique dictionnair électroniqueset grammair local présentation constituer quune priser enmain dUnitexGramLab refléter partiellement possibilité dulogiciel champ dutilisation lextraction dinformationqui sétend monder rechercher lindustrie
68	Revue des Nouvelles Technologies de l'Information	EGC	2018	Universal-endpoint.com : une plateforme d'accès simple au Web des Données	Universal-endpoint.com est une plateforme web permettant un accèssimple au Web des Données par trois aspects : (i) une plateforme de correspondance,pour l'accès aux bases du Web des Données depuis un seul point d'accèscentralisé, (ii) le langage SimplePARQL, pour une écriture intuitive de requêtessous forme de triplets à la manière de SPARQL mais ne nécessitant pas uneconnaissance préalable des bases du Web des Données, et (iii) une aide à larédaction de requêtes SPARQL.	Thomas Raimbault, Abdellah Sabry, Sonia Djebali	http://editions-rnti.fr/render_pdf.php?p1&p=1002429	http://editions-rnti.fr/render_pdf.php?p=1002429	Universalendpointcom plateforme web permettre accèssimple web donnée aspect   ie plateforme correspondancepour laccè base web donnée poindre daccèscentraliser ii langage simpleparql écriture intuitif requêtessous former triplet manière sparql nécessiter uneconnaissance préalable base web donnée iii aider larédaction requête sparql
69	Revue des Nouvelles Technologies de l'Information	EGC	2018	Utilisation de techniques de modélisation thématiques pour la détection de nouveauté dans des flux de données textuelles.	Avec l'avènement des réseaux sociaux et la multiplication des messagesproduits au sujet des entreprises, mieux comprendre les retours clients estdevenu un enjeu primordial. Des techniques de classification automatique et demodélisation thématique permettent d'ors déjà d'observer les principales tendancesobservées dans ces données. Il est intéressant, dans une optique d'anticipation,d'observer les thématiques émergentes et de les identifier avant qu'ellesne prennent de l'ampleur. Afin de résoudre cette problématique, nous avons étudiéla piste de l'utilisation de modèles LDA pour détecter les documents relatifsà ces thématiques émergentes. Nous avons testé trois systèmes sur plusieurs scénariosd'arrivées de la nouveauté dans le flux de données. Nous montrons queles modèles thématiques permettent de détecter cette nouveauté mais que celadépend du scénario envisagé.	Clément Christophe, Julien Velcin, Manel Boumghar	http://editions-rnti.fr/render_pdf.php?p1&p=1002383	http://editions-rnti.fr/render_pdf.php?p=1002383	Avec lavènemer réseau social multiplication messagesproduit entreprise mieux comprendre client estdevenu enjeu primordial technique classification automatique demodélisation thématique permettre dormir déjà dobserver principal tendancesobserver dan donnée intéresser dan optique danticipationdobserver thématique émergent identifier quellesne prendre lampleur Afin résoudre problématique étudiéler pist lutilisation modèle LDA détecter document relatifsà thématique émergent tester système scénariosdarrivée nouveauté dan flux donnée montrer modèle thématique permettre détecter nouveauté celadépend scénario envisager
70	Revue des Nouvelles Technologies de l'Information	EGC	2018	Visualisation dynamique de connaissances : application aux interactions entre facteurs de risque des maladies cardiovasculaires		Rabia Azzi, Sylvie Desprès, Jérôme Nobécourt	http://editions-rnti.fr/render_pdf.php?p1&p=1002407	http://editions-rnti.fr/render_pdf.php?p=1002407	
71	Revue des Nouvelles Technologies de l'Information	EGC	2017	A Hybrid Approach for Detecting Influencers in Social Media	La détection d'influenceurs dans les réseaux sociaux s'appuie généra-lement sur une structure de graphe représentant les utilisateurs et leurs interac-tions. Récemment, cette tâche a tenu compte, en sus de la structure du graphe,du contenu textuel généré par les utilisateurs. Notre approche s'inscrit dans cettelignée : des informations sont extraites du contenu textuel par des règles linguis-tiques puis sont intégrées dans un système d'apprentissage automatique. Nousmontrerons le prototype développé et son interface de visualisation qui facilitel'interprétation des résultats.	Ioannis Partalas, Cédric Lopez, Pierre-Alain Avouac, Matthieu Osmuk, Domoina Rabarijaona, Dana Popovici, Frédérique Segond	http://editions-rnti.fr/render_pdf.php?p1&p=1002325	http://editions-rnti.fr/render_pdf.php?p=1002325	détection dinfluenceur dan réseau social sappuie généralement structurer graph représenter utilisateur interaction récemment tâcher compter savoir structurer graphedu contenir textuel générer utilisateur approcher sinscrit dan cettelignée   information extraire contenir textuel règle linguistique pouvoir intégrer dan système dapprentissage automatique nousmontreron prototype développer interface visualisation facilitelinterprétation résultat
72	Revue des Nouvelles Technologies de l'Information	EGC	2017	Analyse des dynamiques spatio-temporelles à partir de séries temporelles d'images satellitaires	La télédétection est un domaine qui regroupe les techniques et lesoutils permettant l'observation de la terre, notamment l'acquisition d'images sa-tellitaires. La méthode proposée dans cet article permet une analyse automatiquede séries temporelles de telles images. Nos travaux introduisent un nouvelle ap-proche pour l'analyse et le clustering de Séries Temporelles d'Images Satelli-taire (STIS). Ce processus se divise en deux parties. Dans un premier temps,nous retraçons les changements radiométriques d'une zone en représentant sonévolution au cours du temps par un graphe dit graphe d'évolution. Dans undeuxième temps, nous introduisons une représentation synthétique des graphesd'évolutions afin de pouvoir appliquer un algorithme de clustering permettantun regroupement par types d'évolutions identifiées. Les expérimentations me-nées nous ont permis de valider notre approche sur une zone d'étude.	Lynda Khiali, Dino Ienco, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1002286	http://editions-rnti.fr/render_pdf.php?p=1002286	télédétection domaine regrouper technique lesoutil permettre lobservation terrer lacquisition dimag satellitaire méthode proposer dan article permettre analyser automatiqued séri temporel image travail introduire approcher lanalyse clustering série Temporelles dImages Satellitaire stis processus diviser party Dans tempsnous retracer changement radiométrique dune zone représenter sonévolution cours temps graphe graph dévolution Dans undeuxième temps introduire représentation synthétique graphesdévolution pouvoir appliquer algorithme clustering permettantun regroupement type dévolution identifier expérimentation mener permettre valider approcher zone détude
73	Revue des Nouvelles Technologies de l'Information	EGC	2017	Analyse exploratoire de corpus textuels pour le journalisme d'investigation	Nous proposons un outil de visualisation analytique conçu pour etavec une journaliste d'investigation pour l'exploration de corpus textuels. Notreoutil combine une technique de biclustering disjoint pour extraire des sujets dehaut niveau, avec une méthode de biclustering non-disjoint pour révéler plus fi-nement les variantes de sujets. Une vue d'ensemble des sujets de haut niveau estproposée sous forme d'une treemap, puis une visualisation hiérarchique radialecoordonnée avec une heatmap permet d'inspecter et de comparer les variantesde sujet et d'accéder aux contenus d'origine à la demande.	Nicolas Médoc, Mohammad Ghoniem, Mohamed Nadif	http://editions-rnti.fr/render_pdf.php?p1&p=1002328	http://editions-rnti.fr/render_pdf.php?p=1002328	proposer outil visualisation analytique concevoir etavec journaliste dinvestigation lexploration corpus textuel Notreoutil combiner technique biclustering disjoindre extraire dehaut niveau méthode biclustering nondisjoint révéler plaire finement variante densembl niveau estproposer sou former dune treemap pouvoir visualisation hiérarchique radialecoordonner heatmap permettre dinspecter comparer variantesde daccéder contenir dorigine demander
74	Revue des Nouvelles Technologies de l'Information	EGC	2017	Anonymiser des données multidimensionnelles à l'aide du coclustering	Dans cet article, nous proposons une méthodologie pour anonymiserune table de données multidimensionnelles contenant des données individuelles(soit n individus décrits par m variables). L'objectif est de publier une table ano-nyme construite à partir d'une table initiale qui protège contre le risque de ré-identification. En d'autres termes, on ne doit pas pouvoir retrouver dans les don-nées publiées un individu présent dans la table originale. La solution proposéeconsite à agréger les données à l'aide d'une technique de coclustering, puis à uti-liser le modèle produit pour générer une table de données synthétiques du mêmeformat que les données initiales. Les données synthétiques, qui contiennent desindividus fictifs, peuvent maintenant être publiées. Les données produites sontévaluées en termes d'utilité pour différentes tâches de fouille (analyse explora-toire, classification) et de niveau de protection.	Françoise Fessant, Tarek Benkhelif, Fabrice Clérot	http://editions-rnti.fr/render_pdf.php?p1&p=1002277	http://editions-rnti.fr/render_pdf.php?p=1002277	Dans article proposer méthodologie anonymiserune tabler donnée multidimensionnel contenir donnée individuellessoit individu décrire variable Lobjectif publier tabler anonyme construire partir dune tabler initial protéger contrer risquer réidentification En dautr terme devoir pouvoir retrouver dan donnée publier individu présent dan tabler original solution proposéeconsit agréger donnée laid dune technique coclustering pouvoir utiliser modeler produire générer tabler donnée synthétique mêmeformat donnée initial donnée synthétique contenir desindividus fictif pouvoir maintenir publier donnée produire sontévaluer terme dutilité tâche fouiller analyser exploratoire classification niveau protection
75	Revue des Nouvelles Technologies de l'Information	EGC	2017	Application du coclustering à l'analyse exploratoire d'une table de données	La classification croisée est une technique d'analyse non superviséequi permet d'extraire la structure sous-jacente existante entre les individus et lesvariables d'une table de données sous forme de blocs homogènes. Cette tech-nique se limitant aux variables de même nature, soit numériques soit catégo-rielles, nous proposons de l'étendre en proposant une méthodologie en deuxétapes. Lors de la première étape, toutes les variables sont binarisées selon unnombre de parties choisi par l'analyste, par discrétisation en fréquences égalesdans le cas numérique ou en gardant les valeurs les plus fréquentes dans le cascatégoriel. La deuxième étape consiste à utiliser une méthode de coclusteringentre individus et variables binaires, conduisant à des regroupements d'indivi-dus d'une part, et de parties de variables d'autre part. Nous appliquons cetteméthodologie sur plusieurs jeux de donnée en la comparant aux résultats d'uneanalyse par correspondances multiples ACM, appliquée aux même données bi-narisées.	Aichetou Bouchareb, Marc Boullé, Fabrice Clérot, Fabrice Rossi	http://editions-rnti.fr/render_pdf.php?p1&p=1002279	http://editions-rnti.fr/render_pdf.php?p=1002279	classification croiser technique danalyse superviséequi permettre dextraire structurer sousjacent existant entrer individu lesvariable dune tabler donnée sou former bloc homogène technique limiter variable nature numérique catégoriel proposer létendre proposer méthodologie deuxétape étape variable binariser unnombre party choisir lanalyste discrétisation fréquence égalesdans cas numérique garder plaire fréquent dan cascatégoriel étape consister utiliser méthode coclusteringentre individu variable binaire conduire regroupement dindividus dune partir party variable dautr partir appliquer cetteméthodologie jeu donner comparer résultat duneanalyse correspondance acm appliquer donnée binariser
76	Revue des Nouvelles Technologies de l'Information	EGC	2017	Application mobile pour l'évaluation d'un algorithme de calcul de distance entre des items musicaux	Les systèmes de recommandation permettent de présenter à un utilisa-teur des éléments susceptibles de l'intéresser. La mise en place de tels systèmesdans les domaines culturels soulève souvent le questionnement de la place de ladiversité, de la nouveauté, et surtout de la découverte. Nous pensons que l'êtrehumain, bien qu'ayant ordinairement une tendance à se placer dans une zonede confort correspondant à ce qu'il connaît, apprécie occasionnellement d'êtrepoussé à des explorations le faisant sortir de sa routine. Nous avons développédans cette optique une méthode, basée sur la dissimilarité, qui élargit les centresd'intérêt des utilisateurs. Nous avons réussi à délimiter une zone intermédiaireentre des items « trop similaires » et des items « trop différents ». Afin de vali-der cette hypothèse, nous avons développé une application qui permet de testeret de valider cette méthode. Dans cet article de démonstration, nous expliquonsle concept de « zone intermédiaire », nous détaillons le fonctionnement de l'ap-plication, puis nous présentons les résultats obtenus à partir des tests effectués.	Pierre-René Lhérisson, Fabrice Muhlenbach, Pierre Maret	http://editions-rnti.fr/render_pdf.php?p1&p=1002324	http://editions-rnti.fr/render_pdf.php?p=1002324	système recommandation permettre poster utilisateur élément susceptible lintéresser miser placer systèmesdans domaine culturel soulever questionnement placer ladiversité nouveauté découvrir penser lêtrehumain quayant ordinairement tendance placer dan zonede confort correspondre quil connaître apprécier occasionnellement dêtrepousser exploration faire sortir routine développédans optique méthode baser dissimilarité élargir centresdintérêt utilisateur réussir délimiter zone intermédiaireentre item « similaire » item « » Afin valider hypothèse développer application permettre testeret valider méthode Dans article démonstration expliquonsl concept « zone intermédiaire » détailler fonctionnement lapplication pouvoir présenter résultat obtenir partir test effectuer
77	Revue des Nouvelles Technologies de l'Information	EGC	2017	Apprentissage d'espaces prétopologiques dans un cadre multi-instance pour la structuration de données	Nous présentons dans cet article une méthode supervisée de structu-ration (en DAG) d'un ensemble d'éléments. Étant donnés une structure cible etun ensemble de relations sur ces éléments, il s'agit d'apprendre un modèle destructuration par combinaison des relations initiales. Nous formalisons ce pro-blème dans le cadre de la théorie de la prétopologie qui permet d'atteindre desmodèles de structuration complexes.Nous montrons que la non-idempotence de la fonction d'adhérence rentre dansle cadre du formalisme de l'apprentissage (supervisé) multi-instance et nous pro-posons un algorithme d'apprentissage reposant sur le dénombrement des «sacs»positifs et négatifs plutôt que sur un ensemble d'apprentissage standard.Une première expérimentation de cette méthode est présentée dans un cadreapplicatif de fouille de textes, consistant à apprendre un modèle de structurationtaxonomique d'un ensemble de termes.	Gaëtan Caillaut, Guillaume Cleuziou	http://editions-rnti.fr/render_pdf.php?p1&p=1002300	http://editions-rnti.fr/render_pdf.php?p=1002300	présenter dan article méthode superviser structuration DAG dun ensemble délément donner structurer cibl etun ensembl relation élément sagit dapprendre modeler destructuration combinaison relation initial formaliser problème dan cadrer théorie prétopologie permettre datteindre desmodèle structuration complexesnous montrer nonidempotence fonction dadhérence rentrer dansle cadrer formalisme lapprentissage superviser multiinstance proposer algorithme dapprentissage reposer dénombremer « sacs»positif négatif ensemble dapprentissage standardun expérimentation méthode présenter dan cadreapplicatif fouiller texte consister modeler structurationtaxonomiqu dun ensemble terme
78	Revue des Nouvelles Technologies de l'Information	EGC	2017	Apprentissage de structures séquentielles pour l'extraction d'entités et de relations dans des textes d'appels d'offres	Dans cet article nous présentons une étude exploitant des méthodesd'apprentissage automatique de structures séquentielles pour extraire des rela-tions sémantiques dans des textes issus de bases d'appels d'offres. L'une desrelations que nous considérons concerne l'emprise d'un projet d'aménagement,caractérisée par une association entre les concepts qui définissent les infrastruc-tures (bâtiments) et les concepts qui définissent leur(s) surface(s) d'implantation.L'étude propose une analyse comparée d'approches à base de champs condi-tionnels aléatoires (CRF), de CRF d'ordre supérieur (H-CRF), de CRF semi-Markoviens, Modèles de Markov cachés (HMM) et de perceptrons structurés.	Oussama Ahmia, Nicolas Béchet, Pierre-François Marteau	http://editions-rnti.fr/render_pdf.php?p1&p=1002297	http://editions-rnti.fr/render_pdf.php?p=1002297	Dans article présenter étude exploiter méthodesdapprentissage automatique structure séquentiel extraire relation sémantique dan texte issu base dappel doffr lune desrelation considérer concern lemprise dun projet daménagementcaractériser association entrer concept définir infrastructure bâtiment concept définir surface dimplantationlétude proposer analyser comparer dapprocher baser champ conditionnel aléatoire CRF CRF dordre supérieur hcrf CRF semimarkovien Modèles Markov cacher hmm perceptron structuré
79	Revue des Nouvelles Technologies de l'Information	EGC	2017	Approche préventive pour une gestion élastique du traitement parallèle et distribué de flux de données	Dans un contexte de traitement de flux de données, il est importantde garantir à l'utilisateur des propriétés de performance, qualité des résultats etpassage à l'échelle. Mettre en adéquation ressources et besoins, pour n'allouerque les ressources nécessaires au traitement efficace des flux, est un défi d'actualitémajeur au croisement des problématiques du Big Data et du Green IT.L'approche que nous suggérons permet d'adapter dynamiquement et automatiquementle degré de parallélisme des différents opérateurs composant une requêtecontinue selon l'évolution du débit des flux traités. Nous proposons i) unemétrique permettant d'estimer l'activité future des opérateurs selon l'évolutiondes flux en entrée, ii) l'approche AUTOSCALE évaluant a priori l'intérêt d'unemodification du degré de parallélisme des opérateurs en prenant en compte l'impactsur le traitement des données dans sa globalité iii) grâce à une intégrationde notre proposition à Apache Storm, nous exposons des tests de performancecomparant notre approche par rapport à la solution native de cet outil.	Roland Kotto-Kombi, Nicolas Lumineau, Phillipe Lamarre	http://editions-rnti.fr/render_pdf.php?p1&p=1002269	http://editions-rnti.fr/render_pdf.php?p=1002269	Dans contexte traitement flux donnée importantde garantir lutilisateur propriété performance qualité résultat etpassage léchelle mettre adéquation ressourc besoin nallouerque ressource nécessaire traitement efficace flux défi dactualitémajeur croisemer problématique Big Data Green ITLapproche suggérer permettre dadapter dynamiquement automatiquementle degré parallélisme opérateur composer requêtecontinue lévolution débit flux traiter proposer ie unemétrique permettre destimer lactivité futur opérateur lévolutiond flux entrer ii lapproche autoscal évaluer priori lintérêt dunemodification degré parallélisme opérateur prendre compter limpactsur traitement donnée dan globalité iii grâce intégrationde proposition apache Storm exposer test performancecomparer approcher rapport solution natif outil
80	Revue des Nouvelles Technologies de l'Information	EGC	2017	Cadre d'Evaluation pour la Méta Analyse de Données		William Raynaut, Chantal Soulé-Dupuy, Nathalie Vallès-Parlangeau	http://editions-rnti.fr/render_pdf.php?p1&p=1002314	http://editions-rnti.fr/render_pdf.php?p=1002314	
81	Revue des Nouvelles Technologies de l'Information	EGC	2017	Classification ascendante hiérarchique à noyaux et une application aux données textuelles	La formule de Lance etWilliams permet d'unifier plusieurs méthodesde classification ascendante hiérarchique (CAH). Dans cet article, nous suppo-sons que les données sont représentées dans un espace euclidien et nous établis-sons une nouvelle expression de cette formule en utilisant les similarités cosinusau lieu des distances euclidiennes au carré. Notre approche présente les avan-tages suivants. D'une part, elle permet d'étendre naturellement les méthodesclassiques de CAH aux fonctions noyau. D'autre part, elle permet d'appliquerdes méthodes d'écrêtage permettant de rendre la matrice de similarités creuseafin d'améliorer la complexité de la CAH. L'application de notre approche surdes tâches de classification automatique de données textuelles montre d'une part,que le passage à l'échelle est amélioré en mémoire et en temps de traitement;d'autre part, que la qualité des résultats est préservée voire améliorée.	Julien Ah-Pine, Xinyu Wang	http://editions-rnti.fr/render_pdf.php?p1&p=1002306	http://editions-rnti.fr/render_pdf.php?p=1002306	formuler lancer etwilliams permettre dunifier méthodesde classification ascendant hiérarchique CAH Dans article supposer donnée représenter dan espacer euclidien établir expression formuler utiliser similarité cosinusau lieu distance euclidien carrer approcher présenter avantage dune partir permettre détendre naturellement méthodesclassique CAH fonction noyau dautr partir permettre dappliquerdes méthode décrêtage permettre matrice similarité creuseafin daméliorer complexité CAH lapplication approcher surd tâche classification automatique donnée textuel montrer dune partque passage léchelle améliorer mémoire temps traitementdautre partir qualité résultat préserver voire améliorer
82	Revue des Nouvelles Technologies de l'Information	EGC	2017	Classification d'objets 3D par extraction aléatoire de sous-parties discriminantes pour l'étude du sous-sol en prospection pétrolière	Dans cet article, nous proposons une nouvelle approche de classifi-cation d'objets 3D inspirée des Time Series Shapelets de Ye et Keogh (2009).L'idée est d'utiliser des sous-surfaces discriminantes pour la classification concer-née afin de prendre en compte la nature locale des éléments pertinents. Celapermet à l'utilisateur d'avoir connaissance des sous-parties qui ont été utilespour déterminer l'appartenance d'un objet à une classe. Les résultats obtenusconfirment l'intérêt de la sélection aléatoire de caractéristiques candidates pourla pré-sélection d'attributs en classification supervisée.	François Meunier, Christophe Marsala, Laurent Castanié, Bruno Conche	http://editions-rnti.fr/render_pdf.php?p1&p=1002283	http://editions-rnti.fr/render_pdf.php?p=1002283	Dans article proposer approcher classification dobjet 3D inspirer Time serie shapelet Ye Keogh 2009lidée dutiliser soussurface discriminanter classification concerner prendre compter nature local élément pertinent celapermet lutilisateur davoir connaissance souspartie utilespour déterminer lappartenance dun objet classer résultat obtenusconfirmer lintérêt sélection aléatoire caractéristique candidat pourla présélection dattributs classification superviser
83	Revue des Nouvelles Technologies de l'Information	EGC	2017	Classification multi-labels graduée: Apprendre les relations entre les labels ou limiter la propagation d'erreur ?	La classification multi-labels graduée est la tâche d'affecter àchaque donnée l'ensemble des labels qui lui correspondent selon une échellegraduelle de degrés d'appartenance. Les labels peuvent donc avoir à la foisdes relations d'ordre et de co-occurrence.D'un côté, le fait d'ignorer les relations entre les labels risque d'aboutirà des prédictions incohérentes, et d'un autre côté, le fait de prendre encompte ces relations risque de propager l'erreur de prédiction d'un labelà tous les labels qui lui sont reliés.Les approches de l'état d'art permettent soit d'ignorer les relations entreles labels, soit d'apprendre uniquement les relations correspondant à unestructure de dépendance figée. L'approche que nous proposons permetl'apprentissage des relations entre les labels sans fixer une structure dedépendance au préalable. Elle est basée sur un ensemble de classifieursmono-labels, un pour chaque label. L'idée est d'apprendre d'abord toutesles relations entre les labels y compris les relations cycliques. Ensuite lesdépendances cycliques sont résolues en supprimant les relations d'intérêtminimal. Des mesures sont proposées pour évaluer l'intérêt d'apprendrechaque relation. Ces mesures permettent d'agir sur le compromis entrel'apprentissage de relations pour une prédiction cohérente et la minimisa-tion du risque de la propagation d'erreur de prédiction.	Khalil Laghmari, Christophe Marsala, Mohammed Ramdani	http://editions-rnti.fr/render_pdf.php?p1&p=1002302	http://editions-rnti.fr/render_pdf.php?p=1002302	classification multilabel graduer tâcher daffecter àchaqu donner lensembl label luire correspondre échellegraduelle degré dappartenance label pouvoir foisder relation dordre cooccurrencedun côter faire dignorer relation entrer label risqu daboutirà prédiction incohérent dun côté faire prendre encompte relation risquer propager lerreur prédiction dun labelà tou label luire reliésles approche létat dart permettre dignorer relation entrel label dapprendre uniquement relation correspondre unestructure dépendanc figer Lapproche proposer permetlapprentissage relation entrer label fixer structurer dedépendance préalable baser ensemble classifieursmonolabel label Lidée dapprendre dabord toutesl relation entrer label yu relation cyclique ensuite lesdépendance cyclique résoudre supprimer relation dintérêtminimal mesure proposer évaluer lintérêt dapprendrechaqu relation mesure permettre dagir compromis entrelapprentissage relation prédiction cohérent minimisation risquer propagation derreur prédiction
84	Revue des Nouvelles Technologies de l'Information	EGC	2017	Classification parcimonieuse pour l'aide à la reconnaissance de cibles radar	Dans le présent papier, nous proposons l'étude et l'application d'unenouvelle approche pour l'aide à la reconnaissance automatique de cibles (ATR,pour Automatic Target Recognition) à partir des images à synthèse d'ouvertureinverse (ISAR, pour Inverse Synthetic Aperture Radar). Cette approche est com-posée de deux phases principales. Dans la première phase, nous utilisons deuxméthodes statistiques pour extraire les caractéristiques discriminants à partir desimages ISAR. Nous nous intéressons dans ce travail aux deux descripteurs multi-échelles issus des deux méthodes SIFT (Scale-Invariant Feature Transform) etla décomposition en ondelettes complexes DT-CWT (Dual-Tree Complex Wa-velet Transform) qui sont calculées disjointement. Ensuite, nous modélisons sé-parément les descripteurs issus des deux méthodes précédentes (SIFT et DT-CWT) par la loi Gamma. Les paramètres statistiques estimés sont utilisés pourla deuxième phase dédiée à la classification. Dans cette deuxième phase, uneclassification parcimonieuse (SRC, pour Sparse Representation-based Classifi-cation) est proposée. Afin d'évaluer et valider notre approche, nous avons eurecours aux données réelles d'images issues d'une chambre anéchoïque. Les ré-sultats expérimentaux montrent que l'approche proposée peut atteindre un tauxde reconnaissance élevé et dépasse largement l'utilisation du même descripteuravec le classifieur machine à vecteurs de support (SVM, pour Support VectorMachine).	Ayoub Karine, Abdelmalek Toumi, Ali Khenchaf, Mohammed El Hassouni	http://editions-rnti.fr/render_pdf.php?p1&p=1002305	http://editions-rnti.fr/render_pdf.php?p=1002305	Dans présent papier proposer létude lapplication dunenouvell approcher laid reconnaissance automatique cibl ATRpour Automatic Target recognition partir image synthèse douvertureinverse ISAR Inverse Synthetic Aperture Radar approcher composer phase principal Dans phase utiliser deuxméthodes statistique extraire caractéristique discriminant partir desimage ISAR intéresser dan travail descripteur multiéchell issu méthode sift scaleinvarier Feature Transform etla décomposition ondelette complexe DTCWT DualTree Complex Wavelet Transform calculer disjointement ensuite modéliser séparément descripteur issu méthode précédent sift DTCWT loi Gamma paramètre statistique estimer utiliser pourla phase dédier classification Dans phase uneclassification parcimonieux src Sparse Representationbased Classification proposer Afin dévaluer valider approcher eurecour donnée réel dimage issu dune chambrer anéchoïque résultat expérimental montrer lapproche proposer pouvoir atteindre tauxde reconnaissance élever dépasser largement lutilisation descripteuravec classifieur machiner vecteur support svm Support vectormachine
85	Revue des Nouvelles Technologies de l'Information	EGC	2017	Co-clustering de données mixtes à base des modèles de mélange	La classification croisée (co-clustering) est une technique non super-visée qui permet d'extraire la structure sous-jacente existante entre les lignes etles colonnes d'une table de données sous forme de blocs. Plusieurs approchesont été étudiées et ont démontré leur capacité à extraire ce type de structure dansune table de données continues, binaires ou de contingence. Cependant, peu detravaux ont traité le co-clustering des tables de données mixtes. Dans cet article,nous étendons l'utilisation du co-clustering par modèles à blocs latents au casdes données mixtes (variables continues et variables binaires). Nous évaluonsl'efficacité de cette extension sur des données simulées et nous discutons seslimites potentielles.	Aichetou Bouchareb, Marc Boullé, Fabrice Rossi	http://editions-rnti.fr/render_pdf.php?p1&p=1002276	http://editions-rnti.fr/render_pdf.php?p=1002276	classification croisé coclustering technique superviser permettre dextraire structurer sousjacent existant entrer ligne etl colonne dune tabler donnée sou former bloc approchesont étudier démontrer capacité extraire typer structurer dansun tabler donnée continu binaire contingence detravaux traiter coclustering table donnée mixte Dans articlenou étendre lutilisation coclustering modèle bloc latent casde donnée mixte variable continu variable binaire évaluonslefficacité extension donnée simuler discuter seslimit potentiel
86	Revue des Nouvelles Technologies de l'Information	EGC	2017	Comparaison et Évaluation de Mesures de Similarité entre Concepts d'un Treillis	Cet article se situe dans le cadre de l'analyse de concepts formels(ACF) qui fournit des classes (les extensions) d'objets partageant des carac-tères similaires (les intensions), une description par des attributs étant associéeà chaque classe. Dans un article récent, une nouvelle mesure de similarité entredeux concepts dans un treillis de concepts a été introduite, permettant une nor-malisation par la taille du treillis. Dans cet article, nous comparons cette mesurede similarité avec des mesures existantes, soit basées sur la cardinalité des en-sembles ou issues de la conception d'ontologies et basées sur la structure hiérar-chique du treillis. Une comparaison statistique avec des méthodes existantes esteffectuée et testée pour leur consistance.	Florent Domenach, George Portides	http://editions-rnti.fr/render_pdf.php?p1&p=1002289	http://editions-rnti.fr/render_pdf.php?p=1002289	article situer dan cadrer lanalyse concept formelsacf fournir classe extension dobjet partager caractère similaire intension description attribut associéeà classer Dans article récent mesurer similarité entredeux concept dan treillis concept introduire permettre normalisation tailler treillis Dans article comparer mesurede similarité mesure existant baser cardinalité ensemble issu conception dontologier baser structurer hiérarchique treillis comparaison statistique méthode existant esteffectuer tester consistance
87	Revue des Nouvelles Technologies de l'Information	EGC	2017	Conception d'un modèle généraliste pour l'évaluation d'un test A/B		Emmanuelle Claeys, Pierre Gançarski, Myriam Maumy-Bertrand, Hubert Wassner	http://editions-rnti.fr/render_pdf.php?p1&p=1002309	http://editions-rnti.fr/render_pdf.php?p=1002309	
88	Revue des Nouvelles Technologies de l'Information	EGC	2017	Découverte de sous-groupes avec les arbres de recherche de Monte Carlo	Découvrir des règles qui distinguent clairement une classe d'une autrereste un problème difficile. De tels motifs permettent de suggérer des hypothèsespouvant expliquer une classe. La découverte de sous-groupes (Subgroup Disco-very, SD), un cadre qui définit formellement cette tâche d'extraction de motifs,est toujours confrontée à deux problèmes majeurs: (i) définir des mesures dequalité appropriées qui caractérisent la singularité d'un motif et (ii) choisir uneheuristique d'exploration de l'espace de recherche correcte lorsqu'une énuméra-tion complète est irréalisable. À ce jour, les algorithmes de SD les plus efficacessont basés sur une recherche en faisceau (Beam Search, BS). La collection demotifs extraits manque cependant de diversité en raison de la nature gloutonne del'exploration. Nous proposons ici d'utiliser une technique d'exploration récente,la recherche arborescente de Monte Carlo (Monte Carlo Tree Search, MCTS).Le compromis entre l'exploitation et l'exploration ainsi que la puissance de larecherche aléatoire permettent d'obtenir une solution disponible à tout momentet de surpasser généralement les approches de type BS. Notre étude empirique,avec plusieurs mesures de qualité, sur divers jeux de données de référence et dumonde réel démontre la qualité de notre approche.	Guillaume Bosc, Jean-François Boulicaut, Chedy Raïssi, Mehdi Kaytoue	http://editions-rnti.fr/render_pdf.php?p1&p=1002287	http://editions-rnti.fr/render_pdf.php?p=1002287	découvrir règle distinguer clairement classer dune autrerest problème difficile De motif permettre suggérer hypothèsespouvant expliquer classer découvrir sousgroupe Subgroup Discovery SD cadrer définir formellement tâcher dextraction motifsest confronter problème majeur ie définir mesure dequalité approprier caractériser singularité dun motif ii choisir uneheuristiqu dexploration lespace rechercher correct lorsquune énumération complet irréalisable À jour algorithme SD plaire efficacessont baser rechercher faisceau Beam Search BS collection demotif extrait manqu diversité raison nature glouton delexploration proposer dutiliser technique dexploration récentela rechercher arborescent Monte Carlo Monte Carlo Tree search mctsle compromettre entrer lexploitation lexploration puissance larecherche aléatoire permettre dobtenir solution disponible momentet surpasser généralement approche typer BS étude empiriqueavec mesure qualité jeu donnée référence dumonde réel démontrer qualité approcher
89	Revue des Nouvelles Technologies de l'Information	EGC	2017	Deep Dive on Smart Cities by Scaling Reasoning and Interpreting the Semantics of IoT	Modern cities are facing tremendous amount of information, captured from internal in-frastructures and/or exogenous sensors, humanincluded. This talk presents how big and het-erogenous city data has been captured, represented, unified to serve one of the most pressingcity objective: improving quality of city, in particular how understanding and reducing traf-fic congestion. We will also present lessons learnt from the deployment of our system andexperimentation in Dublin (Ireland), Bologna (Italy), Miami (USA) and Rio (Brazil).	Freddy Lécué	http://editions-rnti.fr/render_pdf.php?p1&p=1002264	http://editions-rnti.fr/render_pdf.php?p=1002264	Modern citier are facing tremendous amount of information captured from internal infrastructur andor exogenous sensors humanincluded This talk present how big and heterogenou city dater has been captured represented unified to servir one of the most pressingcity objectif improving quality of city in particular how understanding and reducing traffic congestion We will also present lesser learnt from the deployment of our system andexperimentation in Dublin Ireland Bologna Italy Miami USA and Rio Brazil
90	Revue des Nouvelles Technologies de l'Information	EGC	2017	Défi EGC 2017: Modélisation Cost-Sensitive et Enrichissement de données	La conférence EGC'2017 propose un défi dont le contexte est la gestiondes espaces verts pour la ville de Grenoble, et notamment des arbres qui ysont présents. L'objectif est de proposer un modèle basé sur des données fourniesqui permettrait de prédire au mieux les arbres malades, ainsi que la localisationpotentielle de la maladie. Après avoir obtenu quelques résultats intéressantsavec des modèles standards, notre approche utilisant un modèle Cost-SensitiveOne Against All (CSOAA) nous permet d'obtenir une exactitude de 0,86, uneprécision de 0,88, et un rappel de 0,91 sur la prédiction unilabel, et une précision/rappel micro de 0,82/0,74 ainsi qu'une précision/rappel macro de 0,66/0,46pour la prédiction multilabel. L'extraction de connaissances pour la tâche 2 nousa permis de mettre en relief l'intérêt de l'ajout de données sur la nature des maladieset la concentration de la pollution dans la ville.	Vincent Levorato, Michel Lutz, Matthieu Lagacherie	http://editions-rnti.fr/render_pdf.php?p1&p=1002268	http://editions-rnti.fr/render_pdf.php?p=1002268	conférence egc2017 proposer défi contexte gestionde espace vert ville Grenoble arbre ysont présent Lobjectif proposer modeler baser donnée fourniesqui permettre prédire mieux arbre malade localisationpotentielle maladie Après obtenir résultat intéressantsavec modèle standard approcher utiliser modeler costsensitiveone Against All CSOAA permettre dobtenir exactitude 086 uneprécision 088 rappel 091 prédiction unilabel précisionrappel micro 082074 quun précisionrappel macro 066046pour prédiction multilabel lextraction connaissance tâcher 2 nousa permettre mettre relief lintérêt lajout donnée nature maladieset concentration pollution dan ville
91	Revue des Nouvelles Technologies de l'Information	EGC	2017	Description interactive de l'intérêt de l'utilisateur via l'échantillonnage de motifs	La plupart des méthodes d'extraction de motifs requièrent que l'uti-lisateur formalise son intérêt avec une mesure d'intérêt et des seuils. L'utili-sateur est souvent incapable d'expliciter son intérêt mais il saura juger si unmotif donné est pertinent ou non. Dans cet article, nous proposons une nou-velle méthode de découverte de motifs interactive en supposant que seule unepartie des données est intéressante pour l'utilisateur. En intégrant le retour utili-sateur de motifs proposés un à un, notre méthode vise à échantillonner des mo-tifs avec une probabilité proportionnelle à leur fréquence d'apparition au seindes transactions implicitement préférées par l'utilisateur. Nous démontrons quenotre méthode identifie exactement les transactions implicitement préférées parl'utilisateur sous réserve de la consistance de ses retours. Des expérimentationsmontrent les bonnes performances de l'approche en terme de précision et rappel.	Moditha Hewasinghage, Suela Isaj, Arnaud Giacometti, Arnaud Soulet	http://editions-rnti.fr/render_pdf.php?p1&p=1002332	http://editions-rnti.fr/render_pdf.php?p=1002332	méthode dextraction motif requérir lutilisateur formaliser intérêt mesurer dintérêt seuil Lutilisateur incapable dexpliciter intérêt savoir juger unmotif donner pertinent Dans article proposer méthode découvrir motif interactif supposer unepartie donnée intéressant lutilisateur En intégrer utilisateur motif proposer méthode viser échantillonner motif probabilité proportionnel fréquence dapparition seinde transaction implicitement préférer lutilisateur démontrer quenotr méthode identifier transaction implicitement préférer parlutilisateur sou réserver consistance expérimentationsmontrer performance lapproche terme précision rappel
92	Revue des Nouvelles Technologies de l'Information	EGC	2017	Détection de fausses informations dans les réseaux sociaux : vers des approches multi-modales		Cédric Maigrot, Vincent Claveau, Ewa Kijak	http://editions-rnti.fr/render_pdf.php?p1&p=1002316	http://editions-rnti.fr/render_pdf.php?p=1002316	
93	Revue des Nouvelles Technologies de l'Information	EGC	2017	Enhanced user-user collaborative filtering recommendation algorithm based on semantic ratings		Wen Zhang, Raja Chiky, Manuel Pozo	http://editions-rnti.fr/render_pdf.php?p1&p=1002318	http://editions-rnti.fr/render_pdf.php?p=1002318	
94	Revue des Nouvelles Technologies de l'Information	EGC	2017	Evolution temporelle de communautés représentatives : mesures et visualisation	La problématique de ce papier est d'identifier dans un graphe dyna-mique les communautés les plus représentatives sur une période donnée, de me-surer leur stabilité, et d'en visualiser les évolutions majeures. Notre cas d'usageconcerne l'étude de la visibilité médiatique des communautés et des individusgrâce aux données relatives aux émissions télévisuelles et radiophoniques entre2011 et 2015. A partir d'une détection de communautés sur l'intégralité de lapériode, nous proposons des mesures de stabilité et d'activité des communautéset proposons une visualisation de leur évolution temporelle.	Haolin Ren, Marie-Luce Viaud, Guy Melançon	http://editions-rnti.fr/render_pdf.php?p1&p=1002308	http://editions-rnti.fr/render_pdf.php?p=1002308	problématique papier didentifier dan graphe dynamique communauté plaire représentatif période donner mesurer stabilité den visualiser évolution majeur cas dusageconcern létude visibilité médiatique communauté individusgrâce donnée relatif émission télévisuel radiophonique entre2011 2015 partir dune détection communauter lintégralité lapériode proposer mesure stabilité dactiviter communautéset proposer visualisation évolution temporel
95	Revue des Nouvelles Technologies de l'Information	EGC	2017	Expression des connaissances en langage naturel : singularité et normalité d'une sélection		Jérémy Vizzini, Cyril Labbé, François Portet	http://editions-rnti.fr/render_pdf.php?p1&p=1002319	http://editions-rnti.fr/render_pdf.php?p=1002319	
96	Revue des Nouvelles Technologies de l'Information	EGC	2017	Extraction automatique de paysages en imagerie satellitaire et enrichissement sémantique	Nous présentons ici une méthode originale pour l'automatisation dela détection de paysages dans une image satellite. Deux enjeux majeurs ap-paraissent dans ce processus. Le premier réside dans la faculté à prendre encompte l'ensemble des connaissances expertes tout au long du travail d'analysede l'image. Le second est de réussir à structurer et pérenniser ces connaissancesde façon à les rendre interopérables et exploitables dans le cadre du web de don-nées. Nous présentons en quoi la collaboration de plusieurs stratégies alliant lestraitements de l'image, le calcul de caractéristiques spécifiques et la program-mation logique inductive (PLI), vient alimenter le processus d'automatisation,et comment l'intégration de la connaissance, au travers de la construction d'on-tologies dédiées, permet de répondre pleinement à ces enjeux.	Anne Toulet, Emmanuel Roux, Anne-Elisabeth Laques, Eric Delaître, Laurent Demagistri, Isabelle Mougenot	http://editions-rnti.fr/render_pdf.php?p1&p=1002285	http://editions-rnti.fr/render_pdf.php?p=1002285	présenter méthode original lautomatisation dela détection paysage dan imager satellite Deux enjeu majeur apparaître dan processus résider dan faculté prendre encompte lensembl connaissance expert long travail danalysede limage second réussir structurer pérenniser connaissancesde interopérable exploitable dan cadrer web donnée présenter collaboration stratégie allier lestraitement limage calcul caractéristique spécifique programmation logique inductif PLI venir alimenter processus dautomatisationet lintégration connaissance travers construction dontologie dédier permettre répondre pleinement enjeu
97	Revue des Nouvelles Technologies de l'Information	EGC	2017	Extraction de chroniques discriminantes	L'extraction de motifs séquentiels vise à extraire des comportementsrécurrents dans un ensemble de séquences. Lorsque ces séquences sont étique-tées, l'extraction de motifs discriminants engendre des motifs caractéristiquesde chaque classe de séquences. Cet article s'intéresse à l'extraction des chro-niques discriminantes où une chronique est un type de motif temporel représen-tant des durées inter-évènements quantitatives. L'article présente l'algorithmeDCM dont l'originalité réside dans l'utilisation de méthodes d'apprentissageautomatique pour extraire les intervalles temporels. Les performances compu-tationnelles et le pouvoir discriminant des chroniques extraites sont évalués surdes données synthétiques et réelles.	Yann Dauxais, David Gross-Amblard, Thomas Guyet, André Happe	http://editions-rnti.fr/render_pdf.php?p1&p=1002278	http://editions-rnti.fr/render_pdf.php?p=1002278	lextraction motif séquentiel viser extraire comportementsrécurrent dan ensemble séquence Lorsque séquence étiqueter lextraction motif discriminant engendrer motif caractéristiquesd classer séquence article sintéresse lextraction chronique discriminanter chroniquer typer motif temporel représenter durée interévènement quantitative Larticle présenter lalgorithmedcm loriginalité résider dan lutilisation méthode dapprentissageautomatiqu extraire intervalle temporel performance computationnel pouvoir discriminer chronique extraite évaluer surdes donnée synthétique réel
98	Revue des Nouvelles Technologies de l'Information	EGC	2017	Extraction de relations pour le peuplement d'une base de connaissance à partir de tweets	Dans une base de connaissance, les entités se veulent pérennes maiscertains événements induisent que les relations entre ces entités sont instables.C'est notamment le cas pour des relations entre organisations, produits, ou marques,entités qui peuvent être rachetées. Dans cet article, nous proposons une approchepermettant d'extraire des relations d'appartenance entre deux entités afin de peu-pler une base de connaissance. L'extraction des relations à partir d'une sourcedynamique d'informations telle que Twitter permet d'atteindre cet objectif entemps réel. L'approche consiste à modéliser les événements en s'appuyant surune ressource lexico-sémantique. Une fois les entités liées au Web des donnéesouvertes (en particulier DBpedia), des règles linguistiques sont appliquées pourfinalement générer les triplets RDF qui représentent les événements.	Cédric Lopez, Elena Cabrio, Frédérique Segond	http://editions-rnti.fr/render_pdf.php?p1&p=1002301	http://editions-rnti.fr/render_pdf.php?p=1002301	Dans baser connaissance entité vouloir pérenne maiscertains événement induire relation entrer entité instablescest cas relation entrer organisation produit marquesentité pouvoir racheter Dans article proposer approchepermetter dextraire relation dappartenance entrer entité peupler baser connaissance lextraction relation partir dune sourcedynamique dinformation twitter permettre datteindre objectif entemp réel Lapproche consister modéliser événement sappuyer surune ressource lexicosémantiqu entité lier web donnéesouverte dbpedia règle linguistique appliquer pourfinalement générer triplet RDF représenter événement
99	Revue des Nouvelles Technologies de l'Information	EGC	2017	Extraction des évolutions récurrentes dans un unique graphe dynamique attribué	Un grand nombre d'applications nécessitent d'analyser un unique grapheattribué évoluant dans le temps. Cette tâche est particulièrement complexe car lastructure du graphe et les attributs associés à chacun de ses noeuds ne sont pasfigés. Dans ce travail, nous nous focalisons sur la découverte de motifs récurrentsdans un tel graphe. Ces motifs, des séquences de sous-graphes connexes, représententles évolutions récurrentes de sous-ensembles de noeuds et de leurs attributs.Différentes contraintes ont été définies (e.g. fréquence, volume, connectivité,non redondance, continuité) et un algorithme original a été proposé. Lesexpérimentations réalisées sur des jeux de données synthétiques et réelles démontrentl'intérêt de l'approche proposée et son passage à l'échelle.	Zhi Cheng, Frédéric Flouvat, Nazha Selmaoui-Folcher	http://editions-rnti.fr/render_pdf.php?p1&p=1002273	http://editions-rnti.fr/render_pdf.php?p=1002273	grand nombre dapplication nécessiter danalyser grapheattribué évoluer dan temps tâcher complexe lastructure graphe attribut associer noeud pasfiger Dans travail focaliser découvrir motif récurrentsdans graph motif séquence sousgraphe connexe représententl évolution récurrent sousensemble noeud attributsdifférente contraint définir eg fréquence volume connectiviténon redondance continuité algorithme original proposer Lesexpérimentations réaliser jeu donnée synthétique réel démontrentlintérêt lapproche proposer passage léchelle
100	Revue des Nouvelles Technologies de l'Information	EGC	2017	Extraction et chaînage supervisés de connaissances d'un corpus d'entretiens en histoire des sciences		Benjamin Hervy, Matthieu Quantin, Pierre Teissier	http://editions-rnti.fr/render_pdf.php?p1&p=1002311	http://editions-rnti.fr/render_pdf.php?p=1002311	
101	Revue des Nouvelles Technologies de l'Information	EGC	2017	Extraction et Inférence de Connaissances à partir d'Assemblages Mécaniques Définis par une Représentation CAO 3D	L'extraction de connaissances à partir de modèles géométriques 3Det les raisonnements associés constituent un enjeu important pour permettre ledéveloppement d'ontologies capables de décrire fonctionnellement des produitsmanufacturés. Dans ce contexte, nous nous appuyons sur la logique déductiveapportée par une base de connaissances étroitement couplée à un modeleur géométrique3D. Les raisonnements faisant appel au concept de forme 3D restentdifficiles à formaliser et les informations géométriques difficiles à extraire. Nousproposons une formalisation de propriétés telles que 'à la même forme que','est de la même famille que' pour montrer comment l'extraction d'informationsgéométriques 3D est reliée à ces propriétés. Par la suite, une formalisation depropriétés telles que 'est un empilage', 'est un regroupement' est introduite pourmontrer les raisonnements qui contribuent à la structuration d'assemblages 3D.Ces propriétés sont illustrées à l'aide d'un exemple de pompe hydraulique.	Harold Vilmart, Jean-Claude Léon, Federico Ulliana	http://editions-rnti.fr/render_pdf.php?p1&p=1002266	http://editions-rnti.fr/render_pdf.php?p=1002266	lextraction connaissance partir modèle géométrique 3Det raisonnement associer constituer enjeu importer permettre ledéveloppement dontologie capable décrir fonctionnellement produitsmanufacturé Dans contexte appuyer logique déductiveapporter baser connaissance étroitement coupler modeleur géométrique3d raisonnement faire appel concept former 3D restentdifficile formaliser information géométrique difficile extraire nousproposon formalisation propriété former queest famille montrer lextraction dinformationsgéométriquer 3d relier propriété Par suite formalisation depropriéter empilage regroupement introduire pourmontrer raisonnement contribuer structuration dassemblager 3DCes propriété illustrer laid dun exemple pomper hydraulique
102	Revue des Nouvelles Technologies de l'Information	EGC	2017	Face2Graph: Base de données graphe et visualisation pour l'annotation d'archives vidéos	Nous proposons dans ce travail d'utiliser la flexibilité des modèlesde base de données graphe, et la représentation intuitive du réseau social afinde visuellement explorer, annoter, et vérifier des détections de visages dans unearchive de 15 années de journaux télévisés.	Adrien Dufraux, Benjamin Renoust, Shin’Ichi Satoh	http://editions-rnti.fr/render_pdf.php?p1&p=1002322	http://editions-rnti.fr/render_pdf.php?p=1002322	proposer dan travail dutiliser flexibilité modèlesde baser donnée graph représentation intuitif réseau social afinde visuellemer explorer annoter vérifier détection visage dan unearchive 15 année journal téléviser
103	Revue des Nouvelles Technologies de l'Information	EGC	2017	Faciliter les contributions personnelles pour préserver la mémoire des événements historiques	Un aspect essentiel dans la préservation du patrimoine culturel résidedans la collecte et l'assemblage des témoignages provenant de citoyens ordi-naires. Dans cet article, nous présentons une architecture logicielle facilitant lasaisie et le partage de témoignages concernant la période de la construction eu-ropéenne au Luxembourg. En rédigeant son témoignage, l'utilisateur obtient lesrésultats d'une extraction de connaissances sur le contenu saisi, indiquant no-tamment des entités et informations liées.	Pierrick Bruneau, Olivier Parisot, Thomas Tamisier	http://editions-rnti.fr/render_pdf.php?p1&p=1002290	http://editions-rnti.fr/render_pdf.php?p=1002290	aspect essentiel dan préservation patrimoine culturel résidedans collecter lassemblage témoignage provenir citoyen ordinaire Dans article présenter architecturer logiciel faciliter lasaisie partager témoignage concerner période construction européen Luxembourg En rédiger témoignage lutilisateur obtenir lesrésultat dune extraction connaissance contenir saisir indiquer entité information lier
104	Revue des Nouvelles Technologies de l'Information	EGC	2017	Génération de RDF à partir de sources de données aux formats hétérogènes	Contrairement à ce que promeut le Web des données, les données exposéespar la plupart des organisations sont dans des formats non-RDF tels queCSV, JSON, ou XML. De plus sur le Web des objets, les objets contraints préférerontdes formats binaires tels que EXI ou CBOR aux formats RDF textuels.Dans ce contexte, RDF peut toutefois servir de lingua franca pour l'interopérabilitésémantique, l'intégration de données aux formats hétérogènes, le raisonnement,et le requêtage. Dans ce but, plusieurs outils et formalismes permettentde transformer des documents non-RDF vers RDF, les plus flexibles étant baséssur des langages de transformation ou de correspondance (GRDDL, XSPARQL,R2RML, RML, CSVW, etc.). Cet article définit un nouveau langage, SPARQLGenerate,qui permet de générer du RDF à partir: (i) d'une base de données RDF,et (ii) d'un nombre quelconque de documents aux formats arbitraires. L'originalitéde SPARQL-Generate est qu'il étend SPARQL 1.1, et peut donc (i) êtreappris facilement par les ingénieurs de la connaissance familiers de SPARQL,(ii) être implémenté au dessus de n'importe quel moteur SPARQL existant, (iii)tirer parti des mécanismes d'extension de SPARQL pour prendre en compte defuturs formats.	Maxime Lefrançois, Antoine Zimmermann, Noorani Bakerally	http://editions-rnti.fr/render_pdf.php?p1&p=1002272	http://editions-rnti.fr/render_pdf.php?p=1002272	contrairement promouvoir web donnée donnée exposéespar organisation dan format nonrdf quecsv JSON xml De plaire web objet objet contraint préférerontd format binaire exi CBOR format rdf textuelsDans contexte RDF pouvoir servir lingua francer linteropérabilitésémantiqu lintégration donnée format hétérogène raisonnementet requêtage Dans boire outil formalism permettentde transformer document nonrdf ver rdf plaire flexible baséssur langage transformation correspondance grddl XSPARQLR2RML RML csvw article définir langage sparqlgeneratequi permettre générer rdf partir ie dune baser donnée RDFet ii dun nombre document format arbitraire Loriginalitéde SPARQLGenerate quil étendre sparql 11 pouvoir ie êtreappris facilement ingénieur connaissance familier sparqlii implémenter nimporte moteur sparql exister iiitirer partir mécanisme dextension sparql prendre compter defuturs format
105	Revue des Nouvelles Technologies de l'Information	EGC	2017	Gestion de Connaissances en Temps Réel depuis des Flux Massifs de Données et Apprentissage Automatique	L'analyse en temps-réel de données massives envoyées par des cap-teurs a connu ces dernières années un essor important. Du fait de l'hétérogénéitéde ces données, l'application de modèles de machine learning spécialement ca-librés pour des cas d'usages précis a permis d'extraire et d'inférer des infor-mations de très grandes valeurs. Néanmoins, peu de systèmes proposent uneimplémentation distribuée sur un vrai cluster industriel permettant de tirer profitde capacités de calcul décuplées. Nous présentons ici une démonstration de dé-tection d'anomalie sur réseau souterrain d'eau potable en île-de-France réaliséavec notre plateforme, dénotée WAVES.	Badre Belabbess, Jérémy Lhez, Olivier Curé	http://editions-rnti.fr/render_pdf.php?p1&p=1002329	http://editions-rnti.fr/render_pdf.php?p=1002329	lanalyse tempsréel donnée massif envoyer capteur connaître année essor importer faire lhétérogénéitéde donnée lapplication modèle machiner learning spécialement calibrer cas dusag précis permettre dextrair dinférer information grand système proposer uneimplémentation distribuer vrai cluster industriel permettre tirer profitd capacité calcul décupler présenter démonstration détection danomalie réseau souterrain deau potable îledefrance réaliséavec plateforme dénoter wav
106	Revue des Nouvelles Technologies de l'Information	EGC	2017	Interopérabilité sémantique libérale pour les services et les objets	Le Web des données promeut l'utilisation de RDF comme modèlepour les données structurées sur le Web. Cependant, la majorité des servicesWeb consomment et exposent principalement du CSV, JSON, ou XML, des formatnon-RDF. Il est peu probable que tous ces services se convertissent un jouraux formats RDF existants. Ceci est d'autant plus vrai dans le contexte du Webdes objets, puisque les formats RDF sont pour la plupart textuels alors que lesobjets contraints préféreront des formats binaires tels que EXI ou CBOR. Danscet article, nous proposons une approche pour permettre l'interopérabilité sémantiquede ces services et objets, tout en leur laissant la liberté d'utiliser leursformats préférés. Notre approche s'ancre sur les principes de l'architecture duWeb et ceux du Web des données liées, et repose sur la définition de PrésentationRDF. En supposant qu'une Présentation RDF soit identifiée par une IRI etdéréférençable sur le Web, nous montrons comment, avec différents protocolesdu Web, un client/serveur peut faire comprendre à l'autre partie comment lecontenu d'une message peut être interprété en RDF, ou généré à partir de RDF.Nous nommons ceci la négociation de Présentation RDF. En utilisant ces principes,nous montrons comment les services et objets existants pourraient êtrerendus interopérables à moindre coût sur le Web Sémantique.	Maxime Lefrançois	http://editions-rnti.fr/render_pdf.php?p1&p=1002271	http://editions-rnti.fr/render_pdf.php?p=1002271	web donnée promouvoir lutilisation RDF modèlepour donnée structurer web majorité servicesweb consommer exposer principalement csv JSON xml formatnonrdf tou service convertir jouraux format rdf existant dautant plaire vrai dan contexte Webdes objet format RDF textuel lesobjet contraint préférer format binaire exi CBOR Danscet article proposer approcher permettre linteropérabilité sémantiquede service objet liberté dutiliser leursformat préférer approcher sancr principe larchitecture duweb web donnée lier reposer définition présentationrdf En supposer quune présentation RDF identifier iri etdéréférençable Web montrer protocolesdu web clientserveur pouvoir faire comprendre lautre partir lecontenu dune message pouvoir interpréter RDF générer partir rdfnous nommon négociation Présentation RDF En utiliser principesnou montrer service objet existant pouvoir êtrerendus interopérabler moindre coût web Sémantique
107	Revue des Nouvelles Technologies de l'Information	EGC	2017	K-Spectral Centroïd pour des données massives	Nous nous intéressons à la classification non supervisée de séries chro-nologiques. Pour ce faire, nous utilisons l'algorithme K-Spectral Centroïd (K-SC), une variante des K-Means. K-Spectral Centroïd utilise une mesure de dis-similarité entre séries chronologiques, invariante par translation et par change-ment d'échelle. Cet algorithme est coûteux en temps de calcul : lors de la phased'affectation, il nécessite de tester toutes les translations possibles pour identifierla meilleure ; lors de la phase de représentation, le calcul du nouveau barycentrenécessite l'extraction de la plus petite valeur propre d'une matrice. Nous propo-sons dans ce travail trois optimisations de K-SC. L'identification de la meilleuretranslation peut être réalisée efficacement en utilisant la transformée de Fou-rier discrète. Chaque matrice peut être calculée incrémentalement. Le calcul dunouveau barycentre peut s'effectuer à moindre coût grâce à la méthode de lapuissance itérée. Ces trois optimisations fournissent exactement la même classi-fication que K-SC.	Brieuc Conan-Guez, Alain Gély, Lydia Boudjeloud-Assala, Alexandre Blansché	http://editions-rnti.fr/render_pdf.php?p1&p=1002275	http://editions-rnti.fr/render_pdf.php?p=1002275	intéresser classification superviser série chronologique Pour faire utiliser lalgorithme kspectral Centroïd KSC variant kmean KSpectral Centroïd utiliser mesurer dissimilarité entrer série chronologique invariante translation changement déchell algorithme coûteux temps calcul   phasedaffectation nécessiter tester translation identifierla meilleur   phase représentation calcul barycentrenécessite lextraction plaire petit propre dune matrice proposer dan travail optimisation KSC Lidentification meilleuretranslation pouvoir réaliser efficacement utiliser transformer Fourier discret matrice pouvoir calculer incrémentalemer calcul dunouveau barycentre pouvoir seffectuer moindre coût grâce méthode lapuissance itérer optimisation fournir classification KSC
108	Revue des Nouvelles Technologies de l'Information	EGC	2017	Machine Learning Based Classification of Android Apps through Text Features		Mohamed Guendouz, Abdelmalek Amine, Reda Mohamed Hamou	http://editions-rnti.fr/render_pdf.php?p1&p=1002312	http://editions-rnti.fr/render_pdf.php?p=1002312	
109	Revue des Nouvelles Technologies de l'Information	EGC	2017	Machine Learning for the Semantic Web: filling the gaps in Ontology Mining	In the Semantic Web view, ontologies play a key role. They act as shared vocabulariesto be used for semantically annotating Web resources and they allow to perform deductivereasoning for making explicit knowledge that is implicitly contained within them. However,noisy/inconsistent ontological knowledge bases may occur, being the Web a shared and dis-tributed environment, thus making deductive reasoning no more straightforwardly applicable.Machine learning techniques, and specifically inductive learning methods, could be fruitfullyexploited in this case. Additionally, machine learning methods, jointly with standard reason-ing procedure, could be usefully employed for discovering new knowledge from an ontologicalknowledge base, that is not logically derivable. The focus of the talk will be on various ontol-ogy mining problems and on how machine learning methods could be exploited for coping withthem. For ontology mining are meant all those activities that allow to discover hidden knowl-edge from ontological knowledge bases, by possibly using only a sample of data. Specifically,by exploiting the volume of the information within an ontology, machine learning methodscould be of great help for (semi-)automatically enriching and refining existing ontologies, fordetecting concept drift and novelties within ontologies and for discovering hidden knowledgepatterns (also possibly exploiting other sources of information). If on one hand this means toabandon sound and complete reasoning procedures for the advantage of uncertain conclusions,on the other hand this could allow to reason on large scale and to to dial with the intrinsic uncer-tainty characterizing the Web, that, for its nature, could have incomplete and/or contradictoryinformation.	Claudia d’Amato	http://editions-rnti.fr/render_pdf.php?p1&p=1002262	http://editions-rnti.fr/render_pdf.php?p=1002262	In the Semantic Web view ontologier play key role They act shared vocabulariesto be used for semantically annotating Web resource and they allow to perform deductivereasoning for making explicit knowledge that is implicitly contained within them Howevernoisyinconsistent ontological knowledge baser may occur being the Web shared and distributed environmer thus making deductiv reasoning no more straightforwardly applicablemachine learning technique and specifically inductif learning method could be fruitfullyexploited in this caser Additionally machiner learning method jointly with standard reasoning procedur could be usefully employed for discovering new knowledge from an ontologicalknowledge baser that is not logically derivabl The focus of the talk will be various ontology mining problems and how machiner learning method could be exploited for coping withthem For ontology mining are meer all thos activitie that allow to discover hidden knowledge from ontological knowledge baser by possibly using only sample of dater Specificallyby exploiting the volume of the information within an ontology machiner learning methodscould be of great help for semiautomatically enriching and refining existing ontologie fordetecting concept drift and noveltier within ontologie and for discovering hidden knowledgepatterns also possibly exploiting other source of information if one hand this mean toabandon sound and complete reasoning procedur for the advantage of uncertain conclusionson the other hand thi could allow to reason large scal and to to dial with the intrinsic uncertainty characterizing the Web that for its nature could hav incomplete andor contradictoryinformation
110	Revue des Nouvelles Technologies de l'Information	EGC	2017	Mesure de la confiance dans les systèmes d'information : application aux données de navires	Ces dernières années, la prolifération rapide des capteurs et des objetscommunicants de tous types a significativement enrichi le contenu des systèmesd'information. Cependant, cela suscite de nouvelles questions quant à la confianceque l'on peut accorder aux informations et aux sources d'informations. Eneffet, ces sources peuvent être leurrées ou sous l'emprise d'un tiers qui falsifieou altère les informations. Cet article propose donc d'aborder la sécurité dessystèmes d'informations sous l'angle de la confiance dans les sources d'informations.En premier lieu, la définition puis l'évaluation de la confiance dans un réseau hétérogènesont introduits. Une modélisation des sources est ensuite proposée. Laconfiance dans ces sources d'informations est abordée au travers de deux caractéristiques: la compétence et la sincérité. L'extraction de la confiance est réaliséevia un ensemble de mesures de ces deux caractéristiques. Une expérience baséesur plusieurs sources simulées à partir d'un jeu de données réelles montrent lapertinence de l'approche; approche qui peut être transposée à d'autres systèmesd'information. Cette étude est appliquée à l'analyse des données de navigationet de positionnement d'un navire.	Benjamin Costé, Cyril Ray, Gouenou Coatrieux	http://editions-rnti.fr/render_pdf.php?p1&p=1002274	http://editions-rnti.fr/render_pdf.php?p=1002274	dernière année prolifération rapide capteur objetscommunicant type significativement enrichir contenir systèmesdinformation celer susciter question confianceque lon pouvoir accorder information source dinformation eneffet source pouvoir leurrer sou lemprise dun tiers falsifieou altérer information article proposer daborder sécurité dessystèm dinformation sou langle confiance dan source dinformationsen lieu définition pouvoir lévaluation confiance dan réseau hétérogènesont introduit modélisation source ensuite proposer laconfiance dan source dinformation aborder travers caractéristique compétence sincérité lextraction confiance réaliséevier ensemble mesure caractéristique expérience baséesur source simuler partir dun jeu donnée réel montrer lapertinence lapproche approcher pouvoir transposer dautre systèmesdinformation étude appliquer lanalyse donnée navigationet positionnement dun navir
111	Revue des Nouvelles Technologies de l'Information	EGC	2017	Mesure de Similarité entre Treillis Basée sur des Correspondances Explicites	Ce document se situe dans le cadre de l'analyse de concepts formels(ACF), une méthode de hiérarchisation algébrique des données basée sur la no-tion d'intension / extension, partageant maximalement attributs et objets. Nousprésentons ici une mesure de similarité basée sur des correspondances entre deuxtreillis de Galois, définie par un modèle expressif utilisant des correspondancesentre objets et entre attributs des deux treillis. Un point clé de notre approcheest que ces correspondances peuvent ne pas être des fonctions, associant un ob-jet (resp. attribut) d'un treillis avec plusieurs objets (resp. attributs) de l'autretreillis.	Florent Domenach	http://editions-rnti.fr/render_pdf.php?p1&p=1002299	http://editions-rnti.fr/render_pdf.php?p=1002299	document situer dan cadrer lanalyse concept formelsACF méthode hiérarchisation algébrique donnée baser notion dintension   extension partager maximalemer attribut objet nousprésenton mesurer similarité baser correspondance entrer deuxtreilli Galois définir modeler expressif utiliser correspondancesentre objet entrer attribut treillis poindre cler approcheest correspondance pouvoir fonction associer objet resp attribut dun treillis objet resp attribut lautretreilli
112	Revue des Nouvelles Technologies de l'Information	EGC	2017	Nouveau modèle pour un passage à l'échelle de la 0-subsomption	Le test de -subsomption, opération fondamentale en ProgrammationLogique Inductive (PLI) pour tester la validité d'une hypothèse sur les exemples,est particulièrement coûteux. Ainsi, les systèmes d'apprentissage de PLI les plusrécents ne passent pas à l'échelle. Nous proposons donc un nouveau modèle de-subsomption fondé sur un réseau d'acteurs, dans le but de pouvoir décider lasubsomption sur de très grandes clauses.	Hippolyte Léger, Dominique Bouthinon, Mustapha Lebbah, Hanane Azzag	http://editions-rnti.fr/render_pdf.php?p1&p=1002295	http://editions-rnti.fr/render_pdf.php?p=1002295	test subsomption opération fondamental programmationlogiqu Inductive pli tester validité dune hypothèse exemplesest coûteux système dapprentissage pli plusrécent passer léchelle proposer modeler desubsomption fonder réseau dacteur dan boire pouvoir décider lasubsomption grand clause
113	Revue des Nouvelles Technologies de l'Information	EGC	2017	Optimisation des performances dans les entrepôts de données NoSQL en colonnes	Le modèle NoSQL orienté colonnes propose un schéma de donnéesflexible et hautement dénormalisé. Dans cet article, nous proposonsune méthode d'implantation d'un entrepôt de données dans un systèmeNoSQL en colonnes. Notre méthode est basée sur une stratégie de regroupementdes attributs issus des tables de faits et de dimensions, sous formede familles de colonnes. Nous utilisons deux algorithmes OEP et k-means.Pour évaluer notre méthode, nous avons effectué plusieurs tests sur lebenchmark TPC-DS au sein du SGBD NoSQL orienté colonnes Hbase,avec une architecture de type MapReduce sur une plateforme Hadoop.	Mohamed Boussahoua, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1002270	http://editions-rnti.fr/render_pdf.php?p=1002270	modeler nosql orienter colonne proposer schéma donnéesflexibl hautement dénormaliser Dans article proposonsune méthode dimplantation dun entrepôt donnée dan systèmenosql colonne méthode baser stratégie regroupementdes attribut issu table dimension sou formede famille colonne utiliser algorithme OEP kmeanspour évaluer méthode effectuer test lebenchmark tpcd sgbd nosql orienter colonne Hbaseavec architecturer typer MapReduce plateforme hadoop
114	Revue des Nouvelles Technologies de l'Information	EGC	2017	Pharmacovigilance du Web Social par une approche fondée sur les bases de connaissances du Web Sémantique		Damien Leprovost, Marie-Christine Jaulent	http://editions-rnti.fr/render_pdf.php?p1&p=1002320	http://editions-rnti.fr/render_pdf.php?p=1002320	
115	Revue des Nouvelles Technologies de l'Information	EGC	2017	PORGY : a Visual Analytics Platform for System Modelling and Analysis Based on Graph Rewriting	PORGY est un environnement interactif utilisé pour la modélisationde systèmes obtenus àpartir de règles de réécriture, pilotés à l'aide de stratégies et basées sur des graphes utilisantdes noeuds à ports. Cette démonstration présente quelques uns des aspects de visualisation ana-lytique proposés par PORGY. Cette dernière facilite la modélisation du système, sa simulationainsi que l'analyse des résultats à différentes échelles.	Bruno Pinaud, Oana Andrei, Maribel Fernández, Hélène Kirchner, Guy Melançon, Jason Vallet	http://editions-rnti.fr/render_pdf.php?p1&p=1002327	http://editions-rnti.fr/render_pdf.php?p=1002327	porgy environnement interactif utiliser modélisationde système obtenir àpartir règle réécriture piloter laid stratégie baser graphe utilisantd noeud port démonstration présenter aspect visualisation analytique proposer porgy faciliter modélisation système simulationainsi lanalyse résultat échelle
116	Revue des Nouvelles Technologies de l'Information	EGC	2017	Prédiction de défauts dans les arbres du parc végétal Grenoblois et préconisations pour les futures plantations	Nous décrivons dans cet article notre réponse au défi EGC 2017. Uneanalyse exploratoire des données a tout d'abord permis de comprendre les distri-butions des différentes variables et de détecter de fortes corrélations. Nous avonsdéfini deux variables supplémentaires à partir des variables du jeu de données.Plusieurs algorithmes de classification supervisée ont été expérimentés pour ré-pondre à la tâche numéro 1 du défi. Les performances ont été évaluées par va-lidation croisée. Cela nous a permis de sélectionner les meilleurs classifieursuni-label et multi-label. Autant sur la tâche uni-label que multi-label, le meilleurclassifieur dépasse les références d'environ 2%. Nous avons également exploréla tâche numéro 2 du défi. D'une part, des règles d'association ont été recher-chées. D'autre part, le jeu de données a été enrichi avec des connaissances tellesque des données climatiques (pluviométrie, température, vent) ou des donnéestaxonomiques dans le domaine de la botanique (famille, ordre, super-ordre). Enoutre, des données géographiques et cartographiques sont exploitées dans unoutil de visualisation d'une partie des données sur les arbres.	Yelen Per, Kevin Dalleau, Malika Smail-Tabbone	http://editions-rnti.fr/render_pdf.php?p1&p=1002284	http://editions-rnti.fr/render_pdf.php?p=1002284	décrire dan article réponse défi EGC 2017 Uneanalyse exploratoire donnée dabord permettre comprendre distribution variable détecter forte corrélation avonsdéfini variable supplémentaire partir variable jeu donnéesplusieurs algorithme classification superviser expérimenter répondre tâcher numéro 1 défi performance évaluer validation croiser celer permettre sélectionner meilleur classifieursunilabel multilabel autant tâcher unilabel multilabel meilleurclassifieur dépasser référence denviron 2 également exploréla tâch numéro 2 défi dune partir règle dassociation rechercher Dautre partir jeu donnée enrichir connaissance tellesqu donnée climatique pluviométrie température vent donnéestaxonomique dan domaine botanique famille ordre superordre enoutre donnée géographique cartographique exploiter dan unoutil visualisation dune partir donnée arbre
117	Revue des Nouvelles Technologies de l'Information	EGC	2017	Prédiction du montant levé lors d'une campagne de financement participatif par la méthode des plus proches voisins	Le financement participatif est un mode de financement d'unprojet faisant appel à un grand nombre de personnes, contrairement auxmodes de financement traditionnels. Il a connu une forte croissance avecl'émergence d'Internet et des réseaux sociaux. Cependant plus de 60 %des projets ne sont pas financés, il est donc important de bien préparersa campagne de financement. De plus, en cours de campagne, il est cru-cial d'avoir une estimation rapide de son succès afin de pouvoir réagirrapidement (restructuration, communication) : des outils de prédictionsont alors indispensables. Nous proposons dans cet article une méthodede prédiction du montant final levé lors d'une campagne de financementparticipatif utilisant l'algorithme k-NN : en utilisant l'historique de cam-pagnes passées, nous déterminons celles qui sont les plus similaires à unecampagne en cours. Nous utilisons alors les montants finaux pour faireune estimation. Nous comparons plusieurs mesures de distance pour dé-terminer les plus proches voisins. Nos résultats indiquent que le dernierétat d'une campagne seul est suffisant pour obtenir une bonne prédiction.	Alexandre Blansché, Dylan Da Conceicao, Dylan Koby	http://editions-rnti.fr/render_pdf.php?p1&p=1002303	http://editions-rnti.fr/render_pdf.php?p=1002303	financement participatif mode financement dunprojet faire appel grand nombre contrairement auxmod financement traditionnel connaître fort croissance aveclémergence dInternet réseau social plaire 60 projet financer importer préparersa campagne financement De plaire cours campagne crucial davoir estimation rapide succès pouvoir réagirrapidement restructuration communication   outil prédictionsont indispensable proposer dan article méthodede prédiction monter final lever dune campagne financementparticipatif utiliser lalgorithm knn   utiliser lhistoriqu campagne passer déterminer plaire similaire unecampagne cours utiliser montant final faireune estimation comparer mesure distancer déterminer plaire voisin résultat indiquer dernierétat dune campagne suffire obtenir prédiction
118	Revue des Nouvelles Technologies de l'Information	EGC	2017	Prévision à court terme des flux de voyageurs du réseau ferré urbain : une approche par les réseaux bayésiens dynamiques	Nous proposons une approche de prévision à court terme des flux devoyageurs du réseau ferré d'Île-de-France basée sur les réseaux bayésiens dy-namiques. La structure du modèle repose sur les relations de causalité entre lesflux adjacents et permet d'intégrer l'offre de transport. En présence de donnéesmanquantes, l'apprentissage est réalisé via l'algorithme espérance-maximisation(EM) structurel. En appliquant notre approche sur une ligne de métro, les résul-tats obtenus sont globalement supérieurs à ceux des autres méthodes testées.	Jérémy Roos, Stéphane Bonnevay, Gérald Gavin	http://editions-rnti.fr/render_pdf.php?p1&p=1002291	http://editions-rnti.fr/render_pdf.php?p=1002291	proposer approcher prévision courir terme flux devoyageur réseau ferrer dîledefrance baser réseau bayésien dynamique structurer modeler reposer relation causalité entrer lesflux adjacent permettre dintégrer loffre transport En présence donnéesmanquante lapprentissage réaliser lalgorithm espérancemaximisationem structurel En appliquer approcher ligne métro résultat obtenir globalement supérieur méthode tester
119	Revue des Nouvelles Technologies de l'Information	EGC	2017	Prototype de clustering exploratoire pour l'aide à la segmentation des clients	Le clustering est une technique largement répandue pour la définitionde profils dans le cadre de l'aide à la gestion de la relation client (CRM). Cepen-dant, les outils classiques sont généralement limités, car ils ne prennent pas encompte la connaissance métier de l'analyste et ne permettent pas l'explorationinteractive des données. Nous décrivons ici un prototype qui permet à un expertmarketing d'explorer interactivement les données pour la recherche de profilsdes clients, mais aussi d'analyser les profils construits à l'aide de différentesvisualisations synthétiques et d'étudier leurs évolutions au cours du temps.	Adnan El Moussawi, Philippe De Guis, Arnaud Giacometti, Nicolas Labroche, Arnaud Soulet	http://editions-rnti.fr/render_pdf.php?p1&p=1002323	http://editions-rnti.fr/render_pdf.php?p=1002323	clustering technique largement répandre définitionde profil dan cadrer laid gestion relation client CRM outil classique généralement limiter prendre encompte connaissance métier lanalyste permettre lexplorationinteractive donnée décrire prototype permettre expertmarketing dexplorer interactivemer donnée rechercher profilsde client danalyser profil construit laid différentesvisualisation synthétique détudier évolution cours temps
120	Revue des Nouvelles Technologies de l'Information	EGC	2017	Recommandations et prédictions de préférences basées sur la combinaison de données sémantiques et de folksonomie	Dans les systèmes de recommandation, l'approche du filtrage sur lecontenu est revenue en force face à celle du filtrage collaboratif grâce à l'arrivéedu paradigme de l'apprentissage profond et des techniques de word embedding.Dans cette même veine, l'avènement des folksonomies et du web sémantique aapporté une meilleure compréhension des profils des utilisateurs et des caracté-ristiques des articles à recommander. Dans cet article, nous nous intéressons audomaine musical et nous introduisons un nouveau calcul de mesure de préfé-rence intégrée dans un système de recommandations basées sur le contenu. Entestant notre approche sur le jeu de données Last.fm, nous montrons que l'utili-sation de termes issus d'une folksonomie associés à des informations issues duweb sémantique permet d'améliorer le processus de recommandation musicale.	Pierre-René Lhérisson, Fabrice Muhlenbach, Pierre Maret	http://editions-rnti.fr/render_pdf.php?p1&p=1002294	http://editions-rnti.fr/render_pdf.php?p=1002294	Dans système recommandation lapproch filtrage lecontenu revenir forcer face filtrage collaboratif grâce larrivéedu paradigm lapprentissage profond technique word embeddingDans veiner lavènemer folksonomie web sémantique aapporter meilleur compréhension profil utilisateur caractéristique article recommander Dans article intéresser audomain musical introduire calcul mesurer préférence intégrer dan système recommandation baser contenir entester approcher jeu donnée Lastfm montrer lutilisation terme issu dune folksonomie associer information issu duweb sémantique permettre daméliorer processus recommandation musical
121	Revue des Nouvelles Technologies de l'Information	EGC	2017	Reconnaissance de sections et d'entités dans les décisions de justice : application des modèles probabilistes HMM et CRF	Une décision de justice est un document textuel rapportant le dénoue-ment d'une affaire judiciaire. Les juristes s'en servent régulièrement commesource d'interprétation de la loi et de compréhension de l'opinion des juges.La masse disponible de décisions exige des solutions automatiques pour aiderles acteurs du droit. Nous proposons d'adresser certains des défis liés à la re-cherche et l'analyse du volume croissant de décisions de justice en France dansun projet plus global. La première phase de ce projet porte sur l'extraction d'in-formation des décisions dans l'objectif de construire une base de connaissancesjurisprudentielles structurant et organisant les décisions. Une telle base facilitel'analyse descriptive et prédictive de corpus de décisions. Cet article présenteune application des modèles probabilistes pour la segmentation des décisions etla reconnaissance d'entités dans leur contenu (lieu, date, participants, règles deloi, ...). Nos tests montrent l'avantage d'approches basées sur les champs aléa-toires conditionnels (CRF) par rapport à des modèles plus simples et rapidesbasés sur les modèles cachés de Markov (HMM). Nous présentons ici les as-pects techniques de la sélection et l'annotation du corpus d'apprentissage, et ladéfinition de descripteurs discriminants. La spécificité des textes est importanteet doit être prise en compte lors de l'application de méthodes d'extraction d'in-formation dans un domaine spécifique.	Gildas Tagny Ngompé, Sébastien Harispe, Guillaume Zambrano, Jacky Montmain, Stéphane Mussard	http://editions-rnti.fr/render_pdf.php?p1&p=1002281	http://editions-rnti.fr/render_pdf.php?p=1002281	décision justice document textuel rapporter dénouement dune affairer judiciaire juriste sen servir régulièrement commesourc dinterprétation loi compréhension lopinion jugesla masser disponible décision exiger solution automatique aiderl acteur droit proposer dadresser défi lier rechercher lanalyse volume croître décision justice France dansun projet plaire global phase projet porter lextraction dinformation décision dan lobjectif construire baser connaissancesjurisprudentielle structurer organiser décision baser facilitelanalyse descriptif prédictif corpus décision article présenteune application modèle probabiliste segmentation décision etla reconnaissance dentiter dan contenir lieu dater participant régler deloi   test montrer lavantage dapproche baser champ aléatoire conditionnel CRF rapport modèle plaire simple rapidesbaser modèle cacher Markov hmm présenter aspect technique sélection lannotation corpu dapprentissage ladéfinition descripteur discriminant spécificité texte importanteet devoir priser compter lapplication méthode dextraction dinformation dan domaine spécifique
122	Revue des Nouvelles Technologies de l'Information	EGC	2017	Sélection ciblée des descripteurs visuels pour la recherche d'images: une approche basée sur les règles d'association		Olfa Allani, Nedra Mellouli, Hajer Baazaoui , Herman Akdag	http://editions-rnti.fr/render_pdf.php?p1&p=1002317	http://editions-rnti.fr/render_pdf.php?p=1002317	
123	Revue des Nouvelles Technologies de l'Information	EGC	2017	Sélection et transformation de variables pour la classification Multi-Label par une approche MDL	La classification multi-label est une extension de la classification su-pervisée au cas de plusieurs labels. Elle a connu un regain d'intérêt récent dansla communauté du machine learning de par son utilité dans plusieurs domaines.Comme pour tout problème de machine learning, le besoin de prétraiter les don-nées multi-label est apparu comme une nécessité afin d'améliorer les perfor-mances des classifieurs. Dans cet article, nous introduisons une nouvelle mé-thode permettant de prétraiter des variables descriptives par discrétisation ougroupement de valeur, dans le cas de plusieurs labels à prédire. Le choix dumeilleur prétraitement est posé comme un problème de sélection de modèle, etest résolu au moyen d'une approche bayésienne. Une étude comparative est réa-lisée avec d'autres méthodes de l'état de l'art afin de positionner la nouvelleméthode et de montrer l'intérêt de la sélection de variables pour la classification.	Sènami C Fréjus Ahomagnon, Nicolas Voisine, Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1002296	http://editions-rnti.fr/render_pdf.php?p=1002296	classification multilabel extension classification superviser cas label connaître regain dintérêt récent dansla communauter machiner learning utilité dan domainescomme problème machiner learning besoin prétraiter donnée multilabel apparaître nécessiter daméliorer performance classifieur Dans article introduire méthode permettre prétraiter variable descriptif discrétisation ougroupemer dan cas label prédire choix dumeilleur prétraitement poser problème sélection modeler etest résoudre moyen dune approcher bayésienn étude comparatif réaliser dautr méthode létat lart positionner nouvelleméthode montrer lintérêt sélection variable classification
124	Revue des Nouvelles Technologies de l'Information	EGC	2017	Subspace Clustering et Visualisation des Flux de Données	Dans ce papier nous proposons une nouvelle approche de subspaceclustering pour les flux de données, permettant à l'utilisateur de suivre visuel-lement le changement dans le comportement du flux. Cette approche détectel'impact des variables sur l'évolution du flux, Tout en visualisant les étapes dusubspace clustering en temps réel. En premier lieu nous appliquons un clusteringsur l'ensemble de variables afin d'identifer les sous-espaces. Ensuite un cluste-ring est appliqué sur les individus dans chaque sous-espace.	Ibrahim Louhi, Lydia Boudjeloud-Assala, Thomas Tamisier	http://editions-rnti.fr/render_pdf.php?p1&p=1002293	http://editions-rnti.fr/render_pdf.php?p=1002293	Dans papier proposer approcher subspaceclustering flux donnée permettre lutilisateur visuellement changement dan comportement flux approcher détectelimpact variable lévolution flux visualiser étape dusubspace clustering temps réel En lieu appliquer clusteringsur lensembl variable didentifer sousespace ensuite clustering appliquer individu dan sousespace
125	Revue des Nouvelles Technologies de l'Information	EGC	2017	Suivi de l'évolution de Clusters de Liens dans des Réseaux Sociaux Dynamiques	De nombreuses méthodes ont été proposées pour extraire des clus-ters des réseaux sociaux. Si un travail important est aujourd'hui mené sur laconception de méthodes innovantes capables de rechercher des clusters de na-ture différente, la plupart des approches font l'hypothèse de réseaux statiques.L'une des récentes méthodes concerne notamment la recherche de liens concep-tuels. Il s'agit d'une nouvelle approche de clustering de liens, qui exploite à lafois la structure du réseau et les attributs des noeuds dans le but d'identifier desliens fréquents entre des groupes de noeuds au sein desquels les noeuds par-tagent des attributs communs. Dans ce travail, nous nous intéressons au suivides liens conceptuels dans des réseaux dynamiques, c'est-à-dire des réseaux quiconnaissent des changements structurels importants. Nous cherchons en parti-culier à comprendre comment les liens conceptuels se forment et évoluent aucours du développement du réseau. Pour ce faire, nous proposons un ensemblede mesures qui visent à capturer des comportements caractérisant l'évolutionde ces clusters. Notre approche est ainsi utilisée pour comprendre l'évolutiondes liens conceptuels extraits sur deux réseaux réels : un réseau de co-auteursd'articles scientifiques et un réseau de communications mobiles. Les résultatsobtenus permettent de mettre en lumière des tendances significatives dans l'évo-lution des clusters sur ces deux réseaux.	Erick Stattner, Martine Collard	http://editions-rnti.fr/render_pdf.php?p1&p=1002280	http://editions-rnti.fr/render_pdf.php?p=1002280	méthode proposer extraire cluster réseau social Si travail importer aujourdhui mener laconception méthode innovant capable rechercher cluster nature approche faire lhypothèse réseau statiqueslun récent méthode concerner rechercher lien conceptuel sagit dune approcher clustering lien exploiter lafois structurer réseau attribut noeud dan boire didentifier deslien fréquent entrer groupe noeud desquel noeud partager attribut commun Dans travail intéresser suivid lien conceptuel dan réseau dynamique cestàdir réseau quiconnaisser changement structurel important chercher comprendre lien conceptuel former évoluer aucour développement réseau Pour faire proposer ensemblede mesure viser capturer comportement caractériser lévolutionde cluster approcher utiliser comprendre lévolutionde lien conceptuel extrait réseau réel   réseau coauteursdarticle scientifique réseau communication mobile résultatsobtenu permettre mettre lumière tendance significatif dan lévolution cluster réseau
126	Revue des Nouvelles Technologies de l'Information	EGC	2017	Support uniforme de types de données personnalisés dans RDF et SPARQL	"Les littéraux sont les noeuds terminaux du modèle de données RDF, etpermettent d'encoder des données telles que des nombres (""12.5""ˆˆxsd:decimal),des dates (""2017-01-26T23:57:15""ˆˆxsd:dateTime), ou tout autre type d'information(""vert pomme""ˆˆex:couleur). Les moteurs RDF/SPARQL savent tester l'égalité oucomparer les littéraux RDF dont le type de données leur est connu (ce qui estle cas de xsd:decimal et xsd:dateTime). Mais lorsqu'un type de données est inconnud'un moteur RDF/SPARQL (comme ex:couleur), il n'a à priori aucun moyen d'en« découvrir » la sémantique. Dans cet article, nous attaquons ce problème et étu-dions comment permettre: (i) aux éditeurs de données de publier la définition detypes de données personnalisés sur leWeb, et (ii) aux moteurs RDF/SPARQL dedécouvrir à la volée ces types de données personnalisés, et de les utiliser de ma-nière uniforme. Nous discutons de différentes solutions possibles qui tirent partiedes principes du Web des données, et détaillons une solution concrète basée surle déréférencement et le langage JavaScript, suffisemment générique pour êtreutilisée pour des types de données personnalisés arbitrairement complexes."	Maxime Lefrançois, Antoine Zimmermann	http://editions-rnti.fr/render_pdf.php?p1&p=1002292	http://editions-rnti.fr/render_pdf.php?p=1002292	littéral noeud terminal modeler donnée RDF etpermetter dencoder donnée nombre 125ˆˆxsddecimalder dat 20170126T235715ˆˆxsddateTime typer dinformationvert pommeˆˆexcouleur moteur RDFSPARQL savoir tester légalité oucomparer littéral RDF typer donnée connaître estle cas xsddecimal xsddatetime Mais lorsquun typer donnée inconnudun moteur RDFSPARQL excouleur priori moyen den « découvrir » sémantique Dans article attaquer problème étudion permettre ie éditeur donnée publier définition detyp donnée personnaliser leweb ii moteur rdfsparql dedécouvrir voler type donnée personnaliser utiliser manière uniforme discuter solution tirer partied principe web donnée détaillon solution concret basé surle déréférencement langage javascript suffisemment générique êtreutiliser type donnée personnaliser arbitrairemer complexe
127	Revue des Nouvelles Technologies de l'Information	EGC	2017	Sur l'évaluation et l'élaboration d'un jeu de données de référence de bonne qualité en télédétection	En analyse d'images de télédétection, les données de référence, ve-nant étiqueter les objets des images, y jouent un rôle crucial mais sont parfois im-précises voire incertaines et en nombre limité. Dans cet article, nous présentonsune méthodologie pour l'amélioration de données de référence pour la télédé-tection en trois étapes : réalignement des données, évaluation via crowdsourcinget création d'un jeu de données de référence de bonne qualité.	Andrés Troya-Galvis, Pierre Gançarski, Isabelle Mougenot, Laure Berti-Equille	http://editions-rnti.fr/render_pdf.php?p1&p=1002304	http://editions-rnti.fr/render_pdf.php?p=1002304	En analyser dimager télédétection donnée référence venir étiqueter objet image yu jouer rôle crucial imprécis voire incertaine nombre limité Dans article présentonsune méthodologie lamélioration donnée référence télédétection étape   réalignemer donnée évaluation crowdsourcinget création dun jeu donnée référence qualité
128	Revue des Nouvelles Technologies de l'Information	EGC	2017	Un critère d'évaluation pour les K-moyennes prédictives	L'algorithme des K-moyennes prédictives est un des algorithmes declustering prédictif visant à décrire et à prédire d'une manière simultanée. Contr-airement à la classification supervisée et au clustering traditionnel, la perfor-mance de ce type d'algorithme est étroitement liée à sa capacité à réaliser unbon compromis entre la description et la prédiction. Or, à notre connaissance,il n'existe pas dans la littérature un critère analytique permettant de mesurer cecompromis. Cet article a pour objectif de proposer une version modifiée de l'in-dice Davies-Bouldin, nommée SDB, permettant ainsi d'évaluer la qualité des ré-sultats issus de l'algorithme des K-moyennes prédictives. Cette modification sebase sur l'intégration d'une nouvelle mesure de dissimilarité permettant d'éta-blir une relation entre la proximité des observations en termes de distance etleur classe d'appartenance. Les résultats expérimentaux montrent que la versionmodifiée de l'indice DB parvient à mesurer la qualité des résultats issus de l'al-gorithme des K-moyennes prédictives.	Oumaima Alaoui Ismaili, Vincent Lemaire, Antoine Cornuéjols	http://editions-rnti.fr/render_pdf.php?p1&p=1002288	http://editions-rnti.fr/render_pdf.php?p=1002288	lalgorithm kmoyenne prédictive algorithme declustering prédictif viser décrir prédire dune manière simultané contrairement classification superviser clustering traditionnel performance typer dalgorithm étroitement lier capacité réaliser unbon compromis entrer description prédiction Or connaissanceil nexist dan littérature critère analytique permettre mesurer cecompromi article objectif proposer version modifier lindice daviesbouldin nommé sdb permettre dévaluer qualité résultat issu lalgorithme kmoyenne prédictive modification sebase lintégration dune mesurer dissimilarité permettre détablir relation entrer proximité observation terme distancer etleur classer dappartenance résultat expérimental montrer versionmodifiée lindice DB parvenir mesurer qualité résultat issu lalgorithme kmoyenne prédictif
129	Revue des Nouvelles Technologies de l'Information	EGC	2017	Un générateur de réseaux dynamiques attribués avec structure communautaire	Nous proposons une nouvelle approche pour générer des graphes dy-namiques avec attributs munis d'une structure communautaire reflétant les pro-priétés connues des graphes de terrain comme l'attachement préférentiel ou l'ho-mophilie. Le générateur développé permet de construire une suite de graphesformant ainsi un réseau dynamique. Il offre la possibilité de visualiser l'évolu-tion de ces graphes à travers une interface dédiée. Cette interface présente aussiplusieurs mesures évaluées sur chacun des graphes du réseau pour vérifier dansquelle mesure les propriétés du réseau sont préservées au cours de son évolution.	Oualid Benyahia, Christine Largeron, Baptiste Jeudy, Osmar R. Zaïane	http://editions-rnti.fr/render_pdf.php?p1&p=1002321	http://editions-rnti.fr/render_pdf.php?p=1002321	proposer approcher générer graphe dynamique attribut munir dune structurer communautaire refléter propriété connu graphe terrain lattachement préférentiel lhomophilie générateur développer permettre construire suite graphesformer réseau dynamique offrir possibilité visualiser lévolution graphe travers interface dédier interface présent aussiplusieur mesure évaluer graphe réseau vérifier dansquelle mesurer propriété réseau préserver cours évolution
130	Revue des Nouvelles Technologies de l'Information	EGC	2017	Un Modèle de Factorisation de Poisson pour la Recommandation de Points d'Intérêt	L'explosion des volumes de données circulant sur les réseauxsociaux géo-localisés (LBSN) rend possible l'extraction des préférencesdes utilisateurs. En particulier ces préférences peuvent être utilisées pourrecommander à l'utilisateur des points d'intérêt en adéquation avec sonprofil. Aujourd'hui la recommandation de points d'intérêt est devenueune composante essentielle des LBSN. Malheureusement les méthodesde recommandation traditionnelles échouent à s'adapter aux contraintespropres aux LBSN, telles que la ”sparsité” très élevée des données, ouprendre en compte l'influence géographique. Dans ce papier nous pré-sentons un modèle de recommandation basée sur la factorisation de Pois-son qui offre une solution efficace à ces contraintes. Nous avons testénotre modèle via des expérimentations sur un jeu de données réalisteissu du LBSN Foursquare. Ces expériences nous ont permis de démon-trer une meilleure qualité de recommandation que 3 modèles de l'état-de-l'art.	Jean-Benoît Griesner, Talel Abdesssalem, Hubert Naacke	http://editions-rnti.fr/render_pdf.php?p1&p=1002307	http://editions-rnti.fr/render_pdf.php?p=1002307	lexplosion volume donnée circuler réseauxsociaux géolocaliser LBSN lextraction préférencesde utilisateur En préférence pouvoir utiliser pourrecommander lutilisateur point dintérêt adéquation sonprofil Aujourdhui recommandation point dintérêt devenueune composant essentiel lbsn malheureusement méthodesde recommandation traditionnel échouer sadapter contraintespropre lbsn ” sparsité ” élevé donnée ouprendre compter linfluenc géographique Dans papier présenter modeler recommandation basé factorisation Poisson offrir solution efficace contrainte testénotre modeler expérimentation jeu donnée réalisteissu LBSN foursquare expérience permettre démontrer meilleur qualité recommandation 3 modèle létatdelart
131	Revue des Nouvelles Technologies de l'Information	EGC	2017	Une Approche d'Extraction de Motifs Graduels (Fermés) Fréquents Sous Contrainte de la Temporalité	La fouille de motifs graduels a pour but la découverte de co-variationsfréquentes entre attributs numériques dans une base de données. Plusieurs algo-rithmes d'extraction automatique de tels motifs ont été proposés. La principaledifférence entre ces algorithmes réside dans la sémantique de variation considé-rée. Dans certains domaines d'application, on trouve des bases de données dontles objets sont munis d'une relation d'ordre temporel. Ainsi, du fait de leur sé-mantique de variation, les algorithmes de la littérature sont inadaptés pour detelles données. Dans ce contexte, nous proposons une approche de fouille demotifs graduels sous contrainte d'ordre temporel, qui réduit le nombre de motifsgénérés. Une étude expérimentale sur des bases de données paléoécologiquespermet d'apprendre les groupements d'indicateurs qui modélisent l'évolution dela biodiversité. Les connaissances apportées par ces groupements montre l'inté-rêt de notre approche pour le domaine environnemental.	Jerry Lonlac, Yannick Miras, Aude Beauger, Marie Pailloux, Jean-Luc Peiry , Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1002282	http://editions-rnti.fr/render_pdf.php?p=1002282	fouiller motif graduel boire découvrir covariationsfréquente entrer attribut numérique dan baser donnée algorithm dextraction automatique motif proposer principaledifférence entrer algorithme résider dan sémantique variation considérer Dans domaine dapplication trouver base donnée dontl objet munir dune relation dordre temporel faire sémantique variation algorithme littérature inadapter detell donnée Dans contexte proposer approcher fouiller demotifs graduel sou contraint dordre temporel réduire nombre motifsgénéré étude expérimental base donnée paléoécologiquespermet dapprendre groupement dindicateur modélisent lévolution dela biodiversiter connaissance apporter groupement montr lintérêt approcher domaine environnemental
132	Revue des Nouvelles Technologies de l'Information	EGC	2017	Une approche innovante pour la compréhension des comportements de diffusion : personnalité et neutralité		Didier Henry, Erick Stattner, Martine Collard	http://editions-rnti.fr/render_pdf.php?p1&p=1002315	http://editions-rnti.fr/render_pdf.php?p=1002315	
133	Revue des Nouvelles Technologies de l'Information	EGC	2017	Une approche logique pour la fouille de règles d'association	La découverte de règles d'association à partir de données transaction-nelles est une tâche largement étudiée en fouille de données. Les algorithmesproposés dans ce cadre partagent la même méthodologie en deux étapes à savoirl'énumération des itemsets fréquents suivie par l'étape de génération de règles.Dans cet article, nous proposons une nouvelle approche basée sur la satisfiabilitépropositionnelle pour extraire les règles d'association en une seule étape. Pourmontrer la flexibilité et la déclarativité de notre approche, nous considérons éga-lement deux autres variantes, à savoir la fouille de règles d'association ferméeset la fouille de règles indirectes. Les expérimentation sur plusieurs jeux de don-nées montrent que notre approche offre de meilleures performances comparée àdes approches spécialisées.	Abdelhamid Boudane, Said Jabbour, Lakhdar Sais, Yakoub Salhi	http://editions-rnti.fr/render_pdf.php?p1&p=1002298	http://editions-rnti.fr/render_pdf.php?p=1002298	découvrir règle dassociation partir donnée transactionnel tâcher largement étudier fouiller donnée algorithmesproposé dan cadrer partager méthodologie étape savoirlénumération itemset fréquent létape génération règlesdan article proposer approcher basé satisfiabilitépropositionnelle extraire règle dassociation étape pourmontrer flexibilité déclarativité approcher considérer également variante savoir fouiller règle dassociation ferméeset fouiller règle indirect expérimentation jeu donnée montrer approcher offrir meilleure performance comparer àd approche spécialiser
134	Revue des Nouvelles Technologies de l'Information	EGC	2017	Une approche sociologique de la place des calculs dans les mondes numériques	Dans cette présentation, on souhaite présenter un regard de sociologue sur les transformationssociales, politiques et culturelles du développement des mondes numériques dans nos sociétés.Les enjeux que doivent relever la fabrication d'environnements informatiques prennentaujourd'hui de plus en plus d'importance : protection de la vie privée, personnalisation descalculs, guidage des conduites, ouverture des données, éthique des automates, etc. Commentnos sociétés réagissent-elles et s'adaptent-elles à ces mutations ? Dans cette cnférence, on proposeune réflexion sur le rôle joué par les algorithmes du web dans la construction de l'espacepublic numérique. Comment les calculateurs produisent-ils de la visibilité ? A partir de quelsprincipes le PageRank de Google, les métriques du web social ou les outils de recommandationdécident-ils de donner la prééminence à telle information plutôt qu'à telle autre ? Cesdifférentes familles de calcul cherchent à mesurer et à valoriser des principes différents : lapopularité, l'autorité, la réputation et la prédiction efficace. L'approche proposée dans cetteconférence soutient que les manières de calculer enferment des représentations particulièresdes individus et de leur place dans nos sociétés. Comprendre les algorithmes c'est aussi unmoyen de redonner du pouvoir aux utilisateurs et de favoriser une critique éclairée de la manièredont le calcul s'introduit de plus en plus dans nos vies numériques.	Dominique Cardon	http://editions-rnti.fr/render_pdf.php?p1&p=1002263	http://editions-rnti.fr/render_pdf.php?p=1002263	Dans présentation souhaiter poster regard sociologue transformationssociale politique culturel développement monde numérique dan sociétésles enjeu devoir relever fabrication denvironnement informatique prennentaujourdhui plaire plaire dimportanc   protection vie privé personnalisation descalcul guidage conduite ouverture donnée éthique automate Commentnos société réagissentell sadaptentell mutation   Dans cnférence proposeune réflexion rôle jouer algorithme web dan construction lespacepublic numérique calculateur produisentil visibilité   partir quelsprincipe pagerank Google métrique web social outil recommandationdécidentil donner prééminence information quà   cesdifférent famille calcul chercher mesurer valoriser principe   lapopularité lautorité réputation prédiction efficace lapproche proposer dan cetteconférence soutenir manière calculer enfermer représentation particulièresd individu placer dan société Comprendre algorithme cest unmoyen redonner pouvoir utilisateur favoriser critique éclairé manièredont calcul sintroduit plaire plaire dan vie numérique
135	Revue des Nouvelles Technologies de l'Information	EGC	2017	Une mesure d'expertise pour le crowdsourcing	Le crowdsourcing, un enjeu économique majeur, est le fait d'externaliserune tâche interne d'une entreprise vers le grand-public, la foule. C'estainsi une forme de sous-traitance digitale destinée à toute personne susceptiblede pouvoir réaliser la tâche demandée généralement rapide et non automatisable.L'évaluation de la qualité du travail des participants est cependant un problèmemajeur en crowdsourcing. En effet, les contributions doivent être contrôlées pourassurer l'efficacité et la pertinence d'une campagne. Plusieurs méthodes ont étéproposées pour évaluer le niveau d'expertise des participants. Ce travail a la particularitéde proposer une méthode de calcul de degrés d'expertise en présencede données dont l'ordre de classement est connu. Les degrés d'expertise sont ensuiteconsidérés sur des données sans ordre pré-établi. Cette méthode fondée surla théorie des fonctions de croyance tient compte des incertitudes des réponseset est évaluée sur des données réelles d'une campagne réalisée en 2016.	Hosna Ouni, Arnaud Martin, Laetitia Gros, Mouloud Kharoune, Zoltan Miklos	http://editions-rnti.fr/render_pdf.php?p1&p=1002267	http://editions-rnti.fr/render_pdf.php?p=1002267	crowdsourcing enjeu économique majeur faire dexternaliserun tâcher interne dune entreprendre ver grandpublic fouler Cestainsi former soustraitance digital destiner susceptiblede pouvoir réaliser tâcher demander généralement rapide automatisablelévaluation qualité travail participant problèmemajeur crowdsourcing En contribution devoir contrôler pourassurer lefficacité pertinence dune campagne méthode étéproposer évaluer niveau dexpertise participant travail particularitéde proposer méthode calcul degré dexpertise présencede donner lordre classement connaître degré dexpertise ensuiteconsidérer donnée ordre préétablir méthode fonder surler théorie fonction croyance compter incertitude réponseset évaluer donnée réel dune campagne réaliser 2016
136	Revue des Nouvelles Technologies de l'Information	EGC	2017	Une métrique de sélection de variables appliquée à la centralité et à la détection des rôles communautaires	La F-Mesure de trait est une métrique de sélection de variables statistiquesans paramètres qui a montré de bonnes performances pour la classification,l'étiquetage de clusters ou encore la mesure de qualité des clusters. Danscet article, nous proposons d'évaluer son utilisation dans le contexte des graphesde terrain et de leur structure communautaire pour bénéficier de son systèmesans paramètres et de ses performances bien évaluées. Nous étudions donc surdes graphes synthétiques réalistes les corrélations qui existent entre la F-Mesurede trait et certaines mesures de centralité, mais surtout avec des mesures destinéesà caractériser le rôle communautaire des noeuds. Nous montrons ainsi quecette mesure est liée à la centralité des noeuds du réseau, et qu'elle est particulièrementadaptée à la mesure de leur connectivité au regard de la structurede communautés. Nous observons par ailleurs que les mesures usuelles de détectiondes rôles communautaires sont fortement dépendantes de la taille descommunautés alors que celles que nous proposons sont par définition liées à ladensité de la communauté, ce qui rend les résultats comparables d'un réseau àun autre. Ceci offre donc la possibilité d'applications comme le suivi temporelde la structure des communautés. Enfin, le processus de sélection appliqué auxnoeuds permet de disposer d'un système universel, contrairement aux seuils fixésauparavant empiriquement pour l'établissement des rôles communautaires.	Nicolas Dugué, Jean-Charles Lamirel	http://editions-rnti.fr/render_pdf.php?p1&p=1002265	http://editions-rnti.fr/render_pdf.php?p=1002265	fmesure traire métrique sélection variable statistiquesan paramètre montrer performance classificationlétiquetage cluster mesurer qualité cluster Danscet article proposer dévaluer utilisation dan contexte graphesde terrain structurer communautaire bénéficier systèmesan paramètre performance évaluer étudier surde graphe synthétique réaliste corrélation exister entrer FMesurede traire mesure centralité mesure destinéesà caractériser rôle communautaire noeud montrer quecette mesurer lier centralité noeud réseau particulièrementadapter mesurer connectivité regard structurede communauter observer ailleur mesure usuel détectionde rôl communautaire fortement dépendanter tailler descommunauter proposer définition lier ladensité communauté résultat dun réseau àun offrir possibilité dapplicater temporelde structurer communauté Enfin processus sélection appliquer auxnoeuds permettre disposer dun système universel contrairement seuil fixésauparavant empiriquement létablissement rôle communautaire
137	Revue des Nouvelles Technologies de l'Information	EGC	2017	Une plateforme d'analyse d'opinions en temps réel sur Twitter avec recommandation		Noureddine Azzouza, Karima Akli-Astouati, Samy Ait-Bachir, Amira Oussalah	http://editions-rnti.fr/render_pdf.php?p1&p=1002313	http://editions-rnti.fr/render_pdf.php?p=1002313	
138	Revue des Nouvelles Technologies de l'Information	EGC	2017	Veille d'Information sur le Web avec Re-Watch	Les algorithmes d'apprentissage automatique peuvent être utilisés pourcréer des outils de recommandation qui permettent de prédire la pertinence d'undocument pour une thématique de veille donnée en se basant sur les précédentsjugements de pertinence donnés pour cette thématique pour d'autres documents.Ces outils de recommandation permettent de filtrer dans un flux entrant de do-cuments ceux qui sont susceptibles d'être pertinents sans que l'utilisateur aitbesoin de déterminer lui-même les mots clefs marquant l'adéquation d'un do-cument pour un sujet de la veille. Bien que cette problématique de rechercheait été abondamment abordée, les outils de veille d'information pour le web in-tégrant un apprentissage en sont encore à leur balbutiements. Nous présentonsici l'application web Re-Watch permettant la définition d'un thème de veille, lasélection de sources d'information sur le web relatives à ce thème et l'adaptationdes scores de pertinence des documents aux retours de l'utilisateur. L'applicationpermet aussi, pour chaque thème, une auto-évaluation de la qualité du filtrage etune interrogation du moteur de recherche Google. Cette application encore encours de développement est néanmoins actuellement fonctionnelle et accessiblesur le web à l'url suivante : http://www.specific search.com.	Christophe Brouard, Christian Pomot	http://editions-rnti.fr/render_pdf.php?p1&p=1002331	http://editions-rnti.fr/render_pdf.php?p=1002331	algorithme dapprentissage automatique pouvoir utiliser pourcréer outil recommandation permettre prédire pertinence dundocument thématique veiller donner baser précédentsjugement pertinence donner thématique dautre documentsces outil recommandation permettre filtrer dan flux entrer document susceptible dêtre pertinent lutilisateur aitbesoin déterminer luimême clef marquer ladéquation dun document veiller problématique rechercheer abondamment aborder outil veiller dinformation web intégrer apprentissage balbutiement présentonsici lapplication web ReWatch permettre définition dun thème veiller lasélection source dinformation web relatif thème ladaptationd score pertinence document lutilisateur Lapplicationpermet thème autoévaluation qualité filtrage etun interrogation moteur rechercher Google application encours développement actuellement fonctionnel accessiblesur web lurl   httpwwwspecific searchcom
139	Revue des Nouvelles Technologies de l'Information	EGC	2017	Vers un échantillonnage de flux de données transformé		Olivier Parisot, Thomas Tamisier	http://editions-rnti.fr/render_pdf.php?p1&p=1002310	http://editions-rnti.fr/render_pdf.php?p=1002310	
140	Revue des Nouvelles Technologies de l'Information	EGC	2017	Vers une instance française de NELL : chaîne TLN multilingue et modélisation d'ontologie	Nous présentons les étapes de préparation de la création d'une ins-tance nouvelle de NELL dédiée au français. NELL est à la fois un processusde lecture et de compréhension automatique du Web et un ensemble de basede connaissances de faits en anglais, en portuguais et très prochainement enfrançais. Cette mise en place de la nouvelle instance de NELL a donné lieu àl'amélioration de la chaîne NLP en la généralisant au multilangue, ainsi qu'audéveloppement d'une ontologie par correspondance avec l'ontologie en anglais.Nous présenterons le processus de mise en place et de lancement de la nouvelleinstance NELL Français avec l'interface de visualisation et de supervision hu-maine des données collectées.	Maisa Cristina Duarte, Pierre Maret	http://editions-rnti.fr/render_pdf.php?p1&p=1002326	http://editions-rnti.fr/render_pdf.php?p=1002326	présenter étape préparation création dune instance nell dédier français nell processusde lectur compréhension automatique web ensemble based connaissance anglais portuguai prochainement enfrançai miser placer instance nell donner lieu àlamélioration chaîner NLP généraliser multilangue quaudéveloppement dune ontologie correspondance lontologie anglaisnou présenter processus miser placer lancement nouvelleinstance nell français linterface visualisation supervision humain donnée collecter
141	Revue des Nouvelles Technologies de l'Information	EGC	2017	VIPE : un outil interactif de classification multilabel de messages courts	Nous présentons un outil interactif de classification multilabel déve-loppé au sein du groupe Orange et utilisé pour l'analyse d'opinions. Basé sur unalgorithme de factorisation rapide de matrice, il permet à un utilisateur d'impor-ter des textes courts (tweets, mails, enquêtes, ...), de définir des labels d'intérêts(« client globalement satisfait », « évoque la rapidité du débit »,...) et de propo-ser pour chaque texte des recommandations de labels et pour chaque label desrecommandations de textes.	Franck Meyer, Sylvie Tricot, Pascale Kuntz, Wissam Siblini	http://editions-rnti.fr/render_pdf.php?p1&p=1002330	http://editions-rnti.fr/render_pdf.php?p=1002330	présenter outil interactif classification multilabel développer grouper Orange utiliser lanalyse dopinion baser unalgorithme factorisation rapide matrice permettre utilisateur dimporter texte court tweet mail enquêt   définir label dintérêts « client globalement satisfaire » « évoqu rapidité débit » proposer texte recommandation label label desrecommandation texte
142	Revue des Nouvelles Technologies de l'Information	EGC	2017	“Engage moi”: From retrieval effectiveness, user satisfaction to user engagement	The effective prediction of a click remains a primary challenge in the areas of search, digitalmedia and online advertising. In the context of search, satisfying a userâ&#728;A ´ Zs information needby returning results that they will click on is an important objective in any information retrievalsystem. Consequently, information retrieval systems have had a long and varied history of howto evaluate their effectiveness of responding to a given query. However, building such a systemthat not only only returns relevant results to a user query but also encourages a long-termrelationship between the user and the system is far more challenging. In this talk, we reviewthe current state-of-the-art evaluation approaches for search before exploring other ways ofquantifying more long-term engagement measures. Finally, the talk ends with a proposal ofhow the two approaches can be considered together to create a service that optimises for thequery and the longer term engagement aspects.	Mounia Lalmas	http://editions-rnti.fr/render_pdf.php?p1&p=1002261	http://editions-rnti.fr/render_pdf.php?p=1002261	The effectif prediction of click remain primary challeng in the areer of search digitalmedier and online advertising In the context of search satisfying userâ728a ´ zs information needby returning results that they will click is an importer objectiver in any information retrievalsystem Consequently information retrieval systems hav had long and varied history of howto evaluate their effectiveness of responding to given query However building such systemthat not only only return relever results to user query boire also encourag longtermrelationship between the user and the system is far more challenging In this talk we reviewthe current stateoftheart evaluation approach for search before exploring other ways ofquantifying more longterm engagement measur Finally the talk end with proposal ofhow the two approacher can be considered together to create service that optimiser for thequery and the longer term engagement aspect
143	Revue des Nouvelles Technologies de l'Information	EGC	2016	A Relevant Passage Retrieval and Re-ranking Approach for Open-Domain Question Answering	Les systèmes de questions-réponses (SQR)s visent à retourner directement des réponsesprécises à des questions posées en langage naturel. L'extraction et le reclassement des passagessont considérés comme les tâches les plus difficiles dans un SQR typique et exigent encore uneffort non trivial. Dans cet article, nous proposons une nouvelle approche pour L'extraction etle reclassement des passages en utilisant les n-grammes et SVM. Notre système d'extractionde passages basé sur la technique des n-grammes repose sur une nouvelle mesure de similaritéentre un passage et une question. Les passages extraits sont ensuite réordonnés en utilisant unmodèle basé sur RankSVM combinant différentes mesures de similarité afin de retourner lepassage le plus pertinent pour une question donnée. Nos expériences et nos résultats étaientprometteurs et ont démontré que notre approche est concurrentielle.	Nouha Othman, Rim Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1002161	http://editions-rnti.fr/render_pdf.php?p=1002161	système questionsréponse sqrs viser retourner réponsesprécise question poser langage lextraction reclassement passagessont considérer tâche plaire difficile dan sqr typique exiger uneffort trivial Dans article proposer approcher lextraction etle reclassemer passage utiliser ngramme svm système dextractionde passage baser technique ngramme reposer mesurer similaritéentre passage question passage extrait ensuite réordonner utiliser unmodèl baser ranksvm combiner mesure similarité retourner lepassage plaire pertinent question donner expérience résultat étaientprometteur démontrer approcher concurrentiel
144	Revue des Nouvelles Technologies de l'Information	EGC	2016	Adaptation des Mappings entre Systèmes d'Organisation de la Connaissance du domaine Biomédical	Cette thèse de doctorat propose une approche originale pour adapter les mappingsbasés sur les changements détectés dans l'évolution de SOCs du domaine biomédical.Notre proposition consiste à comprendre précisément les mappings entre SOCs, à exploiterles types de changements intervenant lorsque les SOCs évoluent, puis à proposerdes actions de modification des mappings appropriées. Nos contributions sont multiples: (i) nous avons réalisé un travail expérimental approfondi pour comprendre l'évolutiondes mappings entre SOCs; nous proposons des méthodes automatiques (ii) pour analyserles mappings affectés par l'évolution de SOCs, et (iii) pour reconnaître l'évolutiondes concepts impliqués dans les mappings via des patrons de changement; enfin (iv)nous proposons des techniques d'adaptation des mappings à base d'heuristiques. Nousproposons un cadre complet pour l'adaptation des mappings, appelé DyKOSMap, etun prototype logiciel. Nous avons évalué les méthodes proposées et le cadre formel avecdes jeux de données réelles contenant plusieurs versions de mappings entre SOCs du domaine biomédical. Les résultats des expérimentations ont démontré l'efficacité desprincipes sous-jacents à l'approche proposée. La maintenance des mappings, en grandepartie automatique, est de bonne qualité..	Julio Cesar Dos Reis	http://editions-rnti.fr/render_pdf.php?p1&p=1002152	http://editions-rnti.fr/render_pdf.php?p=1002152	thèse doctorat proposer approcher original adapter mappingsbasé changement détecter dan lévolution SOCs domaine biomédicalnotre proposition consister comprendre précisément mapping entrer SOCs exploiterl type changement intervenir soc évoluer pouvoir proposerd action modification mapping approprier contribution ie réaliser travail expérimental approfondir comprendre lévolutionde mapping entrer soc proposer méthode automatique ii analyserl mapping affecter lévolution SOCs iii reconnaître lévolutionde concept impliquer dan mapping patron changement ivnou proposer technique dadaptation mapping baser dheuristiqu nousproposon cadrer complet ladaptation mapping appeler DyKOSMap etun prototyp logiciel évaluer méthode proposer cadrer formel avecd jeu donnée réel contenir version mapping entrer SOCs domaine biomédical résultat expérimentation démontrer lefficaciter desprincipes sousjacent lapproche proposer maintenance mapping grandepartie automatique qualité
145	Revue des Nouvelles Technologies de l'Information	EGC	2016	Analyse d'activité et exposition de la vie privée sur les médias sociaux	Anonymous use of Social network do not prevent users from privacy risks resulting frominfering and cross-checking information published by themselves or their relationhips. Withthis in mind we have conducted a survey in order to measure sensitiveness of personal datapublished on social media and to analyze the users behaviors. We have shown that 76 %of internet users that have answered the survey are vulnerable to identity or sensitive datadisclosure. Our study is completed by the description of an automatic procedure that showshow easily these vulnerabilities can be exploited and motivates the need for more advancedprotection mechanisms.	Younes Abid, Abdessamad Imine, Amedeo Napoli, Chedy Raïssi, Marc Rigolot, Michaël Rusinowitch	http://editions-rnti.fr/render_pdf.php?p1&p=1002223	http://editions-rnti.fr/render_pdf.php?p=1002223	anonymou user of Social network do not prevent user from privacy risk resulting frominfering and crosschecking information published by themselv or their relationhip Withthis in mind we hav conducted survey in order to measure sensitiveness of personal datapublished social medier and to analyze the user behaviors We hav shown that 76 of internet user that hav answered the survey are vulnerabl to identity or sensitif datadisclosure Our study is completed by the description of an automatic procedure that showshow easily these vulnerabilitier can be exploited and motivat the need for more advancedprotection mechanism
146	Revue des Nouvelles Technologies de l'Information	EGC	2016	Analyse exploratoire par k-Coclustering avec Khiops CoViz	En analyse exploratoire, l'identification et la visualisation des interactionsentre variables dans les grandes bases de données est un défi (Dhillon et al.,2003; Kolda et Sun, 2008). Nous présentons Khiops CoViz, un outil qui permetd'explorer par visualisation les relations importantes entre deux (ou plusieurs)variables, qu'elles soient catégorielles et/ou numériques. La visualisation d'unrésultat de coclustering de variables prend la forme d'une grille (ou matrice) dontles dimensions sont partitionnées: les variables catégorielles sont partitionnéesen clusters et les variables numériques en intervalles. L'outil permet plusieurs variantesde visualisations à différentes échelles de la grille au moyen de plusieurscritères d'intérêt révélant diverses facettes des relations entre les variables.	Bruno Guerraz, Marc Boullé, Dominique Gay, Vincent Lemaire, Fabrice Clérot	http://editions-rnti.fr/render_pdf.php?p1&p=1002206	http://editions-rnti.fr/render_pdf.php?p=1002206	En analyser exploratoire lidentification visualisation interactionsentre variable dan grand base donnée défi Dhillon al2003 Kolda Sun 2008 présenter Khiops coviz outil permetdexplorer visualisation relation important entrer plusieursvariabl catégoriel etou numérique visualisation dunrésultat coclustering variable prendre former dune griller matrice dontl dimension partitionner variable catégoriel partitionnéesen cluster variable numérique intervalle Loutil permettre variantesde visualisation échelle griller moyen plusieurscritère dintérêt révéler facett relation entrer variable
147	Revue des Nouvelles Technologies de l'Information	EGC	2016	Analyse géographique de séries de publications : application aux conférences EGC	Dans cet article, nous présentons une méthodologie originale permettantde faire des analyses scientométriques basées sur trois dimensions (spatiale,temporelle et thématique) à partir d'un corpus de publications. Cette méthodologiecomporte 3 étapes : (1) la préparation et la validation des données pourcompléter les critères usuels tels que les noms d'auteurs, affiliation, ... par descritères spatiaux, temporels et thématiques ; (2) l'indexation des contenus despublications et métadonnées associées ; (3) l'analyse et/ou la recherche d'informationmultidimentionnelle. Les expérimentations sont menées sur la série depublications des conférences EGC de 2004 à 2015.	Eric Kergosien, Marie-Noëlle Bessagnet, Christian Sallaberry, Annig Le Parc - Lacayrelle, Albert Royer	http://editions-rnti.fr/render_pdf.php?p1&p=1002190	http://editions-rnti.fr/render_pdf.php?p=1002190	Dans article présenter méthodologie original permettantde faire analyse scientométriqu baser dimension spatialetemporell thématique partir dun corpu publication méthodologiecomporte 3 étape   1 préparation validation donnée pourcompléter critère usuel nom dauteur affiliation   descritère spatial temporel thématique   2 lindexation contenu despublication métadonné associé   3 lanalys etou rechercher dinformationmultidimentionnell expérimentation mener série depublication conférence egc 2004 2015
148	Revue des Nouvelles Technologies de l'Information	EGC	2016	Analyses synchroniques et diachroniques des thématiques EGC- Défi ECG 2016	Les articles scientifiques publiés dans les actes des conférences EGC,qui se déroulent chaque année depuis 2001, constituent la richesse de ces évènementsmettant en avant le fer de lance de la recherche francophone portantsur la gestion et l'extraction de connaissances. Nous nous sommes penchés surl'analyse de ces publications scientifiques afin d'en extraire l'essence en termesde thématiques de recherches abordées. Premièrement, nous avons analysé lespoints communs et les spécificités des publications dans les différentes éditionsde la conférence ainsi que les principales différences entre les éditions consécutives.Puis nous nous sommes intéressés à la façon dont les publications s'articulentautour des thématiques extraites et sur lesquelles nous avons essayé devisualiser une approximation sémantique. Enfin nous nous sommes intéresséà l'évolution des thématiques depuis les débuts de cette conférence et jusqu'àl'édition 2015.	Sofiane Bouzid, Adrian Tanasescu	http://editions-rnti.fr/render_pdf.php?p1&p=1002195	http://editions-rnti.fr/render_pdf.php?p=1002195	article scientifique publier dan acte conférence egcqui dérouler année 2001 constituer richesse évènementsmetter fer lancer rechercher francophone portantsur gestion lextraction connaissance penché surlanalys publication scientifique den extrair lessenc termesde thématique recherche aborder analyser lespoint commun spécificité publication dan éditionsde conférence principal différencer entrer édition consécutivespuis intéresser publication sarticulentautour thématique extrait essayer devisualiser approximation sémantique intéresséà lévolution thématique conférence jusquàlédition 2015
149	Revue des Nouvelles Technologies de l'Information	EGC	2016	Apprentissage du signal prix de l'électricité. Arbres de régression, séries temporelles et prédictions à long terme	Predicting the price of the electricity commodity in the long term is a challenge that currenttechniques do not meet satisfactorily (Karakatsani et Bunn, 2010; Weron, 2014). In this paper,we introduce a new regression tree based model that yields good predictions on a long-termperiod with low computational resources requirements. Our approach is validated by temporalseries collected from an electricity provider.	Louis-Victor Pasquier, Antoine Cornuéjols, Suzanne Pinson	http://editions-rnti.fr/render_pdf.php?p1&p=1002222	http://editions-rnti.fr/render_pdf.php?p=1002222	Predicting the price of the electricity commodity in the long term is challenge that currenttechniqu do not meet satisfactorily Karakatsani Bunn 2010 Weron 2014 In this paperwe introduc new regression tree based model that yields good prediction longtermperiod with low computational resourc requirement Our approach is validated by temporalserier collected from an electricity provider
150	Revue des Nouvelles Technologies de l'Information	EGC	2016	Approche de Clustering de Flux basée sur les Graphes de Voisinage	We propose a neighborhood-based approach for data streams clustering. Instead of processingeach new element one by one, we propose to process each group of new elements simultaneously.A neighborhood-based clustering is applied on each new group. We also definean incremental construction method of the neighborhood graph based on the stream evolution.To validate the approach, we apply it to multiple data sets and we compare it with variousstream clustering approaches.	Ibrahim Louhi, Lydia Boudjeloud-Assala, Thomas Tamisier	http://editions-rnti.fr/render_pdf.php?p1&p=1002217	http://editions-rnti.fr/render_pdf.php?p=1002217	We proposer neighborhoodbased approach for dater streams clustering Instead of processingeach new element one by one we proposer to process each group of new elements simultaneouslyA neighborhoodbased clustering is applied each new group We also definean incremental construction method of the neighborhood graph based the stream evolutionTo validate the approach we apply it to dater set and we comparer it with variousstream clustering approach
151	Revue des Nouvelles Technologies de l'Information	EGC	2016	Arbres de modèles et flux de données incomplets	Model tree is a useful and convenient method for predictive analytics in data streams.Often, this issue is solved by pre-processing techniques applied prior to the training phase ofthe model. In this article, we propose a new method that estimates and adjusts missing valuesbefore the model tree training. A prototype was developed and tested on several data streams.	Olivier Parisot, Yoanne Didry, Thomas Tamisier, Benoît Otjacques	http://editions-rnti.fr/render_pdf.php?p1&p=1002210	http://editions-rnti.fr/render_pdf.php?p=1002210	model tree is useful and convenient method for predictiv analytics in dater streamsOften this issu is solved by preprocessing technique applied prior to the training phase ofthe model In this article we proposer new method that estimat and adjusts missing valuesbefor the model tree training prototype wa developed and tested several dater stream
152	Revue des Nouvelles Technologies de l'Information	EGC	2016	Associer argumentation et simulation en aide à la décision : Illustration en agroalimentaire	Prendre une décision impliquant plusieurs acteurs aux objectifs divergentsnécessite de considérer des informations tant qualitatives – les préférencesdes acteurs sur les décisions possibles – que quantitatives – les paramètres servantd'indicateurs pour les acteurs. Dans cet article nous nous intéressons à l'associationde ces deux types d'approches. Le modèle qualitatif considéré est l'argumentation.Le modèle quantitatif simulant les scénarios découlant de chaquedécision est la dynamique des systèmes. Cet article s'intéresse aux éléments permettantde connecter les deux formalismes. Un exemple en agroalimentaire vienten appui à cette réflexion.	Rallou Thomopoulos, Sébastien Gaucel, Bernard Moulin	http://editions-rnti.fr/render_pdf.php?p1&p=1002187	http://editions-rnti.fr/render_pdf.php?p=1002187	prendre décision impliquer acteur objectif divergentsnécessite considérer information qualitatif – préférencesde acteur décision – quantitatif – paramètre servantdindicateur acteur Dans article intéresser lassociationd type dapprocher modeler qualitatif considérer largumentationle modeler quantitatif simuler scénario découler chaquedécision dynamique système article sintéresse élément permettantde connecter formalisme exemple agroalimentaire vienten appui réflexion
153	Revue des Nouvelles Technologies de l'Information	EGC	2016	Caractérisation d'instances d'apprentissage pour un méta-mining évolutionnaire	Machine learning has proven to be a powerful tool in diverse fields, and is getting moreand more widely used by non-experts. One of the foremost difficulties they encounter liesin the choice and calibration of the machine learning algorithm to use. Our objective is thusto provide assistance in the matter, using a meta-learning approach based on an evolutionaryheuristic. We introduce here this approach as a potential solution to the limitation of currentdata characterization.	William Raynaut, Chantal Soulé-Dupuy, Nathalie Vallès-Parlangeau, Cédric Dray, Philippe Valet	http://editions-rnti.fr/render_pdf.php?p1&p=1002221	http://editions-rnti.fr/render_pdf.php?p=1002221	machiner learning has proven to be powerful tool in fields and is getting moreand more widely used by nonexperts One of the foremost difficultier they encounter liesin the choice and calibration of the machiner learning algorithm to us Our objectif is thusto provid assistance in the matter using metalearning approach based an evolutionaryheuristic We introduce her this approach potential solution to the limitation of currentdata characterization
154	Revue des Nouvelles Technologies de l'Information	EGC	2016	Catégorisation et Désambiguïsation des Intérêts des Individus dans le Web Social	Cet article présente une approche pour la catégorisation et la désambiguïsationdes intérêts que les individus renseignent sur les réseaux sociaux enutilisant Wikipédia.	Coriane Nana Jipmo, Gianluca Quercini, Nacéra Bennacer	http://editions-rnti.fr/render_pdf.php?p1&p=1002212	http://editions-rnti.fr/render_pdf.php?p=1002212	article présenter approcher catégorisation désambiguïsationd intérêt individu renseigner réseau social enutiliser Wikipédia
155	Revue des Nouvelles Technologies de l'Information	EGC	2016	Clustering par apprentissage de distance guidé par des préférences sur les attributs	Ces dernières années de nombreuses méthodes semi-supervisées declustering ont intégré des contraintes entre paires d'objets ou d'étiquettes declasse, afin que le partitionnement final soit en accord avec les besoins de l'utilisateur.Pourtant dans certains cas où les dimensions d'études sont clairementdéfinies, il semble opportun de pouvoir directement exprimer des contraintessur les attributs pour explorer des données. De plus, une telle formulation permettraitd'éviter les écueils classiques de la malédiction de la dimensionnalitéet de l'interprétation des clusters. Cet article propose de prendre en compte lespréférences de l'utilisateur sur les attributs afin de guider l'apprentissage de ladistance pendant le clustering. Plus précisément, nous montrons comment paramétrerla distance euclidienne par une matrice diagonale dont les coefficientsdoivent être au plus proche des poids fixés par l'utilisateur. Cette approche permetd'ajuster le clustering pour obtenir un compromis entre les approches guidéespar les données et par l'utilisateur. Nous observons que l'ajout des préférencesest parfois essentiel pour atteindre un clustering de meilleure qualité.	Adnan El Moussawi, Ahmed Cheriat, Arnaud Giacometti, Nicolas Labroche, Arnaud Soulet	http://editions-rnti.fr/render_pdf.php?p1&p=1002185	http://editions-rnti.fr/render_pdf.php?p=1002185	année méthode semisuperviser declustering intégré contrainte entrer paire dobjet détiquett declasse partitionnement final accord besoin lutilisateurpourter dan cas dimension détud clairementdéfinier sembler opportun pouvoir exprimer contraintessur attribut explorer donnée De plaire formulation permettraitdéviter écueil classique malédiction dimensionnalitéet linterprétation cluster article proposer prendre compter lespréférence lutilisateur attribut guider lapprentissage ladistance pendre clustering plaire précisément montrer paramétrerler distancer euclidien matrice diagonal coefficientsdoiver plaire poids fixé lutilisateur approcher permetdajuster clustering obtenir compromis entrer approche guidéespar donnée lutilisateur observer lajout préférencesest essentiel atteindre clustering meilleur qualité
156	Revue des Nouvelles Technologies de l'Information	EGC	2016	Clustering visuel semi-interactif	Nous proposons dans cet article une approche de clustering visuelsemi-interactif. L'approche proposée utilise la perception visuelle pour guiderl'utilisateur dans le processus interactif. Les clusters sont extraits de manièresuccessive et itérative, puis évalués selon leur ordre d'extraction. Pour l'utilisateur,l'approche semi-interactive permet non seulement d'évaluer les classes enfonction d'un critère déterminé mais aussi d'évaluer l'influence de l'extractiond'un cluster sur ceux précédemment extraits. Un protocole de test est présentéafin de comparer cette approche avec les approches purement automatiques etpurement interactives. Cet article est un résumé d'un papier accepté 1 pour unjournal international.	Lydia Boudjeloud, Philippe Pinheiro, Alexandre Blansché, Thomas Tamisier, Benoît Otjacques	http://editions-rnti.fr/render_pdf.php?p1&p=1002183	http://editions-rnti.fr/render_pdf.php?p=1002183	proposer dan article approcher clustering visuelsemiinteractif Lapproche proposer utiliser perception visuel guiderlutilisateur dan processus interactif cluster extraire manièresuccessiv itératif pouvoir évaluer ordre dextraction Pour lutilisateurlapproche semiinteractive permettre dévaluer classe enfonction dun critère déterminer dévaluer linfluence lextractiondun cluster précédemment extrait protocole test présentéafin comparer approcher approche purement automatique etpurement interactiver article résumer dun papier accepter 1 unjournal international
157	Revue des Nouvelles Technologies de l'Information	EGC	2016	Combinaison de méthodes numériques et symboliques pour l'analyse de données métabolomiques	Our work consists in developing a workflow using Knowledge Discovery methodologiesto propose advanced predictive biomarkers discovery solutions from metabolomic data. Wepropose to use machine learning algorithms for feature selection and FCA for visualization.	Dhouha-Grissa, Blandine Comte, Estelle Pujos-Guillot, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1002228	http://editions-rnti.fr/render_pdf.php?p=1002228	Our work consist in developing workflow using Knowledge Discovery methodologiesto proposer advanced predictiv biomarker discovery solution from metabolomic dater Wepropose to us machiner learning algorithm for feature selection and FCA for visualization
158	Revue des Nouvelles Technologies de l'Information	EGC	2016	Concept drift vs suicide: comment l'un peut prévenir l'autre?	Le suicide devient d'année en année une problématique plus préoccupante.Les organismes de santé tels que l'OMS se sont engagés à réduire lenombre de suicides de 10% dans l'ensemble des pays membres d'ici 2020. Sile suicide est généralement un geste impulsif, il existe souvent des actes et desparoles qui peuvent révéler un mal être et représenter des signes précurseurs deprédispositions au suicide. L'objectif de cette étude est de mettre en place unsystème pour détecter semi-automatiquement ces comportements et ces parolesau travers des réseaux sociaux. Des travaux précédents ont proposé la classificationde messages issus de Twitter suivant des thèmes liés au suicide : tristesse,blessures psychologiques, état mental, etc. Dans cette étude, nous ajoutons la dimensiontemporelle pour prendre en compte l'évolution de l'état des personnesmonitorées. Nous avons implémenté pour cela différentes méthodes d'apprentissagedont une méthode originale de concept drift. Nous avons expérimenté avecsuccès cette méthode sur des données réelles issues du réseau social Facebook.	Cédric Maigrot, Sandra Bringay, Jérôme Azé	http://editions-rnti.fr/render_pdf.php?p1&p=1002172	http://editions-rnti.fr/render_pdf.php?p=1002172	suicider devenir danner année problématique plaire préoccupantele organism santé loms engager réduire lenombre suicide 10 dan lensembl pays membre dici 2020 Sile suicider généralement geste impulsif exister acte desparol pouvoir révéler mal représenter signe précurseur deprédisposition suicider Lobjectif étude mettre placer unsystèm détecter semiautomatiquement comportement parolesau travers réseau social travail précédent proposer classificationde messag issu twitter thème lier suicider   tristesseblessur psychologique mental Dans étude ajouter dimensiontemporelle prendre compter lévolution létat personnesmonitorée implémenter celer méthode dapprentissagedont méthode original concept drift expérimenter avecsuccè méthode donnée réel issu réseau social Facebook
159	Revue des Nouvelles Technologies de l'Information	EGC	2016	Construction incrémentale d'une structure hiérarchique pour l'exploration visuelle et interactive de larges collections d'images	Dans cet article, nous étudions de manière conjointe la construction etl'exploration visuelle d'une structure de classification pour de très grande based'images. Pour garantir que la structure construite vérifiera les contraintes detaille nécessaires à sa visualisation dans une interface Web tout en reflétant lespropriétés topologiques des données (clusters), nous combinons la classificationhiérarchique de BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies)avec la construction de graphes de voisinage : un graphe de voisinageest créé et mis à jour de manière incrémentale pour représenter les fils de chaquenoeud de l'arbre. De plus, un ensemble d'images représentatives est remonté àchaque noeud interne pour guider l'utilisateur lors de l'exploration visuelle del'arbre. L'ensemble des algorithmes utilisés sont incrémentaux pour gérer l'insertionde nouvelles images dans la collection. Nous présentons les premiersrésultats sur des dizaines de milliers d'images qui peuvent être ainsi structuréesen une minute de temps de calcul. L'exploration dans l'interface est fluide grâceaux propriétés de la structure construite.	Frédéric Rayar, Sabine Barrat, Fatma Bouali, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1002184	http://editions-rnti.fr/render_pdf.php?p=1002184	Dans article étudier manière conjoint construction etlexploration visuel dune structurer classification grand basedimage Pour garantir structurer construire vérifier contrainte detaille nécessaire visualisation dan interface Web refléter lespropriété topologique donnée cluster combiner classificationhiérarchique BIRCH Balanced Iterative Reducing and Clustering using Hierarchiesavec construction graphe voisinage   graphe voisinageest créer mettre jour manière incrémental représenter fils chaquenoeud larbre De plaire ensemble dimag représentatif remonter àchaqu noeud interne guider lutilisateur lexploration visuel delarbr lensembl algorithme utiliser incrémental gérer linsertionde image dan collection présenter premiersrésultat dizaine millier dimage pouvoir structuréesen minuter temps calcul lexploration dan linterface fluide grâceaux propriété structurer construit
160	Revue des Nouvelles Technologies de l'Information	EGC	2016	Contributions à la coloration des hypergraphes basées sur les traverses minimales	In this paper, we propose two contributions about the determination of chromatic numberand the verification of the 2-colorability property. We introduce an unreleased relation betweenthe problem of hypergraph coloring and the computation of minimal transversals hypergraphand, especially, a subset of them. Thereby, we propose two algorithms in order to optimize theverification of the 2-colorability property of hypergraphs and the evaluation of the chromaticnumber. Experiments carried out on several types of hypergraphs, showed that our algorithmobtains very interesting results.	M. Nidhal Jelassi, Sadok Ben Yahia, Christine Largeron	http://editions-rnti.fr/render_pdf.php?p1&p=1002224	http://editions-rnti.fr/render_pdf.php?p=1002224	in this paper we proposer two contributer about the determination of chromatic numberand the verification of the 2colorability property We introduce an unreleased relation betweenthe problem of hypergraph coloring and the computation of minimal transversals hypergraphand especially subset of them Thereby we proposer two algorithm in order to optimize theverification of the 2colorability property of hypergraph and the evaluation of the chromaticnumber experiment carried out several typer of hypergraph showed that our algorithmobtain very interesting result
161	Revue des Nouvelles Technologies de l'Information	EGC	2016	Découverte de labels dupliqués par l'exploration du treillis des classifieurs binaires	L'analyse des données comportementales représente aujourd'hui ungrand enjeu. Tout individu génère des traces d'activité et de mobilité. Lorsqu'ellessont associées aux individus, ou labels, qui les ont créées, il est possiblede construire un modèle qui prédit avec précision l'appartenance d'une nouvelletrace. Sur internet, il est cependant fréquent qu'un utilisateur possède différentesidentités virtuelles, ou labels doublons. Les ignorer provoque une grande réductionde la précision de l'identification. Il est ainsi question dans cet article du problèmede déduplication de labels, et l'on présente une méthode originale baséesur l'exploration du treillis des classifieurs binaires. Chaque sous-ensemble delabels est classifié face à son complémentaire et des contraintes rendent possiblel'identification des labels doublons en élaguant l'espace de recherche. Des expérimentationssont menées sur des données issues du jeu vidéo STARCRAFT 2.Les résultats sont de bonne qualité et encourageants.	Quentin Labernia, Victor Codocedo, Mehdi Kaytoue, Celine Robardet	http://editions-rnti.fr/render_pdf.php?p1&p=1002177	http://editions-rnti.fr/render_pdf.php?p=1002177	lanalyse donnée comportemental représenter aujourdhui ungrand enjeu individu génèr trace dactivité mobilité lorsquellessont associer individu label créer possiblede construire modeler prédire précision lappartenance dune nouvelletrace Sur internet fréquent quun utilisateur posséder différentesidentiter virtuel label doublon ignorer provoquer grand réductiond précision lidentification question dan article problèmede déduplication label lon présenter méthode original baséesur lexploration treillis classifieur binaire sousensembl delabel classifier face complémentaire contrainte possiblelidentification label doublon élaguer lespace rechercher expérimentationssont mener donnée issu jeu vidéo starcraft 2Les résultat qualité encourageant
162	Revue des Nouvelles Technologies de l'Information	EGC	2016	Découverte de motifs intelligibles et caractéristiques d'anomalies dans les traces unitaires	"De nombreuses industries manufacturières s'intéressent aujourd'hui àl'exploitation des grandes collections de traces unitaires. Les applications sontmultiples et vont du simple ""reporting"" à la détection de fraudes en passant parla gestion de retours ou encore la mise en évidence d'incohérences dans lescircuits de distribution. Une étape importante consiste à détecter des anomaliesdans des collections de traces. Si les travaux concernant la détection d'anomaliessont assez nombreux, peu permettent de caractériser les anomalies détectées parune description intelligible. Étant donné un ensemble de traces unitaires, nousdéveloppons une méthode d'extraction de motifs pour détecter et contextualiserdes comportements non conformes à un modèle expert (fourni ou construit àpartir des données). Le degré d'anomalie est alors quantifié grâce à la proportiondu nombre de mouvements des objets qui ne sont pas prévus dans le modèleexpert. Cette recherche est financée partiellement par un programme industrielqui ne permet ni de dévoiler le contexte concret ni de parler des données réelles.Ainsi, nous validons empiriquement la valeur ajoutée de la méthode proposéepar l'étude de traces de mobilité dans un jeu vidéo : nous pouvons alors discuterd'un motif qui explicite les raisons de l'inexpérience de certains joueurs."	Olivier Cavadenti, Victor Codocedo, Mehdi Kaytoue, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1002153	http://editions-rnti.fr/render_pdf.php?p=1002153	industrie manufacturier sintéressent aujourdhui àlexploitation grand collection trace unitaire application sontmultipl aller simple reporting détection fraude passer gestion miser évidence dincohérencer dan lescircuit distribution étape important consister détecter anomaliesdan collection trace Si travail concerner détection danomaliessont permettre caractériser anomalie détecter parune description intelligible donner ensemble trace unitaire nousdéveloppon méthode dextraction motif détecter contextualiserde comportement conforme modeler expert fournir construire àpartir donnée degré danomalie quantifier grâce proportiondu nombre mouvement objet prévoir dan modèleexpert rechercher financer partiellemer programmer industrielqui permettre dévoiler contexte concret donnée réellesainsi valider empiriquement ajouter méthode proposéepar létude trace mobilité dan jeu vidéo   pouvoir discuterdun motif expliciter raison linexpérience joueur
163	Revue des Nouvelles Technologies de l'Information	EGC	2016	Défi EGC 2016 : Analyse par Motifs Fréquents et Topic Modeling	Dans le domaine de l'analyse de textes, l'extraction de motifs est unetechnique très populaire pour mettre en évidence des relations fréquentes entreles mots. De même, les techniques de topic modeling ont largement fait leurspreuves lorsqu'il s'agit de classer automatiquement des ensembles de textes partageantdes thématiques similaires. Ainsi, ce papier a pour ambition de montrerl'intérêt de l'utilisation conjointe de ces deux techniques afin de mettre en évidence,sous la forme d'un graphe biparti, des mots partageant des thématiquessimilaires mais aussi leurs relations fréquentes, intra et inter thématiques. Lesdonnées du Défi EGC 2016 permettent de valider l'intérêt de l'approche, touten montrant l'évolution des thématiques et des mots clés parmi les papiers de laconférence EGC sur ces onze dernières années.	Julien Aligon, Fabrice Guillet, Julien Blanchard, Fabien Picarougne	http://editions-rnti.fr/render_pdf.php?p1&p=1002192	http://editions-rnti.fr/render_pdf.php?p=1002192	Dans domaine lanalyse texte lextraction motif unetechniqu populaire mettre évidence relation fréquent entrel De technique topic modeling largement faire leurspreuves lorsquil sagit classer automatiquement ensemble texte partageantde thématique similaire papier ambition montrerlintérêt lutilisation conjoint technique mettre évidencesous former dun graph biparti partager thématiquessimilaire relation fréquent intra inter thématique lesdonner Défi EGC 2016 permettre valider lintérêt lapproche touten montrer lévolution thématique cler papier laconférence egc année
164	Revue des Nouvelles Technologies de l'Information	EGC	2016	Défi EGC 2016 Vues Conceptuelles des Collaborations aux Conférences EGC depuis 2004: Une modélisation descriptive	Dans ce travail, nous analysons les données concernant les articles publiés à laconférence EGC. Notre objectif est d'identifier et de comprendre les tendances en matièrede collaborations. Pour ce faire, nous adoptons une modélisation descriptive, à travers uneapproche réseau qui consiste à générer tout d'abord le réseau de collaborations des auteursà partir des données. Nous enrichissons ensuite les noeuds de ce réseau d'une dizained'attributs individuels extraits à partir des données. Enfin, nous recherchons des vuesconceptuelles, une approche récente de clustering de liens, qui permet de synthétiser desréseaux en mettant en évidence les ensembles d'attributs retrouvés fréquemment liés dansle réseau. Les résultats obtenus montrent les tendances existantes dans les comportementsde collaborations. Dans ce papier, nous présentons ces tendances et montrons commentelles évoluent selon différents seuils d'extraction.	Erick Stattner	http://editions-rnti.fr/render_pdf.php?p1&p=1002196	http://editions-rnti.fr/render_pdf.php?p=1002196	Dans travail analyser donnée concerner article publier laconférence egc objectif didentifier comprendre tendance matièred collaboration Pour faire adopter modélisation descriptif travers uneapproch réseau consister générer dabord réseau collaboration auteursà partir donnée enrichir ensuite noeud réseau dune dizainedattribut individuel extrait partir donnée rechercher vuesconceptuelle approcher récent clustering lien permettre synthétiser desréseaux mettre évidence ensemble dattribut retrouver fréquemment lier dansle réseau résultat obtenir montrer tendance existant dan comportementsde collaboration Dans papier présenter tendance montron commentell évoluer seuil dextraction
165	Revue des Nouvelles Technologies de l'Information	EGC	2016	Détection de données aberrantes à partir de motifs fréquents sans énumération exhaustive	La détection de données aberrantes (outliers) consiste à détecter desobservations anormales au sein des données. Durant la dernière décennie, desméthodes de détection d'outliers utilisant les motifs fréquents ont été proposées.Elles extraient dans une première phase tous les motifs fréquents, puis assignentà chaque transaction un score mesurant son degré d'aberration (en fonction dunombre de motifs fréquents qui la couvre). Dans cet article, nous proposons deuxnouvelles méthodes pour calculer le score d'aberration fondé sur les motifs fréquents(FPOF). La première méthode retourne le FPOF exact de chaque transactionsans extraire le moindre motif. Cette méthode s'avère en temps polynomialpar rapport à la taille du jeu de données. La seconde méthode est une méthodeapprochée où l'utilisateur final peut contrôler l'erreur maximale sur l'estimationdu FPOF. Une étude expérimentale montre l'intérêt des deux méthodes pour lesjeux de données volumineux où une approche exhaustive échoue à calculer unesolution exacte. Pour un même nombre de motifs, la précision de notre méthodeapprochée est meilleure que celle de la méthode classique.	Arnaud Giacometti, Arnaud Soulet	http://editions-rnti.fr/render_pdf.php?p1&p=1002155	http://editions-rnti.fr/render_pdf.php?p=1002155	détection donnée aberrant outlier consister détecter desobservation anormal donnée Durant décennie desméthode détection doutlier utiliser motif fréquent proposéesell extraire dan phase tou motif fréquent pouvoir assignentà transaction score mesurer degré daberration fonction dunombr motif fréquent couvrir Dans article proposer deuxnouvell méthode calculer score daberration fonder motif fréquentsfpof méthode retourn fpof exact transactionsan extraire moindre motif méthode savère temps polynomialpar rapport tailler jeu donnée second méthode méthodeapprochée lutilisateur final pouvoir chuter lerreur maximal lestimationdu fpof étude expérimental montr lintérêt méthode lesjeux donnée volumineux approcher exhaustif échouer calculer unesolution exact Pour nombre motif précision méthodeapprochée meilleur méthode classique
166	Revue des Nouvelles Technologies de l'Information	EGC	2016	Détection de messages falsifiés de localisation de navires	The Automatic Identification System was initially designed for safety purposes. However,the system is not secured and the messages contain errors and undergo attacks and falsifications.This article proposes a methodological approach for the detection of falsified AISmessages.	Clément Iphar, Aldo Napoli, Cyril Ray	http://editions-rnti.fr/render_pdf.php?p1&p=1002229	http://editions-rnti.fr/render_pdf.php?p=1002229	The Automatic Identification system wa initially designed for safety purposer Howeverthe system is not secured and the messag contain errors and undergo attack and falsificationsthi article propos methodological approach for the detection of falsified aismessage
167	Revue des Nouvelles Technologies de l'Information	EGC	2016	Enrichissement de schéma multidimensionnel en constellation grâce à la Classification Ascendante Hiérarchique	Les hiérarchies sont des structures cruciales dans un entrepôt de donnéespuisqu'elles permettent l'agrégation de mesures dans le but de proposerune vue analytique plus ou moins globale sur les données entreposées, selon leniveau hiérarchique auquel on se place. Cependant, peu de travaux s'intéressentà la construction de hiérarchies, via un algorithme de fouille de données, prenanten compte le contexte multidimensionnel de la dimension concernée. Danscet article, nous proposons donc un algorithme, implémenté sur une architectureROLAP, permettant d'enrichir une dimension avec des données factuelles.	Lucile Sautot, Sandro Bimonte, Ludovic Journaux, Arnaud Larrère, Kévin Saint-Paul, Bruno Faivre	http://editions-rnti.fr/render_pdf.php?p1&p=1002209	http://editions-rnti.fr/render_pdf.php?p=1002209	hiérarchie structure crucial dan entrepôt donnéespuisquelle permettre lagrégation mesure dan boire proposerune analytique plaire global donnée entreposer leniveau hiérarchique placer travail sintéressentà construction hiérarchie algorithme fouiller donnée prenanten compter contexte multidimensionnel dimension concerner Danscet article proposer algorithme implémenter architecturerolap permettre denrichir dimension donnée factuel
168	Revue des Nouvelles Technologies de l'Information	EGC	2016	Évaluation et Prédiction de la Centralité de Groupes de Recherche dans un Réseau de Collaborations Scientifiques	De nos jours, il y a un fort intérêt pour de nouvelles méthodes d'évaluationdes groupes de recherche afin de quantifier l'impact de leur travail surtoute la communauté scientifique et de tenter de prédire leurs performances dansle futur. Dans ce contexte, nous proposons une nouvelle approche hybride quimesure la centralité d'un groupe de chercheurs publiants. Cette mesure profitede l'expressivité et de la capacité d'inférence apportées par une modélisationontologique des groupes et des thématiques inférées, et d'une modélisation engraphe qui permet d'explorer les interactions entre ces différents groupes aufil du temps. Ce modèle permet également de détecter les groupes capables decollaborer avec d'autres tout en maintenant un haut niveau de production, etd'identifier ceux qui sont plus déterminants sur les thématiques déduites, afin dedévelopper des collaborations de recherche plus fructueuses.	Aurélien Bossard, Mario Cataldi, Myriam Lamolle, Chan Le Duc	http://editions-rnti.fr/render_pdf.php?p1&p=1002193	http://editions-rnti.fr/render_pdf.php?p=1002193	De jour yu fort intérêt méthode dévaluationde groupe rechercher quantifier limpact travail surtoute communauté scientifique tenter prédire performance dansle futur Dans contexte proposer approcher hybride quimesure centralité dun grouper chercheur publiant mesurer profitede lexpressivité capacité dinférenc apporter modélisationontologique groupe thématique inféré dune modélisation engraph permettre dexplorer interaction entrer groupe aufil temps modeler permettre également détecter groupe capable decollaborer dautre maintenir niveau production etdidentifier plaire déterminant thématique déduit dedévelopper collaboration rechercher plaire fructueuser
169	Revue des Nouvelles Technologies de l'Information	EGC	2016	Exploration des Données du Défi EGC 2016 à l'aide d'un Système d'Information Logique	Nous présentons dans cet article les méthodes employées et les résultatsobtenus en réponse au Défi EGC 2016. Notre approche repose d'une partsur des chaînes automatiques de traitements linguistiques en français et en anglaisutilisant le plus possible des ressources et outils publics et d'autre part surun environnement d'exploration des données basé sur les systèmes d'informationlogiques ; ces systèmes exploitent une généralisation des treillis de conceptsformels appliquée aux données attribut-valeur ou au web sémantique.	Peggy Cellier, Sébastien Ferré, Annie Foret, Olivier Ridoux	http://editions-rnti.fr/render_pdf.php?p1&p=1002198	http://editions-rnti.fr/render_pdf.php?p=1002198	présenter dan article méthode employé résultatsobtenu réponse Défi EGC 2016 approcher reposer dune partsur chaîne automatique traitement linguistique français anglaisutiliser plaire ressource outil public dautre partir surun environnement dexploration donnée baser système dinformationlogiqu   système exploiter généralisation treillis conceptsformel appliquer donnée attributvaleur web sémantique
170	Revue des Nouvelles Technologies de l'Information	EGC	2016	Extension de C-SPARQL pour l'échantillonnage de flux de graphes RDF	Les technologies du web sémantique sont de plus en plus utiliséespour la gestion de flux de données. Plusieurs systèmes de traitement de fluxRDF ont été proposés : C-SPARQL, CQELS, SPARQLstream, EP-SPARQL,SPARKWAVE, etc. Ces derniers étendent tous à la base, le langage d'interrogationsémantique SPARQL. Les données à l'entrée du système sont volumineuseset générées en continu à un rythme rapide et variable. De ce fait, le stockage etle traitement de la totalité du flux deviennent coûteux et le raisonnement presqueimpossible. Par conséquent, le recours à des techniques permettant de réduire lacharge tout en conservant la sémantique des données, permet d'optimiser les traitementsvoire le raisonnement. Cependant, aucune des extensions de SPARQLn'inclut cette fonctionnalité. Ainsi, dans cet article, nous proposons d'étendre lesystème C-SPARQL pour générer des échantillons à la volée sur flux de graphesRDF. Nous ajoutons trois opérateurs d'échantillonnage (UNIFORM, RESERVOIRet CHAIN) à la syntaxe de C-SPARQL. Les expérimentations montrent laperformance de notre extension en terme de temps d'exécution, et de la préservationde la sémantique des données.	Amadou Fall Dia, Zakia Kazi Aoul, Aliou Boly	http://editions-rnti.fr/render_pdf.php?p1&p=1002167	http://editions-rnti.fr/render_pdf.php?p=1002167	technologie web sémantique plaire plaire utiliséespour gestion flux donnée système traitement fluxrdf proposer   CSPARQL CQELS sparqlstream epsparqlsparkwav étendre tou baser langage dinterrogationsémantiqu sparql donnée lentrée système volumineuseset générer continu rythmer rapide variable De faire stockage etle traitement totalité flux devenir coûteux raisonnement presqueimpossibl Par conséquent recours technique permettre réduire lacharge conserver sémantique donnée permettre doptimiser traitementsvoire raisonnement extension sparqlninclut fonctionnalité dan article proposer détendre lesystèm csparql générer échantillon voler flux graphesrdf ajouter opérateur déchantillonnage UNIFORM RESERVOIRet chain syntaxe CSPARQL expérimentation montrer laperformance extension terme temps dexécution préservationde sémantique donnée
171	Revue des Nouvelles Technologies de l'Information	EGC	2016	Extraction automatique d'affixes pour la reconnaissance d'entités nommées chimiques	Nous détaillerons ici une approche permettant de détecter des affixes àpartir de dictionnaires en se basant sur l'algorithme de la plus longue sous-chaînecommune, dans le cadre de la reconnaissance d'entités nommées chimiques surCHEMDNER. Nous verrons ensuite des méthodes de sélection et de tri afin deles intégrer au mieux dans un système d'apprentissage automatique.	Yoann Dupont, Isabelle Tellier, Christian Lautier, Marco Dinarelli	http://editions-rnti.fr/render_pdf.php?p1&p=1002216	http://editions-rnti.fr/render_pdf.php?p=1002216	détailler approcher permettre détecter affixe àpartir dictionnaire baser lalgorithme plaire long souschaînecommune dan cadrer reconnaissance dentité nommer chimique surchemdner voir ensuite méthode sélection tri del intégrer mieux dan système dapprentissage automatique
172	Revue des Nouvelles Technologies de l'Information	EGC	2016	Extraction de clés de liage de données (résumé étendu)	De grandes quantités de données sont publiées sur le web des données.Les lier consiste à identifier les mêmes ressources dans deux jeux de donnéespermettant l'exploitation conjointe des données publiées. Mais l'extractionde liens n'est pas une tâche facile. Nous avons développé une approche qui extraitdes clés de liage (link keys). Les clés de liage étendent la notion de cléde l'algèbre relationnelle à plusieurs sources de données. Elles sont fondées surdes ensembles de couples de propriétés identifiant les objets lorsqu'ils ont lesmêmes valeurs, ou des valeurs communes, pour ces propriétés. On présenteraune manière d'extraire automatiquement les clés de liage candidates à partir dedonnées. Cette opération peut être exprimée dans l'analyse formelle de concepts.La qualité des clés candidates peut-être évaluée en fonction de la disponibilité(cas supervisé) ou non (cas non supervisé) d'un échantillon de liens. La pertinenceet de la robustesse de telles clés seront illustrées sur un exemple réel.	Jérôme Euzenat	http://editions-rnti.fr/render_pdf.php?p1&p=1002150	http://editions-rnti.fr/render_pdf.php?p=1002150	De grand quantité donnée publier web donnéesles lier consister identifier ressource dan jeu donnéespermetter lexploitation conjoindre donnée publier Mais lextractionde lien nest tâcher facile développer approcher extraitde cler liage link keys clé liage étendre notion cléd lalgèbr relationnel source donnée fonder surd ensemble couple propriété identifier objet lorsquils lesmêm commun propriété présenteraun manière dextraire automatiquement clé liage candidater partir dedonner opération pouvoir exprimer dan lanalyse formel conceptsla qualité clé candidat peutêtre évaluer fonction disponibilitéca superviser cas superviser dun échantillon lien pertinenceet robustesse clé illustrer exemple réel
173	Revue des Nouvelles Technologies de l'Information	EGC	2016	Extraction de commentaires utilisateurs sur le Web	Dans cet article, nous présentons CommentsMiner, une solution d'extractionnon supervisée pour l'extraction de commentaires utilisateurs. Notreapproche se base sur une combinaison de techniques de fouille de sous-arbresfréquents, d'extraction de données et d'apprentissage de classement. Nos expérimentationsmontrent que CommentsMiner permet de résoudre le problèmed'extraction de commentaires sur 84% d'un jeu de données représentatif et publiquementaccessible, loin devant les techniques existantes d'extraction.	Julien Subercaze, Christophe Gravier, Frédérique Laforest	http://editions-rnti.fr/render_pdf.php?p1&p=1002174	http://editions-rnti.fr/render_pdf.php?p=1002174	Dans article présenter commentsminer solution dextractionnon superviser lextraction commentaire utilisateur notreapproche baser combinaison technique fouiller sousarbresfréquents dextraction donnée dapprentissage classement expérimentationsmontrer commentsminer permettre résoudre problèmedextraction commentaire 84 dun jeu donnée représentatif publiquementaccessible loin devoir technique existant dextraction
174	Revue des Nouvelles Technologies de l'Information	EGC	2016	Extraction de connaissances dans les Systèmes d'Information Pervasifs par l'Analyse Formelle de Concepts	Nous présentons une méthode d'extraction de connaissances dans dessystèmes d'information pervasifs. Nous étudions l'impact du contexte (environnement)d'un utilisateur sur les applications qu'il utilise sur son smartphone.Notre proposition pour gérer la complexité des données contextuelles repose surl'Analyse Formelle de Concepts et les treillis de Galois. Nous nous focalisonssur l'automatisation du processus d'interprétation de ces treillis, pour généraliserl'extraction de connaissances et passer à l'échelle. Nous présentons desmétriques originales illustrées sur des données réelles.	Ali Jaffal, Bénédicte Le Grand, Manuele Kirsh-Pinheiro	http://editions-rnti.fr/render_pdf.php?p1&p=1002180	http://editions-rnti.fr/render_pdf.php?p=1002180	présenter méthode dextraction connaissance dan dessystème dinformation pervasif étudier limpact contexte environnementdun utilisateur application quil utiliser smartphonenotre proposition gérer complexité donnée contextuel reposer surlAnalyse Formelle Concepts treillis Galois focalisonssur lautomatisation processus dinterprétation treillis généraliserlextraction connaissance prendre léchelle présenton desmétriqu original illustrer donnée réel
175	Revue des Nouvelles Technologies de l'Information	EGC	2016	Fabrique logicielle de réseaux sociaux spécialisés (aspects fonctionnels)	This paper introduces a software factory for developing social networks. This factory takesan abstract social network and creates a concrete one, using mechanisms such as sub-typingand behavior overloading.	Benjamin Billet, David Fernandez, Didier Parigot	http://editions-rnti.fr/render_pdf.php?p1&p=1002215	http://editions-rnti.fr/render_pdf.php?p=1002215	this paper introduce software factory for developing social network This factory takesan abstract social network and creat concrete one using mechanism such subtypingand behavior overloading
176	Revue des Nouvelles Technologies de l'Information	EGC	2016	Fairness-Aware Data Mining	In data mining we often have to learn from biased data, because, for instance, data comesfrom different batches or there was a gender or racial bias in the collection of social data. Insome applications it may be necessary to explicitly control this bias in the models we learn fromthe data. Recently this topic received considerable interest both in the research community aswell as more general, as witnessed by several recent articles in popular news media such asthe New York Times. In this talk I will introduce and motivate research in fairness-aware datamining. Different techniques in unsupervised and supervised data mining will be discussed,dividing these techniques into three categories: algorithms of the first category adapt the inputdata in such a way to remove harmful biases while the second adapts the learning algorithmsand the third category modifies the output models in such a way that its predictions becomeunbiased. Furthermore different ways to quantify unfairness, and indirect and conditionaldiscrimination will be discussed, each with their own pros and cons. With this talk I hope toconvincingly argument the validity and necessity of this often contested research area.	Toon Calders	http://editions-rnti.fr/render_pdf.php?p1&p=1002147	http://editions-rnti.fr/render_pdf.php?p=1002147	In dater mining we often hav to learn from biased dater becaus for instance dater comesfrom batch or there wa gender or racial bias in the collection of social dater Insome application it may be necessary to explicitly control thi bia in the model we learn fromthe dater Recently this topic received considerabl interest both in the research community aswell more general witnessed by several recent articler in popular new medier such asthe New York time In this talk ie will introduce and motivate research in fairnessaware datamining Different technique in unsupervised and supervised dater mining will be discusseddividing these technique into three categorier algorithms of the first category adapt the inputdata in such way to remove harmful bias while the second adapt the learning algorithmsand the third category modifi the output model in such way that it prediction becomeunbiased Furthermore way to quantify unfairness and indirect and conditionaldiscrimination will be discussed each with their own pro and con With this talk ie toconvincingly argument the validity and necessity of this often contested research area
177	Revue des Nouvelles Technologies de l'Information	EGC	2016	FODOMUST: une plateforme pour la fouille de données multistratégie multitemporelle	La plateforme FODOMUST 1 est une implantation concrète des méthodes,librairies et interfaces proposées au sein d'ICube. Elle intègre une versionmultisource de la méthode de classification collaborative multistratégie SAMARAH.Elle propose aussi un ensemble d'algorithmes de segmentation soitpropres à ICUBE soit faisant appel à l'OTB. Enfin, trois interfaces dédiées chacuneà un type de données différent permettent une interaction avec l'utilisateur.Sa principale originalité est qu'elle permet la classification, basée sur DTW (DynamicTimeWarping) de données temporelles symboliques ou numériques et deséries temporelles d'images	Pierre Gançarski, Abdoul-Djawadou Salaou	http://editions-rnti.fr/render_pdf.php?p1&p=1002204	http://editions-rnti.fr/render_pdf.php?p=1002204	plateforme fodomust 1 implantation concret méthodeslibrairie interface proposer dicube intégrer versionmultisource méthode classification collaboratif multistratégie samarahell proposer ensemble dalgorithme segmentation soitpropr ICUBE faire appel lotb interface dédier chacuneà typer donnée permettre interaction lutilisateursa principal originalité permettre classification basé dtw DynamicTimeWarping donnée temporel symbolique numérique desérie temporel dimag
178	Revue des Nouvelles Technologies de l'Information	EGC	2016	Fouille de motifs séquentiels avec ASP	Cet article présente l'utilisation de la programmation par ensemblesréponses (ASP) pour répondre à une tâche de fouille de motifs séquentiels. Lasyntaxe de l'ASP, proche du Prolog, en fait un langage très pertinent pour représenterdes connaissances de manière aisée et ses mécanismes de résolution,basés sur des solveurs efficaces, en font une solution alternative aux approchesde programmation par contraintes pour la fouille déclarative de motifs. Nousproposons un premier encodage de la tâche classique d'extraction de motifs séquentielset de ses variantes (motifs clos et maximaux). Nous comparons lesperformances calculatoires de ses encodages avec une approche de programmationpar contraintes. Les performances obtenues sont inférieures aux approchesde programmation par contraintes, mais l'encodage purement déclaratif offreplus de perspectives d'intégration de connaissances expertes.	Thomas Guyet, Yves Moinard, Rene Quiniou, Torsten Schaub	http://editions-rnti.fr/render_pdf.php?p1&p=1002154	http://editions-rnti.fr/render_pdf.php?p=1002154	article présent lutilisation programmation ensemblesréponse asp répondre tâcher fouiller motif séquentiel Lasyntaxe lasp Prolog faire langage pertinent représenterdes connaissance manière aisé mécanisme résolutionbaser solveur efficace faire solution alternatif approchesd programmation contrainte fouiller déclaratif motif nousproposon encodage tâcher classique dextraction motif séquentielset variante motif clore maximal comparer lesperformanc calculatoir encodage approcher programmationpar contraint performance obtenu inférieur approchesd programmation contraint lencodage purement déclaratif offreplus perspective dintégration connaissance expert
179	Revue des Nouvelles Technologies de l'Information	EGC	2016	Fusion de données redondantes : une approche explicative	"Nous nous intéressons, dans le cadre du projet ANR Qualinca au traitementdes données redondantes. Nous supposons dans cet article que cette redondancea déjà été établie par une étape préalable de liage de données. Laquestion abordée est la suivante : comment proposer une représentation uniqueen fusionnant les ""duplicats"" identifiés ? Plus spécifiquement, comment décider,pour chaque propriété de la donnée considérée, quelle valeur choisir parmi cellesfigurant dans les ""duplicats"" à fusionner ? Quelle méthode adopter dans le butde pouvoir, par la suite, retracer et expliquer le résultat obtenu de façon transparenteet compréhensible par l'utilisateur ? Nous nous appuyons pour cela surune approche de décision multicritère et d'argumentation."	Fatiha Saïs, Rallou Thomopoulos	http://editions-rnti.fr/render_pdf.php?p1&p=1002189	http://editions-rnti.fr/render_pdf.php?p=1002189	intéresser dan cadrer projet ANR qualinca traitementd donnée redondant supposer dan article redondancea déjà établir étape préalable liage donnée Laquestion aborder   proposer représentation uniqueen fusionner duplicat identifié   plaire spécifiquemer déciderpour propriété donner considérer choisir cellesfigurant dan duplicat fusionner   quell méthode adopter dan butde pouvoir suite retracer expliquer résultat obtenir transparenteet compréhensible lutilisateur   appuyer celer surune approcher décision multicritère dargumentation
180	Revue des Nouvelles Technologies de l'Information	EGC	2016	Génération de contraintes pour le clustering à partir d'une ontologie - Application à la classification d'images satellites	L'utilisation des connaissances a priori peut fortement améliorer laclassification non-supervisée. L'injection de ces connaissances sous forme decontraintes sur les données figure parmi les techniques les plus efficaces de lalittérature. Cependant, la génération des contraintes est très coûteuse et demandel'intervention de l'expert ; la sémantique apportée par l'étiquetage de l'expertest aussi perdue dans ce type de techniques, seuls les contraintes sont retenuespar le clustering. Dans cet article, nous proposons une nouvelle approche hybrideexploitant le raisonnement à base d'ontologie pour générer automatiquementdes contraintes permettant de guider et améliorer le clustering. L'utilisationd'une ontologie comme connaissance a priori a plusieurs avantages. Elle permetl'interprétation automatisée des connaissances, ajoute de la modularité dans lachaîne de traitement et améliore la qualité du clustering en prenant en comptela vision de l'utilisateur. Pour évaluer notre approche, nous l'avons appliquée àla classification d'images satellites et les résultats obtenus démontrent des améliorationsnotables à la fois au niveau de la qualité du clustering et au niveau del'étiquetage sémantique des clusters sans intervention de l'expert.	Hatim Chahdi, Nistor Grozavu, Isabelle Mougenot, Laure Berti-Equille, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1002158	http://editions-rnti.fr/render_pdf.php?p=1002158	lutilisation connaissance priori pouvoir fortement améliorer laclassification nonsupervisée linjection connaissance sou former decontraint donnée figurer technique plaire efficace lalittérature génération contrainte coûteux demandelintervention lexpert   sémantique apporter létiquetage lexpertest perdre dan typer technique contrainte retenuespar clustering Dans article proposer approcher hybrideexploiter raisonnement baser dontologie générer automatiquementde contraint permettre guider améliorer clustering lutilisationdune ontologie connaissance priori avantage permetlinterprétation automatiser connaissance ajouter modularité dan lachaîn traitement améliorer qualité clustering prendre comptela vision lutilisateur Pour évaluer approcher laver appliquer àla classification dimag satellite résultat obtenir démontrer améliorationsnotable niveau qualité clustering niveau delétiquetage sémantique cluster intervention lexpert
181	Revue des Nouvelles Technologies de l'Information	EGC	2016	Identification de Classes Sémantiques Basée sur des Mesures de Proximité Sémantique	Semantic relations are the core of a growing number of knowledge-intensive systems. Theneed to validate automatically such relations remains an up-to-date challenge. In this paper, wepresent a web-based method enabling the automatic identification of the class of a semantic relation.Using measures based on syntactic patterns as entry features for a learning algorithm,we are able to successfully identify 72% of semantic relations divided in 4 classes in a semanticallyrich environment.	Jean Petit, Jean-Charles Risch	http://editions-rnti.fr/render_pdf.php?p1&p=1002213	http://editions-rnti.fr/render_pdf.php?p=1002213	semantic relation are the core of growing number of knowledgeintensiv system Theneed to validate automatically such relation remain an uptodate challenge in this paper wepresent webbased method enabling the automatic identification of the class of semantic relationusing measur based syntactic patterns entry featur for learning algorithmwe are abl to successfully identify 72 of semantic relation divided in 4 classe in semanticallyrich environment
182	Revue des Nouvelles Technologies de l'Information	EGC	2016	Intégration de connaissances lexicales et sémantiques pour l'analyse de sentiments dans les SMS	With the explosive growth of the social media (forums, blogs, and social networks) on theWeb, the exploitation of these new information sources became essential. In this paper, wepresent a new automatic method to integrate knowledge for sentiment detection from a SMScorpus by combining lexical and semantic information.	Wedjene Khiari, Mathieu Roche, Asma Bouhafs Hafsia	http://editions-rnti.fr/render_pdf.php?p1&p=1002227	http://editions-rnti.fr/render_pdf.php?p=1002227	With the explosif growth of the social medier forum blog and social network theweb the exploitation of these new information source became essential In this paper wepresent new automatic method to integrat knowledg for sentiment detection from smscorpus by combining lexical and semantic information
183	Revue des Nouvelles Technologies de l'Information	EGC	2016	Intégration des Influences Géographique et Temporelle pour la Recommandation de Points d'Intérêt	La recommandation de points d'intérêts (ou POI), est devenue un problèmemajeur avec l'émergence des réseaux sociaux (ou LBSN). À la différencedes approches de recommandation traditionnelles, les données des LBSN présententdes caractéristiques géographique et temporelle importantes qui limitentles performances des algorithmes traditionnels existant. L'intégration de ces caractéristiquesdans un unique modèle de factorisation pour augmenter la qualitéde la recommandation n'a pas été un problème très étudié jusqu'à présent. Dansce papier nous présentons GeoMF-TD, une extension d'un modèle de factorisationgéographique avec des dépendances temporelles. Nos expérimentationssur un jeu de données réel montre jusqu'à 20% de gain sur la précision de larecommandation.	Jean-Benoît Griesner, Talel Abdesssalem, Hubert Naacke	http://editions-rnti.fr/render_pdf.php?p1&p=1002166	http://editions-rnti.fr/render_pdf.php?p=1002166	recommandation point dintérêts POI devenir problèmemajeur lémergence réseau social lbsn À différencedes approche recommandation traditionnel donnée lbsn présententd caractéristique géographique temporel important limitentl performance algorithme traditionnel exister lintégration caractéristiquesdans modeler factorisation augmenter qualitéde recommandation problème étudier jusquà présent Dansce papier présenter GeoMFTD extension dun modeler factorisationgéographique dépendance temporel expérimentationssur jeu donnée réel montrer jusquà 20 gain précision larecommandation
184	Revue des Nouvelles Technologies de l'Information	EGC	2016	Khiops: outil d'apprentissage supervisé automatique pour la fouille de grandes bases de données multi-tables	Khiops est un outil d'apprentissage supervisé automatique pour lafouille de grandes bases de données multi-tables. L'importance prédictive desvariables est évaluée au moyen de modèles de discrétisation dans le cas numériqueet de groupement de valeurs dans le cas catégoriel. Dans le cas d'unebase multi-tables, par exemple des clients avec leurs achats, une table d'analyseindividus × variables est produite par construction automatique de variables.Le modèle de classification utilisé est un classifieur Bayésien naïf avec sélectionde variables et moyennage de modèles. L'outil est adapté à l'analyse desgrandes bases de données, avec des millions d'individus, des dizaines de milliersde variables et des centaines de millions d'enregistrements dans les tablessecondaires.	Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1002208	http://editions-rnti.fr/render_pdf.php?p=1002208	Khiops outil dapprentissage superviser automatique lafouille grand base donnée multitabl limportance prédictif desvariabl évaluer moyen modèle discrétisation dan cas numériqueet groupement dan cas catégoriel Dans cas dunebase multitabl exemple client achat tabler danalyseindividus × variable produire construction automatique variablesLe modeler classification utiliser classifieur Bayésien naïf sélectionde variable moyennage modèle Loutil adapter lanalyse desgrandes base donnée million dindividus dizaine milliersde variable centaine million denregistrement dan tablessecondaire
185	Revue des Nouvelles Technologies de l'Information	EGC	2016	L'analyse relationnelle de concepts pour la fouille de données temporelles – Application à l'étude de données hydroécologiques	Cet article présente une méthode d'exploration de données temporelles,fondée sur l'analyse relationnelle de concepts (ARC) et appliquée à desdonnées séquentielles construites à partir d'échantillons physico-chimiques etbiologiques prélevés dans des cours d'eau. Notre but est de mettre au jour dessous-séquences pertinentes et hiérarchisées, associant les deux types de paramètres.Pour faciliter la lecture, ces sous-séquences sont représentées sous laforme de motifs partiellement ordonnés (po-motifs). Le processus de fouille dedonnées se décompose en plusieurs étapes : construction d'un modèle temporelad hoc et mise en oeuvre de l'ARC ; extraction des sous-séquences synthétiséessous la forme de po-motifs ; sélection des po-motifs intéressants grâce à unemesure exploitant la distribution des extensions de concepts. Le processus a ététesté sur un jeu de données réelles et évalué quantitativement et qualitativement.	Cristina Nica, Agnès Braud, Xavier Dolques, Marianne Huchard, Florence Le Ber	http://editions-rnti.fr/render_pdf.php?p1&p=1002178	http://editions-rnti.fr/render_pdf.php?p=1002178	article présenter méthode dexploration donnée temporellesfonder lanalyse relationnel concept ARC appliquer desdonnée séquentiel construire partir déchantillon physicochimique etbiologiqu prélever dan cours deau boire mettre jour dessousséquence pertinent hiérarchiser associer type paramètrespour faciliter lecture sousséquence représenter sou laforme motif partiellemer ordonner pomotif processus fouiller dedonner décomposer étape   construction dun modeler temporelad hoc mettre oeuvrer larc   extraction sousséquence synthétiséessous former pomotif   sélection pomotif intéressant grâce unemesure exploiter distribution extension concept processus ététester jeu donnée réel évaluer quantitativement qualitativement
186	Revue des Nouvelles Technologies de l'Information	EGC	2016	La génération des résumés visuels de flux de données de capteurs météorologiques avec des chorèmes	This paper describes a new approach for the automatic generation of visual summariesdealing with cartographic visualization methods and modeling of data coming from sensors inreal time for meteorology. Indeed the concept of chorems seems to be an interesting candidateto visualize real time geographic database summaries.	Zina Bouattou, Robert Laurini, Hafida Belbachir	http://editions-rnti.fr/render_pdf.php?p1&p=1002214	http://editions-rnti.fr/render_pdf.php?p=1002214	this paper describe new approach for the automatic generation of visual summariesdealing with cartographic visualization method and modeling of dater coming from sensors inreal time for meteorology Indeed the concept of chorems seems to be an interesting candidateto visualiz real tim geographic database summari
187	Revue des Nouvelles Technologies de l'Information	EGC	2016	La r-confiance pour l'identification de trajectoires de patients	Sequential patterns mining consist in identifying frequent sequences of ordered events. Tosolve the problem of the large number of patterns obtained, we extend the interest measurecalled confidence, conventionally used to select association rules to sequential patterns. Wefocused on a case study: myocardial infarction (MI), in order to predict the trajectory of patientswith MI between 2009 and 2013. The results were submitted to an expert for discussionand validation.	Yves Mercadier, Jessica Pinaire, Jérôme Azé, Sandra Bringay, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1002218	http://editions-rnti.fr/render_pdf.php?p=1002218	sequential patterns mining consist in identifying frequent sequence of ordered event Tosolve the problem of the large number of pattern obtained we extend the interest measurecalled confidence conventionally used to select association ruler to sequential pattern Wefocused caser study myocardial infarction MI in order to predict the trajectory of patientswith MI between 2009 and 2013 The results were submitted to an expert for discussionand validation
188	Revue des Nouvelles Technologies de l'Information	EGC	2016	La révolution de l'assurance par la donnée : défis scientifiques de l'extraction à la gestion de connaissances	La quantité de données dans notre monde a explosé et l'analyse de grands ensemblesde données – aussi connu dans l'industrie sous le nom « Big Data » – deviendra un atoutmajeur de compétitivité, principalement dû à une croissance de productivité et surtoutà grâce à plus d'innovation. La croissance exponentielle de données est alimentée parla facilité de la captation et par la multiplication de canaux numériques d'acquisition.On pense non seulement à tous les processus qui sont informatisés aujourd'hui, maisaussi aux médias sociaux et aux objets connectés.L'assurance vie une révolution tout particulière. L'assureur, traditionnellement gestionnairedu risque en s'appuyant sur une longue expérience, qu'on traduirait aujourd'huipar une captation systématique de données, est après la révolution numériquepartiellement exclus de canaux digitaux.Ceci est en même temps une menace et une opportunité. Il s'agit d'un défi puisquel'industrie doit réaliser une forte mutation pour se positionner la où la donnée setrouve aujourd'hui, i.e. dans le digital. Il s'agit d'une opportunité puisque ces nouvellesdonnées permettront de mieux appréhender les risques, et plus particulièrement,permettront d'estimer au plus près les risques à la source, plutôt que passer par devariables intermédiaires, comme peut l'être l'âge pour le risque d'accident en conduite.L'opportunité est d'autant plus grande qu'en accédant aux données au plus près desutilisateurs il est possible de faire de la prévention évitant ainsi des accidents coûteuxpour l'assureur, mais surtout désastreux pour les victimes.Une fois la révolution engagée, ceci implique, un certain nombre de transformationsdans les processus d'extraction et gestion de connaissances. Les défis scientifiques sontnombreux, allant de la captation non-intrusive de la donnée, à la visualisation et gestionde connaissances extraites, en passant par de l'apprentissage artificiel pour pouvoirservir à de millions d'utilisateurs simultanément. Dans cette présentation nous allonscouvrir rapidement chacune de ces thématiques avec une attention particulière auxdéfis scientifiques sous-jacents.Nous allons illustrer notre propos par un exemple phare de cette révolution : lafamille d'offres d'assurance dite « pay as you drive » où généralement on obtient unedécote ou réduction en fonction de sa façon de conduire. Nous allons ce que ceci impliqueen termes d'extraction et de gestion de connaissances.Pour conclure, il est important de mentionner que cette révolution implique d'autreschallenges cruciaux qui dépassent ce qui est abordé ici. En particulier, pour ne mentionnerque deux grands axes : la protection de la vie privée, aussi bien du point de vuetechnique que juridique ; et la transformation de métiers accompagné d'une pénurie detalents déjà entamé.	Marcin Detyniecki	http://editions-rnti.fr/render_pdf.php?p1&p=1002148	http://editions-rnti.fr/render_pdf.php?p=1002148	quantité donnée dan monder exploser lanalyse grand ensemblesde donné – connaître dan lindustrie sou nom « Big Data » – devenir atoutmajeur compétitivité principalement devoir croissance productivité surtoutà grâce plaire dinnovation croissance exponentiel donnée alimenter faciliter captation multiplication canal numérique dacquisitionon penser tou processus informatiser aujourdhui maisaussi média social objet connectéslassurance vie révolution Lassureur traditionnellemer gestionnairedu risquer sappuyer long expérience quon traduire aujourdhuipar captation systématique donnée révolution numériquepartiellement exclu canal digitauxceci temps menacer opportunité sagit dun défi puisquelindustrie devoir réaliser fort mutation positionner donner setrouve aujourdhui ie dan digital sagit dune opportunité nouvellesdonner permettre mieux appréhender risque plaire particulièrementpermettront destimer plaire risque source prendre devariabl intermédiaire pouvoir lêtr lâge risquer daccident conduitelopportunité dautant plaire grand quen accéder donnée plaire desutilisateur faire prévention éviter accident coûteuxpour lassureur désastreux victimesune révolution engagé impliquer nombre transformationsdan processus dextraction gestion connaissance défi scientifique sontnombreux aller captation nonintrusiv donner visualisation gestiond connaissance extrait passer lapprentissage artificiel pouvoirservir million dutilisateur simultanément Dans présentation allonscouvrir rapidement thématique attention auxdéfi scientifique sousjacentsnous aller illustrer propos exemple phare révolution   lafamill doffr dassurance « pay you driv » généralement obtenir unedécot réduction fonction conduire aller impliqueen terme dextraction gestion connaissancesPour conclure importer mentionner révolution impliqu dautreschalleng crucial dépasser aborder En mentionnerqu grand axe   protection vie priver poindre vuetechniqu juridique   transformation métier accompagner dune pénurie detalent déjà entamer
189	Revue des Nouvelles Technologies de l'Information	EGC	2016	Learning from Massive, Incompletely annotated & Structured Data	The MAESTRA project (http://maestra-project.eu/) addresses the ambitious taskof predicting different types of structured outputs in several challenging settings, suchas semi-supervised learning, mining data streams and mining network data. It developsmachine learning methods that work in each of these settings, as well as combinationsthereof. The techniques developed are applied to problems from the area of biology andbioinformatics, sensor data analysis, multimedia annotation and retrieval, and socialnetwork analysis. The talk will give an introduction to the project and the topicsit addresses, an overview of the results of the project, and a detailed description ofselected techniques and applications: Semi-supervised learning for structured-outputprediction (SOP) and SOP on data streams will be discussed for the task of multitargetregression (MTR), as well as applications of MTR for the annotation/retrievalof images.	Saso Dzeroski	http://editions-rnti.fr/render_pdf.php?p1&p=1002149	http://editions-rnti.fr/render_pdf.php?p=1002149	The MAESTRA project httpmaestraprojecteu addresse the ambitious taskof predicting type of structured outputs in several challenging settings suchas semisupervised learning mining dater stream and mining network dater it developsmachine learning method that work in each of these setting well combinationsthereof The technique developed are applied to problem from the area of biology andbioinformatic sensor dater analysis multimedia annotation and retrieval and socialnetwork analysis The talk will giv an introduction to the project and the topicsit addresse an overview of the results of the project and detailed description ofselected technique and application Semisupervised learning for structuredoutputprediction sop and sop dater stream will be discussed for the task of multitargetregression MTR well application of MTR for the annotationretrievalof imag
190	Revue des Nouvelles Technologies de l'Information	EGC	2016	LibRe: Protocole de gestion de la cohérence dans les systèmes de stockage distribués	Nous présentons dans ce papier un protocole de gestion de la cohérenceappelé LibRe adapté aux systèmes de stockage orientés Cloud (telles queles bases de données NoSQL). Ce protocole garantit l'accès à la donnée la plusrécente tout en ne consultant qu'une seule réplique. Cet algorithme est évaluépar simulation et est également implémenté au sein du système de stockage Cassandra.Les résultats de ces expérimentations ont démontré l'efficacité de notreapproche.	Raja Chiky, Sathya Prabhu Kumar, Sylvain Lefebvre, Eric Gressier-Soudan	http://editions-rnti.fr/render_pdf.php?p1&p=1002171	http://editions-rnti.fr/render_pdf.php?p=1002171	présenter dan papier protocole gestion cohérenceappelé libre adapter système stockage orienter cloud base donnée NoSQL protocole garantir laccè donner plusrécente consulter quune répliqu algorithme évaluépar simulation également implémenter système stockage cassandral résultat expérimentation démontrer lefficaciter notreapproche
191	Revue des Nouvelles Technologies de l'Information	EGC	2016	Manipulation interactive d'ensemble de motifs : application aux parcours hospitaliers	Dans cette démonstration, nous proposons une application de visualisationdes résultats de la fouille de données séquentielles. Pour illustrer le fonctionnementde cette application, nous avons utilisé des données PMSI hospitalières,plus précisément dans le cas de l'infarctus du myocarde (IM). Les résultatsobtenus ont été soumis à un spécialiste pour discussion et validation.	Yves Mercadier, Jessica Pinaire, Jérôme Azé, Sandra Bringay, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1002200	http://editions-rnti.fr/render_pdf.php?p=1002200	Dans démonstration proposer application visualisationde résultat fouiller donnée séquentiel Pour illustrer fonctionnementde application utiliser donnée pmsi hospitalièresplu précisément dan cas linfarctu myocarde im résultatsobtenu soumettre spécialiste discussion validation
192	Revue des Nouvelles Technologies de l'Information	EGC	2016	Nettoyage de données guidé par la sémantique inter-colonnes	Today, the volume of unstructured and heterogeneous data is exploding, coming from multiplesources with different levels of quality. Therefore, it is very likely to manipulate datawithout knowledge about their structures and their semantics. In fact, the meta-data may beinsufficient or totally absent. Data anomalies may be due to the poverty of their semantic descriptions,or even the absence of their descriptions. We propose an approach to understandbetter the semantics and the structure of the data. It helps to correct the intra-column anomalies(homogenization) and then the inter-columns ones caused by the violation of semanticdependencies.	Houda Zaidi, Faouzi Boufarès, Yann Pollet	http://editions-rnti.fr/render_pdf.php?p1&p=1002225	http://editions-rnti.fr/render_pdf.php?p=1002225	Today the volume of unstructured and heterogeneou dater is exploding coming from multiplesource with level of quality Therefore it is very likely to manipulate datawithout knowledge about their structur and their semantic In fact the metadata may beinsufficient or totally absent Data anomalie may be to the poverty of their semantic descriptionsor even the absence of their description We proposer an approach to understandbetter the semantics and the structurer of the dater it help to correct the intracolumn anomalieshomogenization and then the intercolumns caused by the violation of semanticdependencie
193	Revue des Nouvelles Technologies de l'Information	EGC	2016	Nouveaux algorithmes de fouilles de données relationnelles de clowdflows	Clowdflows est un logiciel open source qui permet à un utilisateur deréaliser des processus entiers de fouille de données à partir d'un navigateur etd'une connexion internet. Les calculs sont réalisés dans le “nuage”, c'est-à-direde façon transparente sur plusieurs serveurs exécutant les calculs ou hébergeantles données. Dans cet article, nous rappelons les points forts de clowdflows etnous présentons trois familles d'algorithmes de fouille de données relationnellesque nous venons d'y intégrer. En effet clowdflows est la seule plateforme webpermettant d'exécuter, voire comparer, plusieurs techniques de fouille de donnéesrelationnelles, souvent appelée programmation logique inductive.	Nicolas Lachiche, Alain Shakour	http://editions-rnti.fr/render_pdf.php?p1&p=1002207	http://editions-rnti.fr/render_pdf.php?p=1002207	Clowdflows logiciel open source permettre utilisateur deréaliser processus entier fouiller donnée partir dun navigateur etdun connexion internet calcul réaliser dan “ nuage ” cestàdirede transparent serveur exécuter calcul hébergeantle donner Dans article rappeler point fort clowdflow etnous présenton famille dalgorithme fouiller donnée relationnellesqu venir dy intégrer En clowdflow plateforme webpermetter dexécuter voire comparer technique fouiller donnéesrelationnelle appeler programmation logique inductif
194	Revue des Nouvelles Technologies de l'Information	EGC	2016	Nouvelle méthode de calcul de la réputation dans les forums de santé	De plus en plus de forums, tels que Slashdot ou Stack Exchange, proposentdes systèmes de réputations qui se basent sur le vote collaboratif. Lesutilisateurs peuvent ainsi donner un score à chaque message posté selon sa pertinenceou son utilité. Cependant, ces fonctionnalités de vote sont rarement utiliséesdans de nombreuses communautés en ligne tels que les forums de santé.Dans ces forums, les utilisateurs préfèrent poster un nouveau message exprimantde l'accord ou du remerciement vis à vis des messages pertinents plutôtque de cliquer sur un bouton de vote. Dans ce travail, nous proposons d'utiliserces formes implicites d'expression de la confiance pour estimer la réputation desutilisateurs dans les forums de santé.	Amine Abdaoui	http://editions-rnti.fr/render_pdf.php?p1&p=1002173	http://editions-rnti.fr/render_pdf.php?p=1002173	De plaire plaire forum Slashdot Stack Exchange proposentde système réputation baser voter collaboratif Lesutilisateurs pouvoir donner score message poster pertinenceou utilité fonctionnalité voter utiliséesdans communauter ligne forum santéDans forum utilisateur préférer poster message exprimantde laccord remerciemer vis vis message pertinent plutôtqu cliquer bouton voter Dans travail proposer dutiliserces forme implicite dexpression confiance estimer réputation desutilisateurs dan forum santé
195	Revue des Nouvelles Technologies de l'Information	EGC	2016	Observations sur les distributions latentes aux matrices laplaciennes de graphes	L'algorithme de clustering spectral permet en principe d'extraire desclusters de formes arbitraires à partir de données numériques. Cette propriété acontribué à sa popularité, et même si ses bases théoriques sont établies depuisplus d'une décennie, des variantes en ont été proposées jusqu'à récemment. Sonfonctionnement repose sur une transformation vers un espace latent dans lequeldes formes de clusters arbitraires sont converties en structures faciles à traiterpar un algorithme tel que k-means. Toutefois, les distributions dans cet espacelatent n'ont été que peu discutées, beaucoup d'auteurs supposant que les propriétésprédites par la théorie sont vérifiées. Cet article propose alternativementune approche qualitative pour vérifier si cette structure idéale est effectivementobtenue en pratique. Le travail consiste également à identifier les paramètresde variabilité commandant à la transformation vers l'espace latent, via un étatde l'art synthétique de la théorie sous-jacente au clustering spectral. Les observationstirées de nos expériences permettent d'identifier les combinaisons deparamètres efficaces, et les conditions de cette efficacité.	Pierrick Bruneau, Benoît Otjacques	http://editions-rnti.fr/render_pdf.php?p1&p=1002159	http://editions-rnti.fr/render_pdf.php?p=1002159	Lalgorithme clustering spectral permettre principe dextrair descluster forme arbitraire partir donnée numérique propriété acontribuer popularité base théorique établir depuisplus dune décennie variante proposer jusquà récemment Sonfonctionnement reposer transformation ver espacer latent dan lequelde forme cluster arbitraire convertir structure facile traiterpar algorithme kmean distribution dan espacelatent nont discuter dauteur supposer propriétésprédite théorie vérifier article proposer alternativementune approcher qualitatif vérifier structurer idéal effectivementobtenue pratiquer travail consister également identifier paramètresde variabilité commander transformation ver lespace latent étatde lart synthétique théorie sousjacent clustering spectral observationstirée expérience permettre didentifier combinaison deparamètr efficace condition efficacité
196	Revue des Nouvelles Technologies de l'Information	EGC	2016	PersoRec : un système personnalisé de recommandations pour les folksonomies basé sur les concepts quadratiques	Nous proposons un nouveau système appelé PersoRec afin de personnaliserles recommandations (d'amis, de tags ou de ressources) faites aux utilisateursdans les folksonomies. La personnalisation des recommandations estréalisée en prenant en compte le profil des utilisateurs. Cette nouvelle donnéepermet de proposer aux utilisateurs des tags ou/et ressources plus adaptées àleurs besoins. En plus du profil des utilisateurs, nous avons recours à leur historiquede partage de tags et de ressources dans le but de regrouper les utilisateursayant partagé des tags et des ressources en commun tout en ayant des profilséquivalents (i.e., des structures appelées concepts quadratiques). Ces deux donnéesprises en compte au moment du processus de recommandation a permisd'améliorer la qualité des recommandations faites aux utilisateurs. PersoRec estdonc capable de générer une recommandation personnalisée pour chaque utilisateurselon le mode de recommandation qu'il désire (recommandation d'amis,de tags ou de ressources) et selon le profil qu'il possède.	Mohamed Nader Jelassi, Sadok Ben Yahia, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1002205	http://editions-rnti.fr/render_pdf.php?p=1002205	proposer système appeler persorec personnaliserl recommandation damis tag ressource utilisateursdans folksonomie personnalisation recommandation estréaliser prendre compter profil utilisateur donnéepermet proposer utilisateur tag ouet ressourcer plaire adapter àleurs besoin En plaire profil utilisateur recourir historiquede partager tag ressource dan boire regrouper utilisateursayant partager tag ressource commun profilséquivalent ie structure appeler concept quadratique donnéesprise compter momer processus recommandation permisdaméliorer qualité recommandation utilisateur persorec estdonc capable générer recommandation personnalisé utilisateurselon mode recommandation quil désir recommandation damisd tag ressource profil quil posséder
197	Revue des Nouvelles Technologies de l'Information	EGC	2016	Plongement de métrique pour le calcul de similarité sémantique à l'échelle	Nous explorons le plongement de la métrique de plus court chemindans l'hypercube de Hamming, dans l'objectif d'améliorer les performances desimilarité sémantique dans Wordnet (Subercaze et al. (2015)). Nous montronsque bien qu'un plongement isométrique est impossible en pratique, nous obtenonsde très bons plongements non isométriques. Nous obtenons une améliorationdes performances de trois ordres de grandeur pour le calcul de la similaritéde Leacock et Chodorow (LCH).	Julien Subercaze, Christophe Gravier, Frédérique Laforest	http://editions-rnti.fr/render_pdf.php?p1&p=1002163	http://editions-rnti.fr/render_pdf.php?p=1002163	explorer plongement métrique plaire courir chemindans lhypercub Hamming dan lobjectif daméliorer performance desimilarité sémantique dan Wordnet Subercaze al 2015 montronsqu quun plongement isométrique impossible pratiquer obtenonsde plongement isométrique obtenir améliorationde performance ordre grandeur calcul similaritéde Leacock Chodorow LCH
198	Revue des Nouvelles Technologies de l'Information	EGC	2016	Prédiction de la qualité dans les plateformes collaboratives : une approche générique par les graphes hétérogènes	La qualité des contenus sur les plateformes collaboratives est très hétérogène.Dans la littérature scientifique, les algorithmes d'analyse structurelleappliqués à la tâche de détection de contenu de qualité reposent généralement surdes graphes définis à partir d'un seul type de noeuds et de relations. Pourtant lesgraphes sur lesquels reposent ces récentes plateformes présentent de nombreusessémantiques de noeuds et relations différentes, e.g., producteurs/consommateurs,questions/réponses, etc. Ces solutions souffrent d'un manque de généricité et nepeuvent s'adapter facilement à l'évolution des plateformes. Nous proposons unemodélisation générique de ces platformes par les graphes hétérogènes pouvantintégrer automatiquement de nouvelles sémantiques de noeuds et de relations. Unalgorithme de prédiction de qualité des contenus reposant sur ce modèle est proposé.Nous montrons qu'il généralise plusieurs travaux de la littérature. Enfin,en intégrant certaines relations inter-utilisateurs, nous montrons que notre solution,évaluée surWikipedia et Stack Exchange, améliore la tâche de détection decontenu de qualité.	Baptiste de La Robertie, Yoann Pitarch, Olivier Teste	http://editions-rnti.fr/render_pdf.php?p1&p=1002175	http://editions-rnti.fr/render_pdf.php?p=1002175	qualité contenu plateforme collaboratif hétérogèneDans littérature scientifique algorithme danalyse structurelleappliquer tâcher détection contenir qualité reposer généralement surde graphe définir partir dun typer noeud relation pourtant lesgrapher reposer récent plateforme présenter nombreusessémantique noeud relation eg producteursconsommateursquestionsrépons solution souffrir dun manqu généricité nepeuvent sadapter facilement lévolution plateforme proposer unemodélisation générique platforme graphe hétérogène pouvantintégrer automatiquement sémantique noeud relation Unalgorithme prédiction qualité contenu reposer modeler proposénous montron quil généraliser travail littérature Enfinen intégrer relation interutilisateur montrer solutionévaluée surWikipedia Stack Exchange améliorer tâcher détection decontenu qualité
199	Revue des Nouvelles Technologies de l'Information	EGC	2016	Recherche de groupes parallèles en classification non-supervisée	"Dans cet article, nous nous intéressons à une situation de classificationnon supervisée dans laquelle nous souhaitons imposer une ""forme"" commune àtous les clusters. Dans cette approche, la ""forme"" commune sera caractérisée parun hyperplan qui sera le même pour tous les groupes, à une translation près.Les points sont donc supposés être distribués autour d'hyperplans parallèles. Lafonction objectif utilisée peut naturellement s'exprimer comme la minimisationde la somme des distances de chaque point à son hyperplan. Comme pour le casde k-means, la résolution est effectuée par l'alternance de phases d'affectationde chaque point à l'hyperplan le plus proche et de phases de calcul de l'hyperplanqui ajuste au mieux l'ensemble des points qui lui sont affectés. L'objectifétant d'obtenir des hyperplans parallèles, cette phase de calcul est menée simultanémentpour tous les hyperplans, par une méthode de régression."	Lionel Martin, Matthieu Exbrayat, Teddy Debroutelle, Aladine Chetouani, Sylvie Treuillet, Sébastien Jesset	http://editions-rnti.fr/render_pdf.php?p1&p=1002157	http://editions-rnti.fr/render_pdf.php?p=1002157	Dans article intéresser situation classificationnon superviser dan souhaiter imposer former commun àtous cluster Dans approcher former commun caractériser parun hyperplan tou groupe translation prèsles point supposer distribuer autour dhyperplans parallèle Lafonction objectif utiliser pouvoir naturellement sexprimer minimisationde sommer distance poindre hyperplan Comme casde kmeans résolution effectuer lalternance phase daffectationde poindre lhyperplan plaire phase calcul lhyperplanqui ajuster mieux lensembl point luire affecter lobjectifétant dobtenir hyperplan parallèle phase calcul mener simultanémentpour tou hyperplan méthode régression
200	Revue des Nouvelles Technologies de l'Information	EGC	2016	Régression logistique pour la classification d'images à grande échelle	Nous présentons un nouvel algorithme parallèle de régression logistique(PAR-MC-LR) pour la classification d'images à grande échelle. Nous proposonsplusieurs extensions de l'algorithme original de régression logistique àdeux classes pour en développer une version efficace pour les grands ensemblesde données d'images avec plusieurs centaines de classes. Nous présentons unnouvel algorithme LR-BBatch-SGD de descente de gradient stochastique de régressionlogistique en batch équilibré avec un apprentissage parallèle (approcheun contre le reste) multi-classes sur de multiples coeurs. Les résultats expérimentauxsur des ensembles de données d'ImageNet montrent que notre algorithmeest efficace comparés aux algorithmes de classification linéaires de l'état de l'art.	Thanh-Nghi Do, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1002182	http://editions-rnti.fr/render_pdf.php?p=1002182	présenter nouvel algorithme parallèle régression logistiqueparmclr classification dimager grand échelle proposonsplusieur extension lalgorithme original régression logistique àdeux classe développer version efficace grand ensemblesde donner dimager centaine classe présenter unnouvel algorithme LRBBatchSGD descente gradient stochastique régressionlogistiqu batch équilibrer apprentissage parallèle approcheun contrer rester multiclass coeur résultat expérimentauxsur ensemble donnée dimagenet montrer algorithmeest efficace comparer algorithme classification linéaire létat lart
201	Revue des Nouvelles Technologies de l'Information	EGC	2016	Relaxation des Requêtes Skyline : Une Approche Centrée Utilisateur	Les requêtes skyline constituent un outil puissant pour l'analyse dedonnées multidimensionnelles et la décision multicritère. En pratique, le calculdu skyline peut conduire à deux scénarios : soit (i) un nombre important d'objetssont retournés, soit (ii) un nombre réduit d'objets sont retournés, ce qui peut êtreinsuffisant pour la prise de décisions. Dans cet article, nous abordons le secondproblème et proposons une approche permettant de le traiter. L'idée consiste àrendre le skyline plus permissive en lui ajoutant les objets, non skyline, les pluspréférés. L'approche s'appuie sur une nouvelle relation de dominance floue appelée«Much Preferred». Un algorithme efficace pour calculer le skyline relaxéest proposé. Une série d'expériences sont menées pour démontrer la pertinencede l'approche et la performance de l'algorithme proposé.	Djamal Belkasmi, Allel HadjAli, Hamid Azzoune	http://editions-rnti.fr/render_pdf.php?p1&p=1002188	http://editions-rnti.fr/render_pdf.php?p=1002188	requête skyline constituer outil puisser lanalyse dedonnées multidimensionnel décision multicritère En pratiquer calculdu skyline pouvoir conduire scénario   ie nombre importer dobjetssont retourner ii nombre réduire dobjet retourner pouvoir êtreinsuffisant priser décision Dans article aborder secondproblème proposon approcher permettre traiter Lidée consister àrendre skyline plaire permissif luire ajouter objet skyline pluspréféré Lapproche sappuie relation dominance flouer appelée«Much Preferred » algorithme efficace calculer skyline relaxéest proposer série dexpérience mener démontrer pertinencede lapproch performance lalgorithm proposer
202	Revue des Nouvelles Technologies de l'Information	EGC	2016	Requêtes discriminantes pour l'exploration des données	À l'ère du Big Data, les profils d'utilisateurs deviennent de plus enplus diversifiés et les données de plus en plus complexes, rendant souvent trèsdifficile l'exploration des données. Dans cet article, nous proposons une techniquede réécriture de requêtes pour aider les analystes à formuler leurs interrogations,pour explorer rapidement et intuitivement les données. Nous introduisonsles requêtes discriminantes, une restriction syntaxique de SQL, avecune condition de sélection qui dissocie des exemples positifs et négatifs. Nousconstruisons un ensemble de données d'apprentissage dont les exemples positifscorrespondent aux résultats souhaités par l'analyste, et les exemples négatifs àceux qu'il ne veut pas. En utilisant des techniques d'apprentissage automatique,la requête initiale est reformulée en une nouvelle requête, qui amorce un processusitératif d'exploration des données. Nous avons implémenté cette idée dansun prototype (iSQL) et nous avons mené des expérimentations dans le domainede l'astrophysique.	Julien Cumin, Jean-Marc Petit, Fabien Rouge, Vasile-Marian Scuturici, Christian Surace, Sabine Surdu	http://editions-rnti.fr/render_pdf.php?p1&p=1002170	http://editions-rnti.fr/render_pdf.php?p=1002170	À lère Big Data profil dutilisateur devenir plaire enplu diversifié donnée plaire plaire complexe trèsdifficil lexploration donnée Dans article proposer techniquede réécritur requête aider analyste formuler interrogationspour explorer rapidement intuitivemer donnée introduisonsl requêt discriminanter restriction syntaxique sql avecun condition sélection dissocier exemple positif négatif nousconstruison ensemble donnée dapprentissage exemple positifscorresponder résultat souhaité lanalyste exemple négatif àceux quil vouloir En utiliser technique dapprentissage automatiquela requête initial reformuler requête amorcer processusitératif dexploration donnée implémenter idée dansun prototype isql mener expérimentation dan domainede lastrophysiqu
203	Revue des Nouvelles Technologies de l'Information	EGC	2016	SAFFIET : un système d'extraction de règles d'associations spatiales et fonctionnelles dans les séries de données géographiques	Nous partons de l'hypothèse que les dynamiques spatiales et l'évolutiondes usages des objets géographiques peuvent en partie être explicitées(voire anticipées) par leurs différentes évolutions précédentes et les configurationsspatiales dans lesquelles ils se situent. Aussi afin d'analyser et comprendreles changements de fonction des objets géographiques au cours du temps, et endéduire un modèle prospectif et puis prédictif, nous proposons l'outil SAFFIETqui exploite la recherche des motifs fréquents et des règles d'associations, pourextraire des règles d'évolution régissant les dynamiques spatiales.	Asma Gharbi, Cyril de Runz, Sami Faiz, Herman Akdag	http://editions-rnti.fr/render_pdf.php?p1&p=1002202	http://editions-rnti.fr/render_pdf.php?p=1002202	partir lhypothèse dynamique spatiale lévolutiond usag objet géographique pouvoir partir explicitéesvoir anticiper évolution précédent configurationsspatiale dan situer danalyser comprendrel changement fonction objet géographique cours temps endéduire modeler prospectif pouvoir prédictif proposer loutil saffietqui exploiter rechercher motif fréquent règle dassociation pourextraire règle dévolution régir dynamique spatiale
204	Revue des Nouvelles Technologies de l'Information	EGC	2016	SArEM: Un méta-modèle pour la spécification des processus d'extraction d'architectures logicielles	We propose a meta-model, called SArEM, that specifies the basic elements of the softwarearchitecture extraction. SArEM serves as a tool to compare the different software architectureextraction approaches that aim to extract a system architecture from the source code.	Mira Abboud, Hala Naja, Mourad Oussalah, Mohamad Dbouk	http://editions-rnti.fr/render_pdf.php?p1&p=1002211	http://editions-rnti.fr/render_pdf.php?p=1002211	We proposer metamodel called sarem that specifier the basic elements of the softwarearchitecture extraction sarem serve tool to comparer the softwar architectureextraction approaches that aim to extract system architecturer from the source coder
205	Revue des Nouvelles Technologies de l'Information	EGC	2016	Segmentation comportementale à l'aide des réseaux communautaires	La mise en place d'actions marketing efficaces passe par la segmentationde la clientèle. C'est-à-dire que les clients sont regroupés en ensembles homogènesen fonction de leurs habitudes de consommation, ce qui rend possibleles actions ciblées. Ces dernières, en personnalisant l'offre permettent d'obtenirdes taux de transformation plus importants et de meilleures ventes.Dans cet article, une méthode originale de segmentation comportementale de laclientèle est présentée. Elle permet de visualiser les segments de clients à traversdes réseaux de communautés et de déceler aisément des mutations soudainesou graduelles dans les comportements de quelques individus ou d'un ensembleplus important. L'analyste bénéficie alors d'une meilleure visibilité et peut adapterl'offre à tout moment.	Gaël Bardury, Teddy Boula	http://editions-rnti.fr/render_pdf.php?p1&p=1002176	http://editions-rnti.fr/render_pdf.php?p=1002176	miser placer dactions marketing efficace passer segmentationde clientèle Cestàdire client regrouper ensembl homogènesen fonction habitude consommation possiblel action cibler dernière personnaliser loffre permettre dobtenirde taux transformation plaire important meilleure ventesDans article méthode original segmentation comportemental laclientèle présenter permettre visualiser segment client traversde réseau communauté déceler aisémer mutation soudainesou graduel dan comportement individu dun ensembleplus importer lanalyste bénéficier dune meilleur visibilité pouvoir adapterloffre moment
206	Revue des Nouvelles Technologies de l'Information	EGC	2016	Sélection topologique de variables dans un contexte de discrimination	"En apprentissage automatique, la présence d'un grand nombre de variablesexplicatives conduit à une plus grande complexité des algorithmes et àune forte dégradation des performances des modèles de prédiction. Pour cela,une sélection d'un sous-ensemble optimal discriminant de ces variables s'avèrenécessaire. Dans cet article, une approche topologique est proposée pour la sélectionde ce sous-ensemble optimal. Elle utilise la notion de graphe de voisinagepour classer les variables par ordre de pertinence, ensuite, une méthode pas à pasde type ascendante ""forward"" est appliquée pour construire une suite de modèlesdont le meilleur sous-ensemble est choisi selon son degré d'équivalence topologiquede discrimination. Pour chaque sous-ensemble, le degré d'équivalence estmesuré en comparant la matrice d'adjacence induite par la mesure de proximitéchoisie à celle induite par la ""meilleure"" mesure de proximité discriminante ditede référence. Les performances de cette approche sont évaluées à l'aide de donnéessimulées et réelles. Des comparaisons de sélection de variables en discriminationavec une approche métrique montrent une bien meilleure sélection àpartir de l'approche topologique proposée."	Fatima-Zahra Aazi, Rafik Abdesselam	http://editions-rnti.fr/render_pdf.php?p1&p=1002162	http://editions-rnti.fr/render_pdf.php?p=1002162	En apprentissage automatique présence dun grand nombre variablesexplicative conduire plaire grand complexité algorithme àune fort dégradation performance modèle prédiction Pour celaune sélection dun sousensembl optimal discriminer variable savèrenécessair Dans article approcher topologique proposer sélectionde sousensemble optimal utiliser notion graphe voisinagepour classer variable ordre pertinence ensuite méthode pasd typer ascendant forward appliquer construire suite modèlesdont meilleur sousensembl choisir degré déquivalence topologiquede discrimination Pour sousensemble degré déquivalence estmesurer comparer matrice dadjacence induire mesurer proximitéchoisie induire meilleur mesurer proximité discriminant ditede référence performance approcher évaluer laid donnéessimulée réel comparaison sélection variable discriminationavec approcher métrique montrer meilleur sélection àpartir lapproche topologique proposer
207	Revue des Nouvelles Technologies de l'Information	EGC	2016	Slider : un Raisonneur Incrémental Évolutif	The main drawbacks of current reasoning methods over ontologies are they struggle toprovide scalability for large datasets. The batch processing reasoners who provide the bestscalability so far are unable to infer knowledge from evolving data. We contribute to solvingthese problems by introducing Slider, an efficient incremental reasoner. Slider exhibits a performanceimprovement by more than a 70% compared to the OWLIM-SE reasoner. Slider isconceived to handle expanding data from streams with a growing background knowledge base.It natively supports df and RDFS, and its architecture allows to extend it to more complexfragments with a minimal effort.	Jules Chevalier, Julien Subercaze, Christophe Gravier, Frédérique Laforest	http://editions-rnti.fr/render_pdf.php?p1&p=1002219	http://editions-rnti.fr/render_pdf.php?p=1002219	The main drawback of current reasoning methods over ontologie are they struggle toprovid scalability for large dataset The batch processing reasoner who provide the bestscalability so far are unabl to infer knowledge from evolving dater We contribute to solvingthes problems by introducing Slider an efficient incremental reasoner Slider exhibit performanceimprovement by more than 70 compared to the OWLIMSE reasoner Slider isconceived to handle expanding dater from streams with growing background knowledge baseit natively support df and RDFS and its architecturer allow to extend it to more complexfragment with minimal effort
208	Revue des Nouvelles Technologies de l'Information	EGC	2016	Structures de haies dans un paysage agricole : une étude par chemin de Hilbert adaptatif et chaînes de Markov	Dans cet article nous présentons une approche couplant une courberemplissant l'espace et une chaîne de Markov pour analyser des données spatialesconcernant la localisation de haies. Du fait de l'hétérogénéité spatiale desdonnées, nous utilisons une courbe adaptative de Hilbert qui permet de linéariserl'espace en s'ajustant localement à la densité des données. Pour ensuite exploiterla séquence produite, il est nécessaire de caractériser la distance entre un pointet son prédecesseur sur la courbe ainsi que la densité locale. Nous proposonsde calculer un temps d'accès à un point à partir du point précédent en utilisantla notion de profondeur de découpe. Cette variable, couplée avec les variablescaractérisant les haies est ensuite analysée avec un modèle de Markov. Nousprésentons et interprétons les résultats obtenus sur un jeu de données d'environ10000 segments de haies d'une zone de la Basse vallée de la Durance.	Sébastien Da Silva, Florence Le Ber, Claire Lavigne	http://editions-rnti.fr/render_pdf.php?p1&p=1002179	http://editions-rnti.fr/render_pdf.php?p=1002179	Dans article présenter approcher coupler courberemplisser lespace chaîner Markov analyser donnée spatialesconcernant localisation haie faire lhétérogénéité spatial desdonner utiliser courber adaptatif Hilbert permettre linéariserlespace sajuster localement densité donnée Pour ensuite exploiterla séquence produire nécessaire caractériser distancer entrer pointet prédecesseur courber densité local proposonsde calculer temps daccè poindre partir poindre précédent utilisantla notion profondeur découper variable couplé variablescaractérisant haie ensuite analyser modeler Markov Nousprésentons interpréter résultat obtenir jeu donnée denviron10000 segment haie dune zone Basse vallée durance
209	Revue des Nouvelles Technologies de l'Information	EGC	2016	Supervision de comportements remarquables d'objets mobiles à partir du suivi et de l'analyse de leurs trajectoires spatio-temporelles	We propose a new generic knowledge model dedicated to the consideration of temporaland spatial dimensions of moving objects. We extend usual approaches to meet the specificityof the representation of moving objects and their trajectories. An application on shipping andboat trip scenarii is done.	Mojdeh Soltanmohammadi, Isabelle Mougenot, Thérèse Libourel, Christophe Fagot	http://editions-rnti.fr/render_pdf.php?p1&p=1002230	http://editions-rnti.fr/render_pdf.php?p=1002230	We proposer new generic knowledge model dedicated to the consideration of temporaland spatial dimension of moving objects We extend usual approacher to meet the specificityof the representation of moving objects and their trajectori an application shipping andboat trip scenarii is done
210	Revue des Nouvelles Technologies de l'Information	EGC	2016	TOM: A library for topic modeling and browsing	In this paper, we present TOM (TOpic Modeling), a Python libraryfor topic modeling and browsing. Its objective is to allow for an efficient analysisof a text corpus from start to finish, via the discovery of latent topics. To thisend, TOM features advanced functions for preparing and vectorizing a text corpus.It also offers a unified interface for two topic models (namely LDA usingeither variational inference or Gibbs sampling, and NMF using alternating leastsquarewith a projected gradient method), and implements three state-of-the-artmethods for estimating the optimal number of topics to model a corpus. What ismore, TOM constructs an interactive Web-based browser that makes exploringa topic model and the related corpus easy.	Adrien Guille, Edmundo-Pavel Soriano-Morales	http://editions-rnti.fr/render_pdf.php?p1&p=1002199	http://editions-rnti.fr/render_pdf.php?p=1002199	in this paper we preser tom TOpic Modeling Python libraryfor topic modeling and browsing it objectif is to allow for an efficient analysisof text corpu from start to finish the discovery of latent topics to thisend tom featur advanced function for preparing and vectorizing text corpusIt also offer unified interface for two topic model namely LDA usingeither variational inference or Gibbs sampling and NMF using alternating leastsquarewith projected gradient method and implement three stateoftheartmethods for estimating the optimal number of topics to model corpus what ismore tom construct an interactif Webbased browser that make exploringer topic model and the related corpu easy
211	Revue des Nouvelles Technologies de l'Information	EGC	2016	Topic modeling and hypergraph mining to analyze the EGC conference history	Dans le cadre du défi proposé à l'édition 2016 de la conférence EGC, nous exploitons lesarticles qui y ont été publiés de 2004 à 2015, avec pour but d'expliquer sa structure et sonévolution. A partir des thématiques latentes découvertes et d'autres propriétés des articles (e.g.auteurs, affiliations), nous mettons en lumière des caractéristiques intéressantes des structuresthématique et collaborative d'EGC. A l'aide d'une méthode d'extraction d'itemsets dans leshyper-graphes nous mettons aussi en avant des liens latents entre auteurs ou entre thématiques.De plus, nous proposons des recommandations d'auteurs ou de thématiques. Enfin, nous décrivonsune interface Web pour explorer les connaissances découvertes.	Adrien Guille, Edmundo-Pavel Soriano-Morales, Ciprian-Octavian Truica	http://editions-rnti.fr/render_pdf.php?p1&p=1002191	http://editions-rnti.fr/render_pdf.php?p=1002191	Dans cadrer défi proposer lédition 2016 conférence EGC exploiter lesarticl yu publier 2004 2015 boire dexpliquer structurer sonévolution A partir thématique latent découverte dautr propriété article egauteur affiliation mettre lumière caractéristique intéressant structuresthématique collaborative degc laid dune méthode dextraction ditemset dan leshypergraphes mettre lien latent entrer auteur entrer thématiquesde plaire proposer recommandation dauteur thématique décrivonsune interfac Web explorer connaissance découvert
212	Revue des Nouvelles Technologies de l'Information	EGC	2016	Towards generic and efficient constraint-based mining, a constraint programming approach	In today's data-rich world, pattern mining techniques allow us to extract knowledge fromdata. However, such knowledge can take many forms and often depends on the application athand. This calls for generic techniques that can be used in a wide range of settings. In recentyears, constraint programming has been shown to offer a generic methodology that fits manypattern mining settings, including novel ones. Existing constraint programming solvers do notscale very well though. In this talk, I will review different ways in which this limitation hasbeen overcome. Often, this is through principled integration of techniques and data structuresfrom pattern mining into the constraint solvers.	Tias Guns	http://editions-rnti.fr/render_pdf.php?p1&p=1002151	http://editions-rnti.fr/render_pdf.php?p=1002151	In today datarich world pattern mining technique allow us to extract knowledg fromdata However such knowledge can take many form and often depends the application athand This calls for generic technique that can be used in wide rang of setting In recentyears constraint programming has been shown to offer generic methodology that fit manypattern mining setting including novel existing constraint programming solver do notscal very well though in this talk ie will review differer ways in which this limitation hasbeen overcom Often this is through principled integration of technique and dater structuresfrom pattern mining into the constraint solver
213	Revue des Nouvelles Technologies de l'Information	EGC	2016	Transmute : un outil interactif pour assister l'extraction de connaissances à partir de traces	Alors que l'extraction de connaissances à partir de données(ecd) est un processus qualifié d'interactif et d'itératif, l'interactivité desoutils est souvent limitée et son étude est relativement récente. Elle estpourtant déterminante lors de l'interprétation pour choisir les motifs quideviendront des connaissances. Nous proposons Transmute, un outild'assistance à l'interprétation dans le processus d'ecd, dans le cadre dela recherche d'épisodes séquentiels à partir de traces. La phase d'interprétationest itérative et à chaque itération les résultats de la fouille sontmis à jour dynamiquement en fonction des interactions avec l'analyste.Des outils de visualisation et des mesures de qualité indépendantes dudomaine permettent de caractériser l'intérêt des motifs à interpréter pourfaciliter leur choix et accompagner le travail de l'analyste afin de l'aider àse focaliser plus rapidement sur les motifs potentiellement intéressants.	Pierre-Loup Barazzutti, Amélie Cordier, Béatrice Fuchs	http://editions-rnti.fr/render_pdf.php?p1&p=1002201	http://editions-rnti.fr/render_pdf.php?p=1002201	lextraction connaissance partir donnéesecd processus qualifier dinteractif ditératif linteractivité desoutil limité étude récent estpourtant déterminant linterprétation choisir motif quideviendront connaissance proposer Transmute outildassistance linterprétation dan processus decd dan cadrer dela rechercher dépisod séquentiel partir trace phase dinterprétationest itératif itération résultat fouiller sontmis jour dynamiquement fonction interaction lanalystedes outil visualisation mesure qualité indépendant dudomaine permettre caractériser lintérêt motif interpréter pourfaciliter choix accompagner travail lanalyste laider àse focaliser plaire rapidement motif potentiellement intéressant
214	Revue des Nouvelles Technologies de l'Information	EGC	2016	Un cadre collaboratif pour la segmentation et la classification d'images de télédétection	Dans cet article nous présentons CoSC, un cadre collaboratif pour lasegmentation et la classification d'images de télédétection permettant d'extraireles objets d'une classe thématique donnée. Le processus de collaboration estguidé par la qualité des données évaluée par des critères d'homogénéité ainsique des critères implicitement liés à la sémantique des objets afin d'extraire uneclasse thématique donnée. Nos expériences montrent que CoSC atteint des bonsrésultats en termes de classification, et améliore notablement la segmentation del'image de manière globale.	Andrés Troya-Galvis, Pierre Gançarski, Laure Berti-Equille	http://editions-rnti.fr/render_pdf.php?p1&p=1002181	http://editions-rnti.fr/render_pdf.php?p=1002181	Dans article présenter CoSC cadrer collaboratif lasegmentation classification dimag télédétection permettre dextrairel objet dune classer thématique donner processus collaboration estguider qualité donnée évaluer critère dhomogénéité ainsiqu critère implicitement lier sémantique objet dextraire uneclasse thématique donner expérience montrer CoSC atteindre bonsrésultat terme classification améliorer notablemer segmentation delimage manière global
215	Revue des Nouvelles Technologies de l'Information	EGC	2016	Un outil d'exploration pour le Défi EGC 2016	Dans le cadre du défi EGC 2016, nous avons développé une applicationweb pour explorer les données décrivant les articles publiés depuis 2004 lorsdes conférences EGC. L'outil permet de découvrir les thèmes importants qui ontété abordés dans ces papiers. De plus, il permet de déterminer automatiquementles articles sémantiquement similaires à des thèmes donnés.	Olivier Parisot, Yoanne Didry, Thomas Tamisier	http://editions-rnti.fr/render_pdf.php?p1&p=1002197	http://editions-rnti.fr/render_pdf.php?p=1002197	Dans cadrer défi EGC 2016 développer applicationweb explorer donnée décrire article publier 2004 lorsde conférence EGC Loutil permettre découvrir thème important ontété aborder dan papier De plaire permettre déterminer automatiquementl article sémantiquemer similaire thème donner
216	Revue des Nouvelles Technologies de l'Information	EGC	2016	Un protocole d'expérimentation sur les propriétés graphémiques avec l'algorithme SOM	Nous présentons une recherche sur la distribution et la classificationnon-supervisée des graphèmes. Nous visons à réduire l'écart entre les résultatsde recherches récentes qui montrent la capacité des algorithmes d'apprentissageet de classification non-supervisée pour détecter les propriétés de phonèmes, etles possibilités actuelles de la représentation textuelle d'Unicode. Nos procéduresdoivent assurer la reproductibilité des expériences et garantir que l'informationrecherchée n'est pas implicitement présente dans le pré-traitement desdonnées. Notre approche est capable de catégoriser correctement de potentielsgraphèmes, ce qui montre que les propriétés phonologiques sont présentes dansles données textuelles, et peuvent être automatiquement extraites à partir desdonnées textuelles brutes en Unicode, sans avoir besoin de les traduire en représentationsphonologiques.	Otman Manad, Nourredine Aliane, Gilles Bernard	http://editions-rnti.fr/render_pdf.php?p1&p=1002160	http://editions-rnti.fr/render_pdf.php?p=1002160	présenter rechercher distribution classificationnonsupervisée graphème viser réduire lécart entrer résultatsde recherche récent montrer capacité algorithme dapprentissageet classification nonsuperviser détecter propriété phonème etl possibilité actuel représentation textuel dunicode procéduresdoivent reproductibilité expérience garantir linformationrechercher nest implicitement présent dan prétraitement desdonner approcher capable catégoriser correctement potentielsgraphème montrer propriété phonologique présenter dansl donnée textuel pouvoir automatiquement extraire partir desdonnée textuel brut unicode besoin traduire représentationsphonologique
217	Revue des Nouvelles Technologies de l'Information	EGC	2016	Un regard lexico-scientométrique sur le défi EGC 2016	Depuis 2001, les conférences EGC ont rassemblé 1 782 chercheursautour de l'extraction et la gestion de connaissances. En 2016, l'associationEGC réfléchit à son histoire et se projette en lançant un défi à sa communauté.Que peut-on révéler sur la communauté EGC via des approches développées enEGC ? Notre étude lexico-scientométrique apporte un éclairage sur les thématiquesdu congrès, les lieux de publication investis par ses auteurs, ou encore lesauteurs sollicitables comme évaluateurs. Les résultats sont intégrés à un site websous-tendu par un système d'information décisionnel.	Guillaume Cabanac, Gilles Hubert, Hong Diep Tran, Cécile Favre, Cyril Labbé	http://editions-rnti.fr/render_pdf.php?p1&p=1002194	http://editions-rnti.fr/render_pdf.php?p=1002194	Depuis 2001 conférence EGC rassembler 1 782 chercheursautour lextraction gestion connaissance En 2016 lassociationegc réfléchir histoire projeter lancer défi communautéque peuton révéler communauté egc approche développer enegc   étude lexicoscientométriqu apporter éclairage thématiquesdu congrè lieu publication investir auteur lesauteur sollicitable évaluateur résultat intégrer site websoustendu système dinformation décisionnel
218	Revue des Nouvelles Technologies de l'Information	EGC	2016	Une approche basée sur des données mixtes – mesures et estimations – pour la détection de défaillances d'un système robotisé	Mettre en place un dispositif de détection de pannes représente denos jours l'un des défis majeurs pour les constructeurs des systèmes robotisés.Le processus de détection nécessite l'utilisation d'un certain nombre de capteursafin de surveiller le fonctionnement de ces systèmes. Or, le coût ainsi queles contraintes liées à la mise en place de ces capteurs conduisent souvent lesconcepteurs à optimiser leurs nombres, ce qui mène à un manque de mesuresnécessaires pour la détection de défaillances. L'une des méthodes pour comblerce manque est d'estimer les paramètres non mesurables à partir d'un modèlemathématique décrivant la dynamique du système réel. Cet article présente uneapproche basée sur des données mixtes (données mesurées et données estimées)pour la détection de défaillances dans les systèmes robotisés. Cette détection esteffectuée en utilisant un classifieur de type arbre de décision. Les données utiliséespour son apprentissage proviennent des mesures prises sur le système réel.Ces données sont ensuite enrichies par des données estimées en provenance d'unobservateur basé sur un modèle analytique. Cet enrichissement sous forme d'attributssupplémentaires a pour but d'augmenter la connaissance du classifieursur le fonctionnement du système et par conséquent améliorer le taux de bonnedétection de défaillances. Une expérience sur un système d'actionnement d'unsiège robotisé, montrant l'intérêt de notre approche, sera présentée à la fin del'article.	Rabah Mazouzi, Rabih Taleb, Lynda Seddiki, Cyril de Runz, Kevin Guelton, Herman Akdag	http://editions-rnti.fr/render_pdf.php?p1&p=1002169	http://editions-rnti.fr/render_pdf.php?p=1002169	mettre placer dispositif détection panne représenter denos jour lun défi majeur constructeur système robotisésLe processus détection nécessit lutilisation dun nombre capteursafin surveiller fonctionnement système Or coût contrainte lier miser placer capteur conduire lesconcepteurs optimiser nombre mener manquer mesuresnécessaire détection défaillance lune méthode comblerce manqu destimer paramètre mesurable partir dun modèlemathématiqu décrire dynamique système réel article présenter uneapproche basé donnée mixte donnée mesuré donnée estiméespour détection défaillance dan système robotiser détection esteffectuer utiliser classifieur typer arbre décision donnée utiliséespour apprentissage provenir mesure prendre système réelces donnée ensuite enrichir donnée estimé provenance dunobservateur baser modeler analytique enrichissemer sou former dattributssupplémentair boire daugmenter connaissance classifieursur fonctionnement système conséquent améliorer taux bonnedétection défaillance expérience système dactionnement dunsiège robotiser montrer lintérêt approcher présenter fin delarticl
219	Revue des Nouvelles Technologies de l'Information	EGC	2016	Une approche combinée pour l'enrichissement d'ontologie à partir de textes et de données du LOD	Cet article porte sur l'étiquetage automatique de documents décrivantdes produits, avec des concepts très spécifiques traduisant des besoins précisd'utilisateurs. La particularité du contexte est qu'il se confronte à une triple difficulté: 1) les concepts utilisés pour l'étiquetage n'ont pas de réalisations terminologiquesdirectes dans les documents, 2) leurs définitions formelles ne sontpas connues au départ, 3) toutes les informations nécessaires ne sont pas forcémentprésentes dans les documents mêmes. Pour résoudre ce problème, nousproposons un processus d'annotation en deux étapes, guidé par une ontologie.La première consiste à peupler l'ontologie avec les données extraites des documents,complétées par d'autres issues de ressources externes. La deuxièmeest une étape de raisonnement sur les données extraites qui recouvre soit unephase d'apprentissage de définitions de concepts, soit une phase d'applicationdes définitions apprises. L'approche SAUPODOC est ainsi une approche originaled'enrichissement d'ontologie qui exploite les fondements du Web sémantique,en combinant les apports du LOD et d'outils d'analyse de texte, d'apprentissageautomatique et de raisonnement. L'évaluation, sur deux domaines d'application,donne des résultats de qualité et démontre l'intérêt de l'approche.	Céline Alec, Chantal Reynaud, Brigitte Safar	http://editions-rnti.fr/render_pdf.php?p1&p=1002168	http://editions-rnti.fr/render_pdf.php?p=1002168	article porter létiquetage automatique document décrivantde produit concept spécifique traduire besoin précisdutilisateur particularité contexte quil confronter tripler difficulté 1 concept utiliser létiquetage nont réalisation terminologiquesdirecter dan document 2 définition formel sontpas connu départir 3 information nécessaire forcémentprésente dan document Pour résoudre problème nousproposer processus dannotation étape guider ontologiela consister peupler lontologie donnée extrait documentscomplétée dautre issu ressource externe deuxièmeest étape raisonnement donnée extrait recouvrir unephase dapprentissage définition concept phase dapplicationde définition apprendre Lapproche SAUPODOC approcher originaledenrichissement dontologie exploiter fondement web sémantiqueen combiner apport lod doutils danalyse texte dapprentissageautomatiqu raisonnement lévaluation domaine dapplicationdonne résultat qualité démontrer lintérêt lapproche
220	Revue des Nouvelles Technologies de l'Information	EGC	2016	Une approche d'évolution du web de données	Sharing knowledge and data coming from different sources is one of the biggest advantageof linked data. Keeping this knowledge graph up to date may take in account both ontologyvocabularies and data since they should be consistent. Our general problem is to deal with webof data evolution in particular: We aim at modifing both levels : A-Box and T-Box.	Fatma Chamekh, Danielle Boulanger, Guilaine Talens	http://editions-rnti.fr/render_pdf.php?p1&p=1002226	http://editions-rnti.fr/render_pdf.php?p=1002226	Sharing knowledge and dater coming from source is one of the biggest advantageof linked dater Keeping this knowledge graph up to dater may take in account both ontologyvocabularies and dater since they should be consister Our general problem is to deal with webof dater evolution in particular we aim at modifing both level   abox and TBox
221	Revue des Nouvelles Technologies de l'Information	EGC	2016	Une approche de réduction de dimensionnalité pour l'agrégation de préférences qualitatives	Nous présentons une méthode de réduction de dimensionnalité pourdes données de préférences multicritères lorsque l'espace des évaluations estun treillis distributif borné. Cette méthode vise à réduire la complexité desprocédures d'apprentissage d'un modèle d'agrégation sur des données qualitatives.Ainsi nous considérons comme modèle d'agrégation l'intégrale de Sugeno.L'apprentissage d'un tel modèle à partir de données empiriques est unproblème d'optimisation à 2n paramètres (où n est le nombre de critères considérés).La méthode de réduction que nous proposons s'appuie sur l'observationde certaines relations entre les éléments de ces données, et nous donnons despremiers résultats d'applications.	Quentin Brabant, Miguel Couceiro, Fabien Labernia, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1002186	http://editions-rnti.fr/render_pdf.php?p=1002186	présenter méthode réduction dimensionnalité pourd donnée préférence multicritère lespace évaluation estun treillis distributif borner méthode viser réduire complexité desprocédur dapprentissage dun modeler dagrégation donnée qualitativesainsi considérer modeler dagrégation lintégral sugenolapprentissage dun modeler partir donnée empirique unproblème doptimisation 2n paramètre nombre critère considérésla méthode réduction proposer sappuie lobservationde relation entrer élément donnée donner despremier résultat dapplication
222	Revue des Nouvelles Technologies de l'Information	EGC	2016	Une mesure de similarité entre phrases basée sur des noyaux sémantiques	Nous proposons une nouvelle approche pour le calcul de similarité sémantiqueentre phrases en utilisant les noyaux sémantiques qui les composent.Ces noyaux, sous la forme de triplets (sujet, verbe et objet) sont supposés porteursde l'information des phrases dont ils sont extraits. Sur la base de la comparaisonsémantique de noyaux, on extrait un ensemble d'indicateurs descriptifs.Nous utilisons ensuite un apprentissage automatique, sur un benchmark contenantdes phrases dont la similarité sémantique a été évaluée par des experts humains,afin de déterminer l'importance de chaque indicateur et de construireainsi un modèle capable de fournir une mesure de similarité sémantique entrephrases. Les expérimentations et les études comparatives, effectuées avec d'autresapproches permettant l'estimation des similarités sémantiques entre phrases,montrent les bonnes performances de notre approche. En se basant sur cette dernière,un outil de navigation sémantique est en cours de développement.	Samir Amir, Adrian Tanasescu, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1002164	http://editions-rnti.fr/render_pdf.php?p=1002164	proposer approcher calcul similarité sémantiqueentre phras utiliser noyau sémantique composentce noyau sou former triplet verb objet supposer porteursde linformation phrase extraire Sur baser comparaisonsémantique noyau extraire ensemble dindicateurs descriptifsnou utiliser ensuite apprentissage automatique benchmark contenantder phrase similarité sémantique évaluer expert humainsafin déterminer limportance indicateur construireainsi modeler capable fournir mesurer similarité sémantique entrephras expérimentation étude comparatif effectuer dautresapproche permettre lestimation similarité sémantique entrer phrasesmontrent performance approcher En baser dernièreun outil navigation sémantique cours développement
223	Revue des Nouvelles Technologies de l'Information	EGC	2016	Une méthode de découverte de motifs contextualisés dans les traces de mobilité d'une personne	Les traces de mobilité générées par les divers capteurs qui nous entourentpeuvent être analysées à des fins prédictives et explicatives pour répondreà divers problèmes du quotidien. Si de nombreuses méthodes ont été proposéespour décrire le comportement d'un individu de manière globale à partir destransitions entre ses différents points d'intérêts (par exemple via un modèle deMarkov), peu de travaux cherchent à l'expliquer de manière locale. Nous proposonsdans cet article une méthode qui permet d'extraire pour un individu donton a une trace de mobilité conséquente des motifs de mobilité dits contextualisés.Chaque motif est composé d'une description sur l'ensemble des visites auxdifférents points d'intérêt de l'individu qui maximise une ou plusieurs mesuresavec une sémantique particulière (le motif décrit une phase sédentaire ou exceptionnelde la mobilité de l'individu). Une expérimentation a été menée à partirde traces de mobilité de véhicules et donne des résultats encourageants.	Aimene Belfodil, Mehdi Kaytoue, Celine Robardet, Marc Plantevit, Julien Zarka	http://editions-rnti.fr/render_pdf.php?p1&p=1002156	http://editions-rnti.fr/render_pdf.php?p=1002156	trace mobilité générer capteur entourentpeuver analyser fin prédictive explicatif répondreà problème quotidien Si méthode proposéespour décrir comportement dun individu manière global partir destransition entrer point dintérêts exemple modeler demarkov travail chercher lexpliquer manière local proposonsdans article méthode permettre dextrair individu donton tracer mobilité conséquent motif mobilité contextualiséschaqu motif composer dune description lensembl visite auxdifférent point dintérêt lindividu maximiser mesuresavec sémantique motif décrire phase sédentaire exceptionnelde mobilité lindividu expérimentation mener partirde trace mobilité véhicule donner résultat encourageant
224	Revue des Nouvelles Technologies de l'Information	EGC	2016	Une méthode supervisée pour initialiser les centres des K-moyennes	Au cours des dernières années, la classification à base de clusterings'est imposée comme un sujet de recherche important. Cette approche vise àdécrire et à prédire un concept cible d'une manière simultanée. Partant du faitque le choix des centres pour l'algorithme des K-moyennes standard a un impactdirect sur la qualité des résultats obtenus, cet article vise alors à tester à quelpoint une méthode d'initialisation supervisée pourrait aider l'algorithme des Kmoyennesstandard à remplir la tâche de la classification à base des K-moyennes.	Oumaima Alaoui Ismaili, Vincent Lemaire, Antoine Cornuéjols	http://editions-rnti.fr/render_pdf.php?p1&p=1002165	http://editions-rnti.fr/render_pdf.php?p=1002165	Au cours dernière année classification baser clusteringsest imposer rechercher importer approcher vis àdécrir prédire concept cibler dune manière simultané partir faitqu choix centre lalgorithme kmoyenne standard impactdirect qualité résultat obtenir article viser tester quelpoint méthode dinitialisation superviser pouvoir aider lalgorithme Kmoyennesstandard remplir tâcher classification baser kmoyenne
225	Revue des Nouvelles Technologies de l'Information	EGC	2016	Vers une approche Visual Analytics pour explorer les variantes de sujets d'un corpus	Our purpose is to implement a Visual Analytics tool for exploring topic variants in textcorpora. The overlapping bi-clustering methods extract multiple topics from the documents,but the interpretation of the results remains difficult. We make the assumption that bi-clusteroverlaps are articulation points between high-level topics, and their multiple variants and viewpoints.We propose to extract and visualize a hierarchical structure of bi-cluster overlaps, allowingto explore the corpus and to discover unsuspected viewpoints.	Nicolas Médoc, Mohammad Ghoniem, Mohamed Nadif	http://editions-rnti.fr/render_pdf.php?p1&p=1002220	http://editions-rnti.fr/render_pdf.php?p=1002220	our purpose is to implement Visual Analytics tool for exploring topic variant in textcorpora The overlapping biclustering method extract topic from the documentsbut the interpretation of the results remain difficult We make the assumption that biclusteroverlap are articulation point between highlevel topic and their variant and viewpointsWe proposer to extract and visualiz hierarchical structurer of bicluster overlap allowingto explorer the corpus and to discover unsuspected viewpoint
226	Revue des Nouvelles Technologies de l'Information	EGC	2016	Visualisation interactive de métadonnées pour aider les utilisateurs d'un logiciel de cartographie statistique à concevoir de meilleures cartes.	CD7Online est l'application SaaS de la 7ème version de Cartes &Données (C & D), le logiciel de cartographie statistique décisionnelle et de géomarketingédité par Articque. C & D permet aux utilisateurs occasionnels de réalisersimplement des cartes à partir de données statistiques et géographiques. 25ans de retours utilisateurs nous ont permis de voir que la qualité des cartes reposeen partie sur la bonne connaissance des données dont disposent les utilisateurset sur leur capacité à choisir des outils d'analyse et de représentation pertinents.Pour aider les utilisateurs à mieux comprendre leurs données et à réaliser descartes de meilleure qualité, nous avons développé une brique sémantique avecun outil de visualisation interactif permettant de visualiser les connaissances extraitesdes espaces de travail des utilisateurs. Nous décrivons ici l'applicationCD7Online ainsi que l'outil de visualisation que nous présenterons lors de ladémonstration logicielle.	Perrine Pittet	http://editions-rnti.fr/render_pdf.php?p1&p=1002203	http://editions-rnti.fr/render_pdf.php?p=1002203	cd7online lapplication saa 7èm version carte donnée   logiciel cartographie statistique décisionnel géomarketingédité Articque   permettre utilisateur occasionnel réalisersimplemer carte partir donnée statistique géographique 25ans utilisateur permettre voir qualité carte reposeen partir connaissance donnée disposer utilisateurset capacité choisir outil danalyse représentation pertinentsPour aider utilisateur mieux comprendre donnée réaliser descart meilleur qualité développer briquer sémantique avecun outil visualisation interactif permettre visualiser connaissance extraitesd espace travail utilisateur décrire lapplicationcd7online loutil visualisation présenter ladémonstration logiciel
227	Revue des Nouvelles Technologies de l'Information	EGC	2015	A Clustering Based Approach for Type Discovery in RDF Data Sources	RDF(S)/OWL data sources are not organized according to a predefined schema, as they are structureless by nature. This lack of schema limits their use to express queries or to understand their content. Our work is a contribution towards the inference of the structure of RDF(S)/OWL data sources. We present an approach relying on density-based clustering to discover the types describing the entities of possibly incomplete and noisy data sets.	Kenza Kellou-Menouer, Zoubida Kedad	http://editions-rnti.fr/render_pdf.php?p1&p=1002113	http://editions-rnti.fr/render_pdf.php?p=1002113	rdfsowl dater source are not organized according to predefined schema they are structureless by nature This lack of schema limit their us to express querier or to understand their conter Our work is contribution towards the inference of the structurer of rdfsowl dater source We preser an approach relying densitybased clustering to discover the type describing the entitier of possibly incomplete and noisy dater set
228	Revue des Nouvelles Technologies de l'Information	EGC	2015	A Framework for Mesh Segmentation and Annotation using Ontologies	La segmentation et annotation de maillages utilisant la sémantique a été l'objet d'un intérêt grandissant avec la démocratisation des techniques de reconstruction 3D. Une approche classique consiste à réaliser cette tâche en deux étapes, tout d'abord en segmentant le maillage, puis en l'annotant. Cependant, cette approche ne permet pas à chaque étape de profiter de l'autre. En traitement d'images, quelques méthodes combinent la segmentation et l'annotation, mais ces approches ne sont pas génériques, et nécessitent des ajustements d'implémentation ou des réécritures pour chaque modification des connaissances expertes. Dans ce travail, nous décrivons un cadre de fonctionnement qui mélange segmentation et annotation afin de réduire le nombre d'étapes de segmentation, et nous présentons des résultats préliminaires qui montrent la faisabilité de l'approche.Notre système fournit une ontologie générique qui décrit sous forme de concepts les propriétés d'un objet (géométrie, topologie, etc.), ainsi que des algorithmes permettant de détecter ces concepts. Cette ontologie peut être étendue par un expert pour décrire formellement un domaine spécifique. La description formelle du domaine est alors utilisée pour réaliser automatiquement l'assemblage de la segmentation et de l'annotation d'objets et de leurs propriétés, en sélectionnant à chaque étape l'algorithme le plus pertinent, étant données les information sémantiques déjà détectées. Cette approche originale comporte plusieurs avantages. Tout d'abord, elle permet de segmenter et d'annoter des objets sans aucune connaissance en traitement d'images ou de maillages, en décrivant uniquement les propriétés de l'objet en terme de concepts ontologiques. De plus, ce cadre de fontionnement peut facilement être réutilisé et appliqué à différents contextes, dès lors qu'une ontologie de domaine a été définie. Finalement, la réalisation conjointe de la segmentation et de l'annotation permet d'utiliser d'une manière efficace la connaissance experte, en réduisant les erreurs de segmentation et le temps de calcul, en lançant toujours l'algorithme le plus pertinent.	Thomas Dietenbeck, Ahlem Othmani, Marco Attene, Jean-Marie Favreau	http://editions-rnti.fr/render_pdf.php?p1&p=1002088	http://editions-rnti.fr/render_pdf.php?p=1002088	segmentation annotation maillage utiliser sémantique lobjet dun intérêt grandir démocratisation technique reconstruction 3D approcher classique consister réaliser tâcher étape dabord segmenter maillage pouvoir lannoter approcher permettre étape profiter lautre En traitement dimager méthode combiner segmentation lannotation approche générique nécessiter ajustement dimplémentation réécriture modification connaissance expert Dans travail décrire cadrer fonctionnement mélanger segmentation annotation réduire nombre détaper segmentation présenter résultat préliminaire montrer faisabilité lapprochenotre système fournir ontologie générique décrire sou former concept propriété dun objet géométri topologie algorithme permettre détecter concept ontologie pouvoir étendre expert décrire formellement domaine spécifique description formel domaine utiliser réaliser automatiquement lassemblage segmentation lannotation dobjet propriété sélectionner étape lalgorithm plaire pertinent donner information sémantique déjà détecter approcher original comporter avantage dabord permettre segmenter dannoter objet connaissance traitement dimag maillage décrire uniquement propriété lobjet terme concept ontologique De plaire cadrer fontionnement pouvoir facilement réutiliser appliquer contexte quune ontologie domaine définir finalement réalisation conjoint segmentation lannotation permettre dutiliser dune manière efficace connaissance expert réduire erreur segmentation temps calcul lancer lalgorithm plaire pertinent
229	Revue des Nouvelles Technologies de l'Information	EGC	2015	Analyse des paramètres de recherche d'information: Etude de l'influence des paramètres sur les résultats	Cet article présente une analyse détaillée d'un ensemble de 2 millions de résultats de recherche d'information obtenus par différents paramétrages de systèmes de recherche d'information. Plus spécifiquement, nous avons utilisé la plateforme Terrier et l'interface RunGeneration pour créer différentes exécutions (run en anglais) en modifiant les modèles d'indexation et de recherche. Nous avons ensuite évalué chacun des résultats obtenus selon différentes mesures de performance de recherche d'information. Une analyse systématique a été menée sur ces données afin de déterminer d'une part quels étaient les paramètres qui ont le plus d'influence, d'autre part quels étaient les valeurs de ces paramètres les plus susceptibles de conduire à de bonnes performances du système.	Josiane Mothe, Marion Moulinou	http://editions-rnti.fr/render_pdf.php?p1&p=1002059	http://editions-rnti.fr/render_pdf.php?p=1002059	article présenter analyser détailler dun ensemble 2 million résultat rechercher dinformation obtenir paramétrage système rechercher dinformation plaire spécifiquement utiliser plateforme Terrier linterfac rungeneration créer exécution run anglais modifier modèle dindexation rechercher ensuite évaluer résultat obtenir mesure performance rechercher dinformation analyser systématique mener donnée déterminer dune partir paramètre plaire dinfluence dautr partir paramètre plaire susceptible conduire performance système
230	Revue des Nouvelles Technologies de l'Information	EGC	2015	Analyse et visualisation d'opinions dans un cadre de veille sur leWeb	L'analyse d'opinions est une tâche qui consiste en l'identification et la classification de textes subjectifs. Dans ce travail, nous nous intéressons au problème d'analyse d'opinions dans un contexte de veille sur le Web. Nous proposons une approche pour visualiser les résultats d'analyse d'opinions, basée sur l'utilisation de termes clés. Nous décrivons également la plateforme de veille sur leWeb AMIEI, au sein de laquelle notre approche a été implémentée. La démonstration consistera en une expérimentation de la plateforme de veille AMIEI et du module d'analyse d'opinions sur un corpus de tweets politiques.	Mohamed Dermouche, Leila Khouas, Sabine Loudcher, Julien Velcin, Eric Fourboul	http://editions-rnti.fr/render_pdf.php?p1&p=1002110	http://editions-rnti.fr/render_pdf.php?p=1002110	lanalys dopinion tâcher consister lidentification classification texte subjectif Dans travail intéresser problème danalyse dopinion dan contexte veiller Web proposer approcher visualiser résultat danalyse dopinion baser lutilisation terme cler décrire également plateforme veiller leweb amiei approcher implémenter démonstration consister expérimentation plateforme veiller amiei moduler danalyse dopinion corpus tweet politique
231	Revue des Nouvelles Technologies de l'Information	EGC	2015	Analyse OLAP sur des tweets et des blogs : un retour d'expérience	Le projet ANR IMAGIWEB dans lequel s'inscrit ce travail s'est donné pour mission d'étudier les images véhiculées sur Internet en se basant sur la détection d'opinions. Deux cas d'étude ont été définis : (1) le premier vise à répondre aux besoins d'analyse de chercheurs en science politique grâce à des données issues de Twitter durant la campagne présidentielle de 2012 ; (2) le second doit permettre à l'entreprise française EDF d'évaluer l'opinion du public en matière de sécurité, d'emploi et de prix à partir de billets de blogs. Dans cet article, nous présentons un retour d'expérience sur l'usage de l'analyse en ligne OLAP (OnLine Analytical Processing) pour des données textuelles, mettant en avant l'intérêt de ce type d'analyse pour les membres du projet.	Brice Olivier, Cécile Favre, Sabine Loudcher	http://editions-rnti.fr/render_pdf.php?p1&p=1002106	http://editions-rnti.fr/render_pdf.php?p=1002106	projet ANR imagiweb dan sinscrit travail sest donner mission détudier image véhiculer Internet baser détection dopinion Deux cas détude définir   1 viser répondre besoin danalyse chercheur science politique grâce donnée issu twitter durer campagne présidentiel 2012   2 second devoir permettre lentreprise français EDF dévaluer lopinion public matière sécurité demploi prix partir billet blog Dans article présenter dexpérience lusage lanalyse ligne OLAP onlin Analytical Processing donnée textuel mettre lintérêt typer danalyse membre projet
232	Revue des Nouvelles Technologies de l'Information	EGC	2015	Analyse visuelle pour la détection des intrusions	"La démocratisation d'Internet, couplée à l'effet de la mondialisation, a pour résultat d'interconnecter les personnes, les états et les entreprises. Le côté déplaisant de cette interconnexion mondiale des systèmes d'information réside dans un phénomène appelé ""Cybercriminalité"". Nous proposons une méthode de visualisation de grands ""graphes"" et l'exploitation d'analyses statiques des flux permettant de détecter les comportements anormaux et dangereux afin d'appréhender les risques d'une façon compréhensible par tous les acteurs."	David Pierrot, Nouria Harbi	http://editions-rnti.fr/render_pdf.php?p1&p=1002079	http://editions-rnti.fr/render_pdf.php?p=1002079	démocratisation dInternet couplé leffet mondialisation résultat dinterconnecter entreprise côté déplaire interconnexion mondial système dinformation résider dan phénomène appeler cybercriminalité proposer méthode visualisation grand graphe lexploitation danalyse statique flux permettre détecter comportement anormal dangereux dappréhender risque dune compréhensible tou acteur
233	Revue des Nouvelles Technologies de l'Information	EGC	2015	Approche d'extraction de classes interlangues à partir de documents multilingues à base de Concepts Fermés	In this article, we highlight the interest and usefulness of Formal Concept Analysis (FCA) in multilingual document clustering. We propose a statistical approach for clustering multilingual documents based on Closed Concepts and vector model partition the documents of one or more collections.An experimental evaluation was conducted on the collection of bilingual documents French-English of CLEF' 2 2003 and showed the merits of this method and the interesting degree of comparability of the obtained bilingual classes.	Mohamed Chebel, Chiraz Latiri	http://editions-rnti.fr/render_pdf.php?p1&p=1002119	http://editions-rnti.fr/render_pdf.php?p=1002119	in this article we highlight the interest and usefulness of Formal Concept Analysis FCA in multilingual document clustering We proposer statistical approach for clustering multilingual document based Closed Concepts and vector model partition the document of one or more collectionsan experimental evaluation wa conducted the collection of bilingual document FrenchEnglish of CLEF 2 2003 and showed the merits of this method and the interesting degree of comparability of the obtained bilingual classe
234	Revue des Nouvelles Technologies de l'Information	EGC	2015	Approche relationnelle de l'apprentissage de séquences	We observe an increasing amount of sequential data, for instance open data sources provide real-time information. In order to apply classical learning algorithms, sequential data are often modelled in an attribute-value setting using a sliding window. In this paper, we propose a relational approach. A first advantage is to let the relational algorithm choose the length of the window. A second advantage is to allow to consider conditions based on the existential quantifier and aggregates. A third advantage is to be able to consider several granularities at the same time.	Clément Charnay, Nicolas Lachiche, Agnès Braud	http://editions-rnti.fr/render_pdf.php?p1&p=1002118	http://editions-rnti.fr/render_pdf.php?p=1002118	We observer an increasing amount of sequential dater for instance open dater source provide realtime information in order to apply classical learning algorithm sequential dater are often modelled in an attributevalue setting using sliding window In this paper we proposer relational approach first advantage is to let the relational algorithm choos the length of the window second advantage is to allow to consider condition based the existential quantifier and aggregater third advantage is to be abl to consider several granularitier at the same time
235	Revue des Nouvelles Technologies de l'Information	EGC	2015	Big Data and the Dawn of Algorithms in Everything	The mainstream adoption of the internet as a source for knowledge and interaction for the past decades has given rise to new data sources that are characterized by large sizes and rapid creation. In addition, sensory data from mobile devices and machinery are on the rise with similar characteristics. All these sources have the commonality that they will tell us something new or something more detailed than before. From a business standpoint these data sources holds the opportunity to create more customized services and improved products in practically anything, however, they also present a challenge since they are big and typically residing outside the traditional server structure of organizations. This talk will explore the challenges of integrating these new, so-called Big Data, in decision processes. Specifically, we will explore the paradigm shifts when external data become equally or more important than internal data. We will also explore the emerging shift in decision making becoming algorithmic as opposed to human discovery driven.	Morten Middelfart	http://editions-rnti.fr/render_pdf.php?p1&p=1002057	http://editions-rnti.fr/render_pdf.php?p=1002057	The mainstream adoption of the internet source for knowledge and interaction for the past decad has given ris to new dater source that are characterized by large sizes and rapid creation in addition sensory dater from mobile devicer and machinery are the ris with similar characteristic all these source hav the commonality that they will tell us something new or something more detailed than before From business standpoint these dater source holds the opportunity to create more customized service and improved product in practically anything however they also preser challenge since they are big and typically residing outsid the traditional server structurer of organization This talk will explorer the challeng of integrating these new socalled Big Data in decision process Specifically we will explorer the paradigm shift when external dater becom equally or more importer than internal dater We will also explorer the emerging shift in decision making becoming algorithmic opposed to human discovery driven
236	Revue des Nouvelles Technologies de l'Information	EGC	2015	Big Data is all about data that we don't have	Big Data is now becoming a buzz word in information technology industry and research. Is Big Data only about large volume of data?, and if it is yes, why is it suddenly becoming a trend. Hasn't the growth of data volume been gigantic in the last decade? From a research point of view, it is not surprising to see researchers from all walks of computer science are trying to align their research to Big Data for the sake of being trendy. The question remains whether it tackles the real Big Data problems. In this talk, I will describe the misconceptions of Big Data, present motivating cases, and discuss the unavoidable challenges faced by industry and research.	David Taniar	http://editions-rnti.fr/render_pdf.php?p1&p=1002055	http://editions-rnti.fr/render_pdf.php?p=1002055	Big Data is now becoming buzz word in information technology industry and research Is Big Data only about large volume of dater and if it is yes why is it suddenly becoming trend Hasnt the growth of dater volume been gigantic in the last decade From research poindre of view it is not surprising to see researcher from all walks of computer science are trying to align their research to Big Data for the sake of being trendy The question remain whether it tackle the real Big Data problems In this talk ie will describe the misconception of Big Data preser motivating cas and discuss the unavoidabl challeng faced by industry and research
237	Revue des Nouvelles Technologies de l'Information	EGC	2015	Challenges and Opportunities in HCI, Visual Analytics and Knowledge Management for the development of Sustainable Cities	While overtly exposed in the media, the challenges faced by our societies to transition towards sustainable energy use are quite formidable. A simple visual refresher of the cold hard facts should amply reveal the importance of visualization to assess the situation. Private companies, such as IBM, and public research centers are joining forces and investing to design and evaluate novel approaches to build and manage Cities, defined as the rational organisation of dense human habitat. Information and Communication technologies are certainly part of the answers, in particular in areas related to knowledge management, data mining, HCI and social computing. Illustrated with telltaling examples of research work carried at IBM, the CSTB and the Efficacity Institute, I will argue that Interactive Information Technologies can help managing the energy transition of cities in 3 key aspects:   — to support the city design process, notably computer supported tooling and information infrastructure that help taming the complexity of the intertwinning actors and interests at play,   — to help understand better the city's dynamics, identifiy inefficiencies and reveal optimization opportunities, where knowledge management and extraction is crucial,   — and foremost, to ease the necessary changes that will have to happen in our mobility and housing habits with novel tools and services that alleviate our energy needs.	Thomas Baudel	http://editions-rnti.fr/render_pdf.php?p1&p=1002058	http://editions-rnti.fr/render_pdf.php?p=1002058	While overtly exposed in the media the challeng faced by our societier to transition towards sustainabl energy user are quite formidable simple visual refresher of the cold hard facts should amply reveal the importance of visualization to asses the situation Private companier such IBM and public research centers are joining and investing to design and evaluate novel approacher to build and manager citi defined the rational organisation of dense human habitat information and communication technologi are certainly partir of the answers in particular in areer related to knowledge management dater mining HCI and social computing Illustrated with telltaling exampl of research work carried at IBM the CSTB and the Efficacity Institute ie will arguer that Interactive information technologi can help managing the energy transition of citier in 3 key aspect    -- to support the city design process notably computer supported tooling and information infrastructur that help taming the complexity of the intertwinning actors and interests at play    -- to help understand better the citys dynamic identifiy inefficiencies and reveal optimization opportunitier where knowledge management and extraction is crucial    -- and foremost to ease the necessary change that will hav to happen in our mobility and housing habit with novel tools and service that alleviat our energy need
238	Revue des Nouvelles Technologies de l'Information	EGC	2015	Choix d'une mesure de proximité discriminante dans un contexte topologique	"Les résultats de toute opération de classification ou de classement d'objets dépendent fortement de la mesure de proximité choisie. L'utilisateur est amené à choisir une mesure parmi les nombreuses mesures de proximité existantes. Or, selon la notion d'équivalence topologique choisie, certaines sont plus ou moins équivalentes. Dans cet article, nous proposons une nouvelle approche de comparaison et de classement de mesures de proximité, dans une structure topologique et dans un objectif de discrimination. Le concept d'équivalence topologique fait appel à la structure de voisinage local.Nous proposons alors de définir l'équivalence topologique entre deux mesures de proximité à travers la structure topologique induite par chaque mesure dans un contexte de discrimination. Nous proposons également un critère pour choisir la ""meilleure"" mesure adaptée aux données considérées, parmi quelques mesures de proximité les plus utilisées dans le cadre de données quantitatives. Le choix de la ""meilleure"" mesure de proximité discriminante peut être vérifié a posteriori par une méthode d'apprentissage supervisée de type SVM, analyse discriminante ou encore régression Logistique, appliquée dans un contexte topologique.Le principe de l'approche proposée est illustré à partir d'un exemple de données quantitatives réelles avec huit mesures de proximité classiques de la littérature. Des expérimentations ont permis d'évaluer la performance de cette approche topologique de discrimination en terme de taille et/ou de dimension des données considérées et de sélection de la ""meilleur"" mesure de proximité discriminante."	Fatima-Zahra Aazi, Rafik Abdesselam	http://editions-rnti.fr/render_pdf.php?p1&p=1002069	http://editions-rnti.fr/render_pdf.php?p=1002069	résultat opération classification classement dobjet dépendre fortement mesurer proximité choisir Lutilisateur amener choisir mesurer mesure proximité existant Or notion déquivalence topologique choisir plaire équivalenter Dans article proposer approcher comparaison classement mesure proximité dan structurer topologique dan objectif discrimination concept déquivalence topologique faire appel structurer voisinage localNous proposon définir léquivalence topologique entrer mesure proximité travers structurer topologique induire mesurer dan contexte discrimination proposer également critère choisir meilleur mesurer adapter donnée considérer mesure proximité plaire utiliser dan cadrer donnée quantitatif choix meilleur mesurer proximité discriminante pouvoir vérifier posteriori méthode dapprentissage superviser typer svm analyser discriminant régression Logistique appliquer dan contexte topologiquel principe lapproche proposer illustrer partir dun exemple donnée quantitatif réel mesure proximité classique littérature expérimentation permettre dévaluer performance approcher topologique discrimination terme tailler etou dimension donnée considérer sélection meilleur mesurer proximité discriminant
239	Revue des Nouvelles Technologies de l'Information	EGC	2015	Classification évidentielle avec contraintes d'étiquettes	Ce papier propose une version améliorée de l'algorithme de classification automatique évidentielle semi-supervisée SECM. Celui-ci bénéficie de l'introduction de données étiquetées pour améliorer la pertinence de ses résultats et utilise la théorie des fonctions de croyance afin de produire une partition crédale qui généralise notamment les concepts de partitions dures et floues. Le pendant de ce gain d'expressivité est une complexité qui est exponentielle avec le nombre de classes, ce qui impose en retour l'utilisation de schémas efficaces pour optimiser la fonction objectif. Nous proposons dans cet article une heuristique qui relâche la contrainte classique de positivité liée aux masses de croyances des méthodes évidentielles. Nous montrons sur un ensemble de jeux de données de test que notre méthode d'optimisation permet d'accélérer sensiblement l'algorithme SECM avec un schéma d'optimisation classique, tout en améliorant également la qualité de la fonction objectif.	Violaine Antoine, Nicolas Labroche	http://editions-rnti.fr/render_pdf.php?p1&p=1002071	http://editions-rnti.fr/render_pdf.php?p=1002071	papier proposer version amélioré lalgorithme classification automatique évidentiell semisupervisé secm Celuici bénéficier lintroduction donnée étiqueter améliorer pertinence résultat utiliser théorie fonction croyance produire partition crédal généraliser concept partition dur floue pendre gain dexpressivité complexité exponentiel nombre classe imposer lutilisation schéma efficace optimiser fonction objectif proposer dan article heuristique relâcher contraint classique positivité lier mass croyance méthode évidentiell montrer ensemble jeu donnée test méthod doptimisation permettre daccélérer sensiblement lalgorithm secm schéma doptimisation classique améliorer également qualité fonction objectif
240	Revue des Nouvelles Technologies de l'Information	EGC	2015	Classification multi-label par raisonnement logique pour l'indexation sémantique de documents	Cet article présente une solution centrée sur les ontologies pour la classification multi-label automatique d'information nécessaire à un système de recommandation d'informations économiques.	David Werner, Christophe Cruz, Aurélie Bertaux	http://editions-rnti.fr/render_pdf.php?p1&p=1002114	http://editions-rnti.fr/render_pdf.php?p=1002114	article présenter solution centrer ontologie classification multilabel automatique dinformation nécessaire système recommandation dinformation économique
241	Revue des Nouvelles Technologies de l'Information	EGC	2015	Clustering topologique pour le flux de données	Actuellement, le clustering de flux de données devient le moyen le plus efficace pour partitionner un très grand ensemble de données. Dans cet article, nous présentons une nouvelle approche topologique, appelée G-Stream, pour le clustering de flux de données évolutives. La méthode proposée est une extension de l'algorithme GNG (Growing Neural Gas) pour gérer le flux de données. G-Stream permet de découvrir de manière incrémentale des clusters de formes arbitraires en ne faisant qu'une seule passe sur les données. Les performances de l'algorithme proposé sont évaluées à la fois sur des données synthétiques et réelles.	Mohammed Ghesmoune, Mustapha Lebbah, Hanane Azzag	http://editions-rnti.fr/render_pdf.php?p1&p=1002072	http://editions-rnti.fr/render_pdf.php?p=1002072	actuellement clustering flux donnée devenir moyen plaire efficace partitionner grand ensemble donnée Dans article présenter approcher topologique appeler GStream clustering flux donnée évolutif méthode proposer extension lalgorithme gng Growing neural Gas gérer flux donnée GStream permettre découvrir manière incrémental cluster forme arbitraire faire quune passer donnée performance lalgorithm proposer évaluer donnée synthétique réel
242	Revue des Nouvelles Technologies de l'Information	EGC	2015	Cohérence des données de bases RDF en évolution constante	Le maintien de la qualité et de la fiabilité de bases de connaissances RDF du Web Sémantique est un problème courant. De nombreuses propositions pour l'intégration de « bonnes » données ont été faites, se basant soit sur les ontologies de ces bases, soit sur des méta-données additionnelles. Dans cet article, nous proposons une approche originale, basée exclusivement sur l'étude des données de la base. Le principe est de déterminer si les modifications apportées par la mise à jour candidate rendent la partie ciblée de la base plus similaire – selon certains critères – à d'autres parties existantes dans la base. La mise à jour est considérée cohérente avec cette base et peut être appliquée.	Pierre Maillot, Thomas Raimbault, David Genest	http://editions-rnti.fr/render_pdf.php?p1&p=1002085	http://editions-rnti.fr/render_pdf.php?p=1002085	maintien qualité fiabilité base connaissance RDF web Sémantique problème courir proposition lintégration « » donnée faire baser ontologie base métadonnée additionnel Dans article proposer approcher original baser exclusivement létude donnée baser principe déterminer modification apporter miser jour candidat partir cibler baser plaire similaire – critère – dautr party existant dan baser miser jour considérer cohérent baser pouvoir appliquer
243	Revue des Nouvelles Technologies de l'Information	EGC	2015	Comparison of linear modularization criteria using the relational formalism, an approach to easily identify resolution limit	La modularisation de grands graphes ou recherche de communautés est abordée comme l'optimisation d'un critère de qualité, l'un des plus utilisés étant la modularité de Newman-Girvan. D'autres critères, ayant d'autres propriétés, aboutissent à des solutions différentes. Dans cet article, nous présentons une réécriture relationnelle de six critères linéaires: Zahn-Condorcet, Owsi´nski- Zadro&#729;zny, l'Ecart à l'Uniformité, l'Ecart à l'Indétermination et la Modularité Equilibrée. Nous utilisons une version générique de l'algorithme d'optimisation de Louvain pour approcher la partition optimale pour chaque critère sur des réseaux réels de différentes tailles. Les partitions obtenues présentent des caractéristiques différentes, concernant notamment le nombre de classes. Le formalisme relationnel nous permet de justifier ces différences d'un point de vue théorique. En outre, cette notation permet d'identifier facilement les critères ayant une limite de résolution (phénomène qui empêche en pratique la détection de petites communautés sur de grands graphes). Une étude de la qualité des partitions trouvées dans les graphes synthétiques LFR permet de confirmer ces résultats.	Patricia Conde-Céspedes, Jean-François Marcotorchino, Emmanuel Viennet	http://editions-rnti.fr/render_pdf.php?p1&p=1002080	http://editions-rnti.fr/render_pdf.php?p=1002080	modularisation grand graphe rechercher communauté aborder loptimisation dun critère qualité lun plaire utiliser modularité newmangirvan dautr critère dautr propriété aboutir solution différenter Dans article présenter réécriture relationnel critère linéaire zahncondorcet owsi´nski zadro729zny lecart luniformité lecart lindétermination modularité equilibrer utiliser version générique lalgorithme doptimisation Louvain approcher partition optimal critère réseau réel taille partition obtenu présenter caractéristique concerner nombre classe formalisme relationnel permettre justifier différence dun poindre théorique En outrer notation permettre didentifier facilement critère limiter résolution phénomène empêcher pratiquer détection petit communauter grand graphe étude qualité partition trouver dan graphe synthétique lfr permettre confirmer résultat
244	Revue des Nouvelles Technologies de l'Information	EGC	2015	Compromis précision-rappel dans l'évaluation des performances 	Dans de nombreux problèmes d'apprentissage automatique la performance des algorithmes est évaluée à l'aide des mesures précision et rappel. Or ces deux mesures peuvent avoir une importance très différente en fonction du contexte. Dans cet article nous étudions le comportement des principaux indices de performance en fonction du couple précision-rappel. Nous proposons un nouvel outil de visualisation de performances et définissons l'espace de compromis qui représente les différents indices en fonction du compromis précision-rappel. Nous analysons les propriétés de ce nouvel espace et mettons en évidence ses avantages par rapport à l'espace précision-rappel.	Blaise Hanczar, Mohamed Nadif	http://editions-rnti.fr/render_pdf.php?p1&p=1002070	http://editions-rnti.fr/render_pdf.php?p=1002070	Dans problème dapprentissage automatique performance algorithme évaluer laid mesure précision rappel Or mesure pouvoir importance fonction contexte Dans article étudier comportement principal indice performance fonction coupler précisionrappel proposer nouvel outil visualisation performance définisson lespace compromis représenter indice fonction compromis précisionrappel analyser propriété nouvel espacer metton évidence avantage rapport lespace précisionrappel
245	Revue des Nouvelles Technologies de l'Information	EGC	2015	Contribution au calcul du skyline par réduction de l'espace candidat	L'opérateur skyline est devenu un paradigme dans les bases de données. Il consiste à localiser Sky l'ensemble des points d'un espace vectoriel qui ne sont pas dominés. Cet opérateur est utile lorsqu'on n'arrive pas à se décider dans les situations conflictuelles. Le calcul des requêtes skyline est pénalisé par le nombre de points que peuvent contenir les bases de données. Dans ce papier, nous présentons une solution analytique pour la réduction de l'espace candidat et nous proposons une méthode efficace pour le calcul de ce type de requêtes	Lougmiri Zekri, Hadjer Belaicha	http://editions-rnti.fr/render_pdf.php?p1&p=1002082	http://editions-rnti.fr/render_pdf.php?p=1002082	lopérateur skyline devenir paradigme dan base donnée consister localiser sky lensembl point dun espac vectoriel dominer opérateur utile lorsquon narriv décider dan situation conflictuel calcul requête skyline pénaliser nombre point pouvoir contenir base donnée Dans papier présenter solution analytique réduction lespace candidat proposer méthode efficace calcul typer requête
246	Revue des Nouvelles Technologies de l'Information	EGC	2015	D113 : une plateforme open-source dédiée à l'analyse des flux et à la détection des intrusions	"Ce travail se situe dans le domaine de la ""Cybersécurité"", le projet ""D113"" permet de visualiser en temps réel les flux transitant sur des équipements de filtrage sans avoir recours au traitement manuel des journaux d'événements. Nous centrerons notre démonstration sur la visualisation de grands ""graphes"" et l'exploitation d'analyses statiques des flux."	David Pierrot, Nouria Harbi	http://editions-rnti.fr/render_pdf.php?p1&p=1002108	http://editions-rnti.fr/render_pdf.php?p=1002108	travail situer dan domaine Cybersécurité projet D113 permettre visualiser temps réel flux transiter équipement filtrage recours traitement manuel journal dévénement centrer démonstration visualisation grand graphe lexploitation danalyse statique flux
247	Revue des Nouvelles Technologies de l'Information	EGC	2015	Découverte de proportions analogiques dans les bases de données : une première approche	Cet article présente un nouveau cadre pour la découverte de connaissances basé sur la notion de proportion analogique qui exprime l'égalité des rapports entre les attributs de deux paires d'éléments. Cette notion est développée dans le contexte des bases de données pour découvrir des parallèles dans les données. Dans un premier temps, nous donnons une définition formelle des proportions analogiques dans le cadre des bases de données relationnelles, puis nous étudions le problème de l'extraction des proportions analogiques. Nous montrons qu'il est possible de suivre une approche de clustering pour découvrir les classes d'équivalence de paires de n-uplets dans le même rapport de proportion analogique. Ce travail constitue unCet article présente un nouveau cadre pour la découverte de connaissances basé sur la notion de proportion analogique qui exprime l'égalité des rapports entre les attributs de deux paires d'éléments. Cette notion est développée dans le contexte des bases de données pour découvrir des parallèles dans les données. Dans un premier temps, nous donnons une définition formelle des proportions analogiques dans le cadre des bases de données relationnelles, puis nous étudions le problème de l'extraction des proportions analogiques. Nous montrons qu'il est possible de suivre une approche de clustering pour découvrir les classes d'équivalence de paires de n-uplets dans le même rapport de proportion analogique. Ce travail constitue une première étape vers l'extension des langages d'interrogation de base de données avec des requêtes « analogiques ».e première étape vers l'extension des langages d'interrogation de base de données avec des requêtes « analogiques ».	William Correa Beltran, Hélène Jaudoin, Olivier Pivert	http://editions-rnti.fr/render_pdf.php?p1&p=1002075	http://editions-rnti.fr/render_pdf.php?p=1002075	article présenter cadrer découvrir connaissance baser notion proportion analogique exprimer légaliter rapport entrer attribut paire déléments notion développer dan contexte base donnée découvrir parallèle dan donnée Dans temps donner définition formel proportion analogique dan cadrer base donnée relationnel pouvoir étudier problème lextraction proportion analogique montrer quil approcher clustering découvrir classe déquivalence paire nuplet dan rapport proportion analogique travail constituer uncet article présenter cadrer découvrir connaissance baser notion proportion analogique exprimer légaliter rapport entrer attribut paire déléments notion développer dan contexte base donnée découvrir parallèle dan donnée Dans temps donner définition formel proportion analogique dan cadrer base donnée relationnel pouvoir étudier problème lextraction proportion analogique montrer quil approcher clustering découvrir classe déquivalence paire nuplet dan rapport proportion analogique travail constituer étape ver lextension langage dinterrogation baser donnée requête « analogique » 7e étape ver lextension langage dinterrogation baser donnée requête « analogique »
248	Revue des Nouvelles Technologies de l'Information	EGC	2015	Détection automatique de reformulations - Correspondance de concepts appliquée à la détection du plagiat	Dans le cadre de la détection du plagiat, la phase de comparaison de deux documents est souvent réduite à une comparaison mot à mot, une recherche de « copier/coller ». Dans cet article, nous proposons une approche naïve de comparaison de deux documents dans le but de détecter automatiquement aussi bien les phrases copiées de l'un des textes dans l'autre que les paraphrases et reformulations, ceci en se focalisant sur l'existence des mots porteurs de sens, ainsi que sur leurs mots de substitution possibles. Nous comparons trois algorithmes utilisant cette approche afin de déterminer la plus efficace pour ensuite l'évaluer face à des méthodes existantes. L'objectif est de permettre la détection des similitudes entre deux textes en utilisant uniquement des mots clefs. L'approche proposée permet de détecter des reformulations non paraphrastiques impossibles à détecter avec des approches conventionnelles faisant appel à une phase d'alignement.	Jérémy Ferrero, Alain Simac-Lejeune	http://editions-rnti.fr/render_pdf.php?p1&p=1002089	http://editions-rnti.fr/render_pdf.php?p=1002089	Dans cadrer détection plagiat phase comparaison document réduire comparaison rechercher « copiercoller » Dans article proposer approcher naïf comparaison document dan boire détecter automatiquement phrase copier lun texte dan lautre paraphrase reformulation focaliser lexistence porteur sens substitution comparer algorithme utiliser approcher déterminer plaire efficace ensuite lévaluer face méthode existant Lobjectif permettre détection similitude entrer texte utiliser uniquement clef Lapproche proposer permettre détecter reformulation paraphrastique impossible détecter approche conventionnel faire appel phase dalignement
249	Revue des Nouvelles Technologies de l'Information	EGC	2015	Détection et regroupement automatique de style d'écriture dans un texte	La détection de plagiat extrinsèque devient vite inefficace lorsque l'on n'a pas accès aux documents potentiellement sources du plagiat ou lorsque l'on se confronte à un espace aussi vaste que leWeb, ce qui est souvent le cas dans les logiciels anti-plagiat actuels. Dès lors la détection intrinsèque devient nettement plus efficace. Dans cet article, nous traitons justement de la détection automatique d'auteurs qui permet de savoir si un passage d'un texte n'appartient pas au même auteur que le reste du texte et donc en théorie de repérer les passages plagiés d'un document. Nous expliquons notre contribution aux procédures déjà existantes et évaluons les limites de notre approche. L'objectif est de permettre la détection et le regroupement de passages d'un document par auteur.	Jérémy Ferrero, Alain Simac-Lejeune	http://editions-rnti.fr/render_pdf.php?p1&p=1002060	http://editions-rnti.fr/render_pdf.php?p=1002060	détection plagiat extrinsèque devenir vite inefficace lon accès document potentiellement source plagiat lon confronter espacer vaste leWeb cas dan logiciel antiplagiat actuel Dès détection intrinsèque devenir nettement plaire efficace Dans article traiter justement détection automatique dauteur permettre savoir passage dun texte nappartient auteur rester texte théorie repérer passage plagié dun document expliquer contribution procédure déjà existant évaluon limite approcher Lobjectif permettre détection regroupement passage dun document auteur
250	Revue des Nouvelles Technologies de l'Information	EGC	2015	Deux approches pour catégoriser le risque	Le risque chimique ou alimentaire couvre les situations où les produits chimiques sont dangereux pour la santé et consommation humaine ou animale, et pour l'environnement. Les experts qui assurent le contrôle et la gestion de ces substances se retrouvent face à de gros volumes de littérature scientifique, qui doit être analysée pour appuyer la prise de décisions. Nous proposons une aide automatique pour l'analyse de cette littérature. Nous abordons la tâche comme une problématique de catégorisation: il s'agit de catégoriser les phrases des textes dans les classes du risque lié aux substances. Nous utilisons deux approches: par apprentissage supervisé et la recherche d'information. Les résultats obtenus avec l'apprentissage supervisé (toute classe confondue, F-mesure autour de 0,8 pour le risque alimentaire, entre 0,61 et 0,64 pour le risque chimique) sont meilleurs que ceux obtenus avec par recherche d'information (toute classe confondue, F-mesure entre 0,18 et 0,226 pour le risque alimentaire, entre 0,20 et 0,32 pour le risque chimique). Le rappel est compétitif avec les deux approches.	Natalia Grabar, Niña Kerry	http://editions-rnti.fr/render_pdf.php?p1&p=1002067	http://editions-rnti.fr/render_pdf.php?p=1002067	risquer chimique alimentaire couvrir situation produit chimique dangereux santé consommation humain animal lenvironnement expert assurer contrôler gestion substance retrouver face gros volume littérature scientifique devoir analyser priser décision proposer aider automatique lanalyse littérature aborder tâcher problématique catégorisation sagit catégoriser phrase texte dan classe risquer lier substance utiliser approche apprentissage superviser rechercher dinformation résultat obtenir lapprentissage superviser classer confondu fmesure autour 08 risquer alimentaire entrer 061 064 risquer chimique meilleur obtenu rechercher dinformation classer confondu fmesure entrer 018 0226 risquer alimentaire entrer 020 032 risquer chimique rappel compétitif approche
251	Revue des Nouvelles Technologies de l'Information	EGC	2015	Échantillonnage de flux de données sémantiques : Une approche orientée graphe	Nowadays, processing online massive data streams with special techniques like load shedding is an unavoidable alternative to optimize system resources use. In this paper, we propose a graph-oriented approach for load shedding semantic data streams. Our approach, unlike the RDF triple based one, preserves the semantic level of the data streams, which improves the responses quality of the RDF data stream processing systems.	Fethi Belghaouti, Amel Bouzeghoub, Zakia Kazi-aoul, Raja Chiky	http://editions-rnti.fr/render_pdf.php?p1&p=1002120	http://editions-rnti.fr/render_pdf.php?p=1002120	nowadays processing online massif dater streams with special technique like load shedding is an unavoidabl alternatif to optimize system resource us in this paper we proposer graphoriented approach for load shedding semantic dater stream Our approach unlike the RDF tripler based one preserv the semantic level of the dater streams which improv the respons quality of the RDF dater stream processing system
252	Revue des Nouvelles Technologies de l'Information	EGC	2015	Etude de La Pertinence lors de La Sélection de Collections dans les Systèmes Distribués	This paper presents a new function of collection selection. Our function is free of any extracollection parameter and is based on the documents relevance. The ranking of a collection is proportional to its number of relevant documents.	Kheira Mechach, Lougmiri Zekri, Mustapha Kamel Abdi	http://editions-rnti.fr/render_pdf.php?p1&p=1002122	http://editions-rnti.fr/render_pdf.php?p=1002122	this paper present new function of collection selection Our function is free of any extracollection parameter and is based the document relevance The ranking of collection is proportional to it number of relever document
253	Revue des Nouvelles Technologies de l'Information	EGC	2015	Extraction complète efficace de chemins pondérés dans un a-DAG	Un nouveau domaine de motifs appelé chemins pondérés condensés a été introduit en 2013 lors de la conférence IJCAI. Le contexte de fouille est alors un graphe acyclique orienté (DAG) dont les sommets sont étiquetés par des attributs. Nous avons travaillé à une implémentation efficace de ce type de motifs et nous montrons que l'algorithme proposé était juste mais incomplet. Nous établissons ce résultat d'incomplétude et nous l'expliquons avant de trouver une solution pour réaliser une extraction complète. Nous avons ensuite développé des structures complémentaires pour calculer efficacement tous les chemins pondérés condensés. L'algorithme est amélioré en performance de plusieurs ordres de magnitude sur des jeux de données artificiels et nous l'appliquons à des données réelles pour motiver qualitativement l'usage des chemins pondérés.	Nazha Selmaoui-Folcher, Frédéric Flouvat, Chengcheng Mu, Jérémy Sanhes, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1002077	http://editions-rnti.fr/render_pdf.php?p=1002077	domaine motif appeler chemin pondéré condensé introduire 2013 conférence ijcer contexte fouiller graphe acyclique orienter DAG sommet étiqueter attribut travailler implémentation efficace typer motif montrer lalgorithm proposer incomplet établir résultat dincomplétude lexpliquer trouver solution réaliser extraction complet ensuite développer structure complémentaire calculer efficacement tou chemin pondéré condensé Lalgorithme améliorer performance ordre magnitude jeu donnée artificiel lappliquer donnée réel motiver qualitativement lusage chemin pondéré
254	Revue des Nouvelles Technologies de l'Information	EGC	2015	Extraction de l'intérêt implicite des utilisateurs dans les attributs des items pour améliorer les systèmes de recommandations	Les systèmes de recommandation ont pour objectif de sélectionner et présenter d'abord les informations susceptibles d'intéresser les utilisateurs. Ce travail expose un système de recommandation qui s'appuie sur deux concepts: des relations sémantiques sur les données et une technique de filtrage collaboratif distribué basée sur la factorisation des matrices (MF). D'une part, les techniques sémantiques peuvent extraire des relations entre les données, et par conséquent, améliorer la précision des recommandations. D'autre part, MF donne des prévisions très précises avec un algorithme facilement parralélisable. Notre proposition utilise cette technique en ajoutant des relations sémantiques au processus. En effet, nous analysons en profondeur les intérêts cachés des utilisateurs dans les attributs des items à recommander. Nous utilisons dans nos expérimentations le jeu de données MovieLens enrichi par la base de données IMDb. Nous comparons notre travail à une technique MF classique. Les résultats montrent une précision dans les recommandations, tout en préservant un niveau élevé d'abstraction du domaine. En outre, nous améliorons le passage à l'échelle du système en utilisant des techniques parallélisables.	Manuel Pozo, Raja Chiky, Elisabeth Métais	http://editions-rnti.fr/render_pdf.php?p1&p=1002093	http://editions-rnti.fr/render_pdf.php?p=1002093	système recommandation objectif sélectionner poster dabord information susceptible dintéresser utilisateur travail exposer système recommandation sappuie concept relation sémantique donnée technique filtrage collaboratif distribuer baser factorisation matrice mf dune partir technique sémantique pouvoir extraire relation entrer donnée conséquent améliorer précision recommandation Dautre partir mf donner prévision précis algorithme facilement parralélisabl proposition utilis technique ajouter relation sémantique processus En analyser profondeur intérêt cacher utilisateur dan attribut item recommander utiliser dan expérimentation jeu donnée movielen enrichir baser donnée imdb comparer travail technique mf classique résultat montrer précision dan recommandation préserver niveau élever dabstraction domaine En outrer améliorer passage léchelle système utiliser technique parallélisable
255	Revue des Nouvelles Technologies de l'Information	EGC	2015	Feedback - Study and Improvement of the Random Forest of the Mahout library in the context of marketing data of Orange	L'apprentissage automatique a fait son apparition dans l'écosystème Hadoop créant, de par la puissance promise, une opportunité sans précédent pour ce domaine. Dans cet écosystème, Apache Mahout est une réponse à la question du temps de calcul et/ou de la volumétrie: il consiste en un entrepôt d'algorithmes d'apprentissage automatique, tous portés afin de s'exécuter sur Map/Reduce. Ce rapport se concentre sur le portage et l'utilisation de l'algorithme des Random Forest dans Mahout. Il montre à travers notre retour d'expérience les difficultés qui peuvent être rencontrées tant pratiques que théoriques et suggère une piste d'amélioration.	C. Thao, Nicolas Voisine, Vincent Lemaire, R. Trinquart	http://editions-rnti.fr/render_pdf.php?p1&p=1002104	http://editions-rnti.fr/render_pdf.php?p=1002104	lapprentissage automatique faire apparition dan lécosystèm hadoop créer puissance promettre opportunité précédent domaine Dans écosystème apache mahout réponse question temps calcul etou volumétrie consister entrepôt dalgorithmer dapprentissage automatique tou porter sexécuter MapReduce rapport concentrer portage lutilisation lalgorithme Random Forest dan mahout montrer travers dexpérience difficulté pouvoir rencontrer pratique théorique suggèr pister damélioration
256	Revue des Nouvelles Technologies de l'Information	EGC	2015	gapIT : Un outil visuel pour l'imputation de valeurs manquantes en hydrologie	Les données manquantes sont problématiques en hydrologie, car elles gênent le calcul de statistiques interannuelles et sur de longues périodes, ainsi que l'analyse et l'interprétation de la variabilité des données. Dans cet article, nous présentons gapIT, une plateforme d'analyse de données permettant d'inspecter visuellement les données manquantes et ensuite de choisir la méthode de correction adéquate. Nous avons utilisé l'outil pour estimer les données manquantes dans des séries temporelles correspondant aux débits mesurés par des stations hydrométriques du Luxembourg.	Olivier Parisot, Laura Giustarini, Olivier Faber, Renaud Hostache, Ivonne Trebs, Mohammad Ghoniem	http://editions-rnti.fr/render_pdf.php?p1&p=1002107	http://editions-rnti.fr/render_pdf.php?p=1002107	donnée manquant problématique hydrologie gêner calcul statistique interannuel long périod lanalyse linterprétation variabilité donnée Dans article présenter gapit plateforme danalyse donnée permettre dinspecter visuellemer donnée manquant ensuite choisir méthode correction adéquat utiliser loutil estimer donnée manquant dan série temporel correspondre débit mesurer station hydrométrique Luxembourg
257	Revue des Nouvelles Technologies de l'Information	EGC	2015	Gestion de l'incertitude dans le cadre d'une extraction des connaissances à partir de texte	The knowledge representation area needs some methods that allow to detect and handle uncertainty. Indeed, a lot of text hold information whose the veracity can be called into question. These information should be managed efficiently in order to represent the knowledge in an explicit way. As first step, we have identified the different forms of uncertainty during a knowledge extraction process, then we have introduce an RDF representation for these kind of knowledge based on an ontologie that we developped for this issue.	Fadhela Kerdjoudj, Olivier Curé	http://editions-rnti.fr/render_pdf.php?p1&p=1002116	http://editions-rnti.fr/render_pdf.php?p=1002116	The knowledge representation areer need some method that allow to detect and handle uncertainty Indeed lot of text hold information whos the veracity can be called into question These information should be managed efficiently in order to represent the knowledge in an explicit way first step we hav identified the form of uncertainty during knowledge extraction proces then we hav introduce an RDF representation for these kind of knowledge based an ontologie that we developped for this issu
258	Revue des Nouvelles Technologies de l'Information	EGC	2015	Heuristiques pour l'adaptation des mappings entre ontologies dynamiques	Les correspondances sémantiques entre ontologies (mappings) jouent un rôle essentiel dans les systèmes d'information. Cependant, en vertu de l'évolution des connaissances, les éléments ontologiques sont sujets à modification invalidant potentiellement les alignements préalablement établis. Des techniques de maintenance sont donc nécessaires pour maintenir la validité des mappings. Dans cet article, nous présentons un ensemble d'heuristiques guidant leur adaptation. Notre approche s'appuie sur l'explication des mappings existants, les informations provenant de l'évolution des ontologies ainsi que les adaptations possibles applicables aux mappings. Nous proposons une validation expérimentale à partir d'ontologies du domaine médical et des mappings qui leur sont associés.	Julio Cesar Dos Reis, Cédric Pruski, Chantal Reynaud-Delaître	http://editions-rnti.fr/render_pdf.php?p1&p=1002084	http://editions-rnti.fr/render_pdf.php?p=1002084	correspondance sémantique entrer ontologie mapping jouer rôle essentiel dan système dinformation vertu lévolution connaissance élément ontologique modification invalider potentiellement alignement préalablement établir technique maintenance nécessaire maintenir validité mapping Dans article présenter ensemble dheuristiqu guider adaptation approcher sappuie lexplication mapping existant information provenir lévolution ontologie adaptation applicable mapping proposer validation expérimental partir dontologie domaine médical mapping associé
259	Revue des Nouvelles Technologies de l'Information	EGC	2015	Identification d'auteurs par apprentissage automatique	Etant donné un ensemble de documents rédigés par un même auteur, le problème d'authentification d'auteurs consiste à décider si un nouveau texte a été rédigé ou non par cet auteur. Pour résoudre ce problème, nous avons proposé et implémenté différentes approches : comptage de similarité, techniques de vote et apprentissage supervisé qui exploitent différents modèles de représentation des documents. Les expérimentations réalisées à partir des collections de la compétition PAN-CLEF 2013 et 2014 ont confirmé l'intérêt de nos approches et leur performance en termes de temps de traitement.	Jordan Frery, Christine Largeron, Mihaela Juganaru-Mathieu	http://editions-rnti.fr/render_pdf.php?p1&p=1002062	http://editions-rnti.fr/render_pdf.php?p=1002062	eter donner ensemble document rédiger auteur problème dauthentification dauteur consister décider texte rédiger auteur Pour résoudre problème proposer implémenter approche   comptage similarité technique voter apprentissage superviser exploiter modèle représentation document expérimentation réaliser partir collection compétition PANCLEF 2013 2014 confirmer lintérêt approche performance terme temps traitement
260	Revue des Nouvelles Technologies de l'Information	EGC	2015	Identification des utilisateurs atypiques dans les systèmes de recommandation sociale	Malgré des performances très satisfaisantes, l'approche sociale de la recommandation ne fournit pas de bonnes recommandations à un sous-ensemble des utilisateurs. Nous supposons ici que certains de ces utilisateurs ont des préférences différentes de celles des autres, nous les qualifions d'atypiques. Nous nous intéressons à leur identification, en amont de la tâche de recommandation, et proposons plusieurs mesures représentant l'atypicité des préférences d'un utilisateur. L'évaluation de ces mesures sur un corpus de l'état de l'art montre qu'elles permettent d'identifier de façon fiable des utilisateurs recevant de mauvaises recommandations.	Benjamin Gras, Armelle Brun, Anne Boyer	http://editions-rnti.fr/render_pdf.php?p1&p=1002097	http://editions-rnti.fr/render_pdf.php?p=1002097	Malgré performance satisfaisant lapproche social recommandation fournir recommandation sousensemble utilisateur supposer utilisateur préférence qualifion datypiqu intéresser identification amont tâcher recommandation proposon mesure représenter latypiciter préférence dun utilisateur lévaluation mesure corpus létat lart montr permettre didentifier fiable utilisateur recevoir mauvais recommandation
261	Revue des Nouvelles Technologies de l'Information	EGC	2015	L'apport d'une approche symbolique pour le repérage des entités nommées en langue amazighe	Le repérage des Entités Nommées (REN) en langue amazighe est un prétraitement éventuellement essentiel pour de nombreuses applications du traitement automatique des langues (TAL), en particulier pour la traduction automatique. Dans cet article, nous présentons une chaîne de repérage des entités nommées en amazighe fondée sur une étude synthétique des spécificités de la langue et des entités nommées en amazighe. L'article met l'accent sur les choix méthodologiques à résoudre les ambiguïtés dues à la langue, en exploitant les technologies existantes pour d'autres langues.	Meryem Talha, Siham Boulaknadel, Driss Aboutajdine	http://editions-rnti.fr/render_pdf.php?p1&p=1002061	http://editions-rnti.fr/render_pdf.php?p=1002061	repérage entité nommer REN langue amazighe prétraitement éventuellement essentiel application traitement automatique langu tal traduction automatique Dans article présenter chaîner repérage entité nommer amazighe fonder étude synthétique spécificité langue entité nommer amazighe Larticle mettre laccent choix méthodologique résoudre ambiguïté langue exploiter technologie existant dautre langu
262	Revue des Nouvelles Technologies de l'Information	EGC	2015	LeveragingWeb 2.0 for Informed Real-Estate Services	The perception about real estate properties, both for individuals and agents, is not formed exclusively by their intrinsic characteristics, such as surface and age, but also from property externalities, such as pollution, traffic congestion, criminality rates, proximity to playgrounds, schools and stimulating social interactions that are equally important. In this paper, we present the Real-Estate 2.0 System that in contrary to existing Real-Estate e-services and applications, takes also into account important externalities. By leveraging Web 2.0 (content from Social Networks, POI listings) applications and Open Data enables the thorough analysis of the current physical and social context of the property, the context-based objective valuation of RE properties, along with an advanced property search and selection experience that unveils otherwise “hidden” property features and significantly reduces user effort and time spent in their RE quest. The system encompasses the above to provide services which assist individuals and agents in making more informed and sound RE decisions.	Papantoniou Katerina, Athanasiadis Marios - Lazaros, Fundulaki Irini, Georgis Christos, Stavrakas Yannis, Troullinos Michalis, Tsitsanis Anastasios	http://editions-rnti.fr/render_pdf.php?p1&p=1002105	http://editions-rnti.fr/render_pdf.php?p=1002105	The perception about real estat propertie both for individual and agent is not formed exclusively by their intrinsic characteristics such surface and age boire also from property externalitie such pollution traffic congestion criminality rater proximity to playground school and stimulating social interaction that are equally importer in this paper we preser the RealEstate 20 system that in contrary to existing RealEstate eservice and application takes also into account importer externalitie By leveraging web 20 conter from Social Networks POI listing application and Open Data enabl the thorough analysis of the current physical and social context of the property the contextbased objectif valuation of RE propertie along with an advanced property search and selection experience that unveil otherwise “ hidden ” property featur and significantly reduc user effort and time spent in their RE quest The system encompasser the above to provide service which assist individual and agent in making more informed and sound RE decision
263	Revue des Nouvelles Technologies de l'Information	EGC	2015	Linked Data Annotation and Fusion driven by Data Quality Evaluation	Dans cet article nous présentons une approche de fusion de données fondée sur l'utilisation d'informations sur la qualité des données pour résoudre les éventuels conflits entre valeurs.	Ioanna Giannopoulou, Fatiha Saïs, Rallou Thomopoulos	http://editions-rnti.fr/render_pdf.php?p1&p=1002086	http://editions-rnti.fr/render_pdf.php?p=1002086	Dans article présenter approcher fusion donnée fonder lutilisation dinformation qualité donnée résoudre éventuel conflit entrer
264	Revue des Nouvelles Technologies de l'Information	EGC	2015	Managing Big Multidimensional Data	Multidimensional database concepts such as cubes, dimensions with hierarchies, and measures have been a cornerstone of analytical business intelligence tools for decades. However, the standard data models and system implementations (OLAP) for multidimensional databases cannot handle “Big Multidimensional Data”, very large amounts of complex and highly dynamic multidimensional data that occur in a number of emerging domains such as energy, transport, logistics, as well as science. This talk will discuss similarities and differences between traditional Business Intelligence (BI) and Big Data, present examples of Big Multidimensional data with the characteristics of large volume, high velocity (fast data), and/or high variety (complex data) and discuss how to manage Big Multidimensional Data, including modeling, algorithmic, implementation, as well as practical, issues.	Torben Bach Pedersen	http://editions-rnti.fr/render_pdf.php?p1&p=1002056	http://editions-rnti.fr/render_pdf.php?p=1002056	Multidimensional database concept such cuber dimension with hierarchi and measur hav been cornerstone of analytical busines intelligence tools for decad However the standard dater model and system implementations olap for multidimensional databas cannot handl “ Big Multidimensional Data ” very large amount of complex and highly dynamic multidimensional dater that occur in number of emerging domain such energy transport logistic well science This talk will discuss similariti and difference between traditional Business Intelligence BI and Big Data preser exampl of Big Multidimensional dater with the characteristics of large volume high velocity fast dater andor high variety complex dater and discuss how to manager Big Multidimensional Data including modeling algorithmic implementation well practical issu
265	Revue des Nouvelles Technologies de l'Information	EGC	2015	Mesure d'influence via les indicateurs de centralité dans les réseaux sociaux	For social network analysis, existing centrality measures emphasize the importance of an actor considering only the structural position in the network regardless of a priori information on this actors such as popularity, accessibility or behavior. In this study new variants of centrality measures are proposed operating both the network structure and the specific attributes of an actor. Experiments have validated the contribution of valuations especially for the detection of broadcasters in social networks.	Oualid Benyahia, Christine Largeron	http://editions-rnti.fr/render_pdf.php?p1&p=1002112	http://editions-rnti.fr/render_pdf.php?p=1002112	for social network analysis existing centrality measur emphasiz the importance of an actor considering only the structural position in the network regardless of priori information this actors such popularity accessibility or behavior In this study new variant of centrality measur are proposed operating both the network structurer and the specific attribut of an actor experiment hav validated the contribution of valuation especially for the detection of broadcasters in social network
266	Revue des Nouvelles Technologies de l'Information	EGC	2015	Méthode alternative à la détection de « copier/coller » : intersection de textes et construction de séquences maximales communes 	La détection du plagiat passe le plus souvent par la phase de recherche de similitudes la plus naïve, la détection de « copier/coller ». Dans cet article, nous proposons une méthode alternative à l'approche standard de comparaison mot à mot. Le principe étant d'effectuer une intersection des deux textes à comparer, récupérant ainsi un tableau des mots qu'ils ont en commun et de ne conserver que les séquences maximales des mots se suivant dans l'un des textes et existant également dans l'autre. Nous montrons que cette méthode est plus rapide et moins coûteuse en ressources que les méthodes de parcours de textes habituellement utilisées. L'objectif étant de détecter les passages identiques entre deux textes plus rapidement que les méthodes de comparaison mot à mot, tout en étant plus efficace que les méthodes n-grammes.	Jérémy Ferrero, Alain Simac-Lejeune	http://editions-rnti.fr/render_pdf.php?p1&p=1002065	http://editions-rnti.fr/render_pdf.php?p=1002065	détection plagiat passer plaire phase rechercher similitude plaire naïf détection « copiercoller » Dans article proposer méthode alternatif lapproche standard comparaison principe deffectuer intersection texte comparer récupérer tableau quils commun conserver séquence maximal dan lun texte exister également dan lautre montrer méthode plaire rapide coûteux ressource méthode parcours texte habituellement utiliser Lobjectif détecter passage identique entrer texte plaire rapidement méthode comparaison plaire efficace méthode ngramm
267	Revue des Nouvelles Technologies de l'Information	EGC	2015	Mining Classes by Multi-label Classification	We propose a new approach to mine potential classes in news documents by examining close relationship between new classes and probability vectors of multiple labeling of the documents. Using EM algorithm to obtain the distribution over linear mixture models, we make clustering and mine classes.	Yuichiro Kase, Takao Miura	http://editions-rnti.fr/render_pdf.php?p1&p=1002066	http://editions-rnti.fr/render_pdf.php?p=1002066	We proposer new approach to miner potential classe in new document by examining clos relationship between new classe and probability vectors of labeling of the document Using EM algorithm to obtain the distribution over linear mixture model we make clustering and min classe
268	Revue des Nouvelles Technologies de l'Information	EGC	2015	"Modèle de Biclustering dans un paradigme ""Mapreduce"""	Biclustering is a main task in a variety of areas of machine learning providing simultaneous observations and features clustering. Biclustering approches are more complex compared to the traditional clustering particularly those requiring large dataset and Mapreduce platforms. We propose a new approach of biclustering based on popular self-organizing maps for cluster analysis of large dataset. We have designed scalable implementations of the new biclustering algorithm using MapReduce with the Spark platform. We report the experiments and demonstrated the performance public dataset using different cores. Using practical examples, we demonstrate that our algorithm works well in practice. The experimental results show scalable performance with near linear speedups across different data and 120 cores.	Tugdual Sarazin, Hanane Azzag, Mustapha Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1002111	http://editions-rnti.fr/render_pdf.php?p=1002111	Biclustering is main task in variety of areer of machiner learning providing simultaneous observation and featur clustering Biclustering approche are more complex compared to the traditional clustering particularly thos requiring large dataset and Mapreduce platforms We proposer new approach of biclustering based popular selforganizing map for cluster analysi of large dataset We hav designed scalabl implementation of the new biclustering algorithm using MapReduce with the Spark platform We report the experiment and demonstrated the performance public dataset using cor Using practical exampl we demonstrate that our algorithm work well in practice The experimental result show scalabl performance with near linear speedup acros dater and 120 core
269	Revue des Nouvelles Technologies de l'Information	EGC	2015	Nouvelle approche de contextualisation de tweets basée sur les règles d'association inter-termes	Tweets are short messages that do not exceed 140 characters. Since they must be written respecting this limitation, a particular vocabulary is used. To make them understandable to a reader, it is therefore necessary to know their context. In this paper, we describe our approach for the tweet contextualization. This approach allows the extension of the tweet's vocabulary by a set of thematically related words using mining association rules between terms.	Meriem Amina Zingla, Mohamed Ettaleb, Chiraz Latiri, Yahia Slimani	http://editions-rnti.fr/render_pdf.php?p1&p=1002121	http://editions-rnti.fr/render_pdf.php?p=1002121	tweet are short messag that do not exceed 140 characters since they must be written respecting this limitation particular vocabulary is used to make them understandabl to reader it is therefore necessary to know their context in this paper we describe our approach for the tweet contextualization This approach allow the extension of the tweet vocabulary by set of thematically related word using mining association ruler between term
270	Revue des Nouvelles Technologies de l'Information	EGC	2015	Pour une meilleure exploitation de la classification croisée dans les systèmes de filtrage collaboratif	Pour la prédiction automatique des items préférés par des utilisateurs sur le Web, différents systèmes de filtrage collaboratif ont été proposés. La plupart d'entre eux sont basés sur la factorisation matricielle et les approches de type k plus proches voisins. Malheureusement ces deux approches requièrent un temps de calcul important. Une partie de ces problèmes a pu être surmontée par la classification croisée ou co-clustering qui s'avère pertinente du fait qu'elle permet par nature une gestion simultanée des ensembles correspondant aux utilisateurs et aux items. Cependant, des travaux doivent encore être menés pour une meilleure prise en compte des données manquantes. Dans ce travail, nous proposons donc une gestion efficace des données non observées permettant une meilleure exploitation du potentiel de la classification croisée dans le domaine des systèmes de recommandation. Nous montrons de plus qu'elle permet d'obtenir des représentations à base de graphes bipartis facilitant l'interprétation interactive des affinités entre des groupes d'utilisateurs et des groupe d'items.	Aghiles Salah, Nicoleta Rogovschi, François Role, Mohamed Nadif	http://editions-rnti.fr/render_pdf.php?p1&p=1002095	http://editions-rnti.fr/render_pdf.php?p=1002095	Pour prédiction automatique item préférer utilisateur web système filtrage collaboratif proposer dentre baser factorisation matriciel approche typer plaire voisin malheureusement approche requérir temps calcul importer partir problème pouvoir surmonter classification croisé coclustering savère pertinent faire permettre nature gestion simultané ensemble correspondre utilisateur item travail devoir mener meilleur priser compter donnée manquant Dans travail proposer gestion efficace donnée observer permettre meilleur exploitation potentiel classification croiser dan domaine système recommandation montrer plaire permettre dobtenir représentation baser graphe biparti faciliter linterprétation interactif affinité entrer groupe dutilisateur grouper ditems
271	Revue des Nouvelles Technologies de l'Information	EGC	2015	Proposition d'outil de clustering visuel et interactif	Cet article présente un nouvel outil visuel de clustering interactif. Il utilise une technique de réduction de dimensionnalité pour permettre une représentation 2D des données et des classes associées, initialement établies de manière non-supervisée. L'originalité de l'outil consiste à autoriser des modifications itératives à la fois du clustering et de la projection 2D. Grâce à des contrôles adaptés, l'utilisateur peut ainsi injecter ses préférences, et observer le changement induit en temps réel. La méthode de projection utilisée suit une métaphore physique, qui facilite le suivi des changements par l'utilisateur. Nous montrons un exemple illustrant l'intérêt pratique de l'outil.	Pierrick Bruneau, Philippe Pinheiro, Bertjan Broeksema, Benoît Otjacques	http://editions-rnti.fr/render_pdf.php?p1&p=1002073	http://editions-rnti.fr/render_pdf.php?p=1002073	article présenter nouvel outil visuel clustering interactif utiliser technique réduction dimensionnalité permettre représentation 2D donnée classe associé initialement établir manière nonsupervisé Loriginalité loutil consister autoriser modification itératif clustering projection 2D grâce contrôle adapté lutilisateur pouvoir injecter préférence observer changement induire temps réel méthode projection utiliser métaphore physique faciliter changement lutilisateur montrer exemple illustrer lintérêt pratiquer loutil
272	Revue des Nouvelles Technologies de l'Information	EGC	2015	Qualité et complexité en évaluation des mesures d'intérêt	Remplacer des hypothèses sur le modèle de données par des informations mesurées sur les données réelles est l'une des forces de la fouille de données. Cet article étudie cet ajustement entre les données et les méthodes de découverte de motifs pour en évaluer la qualité et la complexité. Nous formalisons ce lien entre données et mesures d'intérêt en identifiant les motifs liés qui sont ceux nécessaires pour l'évaluation d'une mesure ou d'une contrainte. Nous formulons alors trois axiomes que devraient satisfaire ces motifs liés pour qu'une méthode d'extraction se comporte bien. En outre, nous définissons la complexité en évaluation qui quantifie finement l'interrelation entre les motifs au sein d'une méthode d'extraction. A la lumière de ces axiomes et de cette complexité en évaluation, nous dressons une typologie de multiples méthodes de découverte de motifs impliquant la fréquence.	Bruno Crémilleux, Arnaud Giacometti, Arnaud Soulet	http://editions-rnti.fr/render_pdf.php?p1&p=1002094	http://editions-rnti.fr/render_pdf.php?p=1002094	remplacer hypothèse modeler donnée information mesuré donnée réel lune fouiller donnée article étudier ajustement entrer donnée méthode découvrir motif évaluer qualité complexité formaliser lien entrer donnée mesure dintérêt identifier motif lier nécessaire lévaluation dune mesurer dune contraint formuler axiome devoir satisfair motif lier quune méthode dextraction comporter En outrer définir complexité évaluation quantifier finement linterrelation entrer motif dune méthode dextraction A lumière axiome complexité évaluation dresser typologie méthode découvrir motif impliquer fréquence
273	Revue des Nouvelles Technologies de l'Information	EGC	2015	RankMerging: Apprentissage supervisé de classements pour la prédiction de liens dans les grands réseaux sociaux	Trouver les liens manquants dans un grand réseau social est une tâche difficile, car ces réseaux sont peu denses, et les liens peuvent correspondre à des environnements structurels variés. Dans cet article, nous décrivons RankMerging, une méthode d'apprentissage supervisé simple pour combiner l'information obtenue par différentes méthodes de classement. Afin d'illustrer son intérêt, nous l'appliquons à un réseau d'utilisateurs de téléphones portables, pour montrer comment un opérateur peut détecter des liens entre les clients de ses concurrents. Nous montrons que RankMerging surpasse les méthodes à disposition pour prédire un nombre variable de liens dans un grand graphe épars.	Lionel Tabourier, Anne-Sophie Libert, Renaud Lambiotte	http://editions-rnti.fr/render_pdf.php?p1&p=1002102	http://editions-rnti.fr/render_pdf.php?p=1002102	trouver lien manquant dan grand réseau social tâcher difficile réseau dense lien pouvoir correspondre environnement structurel varier Dans article décrire rankmerging méthode dapprentissage superviser simple combiner linformation obtenir méthode classement Afin dillustrer intérêt lappliquer réseau dutilisateur téléphone portable montrer opérateur pouvoir détecter lien entrer client concurrent montrer RankMerging surpasser méthode disposition prédire nombre variable lien dan grand graph épar
274	Revue des Nouvelles Technologies de l'Information	EGC	2015	Réduction de la complexité spatiale et temporelle du Compact Prediction Tree pour la prédiction de séquences	La prédiction de séquences de symboles est une tâche ayant de multiples applications. Plusieurs modèles de prédiction ont été proposés tels que DG, All-k-order markov et PPM. Récemment, il a été montré qu'un nouveau modèle nommé Compact Prediction Tree (CPT) utilisant une structure en arbre et un algorithme de prédiction plus complexe, offre des prédictions plus exactes que plusieurs approches de la littérature. Néanmoins, une limite importante de CPT est sa complexité temporelle et spatiale élevée. Dans cet article, nous pallions ce problème en proposant trois stratégies pour réduire la taille et le temps de prédiction de CPT. Les résultats expérimentaux sur 7 jeux de données réels montrent que le modèle résultant nommé CPT+ est jusqu'à 98 fois plus compact et est 4.5 fois plus rapide que CPT, tout en conservant une exactitude très élevée par rapport à All-K-order Markov, DG, Lz78, PPM et TDAG.	Ted Gueniche, Philippe Fournier-Viger	http://editions-rnti.fr/render_pdf.php?p1&p=1002064	http://editions-rnti.fr/render_pdf.php?p=1002064	prédiction séquence symbole tâcher application modèle prédiction proposer dg Allkorder markov PPM Récemment montrer quun modeler nommer Compact Prediction Tree CPT utiliser structurer arbre algorithme prédiction plaire complexe offrir prédiction plaire exact approche littérature limiter important CPT complexité temporel spatial élevé Dans article pallier problème proposer stratégie réduire tailler temps prédiction cpt résultat expérimental 7 jeu donnée réel montrer modeler résulter nommer CPT jusquà 98 plaire compact 45 plaire rapide CPT conserver exactitude élevé rapport AllKorder Markov DG Lz78 ppm TDAG
275	Revue des Nouvelles Technologies de l'Information	EGC	2015	Regroupement d'attributs par règles d'association dans les systèmes d'inférence floue	Dans les systèmes d'apprentissage supervisé par construction de règles de classification floues, un nombre élevé d'attributs descriptifs conduit à une explosion du nombre de règles générées et peut affecter la précision des algorithmes d'apprentissage. Afin de remédier à ce problème, une solution est de traiter séparément des sous-groupes d'attributs. Cela permet de décomposer le problème d'apprentissage en des sous-problèmes de complexité inférieure, et d'obtenir des règles plus intelligibles car de taille réduite. Nous proposons une nouvelle méthode de regroupement des attributs qui se base sur le concept des règles d'association. Ces règles découvrent des relations intéressantes entre des intervalles de valeurs des attributs. Ces liaisons locales sont ensuite agrégées au niveau des attributs mêmes en fonction du nombre de liaisons trouvées et de leur importance. Notre approche, testée sur différentes bases d'apprentissage et comparée à l'approche classique, permet d'améliorer la précision tout en garantissant une réduction du nombre de règles.	Ilef Ben Slima, Amel Borgi	http://editions-rnti.fr/render_pdf.php?p1&p=1002092	http://editions-rnti.fr/render_pdf.php?p=1002092	Dans système dapprentissage superviser construction règle classification flou nombre élever dattributs descriptif conduire explosion nombre règle généré pouvoir précision algorithme dapprentissage Afin remédier problème solution traiter séparément sousgroupe dattributs celer permettre décomposer problème dapprentissage sousproblème complexité inférieur dobtenir règle plaire intelligible tailler réduit proposer méthode regroupement attribut baser concept règle dassociation règle découvrir relation intéressant entrer intervalle attribut liaison local ensuite agréger niveau attribut fonction nombre liaison trouvé importance approcher tester base dapprentissage comparer lapproche classique permettre daméliorer précision garantir réduction nombre règle
276	Revue des Nouvelles Technologies de l'Information	EGC	2015	Régularisation de noyaux temporellement élastiques et analyse en composantes principales non-linéaire pour la fouille de séries temporelles 	Dans le domaine de la fouille de séries temporelles, plusieurs travaux récents exploitent des noyaux construits à partir de distances élastiques de type Dynamic Time Warping (DTW) au sein d'approches à base de noyaux. Pourtant les matrices, apparentées aux matrices de Gram, construites à partir de ces noyaux n'ont pas toujours les propriétés requises ce qui peut les rendre in fine impropres à une telle exploitation. Des approches émergeantes de régularisation de noyaux élastiques peuvent être mises à profit pour répondre à cette insuffisance. Nous présentons l'une de ces méthodes, KDTW, pour le noyau DTW, puis, autour d'une analyse en composantes principales non-linéaire (K-PCA), nous évaluons la capacité de quelques noyaux concurrents (élastiques v.s non élastiques, définis v.s. non définis) à séparer les catégories des données analysées tout en proposant une réduction dimensionnelle importante. Cette étude montre expérimentalement l'intérêt d'une régularisation de type KDTW.	Pierre-François Marteau	http://editions-rnti.fr/render_pdf.php?p1&p=1002063	http://editions-rnti.fr/render_pdf.php?p=1002063	Dans domaine fouiller série temporel travail récent exploiter noyau construit partir distance élastique typer Dynamic Time Warping dtw dapprocher baser noyau pourtant matrice apparenter matrice Gram construire partir noyau nont propriété requis pouvoir in fin impropre exploitation approche émergeant régularisation noyau élastique pouvoir mettre profit répondre insuffisance présenter lune méthode kdtw noyau DTW pouvoir autour dune analyser composante principal nonlinéair KPCA évaluer capacité noyau concurrent élastique vs élastiquer définir vs définir séparer catégorie donnée analyser proposer réduction dimensionnel important étude montr expérimentalement lintérêt dune régularisation typer KDTW
277	Revue des Nouvelles Technologies de l'Information	EGC	2015	Requêtes Skyline en présence des données évidentielles	Dans cet article, nous nous intéressons à la recherche des points les plus intéressants au sens de l'ordre de Pareto, dans les bases de données évidentielles. Nous présentons le modèle skyline évidentiel qui est adapté à la nature des données incertaines. Ensuite, nous présentons une évaluation expérimentale de notre approche.	Sayda Elmi, Karim Benouaret, Allel HadjAli, Mohamed Anis Bach Tobji, Boutheina Ben Yaghlane	http://editions-rnti.fr/render_pdf.php?p1&p=1002081	http://editions-rnti.fr/render_pdf.php?p=1002081	Dans article intéresser rechercher point plaire intéressant sens lordre Pareto dan base donnée évidentiell présenter modeler skyline évidentiel adapter nature donnée incertaine ensuite présenter évaluation expérimental approcher
278	Revue des Nouvelles Technologies de l'Information	EGC	2015	To initiate a corporate memory with a knowledge compendium: ten years of learning from experience with the Ardans method	Ardans method ArdansSas (2006b) and technology ArdansSas (2006a) of knowledge capitalization and structuration are used with different industries (automotive, aerospace, energy, defence, steel, health, etc.) for more than a decade in France and Europe.The proposed solutions in knowledge management and especially in expertise capitalisation have set a lot of feedback over time. With a view toward ongoing improvement, what are the impacts of these feedbacks on the method nowadays? Put into practice into the industry, the return of investment of a capitalization campaign is inferred from the quality of the knowledge base delivered at the end of the campaign. Therefore, the method and the technology are intrinsically connected. How IT tools can assist with the quality diagnosis of the knowledge base?A comparative study was conducted on the basis of the method Mariot et al. (2007) exposed at EGC'2007. This article sets out the results of the changes and improvements of the method, in conjunction with the latest technical and scientific development on the one hand, and the change of the industry needs on the other hand.	Vincent Besson, Alain Berger	http://editions-rnti.fr/render_pdf.php?p1&p=1002103	http://editions-rnti.fr/render_pdf.php?p=1002103	ardan method ardanssa 2006b and technology ardanssas 2006a of knowledge capitalization and structuration are used with industrier automotive aerospace energy defence steel health for more than decade in France and EuropeThe proposed solution in knowledge management and especially in expertiser capitalisation hav set lot of feedback over time With view toward ongoing improvement what are the impact of these feedback the method nowadays Put into practice into the industry the return of investment of capitalization campaign is inferred from the quality of the knowledge baser delivered at the end of the campaign Therefore the method and the technology are intrinsically connected How IT tool can assist with the quality diagnosis of the knowledge baseA comparatif study wa conducted the basis of the method Mariot al 2007 exposed at EGC2007 This article set out the results of the change and improvement of the method in conjunction with the latest technical and scientific development the one hand and the changer of the industry need the other hand
279	Revue des Nouvelles Technologies de l'Information	EGC	2015	Towards Linked Data Extraction From Tweets	Millions of Twitter users post messages every day to communicate with other users in real time information about events that occur in their environment. Most of the studies on the content of tweets have focused on the detection of emerging topics. However, to the best of our knowledge, no approach has been proposed to create a knowledge base and enrich it automatically with information coming from tweets. The solution that we propose is composed of four main phases: topic identification, tweets classification, automatic summarization and creation of an RDF triplestore. The proposed approach is implemented in a system covering the entire sequence of processing steps from the collection of tweets written in English language (based on both trusted and crowd sources) to the creation of an RDF dataset anchored in DBpedia's namespace.	Manel Achichi, Zohra Bellahsene, Dino Ienco, Konstantin Todorov	http://editions-rnti.fr/render_pdf.php?p1&p=1002100	http://editions-rnti.fr/render_pdf.php?p=1002100	million of twitter user post messag every day to communicate with other user in real tim information about event that occur in their environment Most of the studi the conter of tweet hav focused the detection of emerging topics However to the best of our knowledge no approach has been proposed to create knowledge baser and enrich it automatically with information coming from tweets The solution that we proposer is composed of four main phase topic identification tweet classification automatic summarization and creation of an RDF triplestore The proposed approach is implemented in system covering the entir sequence of processing steps from the collection of tweets written in English language based both trusted and crowd source to the creation of an RDF dataset anchored in DBpedias namespac
280	Revue des Nouvelles Technologies de l'Information	EGC	2015	Ultrametricity of Dissimilarity Spaces and Its Significance for Data Mining	Nous introduisons une mesure d'ultramétricité pour les dissimilaritées et examinons les transformations des dissimilaritées et leurs impact sur cette mesure. Ensuite, nous étudions l'influence de l'ultramétricité sur la comportement de deux classes d'algorithmes d'exploration de données (le kNN algorithme de classification et l'algorithme de regroupement PAM) appliqués sur les espaces de dissimilarité. On montre qu'il existe une variation inverse entre ultramétricité et la performance des classificateurs. Pour les clusters, une augmentation d'ultramétricité genere regroupements avec une meilleure séparation. Une diminution de la ultramétricité produit groupes plus compacts.	Dan Simovici, Rosanne Vetro, Kaixun Hua	http://editions-rnti.fr/render_pdf.php?p1&p=1002068	http://editions-rnti.fr/render_pdf.php?p=1002068	introduire mesurer dultramétriciter dissimilaritée examinon transformation dissimilaritée impact mesurer ensuite étudier linfluence lultramétricité comportement classe dalgorithm dexploration donnée knn algorithm classification lalgorithm regroupement PAM appliquer espace dissimilarité montr quil exister variation inverse entrer ultramétricité performance classificateur Pour cluster augmentation dultramétriciter genere regroupement meilleur séparation diminution ultramétricité produire grouper plaire compact
281	Revue des Nouvelles Technologies de l'Information	EGC	2015	Un algorithme EM pour une version parcimonieuse de l'analyse en composantes principales probabiliste	Nous considérons une version parcimonieuse de l'analyse en composantes principales probabiliste. La pénalité `1 imposée sur les composantes principales rend leur interprétation plus aisée en ne faisant dépendre ces dernières que d'un nombre restreint de variables initiales. Un algorithme EM, simple de mise en oeuvre, est proposé pour l'estimation des paramètres du modèle. La méthode de l'heuristique de pente est finalement utilisée pour choisir le coefficient de pénalisation.	Charles Bouveyron, Julien Jacques	http://editions-rnti.fr/render_pdf.php?p1&p=1002074	http://editions-rnti.fr/render_pdf.php?p=1002074	considérer version parcimonieux lanalyse composante principal probabiliste pénalité 1 imposer composante principal interprétation plaire aisé faire dépendre dernière dun nombre restreindre variable initial algorithme em simple miser oeuvrer proposer lestimation paramètre modeler méthode lheuristique pente finalement utiliser choisir coefficient pénalisation
282	Revue des Nouvelles Technologies de l'Information	EGC	2015	Un algorithme ICM basé sur la compacité pour la segmentation des images satellites à très haute résolution	"Dans cet article nous proposons une modification pour l'algorithme ""Iterated Conditional Modes"" (ICM) appliqué à la segmentation d'images à très haute résolution. Pour ce faire, nous introduisons un nouveau critère de convergence basé sur la compacité des clusters et qui repose sur une fonction d'énergie adaptée aux modèles de voisinages irréguliers de ce type d'images. Grâce à cette méthode, nos premières expériences ont montré que nous obtenons des résultats plus fiables en terme de convergence et de meilleure qualité qu'en utilisant l'énergie globale comme critère d'arrêt."	Jérémie Sublime, Younès Bennani, Antoine Cornuéjols	http://editions-rnti.fr/render_pdf.php?p1&p=1002078	http://editions-rnti.fr/render_pdf.php?p=1002078	Dans article proposer modification lalgorithme iterated Conditional Modes icm appliquer segmentation dimager résolution Pour faire introduire critère convergence baser compacité cluster reposer fonction dénergie adapté modèle voisinage irrégulier typer dimager grâce méthode expérienc montrer obtenir résultat plaire fiable terme convergence meilleur qualité quen utiliser lénergie global critèr darrêt
283	Revue des Nouvelles Technologies de l'Information	EGC	2015	Un langage d'interrogation à la SPARQL pour les graphes conceptuels	Cet article propose un langage générique d'interrogation pour le modèle des graphes conceptuels. D'abord, nous introduisons les graphes d'interrogation. Un graphe d'interrogation est utilisé pour exprimer un « ou » entre deux sous-graphes, ainsi qu'une « option » sur un sous-graphe optionnel. Ensuite, nous proposons quatre types de requêtes (interrogation, sélection, description et construction) en utilisant les graphes d'interrogation. Enfin, les réponses à ces requêtes sont calculées à partir d'une opération basée sur l'homomorphisme de graphe.	Marc Legeay, David Genest, Stéphane Loiseau	http://editions-rnti.fr/render_pdf.php?p1&p=1002083	http://editions-rnti.fr/render_pdf.php?p=1002083	article proposer langage générique dinterrogation modeler graphe conceptuel Dabord introduire graphe dinterrogation graph dinterrogation utiliser exprimer « » entrer sousgraphe quune « option » sousgraphe optionnel ensuite proposer type requêt interrogation sélection description construction utiliser graphe dinterrogation réponse requête calculer partir dune opération basé lhomomorphisme graphe
284	Revue des Nouvelles Technologies de l'Information	EGC	2015	Une approche centrée graine pour la détection de communautés dans les réseaux multiplexes	Nous nous intéressons dans ce travail au problème de détection de communautés dans les réseaux multiplexes. Le modèle de réseau multiplexe a été récemment introduit afin de faciliter la modélisation des réseaux multirelationnels, des réseaux dynamiques et/ou des réseaux attribués. Les approches existantes pour la détection de communautés dans ce genre de graphes sont, pour la plupart, basées sur des schémas d'agrégation de couches ou d'agrégation de partitions. Nous proposons ici une nouvelle approche centrée graine qui permet de prendre en compte directement la nature multi-couche d'un réseau multiplexe. Des expérimentations effectuées sur différents réseaux multiplexes montrent que notre approche surpasse les approches de l'état de l'art en termes de qualité des communautés identifiées.	Issam Falih, Manel Hmimida, Rushed Kanawati	http://editions-rnti.fr/render_pdf.php?p1&p=1002099	http://editions-rnti.fr/render_pdf.php?p=1002099	intéresser dan travail problème détection communauter dan réseau multiplex modeler réseau multiplex récemment introduire faciliter modélisation réseau multirelationnel réseau dynamique etou réseau attribuer approche existant détection communauter dan genre graphe baser schéma dagrégation couche dagrégation partition proposer approcher centré graine permettre prendre compter nature multicouche dun réseau multiplex expérimentation effectuer réseau multiplex montrer approcher surpasser approche létat lart terme qualité communauté identifier
285	Revue des Nouvelles Technologies de l'Information	EGC	2015	Une approche de visualisation analytique pour comparer les modèles de propagation dans les réseaux sociaux	Les modèles de propagation d'informations, d'influence et d'actions dans les réseaux sociaux sont nombreux et diversifiés rendant le choix de celui approprié à une situation donnée potentiellement difficile. La sélection d'un modèle pertinent pour une situation exige de pouvoir les comparer. Cette comparaison n'est possible qu'au prix d'une traduction des modèles dans un formalisme commun et indépendant de ceux-ci. Nous proposons l'utilisation de la réécriture de graphes afin d'exprimer les mécanismes de propagation sous la forme d'un ensemble de règles de transformation locales appliquées selon une stratégie donnée. Cette démarche prend tout son sens lorsque les modèles ainsi traduits sont étudiés et simulés à partir d'une plate-forme de visualisation analytique dédiée à la réécriture de graphe. Après avoir décrit les modèles et effectué différentes simulations, nous exhibons comment la plate-forme permet d'interagir avec ces formalismes, et comparer interactivement les traces d'exécution de chaque modèle grâce à diverses mesures soulignant leurs différences.	Jason Vallet, Bruno Pinaud, Guy Melançon	http://editions-rnti.fr/render_pdf.php?p1&p=1002098	http://editions-rnti.fr/render_pdf.php?p=1002098	modèle propagation dinformation dinfluence daction dan réseau social diversifié choix approprier situation donner potentiellement difficile sélection dun modeler pertiner situation exig pouvoir comparer comparaison nest quau prix dune traduction modèle dan formalisme commun indépender ceuxci proposer lutilisation réécriture graphe dexprimer mécanisme propagation sou former dun ensemble règle transformation local appliquer stratégie donner démarcher prendre sens modèle traduire étudier simuler partir dune plateforme visualisation analytique dédier réécriture graphe Après décrire modèle effectuer simulation exhiber plateforme permettre dinteragir formalisme comparer interactivemer trace dexécution modeler grâce mesure souligner différence
286	Revue des Nouvelles Technologies de l'Information	EGC	2015	Une nouvelle formalisation des changements ontologiques composés et complexes	L'évolution d'une ontologie est un processus indispensable dans son cycle de vie. Elle est exprimée et définie par des changements ontologiques de différents types : élémentaires, composés et complexes. Les changements complexes et composés sont très utiles dans le sens où ils aident l'utilisateur à adapter son ontologie sans se perdre dans les détails des changements élémentaires. Cependant, ils cachent derrière une formalisation sophistiquée puisqu'ils affectent, à la fois, plusieurs entités ontologiques et peuvent causer des inconsistances à l'ontologie évoluée. Pour adresser cette problématique, cet article présente une nouvelle formalisation des changements ontologiques composés et complexes basée sur les grammaires de graphes typés. Cette formalisation s'appuie sur l'approche algébrique Simple Pushout (SPO) de transformation de graphes et possède deux principaux avantages : (1) fournir une nouvelle formalisation permettant de contrôler les transformations de graphes et éviter les incohérences d'une manière a priori, (2) simplifier la définition des changements composés et complexes en réduisant le nombre de changements élémentaires nécessaires à leur application.	Mariem Mahfoudh, Laurent Thiry, Germain Forestier, Michel Hassenforder	http://editions-rnti.fr/render_pdf.php?p1&p=1002087	http://editions-rnti.fr/render_pdf.php?p=1002087	lévolution dune ontologie processus indispensable dan cycle vie exprimer définir changement ontologique type   élémentaire composer complexe changement complexe composer utile dan sens aider lutilisateur adapter ontologie perdre dan détail changement élémentaire cacher formalisation sophistiquer puisquils affecter entité ontologique pouvoir causer inconsistance lontologie évoluer Pour adresser problématique article présenter formalisation changement ontologique composer complexe baser grammaire graphe typer formalisation sappuie lapproche algébrique simple pushout SPO transformation graphe posséder principal avantage   1 fournir formalisation permettre chuter transformation graphe éviter incohérence dune manière priori 2 simplifier définition changement composer complexe réduire nombre changement élémentaire nécessaire application
287	Revue des Nouvelles Technologies de l'Information	EGC	2015	Une nouvelle méthode de Web Usage Mining basée sur une analyse sémiotique du comportement de navigation	L'objectif de nos travaux est de proposer une méthode d'analyse automatique du comportement des utilisateurs à des fins de prédiction de leur propension à réaliser une action suggérée. Nous proposons dans cet article une nouvelle méthode de Web Usage Mining basée sur une étude sémiotique des styles perceptifs, considérant l'expérience de l'utilisateur comme élément déterminant de sa réaction à une sollicitation. L'étude de ces styles nous a amené à définir de nouveaux indicateurs (des descripteurs sémiotiques) introduisant un niveau supplémentaire à l'approche sémantique d'annotation des sites. Nous proposons ensuite un modèle neuronal adapté au traitement de ces nouveaux indicateurs. Nous expliquerons en quoi le modèle proposé est le plus pertinent pour traiter ces informations.	Sandra Mellot, Tony Bourdier, Moez Baccouche	http://editions-rnti.fr/render_pdf.php?p1&p=1002090	http://editions-rnti.fr/render_pdf.php?p=1002090	Lobjectif travail proposer méthode danalyse automatique comportement utilisateur fin prédiction propension réaliser action suggéré proposer dan article méthode Web Usage Mining baser étude sémiotique style perceptif considérer lexpérience lutilisateur élément déterminer réaction sollicitation Létude style amener définir indicateur descripteur sémiotique introduire niveau supplémentaire lapproche sémantique dannotation site proposer ensuite modeler neuronal adapter traitement indicateur expliquer modeler proposer plaire pertinent traiter information
288	Revue des Nouvelles Technologies de l'Information	EGC	2015	Une Plateforme ETL parallèle et distribuée pour l'intégration de données massives	Nous nous intéressons, dans ce papier, à l'impact des données massives dans un environnement décisionnel et plus particulièrement sur la phase d'intégration des données. Dans ce contexte, nous avons développé une plateforme, baptisée P-ETL (Parallel-ETL), destinée à l'entreposage de données massives selon le paradigme MapReduce. P-ETL permet le paramétrage de processus ETL (workflow) et un paramétrage avancé relatif à l'environnement parallèle et distribué. Ce papier décrit la plateforme P-ETL en vue d'une démonstration. Face à des jeux de données allant de 244 * 106 à 7, 317 * 109 tuples, les expérimentations menées ont montré l'amélioration significative des performances de P-ETL lorsque la taille du cluster et le nombre des tâches parallèles augmentent.	Mahfoud Bala, Oussama Mokeddem, Omar Boussaid, Zaia Alimazighi	http://editions-rnti.fr/render_pdf.php?p1&p=1002109	http://editions-rnti.fr/render_pdf.php?p=1002109	intéresser dan papier limpact donnée massif dan environnement décisionnel plaire phase dintégration donnée Dans contexte développer plateforme baptiser petl paralleletl destiner lentreposage donnée massif paradigme MapReduce petl permettre paramétrage processus etl workflow paramétrage avancer relatif lenvironnement parallèle distribuer papier décrire plateforme petl dune démonstration fac jeu donnée aller 244   106 7 317   109 tupl expérimentation mener montrer lamélioration significatif performance petl tailler cluster nombre tâche parallèle augmenter
289	Revue des Nouvelles Technologies de l'Information	EGC	2015	Using Social Conversational Context For Detecting Users Interactions on Microblogging Sites	Dans ce travail, nous proposons une nouvelle méthode de détection des conversations sur les sites des réseaux sociaux. Cette méthode est basée sur l'analyse et l'enrichissement de contenu dans le but de présenter un résultat informatif basé sur les interactions des utilisateurs. Nous avons évalué notre méthode sur corpus recueillis de réseau social lié à des sujets spécifiques, et nous avons obtenu des bons résultats.	Rami BELKAROUI, Rim Faiz, Aymen Elkhlifi	http://editions-rnti.fr/render_pdf.php?p1&p=1002101	http://editions-rnti.fr/render_pdf.php?p=1002101	Dans travail proposer méthode détection conversation site réseau social méthode baser lanalyse lenrichissement contenir dan boire poster résultat informatif baser interaction utilisateur évaluer méthode corpu recueillir réseau social lier spécifique obtenir résultat
290	Revue des Nouvelles Technologies de l'Information	EGC	2015	Utilisation des pyramides pour visualiser la contamination des manuscrits	In this paper we present a new codicum stemma visualization method. Don Quentin's modeling is usec to classify the textual tradition.We supplement the genealogical editor's information of betweenness triplets obtained directly from the corpus. A pyramid depicting the family codicum stemma is then constructed on the basis of information obtained by the triplets	Marc Le Pouliquen	http://editions-rnti.fr/render_pdf.php?p1&p=1002115	http://editions-rnti.fr/render_pdf.php?p=1002115	in this paper we preser new codicum stemma visualization method Don quentin modeling is usec to classify the textual traditionwe supplemer the genealogical editors information of betweenness triplet obtained directly from the corpus pyramid depicting the family codicum stemma is then constructed the basis of information obtained by the triplet
291	Revue des Nouvelles Technologies de l'Information	EGC	2015	Vers la découverte de modèles exceptionnels locaux : des règles descriptives liant les molécules à leurs odeurs	Issue d'un phénomène complexe partant d'une molécule odorante jusqu'à la perception dans le cerveau, l'olfaction reste le sens le plus difficile à appréhender par les neuroscientifiques. L'enjeu principal est d'établir des règles sur les propriétés physicochimiques des molécules (poids, nombre d'atomes, etc.) afin de caractériser spécifiquement un sous-ensemble de qualités olfactives (fruité, boisé, etc.). On peut trouver de telles règles descriptives grâce à la découverte de sous-groupes (“subgroup discovery”). Cependant les méthodes existantes permettent de caractériser soit une seule qualité olfactive ; soit toutes les qualités olfactives à la fois (“exceptional model mining”) mais pas un sousensemble. Nous proposons alors une approche de découverte de sous-groupes caractéristiques de seulement certains labels, par une nouvelle technique d'énumération, issue de la fouille de redescriptions. Nous avons expérimenté notre méthode sur une base de données d'olfaction fournie par des neuroscientifiques et pu exhiber des premiers sous-groupes intelligibles et réalistes.	Guillaume Bosc, Mehdi Kaytoue, Marc Plantevit, Fabien De Marchi, Moustafa Bensafi, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1002091	http://editions-rnti.fr/render_pdf.php?p=1002091	issu dun phénomène complexe partir dune molécul odorant jusquà perception dan cerveau lolfaction rester sens plaire difficile appréhender neuroscientifique Lenjeu principal détablir règle propriété physicochimique molécule poids nombre datom caractériser spécifiquement sousensemble qualité olfactif fruiter boiser pouvoir trouver règle descriptif grâce découvrir sousgroupe “ subgroup discovery ” méthode existant permettre caractériser qualité olfactif   qualité olfactif “ exceptional model mining ” sousensemble proposer approcher découvrir sousgroupe caractéristiquer label technique dénumération issu fouiller redescription expérimenter méthode baser donnée dolfaction fournir neuroscientifique pouvoir exhiber sousgroup intelligible réaliste
292	Revue des Nouvelles Technologies de l'Information	EGC	2015	Visualizing Shooting Spots using Geo-tagged Photographs from Social Media Sites	Hotspots, à laquelle de nombreuses photographies ont été prises, pourraient être des lieux intéressants pour beaucoup de gens faire du tourisme. Visualisation des hotspots révèle les intérêts des utilisateurs, ce qui est important pour les industries telles que la recherche et du marketing touristiques. Bien que plusieurs techniques basées sociaux-pour hotspots extraction indépendamment ont été proposés, un hotspot a une relation à d'autres hotspots dans certains cas. Pour organiser ces hotspots, nous proposons une méthode pour détecter et de visualiser les relations entre les hotspots. Notre méthode proposée détecte et évalue les relations de taches de tir et sujets photographiques. Notre approche extrait les relations à l'aide de sous-hotspots, qui sont fendus d'un hotspot qui comprend des photographies de différents types.	Masaharu Hirota, Masaki Endo, Shohei Yokoyama, Hiroshi Ishikawa	http://editions-rnti.fr/render_pdf.php?p1&p=1002076	http://editions-rnti.fr/render_pdf.php?p=1002076	hotspot photographie prendre pouvoir lieu intéressant faire tourisme visualisation hotspot révéler intérêt utilisateur importer industrie rechercher marketing touristique technique baser sociauxpour hotspot extraction indépendamment proposer hotspot relation dautre hotspot dan cas Pour organiser hotspot proposer méthode détecter visualiser relation entrer hotspot méthode proposer détecter évaluer relation tache tir photographique approcher extraire relation laid soushotspot fendre dun hotspot comprendre photographie type
293	Revue des Nouvelles Technologies de l'Information	EGC	2015	XEWGraph : Outil de Visualisation et Analyse des Hypergraphes pour un Système d'Intelligence Economique	The Competitive Intelligence System Xplor EveryWhere helps searching, visualizing, and sharing useful data. In this paper, we will intorduce Xplor EveryWhere and its newest feature called XEWGraph, which is dedicated to the analysis of massive data and visualization of hypergraphs.	Zakaria Boulouard, Amine El Haddadi, Anass El Haddadi, Lahcen Koutti, Abdelhadi Fennan	http://editions-rnti.fr/render_pdf.php?p1&p=1002117	http://editions-rnti.fr/render_pdf.php?p=1002117	The Competitive Intelligence System Xplor EveryWhere help searching visualizing and sharing useful dater In this paper we will intorduce Xplor EveryWhere and it newest featur called XEWGraph which is dedicated to the analysis of massif dater and visualization of hypergraph
294	Revue des Nouvelles Technologies de l'Information	EGC	2014	1d-SAX : une nouvelle représentation symbolique pour les séries temporelles	SAX (Symbolic Aggregate approXimation) est une des techniquesmajeures de symbolisation des séries temporelles. La non prise en compte destendances dans la symbolisation est une limitation bien connue de SAX. Cet articleprésente 1d-SAX, une méthode pour représenter une série temporelle parune séquence de symboles contenant des informations sur la moyenne et la tendancedes fenêtres successives de la série segmentée. Nous comparons l'efficacitéde 1d-SAX vs SAX dans une tâche de classification de séries temporellesd'images satellites. Les résultats montrent que 1d-SAX améliore les taux de classificationpour une quantité d'information identique utilisée.	Simon Malinowski, Thomas Guyet, Rene Quiniou, Romain Tavenard	http://editions-rnti.fr/render_pdf.php?p1&p=1001930	http://editions-rnti.fr/render_pdf.php?p=1001930	sax Symbolic Aggregate approximation techniquesmajeure symbolisation série temporel pris compter destendanc dan symbolisation limitation connaître SAX articleprésente 1dSAX méthode représenter série temporel parune séquence symbole contenir information moyenner tendancede fenêtr successif série segmenter comparer lefficacitéde 1dSAX vs SAX dan tâcher classification série temporellesdimag satellit résultat montrer 1dSAX améliorer taux classificationpour quantité dinformation identique utiliser
295	Revue des Nouvelles Technologies de l'Information	EGC	2014	Agrégation de sac-de-sacs-de-mots pour la recherche d'information par modèles vectoriels	Cet article étudie l'intérêt de représenter les documents textuels nonplus comme des sacs-de-mots, mais comme des sacs-de-sacs-de-mots. Au coeurde l'utilisation de cette représentation, le calcul de similarité entre deux objetsnécessite alors d'agréger toutes les similarités entre sacs de chacun des objets.Nous évaluons cette représentation dans un cadre de recherche d'information,et étudions les propriétés attendues de ces fonctions d'agrégation. Les expériencesrapportées montrent l'intérêt de cette représentation lorsque les opérateursd'agrégation respectent certaines propriétés, avec des gains très importantspar rapport aux représentations standard.	Vincent Claveau	http://editions-rnti.fr/render_pdf.php?p1&p=1001925	http://editions-rnti.fr/render_pdf.php?p=1001925	article étudier lintérêt représenter document textuel nonplus sacsdemot sacsdesacsdemot coeurde lutilisation représentation calcul similarité entrer objetsnécessite dagréger similarité entrer sac objetsnou évaluon représentation dan cadrer rechercher dinformationet étudier propriété fonction dagrégation expériencesrapportée montrer lintérêt représentation opérateursdagrégation respecter propriété gain importantspar rapport représentation standard
296	Revue des Nouvelles Technologies de l'Information	EGC	2014	Alignement d'ontologies : exploitation des ontologies liées sur le web de données	Nous proposons dans cet article une méthode d'alignement d'une ontologiesource avec des ontologies cibles déjà publiées et liées sur le web dedonnées. Nous présentons ensuite un retour d'expérience sur l'alignement d'uneontologie dans le domaine des sciences du vivant et de l'environnement avecAGROVOC et NALT.	Thomas Hecht, Patrice Buche, Juliette Dibie-Barthélemy, Liliana Ibanescu, Cássia Trojahn dos Santos	http://editions-rnti.fr/render_pdf.php?p1&p=1001911	http://editions-rnti.fr/render_pdf.php?p=1001911	proposer dan article méthode dalignemer dune ontologiesource ontologie cibl déjà publier lier web dedonnées présenter ensuite dexpérience lalignement duneontologie dan domaine science vivre lenvironnement avecagrovoc nalt
297	Revue des Nouvelles Technologies de l'Information	EGC	2014	Annotation sémantique de documents administratifs	La numérisation de documents administratifs est un enjeu économiqueet écologique prioritaire dans le contexte sociétal actuel. La dématérialisationmassive de document n'est pas sans conséquence et soulève les problèmes d'organisation,de stockage et d'accès à l'information. Le défi n'est donc plus la numérisationdu document, mais l'extraction des informations qu'ils contiennent.Les documents sont produits par l'Homme et pour l'Homme. Cette propriétépermet de localiser des informations dans les zones saillantes du document (logos).La saillance et la reconnaissance sont deux éléments essentiels pour laclassification rapide de documents. A l'opposé, la recherche d'un document oud'un ensemble de documents repose presque toujours sur le texte brut, il estdonc nécessaire de faire une correspondance entre une requête textuelle et ledocument. Cet article présente une nouvelle approche d'annotation automatiquede documents administratifs qui utilise une approche visuel et une approche defouille de texte.	Benjamin Duthil, Mickaël Coustaty, Vincent Courboulay, Jean-Marc Ogier	http://editions-rnti.fr/render_pdf.php?p1&p=1001913	http://editions-rnti.fr/render_pdf.php?p=1001913	numérisation document administratif enjeu économiqueet écologique prioritaire dan contexte sociétal actuel dématérialisationmassive document nest conséquence soulever problème dorganisationd stockag daccè linformation défi nest plaire numérisationdu document lextraction information quils contiennentles document produire lhomme lhomme propriétépermet localiser information dan zone saillant document logosLa saillance reconnaissance élément essentiel laclassification rapide document lopposer rechercher dun document oudun ensemble document reposer presque texte brut estdonc nécessaire faire correspondance entrer requête textuel ledocumer article présenter approcher dannotation automatiquede document administratif utiliser approcher visuel approcher defouill texte
298	Revue des Nouvelles Technologies de l'Information	EGC	2014	Application du paradigme MapReduce aux données ouvertes Cas : Accessibilité des personnes à mobilité réduite aux musées	Le modèle MapReduce est aujourd'hui l'un des modèles de programmationparallèle les plus utilisés. Définissant une architecture Maître-Esclave,il permet le traitement parallèle de grandes masses de données. Dans ce papier,nous proposons un algorithme basé sur MapReduce qui permet, à partir des donnéespubliques du Ministère Français de la Communication et de la Culture, dedéfinir un classement des galeries et musées nationaux selon leurs degré d'accessibilitéaux personnes handicapées. Tout en profitant de la puissance et de laflexibilité du paradigme MapReduce, les décideurs pourront mettre en place desstratégies efficaces à moindre coût et avoir ainsi une vision plus précise sur lesétablissements culturels et leurs limites relatives à cette catégorie de personnes.L'algorithme que nous proposons peut être exploité et appliqué à d'autres casd'études avec des jeux de données plus volumineux.	Billel Arres, Nadia Kabachi, Fadila Bentayeb, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1001963	http://editions-rnti.fr/render_pdf.php?p=1001963	modeler MapReduce aujourdhui lun modèle programmationparallèle plaire utiliser définir architecturer MaîtreEsclaveil permettre traitement parallèle grand mass donnée Dans papiernous proposer algorithme baser MapReduce permettre partir donnéespublique ministère français communication culture dedéfinir classement galerie muser national degré daccessibilitéaux handicapé profiter puissance laflexibilité paradigme MapReduce décideur pouvoir mettre placer desstratégier efficace moindre coût vision plaire préciser lesétablissement culturel limite relatif catégorie personneslalgorithme proposer pouvoir exploiter appliquer dautre casdétuder jeu donnée plaire volumineux
299	Revue des Nouvelles Technologies de l'Information	EGC	2014	Apprentissage de fonctions de tri pour la prédiction d'interactions protéine-ARN	Les fonctions biologiques dans la cellule mettent en jeu des interactions3D entre protéines et ARN. Les avancées des techniques exérimentalesrestent insuffisantes pour de nombreuse applications. Il faut alors pouvoir prédirein silico les interactions protéine-ARN. Dans ce contexte, nos travaux sontfocalisés sur la construction de fonctions de score permettant d'ordonner les solutionsgénérées par le programme d'amarrage protéine-ARN RosettaDock. Laméthodologie d'évaluation utilisée par RosettaDock impose de trouver une fonctionde score s'exprimant comme une combinaison linéaire de mesures physicochimiques.Avec une approche d'apprentissage supervisé par algorithme génétique,nous avons appris différentes fonctions de score en imposant descontraintes sur la nature des poids recherchés. Les résultats obtenus montrentl'importance de la signification des poids à apprendre et de l'espace de rechercheassocié.	Adrien Guilhot-Gaudeffroy, Jérôme Azé, Julie Bernauer, Christine Froidevaux	http://editions-rnti.fr/render_pdf.php?p1&p=1001960	http://editions-rnti.fr/render_pdf.php?p=1001960	fonction biologique dan cellule mettre jeu interactions3d entrer protéine arn avancée technique exérimentalesrestent insuffisant application falloir pouvoir prédirein silico interaction protéineARN Dans contexte travail sontfocaliser construction fonction scor permettre dordonner solutionsgénérée programmer damarrage protéineARN rosettadock Laméthodologie dévaluation utiliser rosettadock impos trouver fonctionde scor sexprimer combinaison linéaire mesure physicochimiquesAvec approcher dapprentissage superviser algorithme génétiquenous apprendre fonction score imposer descontrainte nature poids recherché résultat obtenu montrentlimportance signification poids lespace rechercheassocié
300	Revue des Nouvelles Technologies de l'Information	EGC	2014	Apprentissage incrémental anytime d'un classifieur Bayésien naïf pondéré	Nous considérons le problème de classification supervisée pour desflux de données présentant éventuellement un très grand nombre de variablesexplicatives. Le classifieur Bayésien naïf se révèle alors simple à calculer etrelativement performant tant que l'hypothèse restrictive d'indépendance des variablesconditionnellement à la classe est respectée. La sélection de variables etle moyennage de modèles sont deux voies connues d'amélioration qui reviennentà déployer un prédicteur Bayésien naïf intégrant une pondération des variablesexplicatives. Dans cet article, nous nous intéressons à l'estimation directe d'untel modèle Bayésien naïf pondéré. Nous proposons une régularisation parcimonieusede la log-vraisemblance du modèle prenant en compte l'informativité dechaque variable. La log-vraisemblance régularisée obtenue étant non convexe,nous proposons un algorithme de gradient en ligne qui post-optimise la solutionobtenue afin de déjouer les minima locaux. Les expérimentations menéess'intéressent d'une part à la qualité de l'optimisation obtenue et d'autre part auxperformances du classifieur en fonction du paramétrage de la régularisation.	Carine Hue, Marc Boullé, Vincent Lemaire	http://editions-rnti.fr/render_pdf.php?p1&p=1001939	http://editions-rnti.fr/render_pdf.php?p=1001939	considérer problème classification superviser desflux donnée présenter éventuellemer grand nombre variablesexplicative classifieur bayésien naïf révéler simple calculer etrelativement performer lhypothèse dindépendance variablesconditionnellement classer respecter sélection variable etle moyennage modèle connu damélioration reviennentà déployer prédicteur bayésien naïf intégrer pondération variablesexplicative Dans article intéresser lestimation direct duntel modeler Bayésien naïf pondérer proposer régularisation parcimonieusede logvraisemblance modeler prendre compter linformativité dechaqu variable logvraisemblance régulariser obtenir convexenou proposer algorithme gradient ligne postoptimise solutionobtenue déjouer minimum local expérimentation menéessintéressent dune partir qualité loptimisation obtenir dautre partir auxperformance classifieur fonction paramétrage régularisation
301	Revue des Nouvelles Technologies de l'Information	EGC	2014	Apprentissage non supervisé de dépendances syntaxiques à partir de texte étiqueté, plusieurs variantes de PCFG légères	L'apprentissage de dépendances est une tâche consistant à établir, àpartir des phrases d'un texte, un modèle de construction d'arbres traduisant unehiérarchie syntaxique entre les mots. Nous proposons un modèle intermédiaireentre l'analyse syntaxique complète de la phrase et les sacs de mots. Il est basésur une grammaire stochastique hors-contexte se traduisant par des relations dedépendance entre les catégories grammaticales d'une phrase. Les résultats expérimentauxobtenus sur des benchmarks attestés dépassent pour cinq langues surdix les scores de l'algorithme de référence DMV, et pour la première fois desscores sont obtenus pour le français. La très grande simplicité de la grammairepermet un apprentissage très rapide, et une analyse presque instantanée.	Marie Arcadias, Guillaume Cleuziou, Edmond Lassalle, Christel Vrain	http://editions-rnti.fr/render_pdf.php?p1&p=1001924	http://editions-rnti.fr/render_pdf.php?p=1001924	lapprentissage dépendance tâcher consister établir àpartir phrase dun texte modeler construction darbr traduire unehiérarchi syntaxique entrer proposer modeler intermédiaireentr lanalyse syntaxique complet phraser sac basésur grammaire stochastique horscontexte traduire relation dedépendance entrer catégorie grammatical dune phraser résultat expérimentauxobtenus benchmark attester dépasser langu surdix score lalgorithme référence DMV desscor obtenir français grand simplicité grammairepermet apprentissage rapide analyser presque instantané
302	Revue des Nouvelles Technologies de l'Information	EGC	2014	Approche formelle de fusion d'ontologies à l'aide des grammaires de graphes typés	L'article propose une approche formelle de fusion d'ontologies se reposantsur les grammaires de graphes typés. Elle se décompose en trois étapes :1) la recherche de similarités entre concepts ; 2) la fusion des ontologies parl'approche algébrique SPO (Simple Push Out) ; 3) l'adaptation d'une ontologieglobale par le biais de règles de réécriture de graphes. Contrairement aux solutionsexistantes, cette méthode offre une représentation formelle de la fusiond'ontologies ainsi qu'une implémentation fonctionnelle basée sur l'outil AGG.	Mariem Mahfoudh, Germain Forestier, Laurent Thiry, Michel Hassenforder	http://editions-rnti.fr/render_pdf.php?p1&p=1001980	http://editions-rnti.fr/render_pdf.php?p=1001980	Larticle proposer approcher formel fusion dontologier reposantsur grammaire graphe typer décomposer étape 1 rechercher similarité entrer concept   2 fusion ontologie parlapproche algébrique SPO simple Push Out   3 ladaptation dune ontologieglobal biais règle réécriture graphe contrairement solutionsexistant méthode offrir représentation formel fusiondontologie quune implémentation fonctionnel baser loutil AGG
303	Revue des Nouvelles Technologies de l'Information	EGC	2014	Approche par motifs pour l'analyse de données multi-résolution	Dans cet article nous nous intéressons aux approches pour l'analysede graphes pouvant évoluer dans le temps et tel qu'un sommet à un temps donnépeut correspondre à plusieurs sommets au temps suivant et où les sommets sontassociés à un ensemble d'attributs catégoriels. Dans ce type de données, nousproposons une nouvelle classe de motifs basée sur des contraintes permettant dedécrire l'évolution de structures homogènes. Ce type d'approche est particulièrementadaptée pour l'analyse d'images multi-résolution sans perte d'information.Nous présentons un résultat qualitatif dans ce domaine.	Pierre-Nicolas Mougel, Frédéric Flouvat, Nazha Selmaoui-Folcher	http://editions-rnti.fr/render_pdf.php?p1&p=1001952	http://editions-rnti.fr/render_pdf.php?p=1001952	Dans article intéresser approche lanalysede graphe pouvoir évoluer dan temps quun sommet temps donnépeut correspondre sommet temps sommet sontassocier ensemble dattribut catégoriel Dans typer donnée nousproposon classer motif baser contrainte permettre dedécrir lévolution structure homogène typer dapproch particulièrementadapter lanalyse dimage multirésolution perte dinformationNous présenton résultat qualitatif dan domaine
304	Revue des Nouvelles Technologies de l'Information	EGC	2014	Automatic correction of SVM for drifted data classification	Concept drift is an important feature of real-world data streams thatcan make usual machine learning techniques rapidly become unsuitable. Thispaper addresses the problem of sudden concept drift in classification problemsfor which standard techniques may fail. To this end, support vector machines(SVMs) are automatically corrected to cope with a new suddenly drifted dataset.Results on real-world datasets with several types of sudden drift indicate that themethod is able to correct the SVM in order to better classify the new data afterthe concept drift, using a correction based on the difference between the initialdataset and the new drifted dataset, even when the new dataset is small.	Alexandra Degeest, Benoît Frénay, Michel Verleysen	http://editions-rnti.fr/render_pdf.php?p1&p=1001942	http://editions-rnti.fr/render_pdf.php?p=1001942	Concept drift is an importer feature of realworld dater streams thatcan mak usual machiner learning technique rapidly become unsuitabl thispaper addresse the problem of sudden concept drift in classification problemsfor which standard technique may fail To thi end support vector machinessvm are automatically corrected to cope with new suddenly drifted datasetresults realworld datasets with several typer of sudden drift indicat that themethod is abl to correct the SVM in order to better classify the new dater afterthe concept drift using correction based the difference between the initialdataset and the new drifted dataset even when the new dataset is small
305	Revue des Nouvelles Technologies de l'Information	EGC	2014	Broad Data: What happens when the Web of Data becomes real?	“Big Data” is used to refer to the very large datasets generated by scientists, to the manypetabytes of data held by companies like Facebook and Google, and to analyzing real-time dataassets like the stream of twitter messages emerging from events around the world. Key areasof interest include technologies to manage much larger datasets (cf. NoSQL), technologies for the visualization and analysis of databases, cloud-based data management and dataminingalgorithms.Recently, however, we have begun to see the emergence of another, and equally compellingdata challenge – that of the “Broad data” that emerges from millions and millions of rawdatasets available on the World Wide Web. For broad data the new challenges that emerge includeWeb-scale data search and discovery, rapid and potentially ad hoc integration of datasets,visualization and analysis of only-partially modeled datasets, and issues relating to the policiesfor data use, reuse and combination. In this talk, we present the broad data challenge anddiscuss potential starting points for solutions. We illustrate these approaches using data froma “meta-catalog” of over 1,000,000 open datasets that have been collected from about twohundred governments from around the world.	Jim Hendler	http://editions-rnti.fr/render_pdf.php?p1&p=1001905	http://editions-rnti.fr/render_pdf.php?p=1001905	“ Big Data ” is used to refer to the very large dataset generated by scientists to the manypetabyt of dater held by companie like Facebook and Google and to analyzing realtime dataasset like the stream of twitter messag emerging from event around the world Key areasof interest includ technologi to manager much larger dataset cf NoSQL technologi for the visualization and analysis of databas cloudbased dater management and dataminingalgorithmsRecently however we hav begun to see the emergence of another and equally compellingdater challenge – that of the “ Broad dater ” that emerg from million and million of rawdataset availabl the World Wide Web For broad dater the new challenge that emerg includewebscal dater search and discovery rapid and potentially ad hoc integration of datasetsvisualization and analysis of onlypartially modeled dataset and issu relating to the policiesfor dater us reus and combination in this talk we preser the broad dater challeng anddiscus potential starting point for solution illustrate these approacher using dater froma “ metacatalog ” of over 1000000 open dataset that hav been collected from about twohundred government from around the world
306	Revue des Nouvelles Technologies de l'Information	EGC	2014	Classification des actions humaines basée sur les descripteurs spatio-temporels	Dans cet article, nous proposons un nouveau descripteurspatio-temporel appelé ST-SURF pour l'analyse et la reconnaissance d'actionsdans des flux vidéo. L'idée principale est d'enrichir le descripteur Speed UpRobust Feature (SURF) en intégrant l'information de mouvement issue du flotoptique. Seuls les points d'intérêts qui ont subi un déplacement sont pris encompte pour générer un dictionnaire de mots visuels (DMV) robuste basé surl'algorithme des k-moyennes (K-means). Le dictionnaire est utilisé lors du processusd'apprentissage et de reconnaissance d'actions basé sur la méthode desmachines à vecteurs supports (SVM). Les résultats obtenus confirment l'intérêtdu descripteur proposé ST-SURF pour l'analyse de scènes et en particulierpour la reconnaissance d'actions. La méthode atteind une précision de reconnaissancede l'ordre de 80.7%, équivalente aux performances des descripteursspatio-temporels de l'état de l'art.	Sameh Megrhi, Azeddine Beghdadi, Wided Souidène	http://editions-rnti.fr/render_pdf.php?p1&p=1001945	http://editions-rnti.fr/render_pdf.php?p=1001945	Dans article proposer descripteurspatiotemporel appeler stsurf lanalyse reconnaissance dactionsdans flux vidéo Lidée principal denrichir descripteur speed UpRobust feature surf intégrer linformation mouvement issu flotoptiqu point dintérêts subir déplacement prendre encompte générer dictionnaire visuel dmv robuste baser surlalgorithme kmoyenne Kmeans dictionnaire utiliser processusdapprentissage reconnaissance dactions baser méthode desmachiner vecteur support svm résultat obtenir confirmer lintérêtdu descripteur proposer stsurf lanalyse scène particulierpour reconnaissance dacter méthode atteind précision reconnaissancede lordre 807 équivalent performance descripteursspatiotemporel létat lart
307	Revue des Nouvelles Technologies de l'Information	EGC	2014	Classification et prédiction du flux solaire	La prédiction du rayonnement solaire horaire dans une journée estun enjeu primordial pour la production d'énergie de type photovoltaïque. Nousprésentons deux stratégies de classification des jours selon leurs rayonnementssolaires puis une méthode de prédiction du flux solaire cohérente avec la classification.	Henri Ralambondrainy, Yves Lechevallier, Jean Daniel Lan-Sun-Luk, Jean-Pierre Chabriat	http://editions-rnti.fr/render_pdf.php?p1&p=1001962	http://editions-rnti.fr/render_pdf.php?p=1001962	prédiction rayonnement solaire horaire dan journée estun enjeu primordial production dénergie typer photovoltaïque nousprésenton stratégie classification jour rayonnementssolaire pouvoir méthode prédiction flux solaire cohérent classification
308	Revue des Nouvelles Technologies de l'Information	EGC	2014	Classifieur naïf de Bayes pondéré pour flux de données	Un classifieur naïf de Bayes est un classifieur probabiliste basé surl'application du théorème de Bayes avec l'hypothèse naïve, c'est-à-dire que lesvariables explicatives (Xi) sont supposées indépendantes conditionnellement àla variable cible (C). Malgré cette hypothèse forte, ce classifieur s'est avéré trèsefficace sur de nombreuses applications réelles et est souvent utilisé sur les fluxde données pour la classification supervisée. Le classifieur naïf de Bayes nécessitesimplement en entrée l'estimation des probabilités conditionnelles parvariable P(Xi|C) et les probabilités a priori P(C). Pour une utilisation sur lesflux de données, cette estimation peut être fournie à l'aide d'un « résumé superviséen-ligne de quantiles ». L'état de l'art montre que le classifieur naïf de Bayespeut être amélioré en utilisant une méthode de sélection ou de pondération desvariables explicatives. La plupart de ces méthodes ne peuvent fonctionner quehors-ligne car elles nécessitent de stocker toutes les données en mémoire et/oude lire plus d'une fois chaque exemple. Par conséquent, elles ne peuvent être utiliséessur les flux de données. Cet article présente une nouvelle méthode baséesur un modèle graphique qui calcule les poids des variables d'entrée en utilisantune estimation stochastique. La méthode est incrémentale et produit un classifieurNaïf de Bayes Pondéré pour flux de données. Cette méthode est comparéeau classique classifieur naïf de Bayes sur les données utilisées lors du challenge« Large Scale Learning ».	Christophe Salperwyck, Vincent Lemaire, Carine Hue	http://editions-rnti.fr/render_pdf.php?p1&p=1001938	http://editions-rnti.fr/render_pdf.php?p=1001938	classifieur naïf Bayes classifieur probabiliste baser surlapplication théorème Bayes lhypothèse naïf cestàdir lesvariable explicatif Xi supposer indépendant conditionnellemer àla variable cibl Malgré hypothèse fort classifieur sest avérer trèsefficac application réel utiliser fluxde donner classification superviser classifieur naïf Bayes nécessitesimplement entrer lestimation probabilité conditionnel parvariabl pxic probabilité priori PC Pour utilisation lesflux donnée estimation pouvoir fournir laid dun « résumer superviséenligne quantil » létat lart montr classifieur naïf Bayespeut améliorer utiliser méthode sélection pondération desvariabl explicatif méthode pouvoir fonctionner quehorsligne nécessiter stocker donnée mémoire etoude lire plaire dune exemple Par conséquent pouvoir utiliséessur flux donnée article présenter méthode baséesur modeler graphique calculer poids variable dentrer utilisantune estimation stochastique méthode incrémental produire classifieurnaïf Bayes Pondéré flux donnée méthode comparéeau classique classifieur naïf Bayes donnée utiliser challenge « Large Scale Learning »
309	Revue des Nouvelles Technologies de l'Information	EGC	2014	Clustering de données relationnelles pour la structuration de flux télévisuels	Les approches existantes pour structurer automatiquement un flux detélévision (i.e. reconstituer un guide de programme exact et complet), sont supervisées.Elles requièrent de grandes quantités de données annotées manuellement,et aussi de définir a priori les types d'émissions (publicités, bandes annonces,programmes, sponsors...). Pour éviter ces deux contraintes, nous proposonsune classification non supervisée. La nature multi-relationnelle de nosdonnées proscrit l'utilisation des techniques de clustering habituelles reposantsur des représentations sous forme attributs-valeurs. Nous proposons et validonsexpérimentalement une technique de clustering capable de manipuler ces donnéesen détournant la programmation logique inductive (PLI) pour fonctionnerdans ce cadre non supervisé.	Vincent Claveau, Patrick Gros	http://editions-rnti.fr/render_pdf.php?p1&p=1001934	http://editions-rnti.fr/render_pdf.php?p=1001934	approche existant structurer automatiquement flux detélévision ie reconstituer guider programmer exact complet superviséeseller requérir grand quantité donnée annoter manuellementet définir priori type démission publiciter bande annoncesprogramm sponsor Pour éviter contrainte proposonsune classification superviser nature multirelationnell nosdonnée proscrire lutilisation technique clustering habituel reposantsur représentation sou former attributsvaleur proposer validonsexpérimentalemer technique clustering capable manipuler donnéesen détourner programmation logique inductif pli fonctionnerdan cadrer superviser
310	Revue des Nouvelles Technologies de l'Information	EGC	2014	Clustering de séquences d'évènements temporels	Nous proposons une nouvelle méthode de clustering et d'analyse deséquences temporelles basée sur les modèles en grille à trois dimensions. Lesséquences sont partitionnées en clusters, la dimension temporelle est discrétiséeen intervalles et la dimension évènement est partitionnée en groupes. La grille decellules 3D forme ainsi un estimateur non-paramétrique constant par morceauxde densité jointe des séquences et des dimensions des évènements temporels.Les séquences d'un cluster sont ainsi groupés car elles suivent une distributionsimilaire d'évènements au cours du temps. Nous proposons aussi une méthoded'exploitation du clustering par simplification de la grille ainsi que des indicateurspermettant d'interpréter les clusters et de caractériser les séquences quiles composent. Les expériences sur des données artificielles ainsi que sur desdonnées réelles issues de DBLP démontrent le bien-fondé de notre approche.	Romain Guigourès, Dominique Gay, Marc Boullé, Fabrice Clérot	http://editions-rnti.fr/render_pdf.php?p1&p=1001929	http://editions-rnti.fr/render_pdf.php?p=1001929	proposer méthode clustering danalyse deséquenc temporel baser modèle griller dimension lesséquenc partitionner cluster dimension temporel discrétiséeen intervalle dimension évènemer partitionner groupe griller decellul 3D former estimateur nonparamétriqu constant morceauxd densité joindre séquence dimension évènement temporelsl séquence dun cluster grouper distributionsimilair dévènement cours temps proposer méthodedexploitation clustering simplification griller indicateurspermettant dinterpréter cluster caractériser séquence quil composer expérience donnée artificiel desdonnée réel issu dblp démontrer bienfondé approcher
311	Revue des Nouvelles Technologies de l'Information	EGC	2014	Clusters dans les réseaux sociaux : intersections entre liens conceptuels fréquents et communautés	La recherche de liens conceptuels fréquents (FCL) est une nouvelleapproche de clustering de réseaux, qui exploite à la fois la structure et les attributsdes noeuds. Bien que les travaux récents se soient déjà intéressés à l'optimisationdes algorithmes de recherche des FCL, peu de travaux sont aujourd'huimenés sur la complémentarité qui existe entre les liens conceptuels et l'approcheclassique de clustering qui consiste en l'extraction de communautés. Ainsi dansce papier, nous nous intéressons à ces deux approches. Notre objectif est d'évaluerles relations potentiellement existantes entre les communautés et les FCLpour comprendre la façon dont les motifs obtenus par chacune des méthodespeuvent correspondre ou s'intersecter ainsi que la connaissance utile résultantde la prise en compte de ces deux types de connaissance. Nous proposons pourcela un ensemble de mesures originales, basées sur la notion d'homogénéité, visantà évaluer le niveau d'intersection des FCL et des communautés lorsqu'ilssont extraits d'un même jeu de données. Notre approche est appliquée à deuxréseaux et démontre l'importance de considérer simultanément plusieurs typesde connaissance et leur intersection.	Erick Stattner, Martine Collard	http://editions-rnti.fr/render_pdf.php?p1&p=1001920	http://editions-rnti.fr/render_pdf.php?p=1001920	rechercher lien conceptuel fréquent FCL nouvelleapproche clustering réseau exploiter structurer attributsde noeud travail récent déjà intéresser loptimisationde algorithme rechercher FCL travail aujourdhuimener complémentarité exister entrer lien conceptuel lapprocheclassiqu clustering consister lextraction communauté dansce papier intéresser approche objectif dévaluerl relation potentiellement existant entrer communauté FCLpour comprendre motif obtenir méthodespeuvent correspondr sintersecter connaissance utile résultantde priser compter type connaissance proposer pourcela ensemble mesure original baser notion dhomogénéité visantà évaluer niveau dintersection FCL communauté lorsquilssont extraire dun jeu donnée approcher appliquer deuxréseaux démontrer limportanc considérer simultanément typesde connaissance intersection
312	Revue des Nouvelles Technologies de l'Information	EGC	2014	Comment Devenir Cybercondriaque ?	"Nous avons tous déjà eu l'occasion d'effectuer des recherches d'ordremédical sur Internet. Si certains sites spécialisés se refusent à tout diagnosticen ligne, préférant le renvoi vers des professionnels de santé, d'autres en revancheconduisent souvent à des déclarations alarmistes faisant état de situationshumaines difficiles. Dans ce travail, nous étudions l'ampleur de ce phénomèneet montrons que quel que soit le syndrome recherché, les résultats obtenusconduisent toujours à l'énoncé des mots ""cancer"" ou ""tumeur""."	Erick Stattner, Martine Collard	http://editions-rnti.fr/render_pdf.php?p1&p=1001986	http://editions-rnti.fr/render_pdf.php?p=1001986	tou déjà loccasion deffectuer recherche dordremédical Internet Si site spécialiser refuser diagnosticen ligne préférer renvoi ver professionnel santé dautr revancheconduiser déclaration alarmiste faire situationshumaine difficile Dans travail étudier lampleur phénomèneet montron syndrome rechercher résultat obtenusconduiser lénoncer cancer tumeur
313	Revue des Nouvelles Technologies de l'Information	EGC	2014	Comparaison de bornes théoriques pour l'accélération du clustering incrémental en une passe	Le clustering incrémental en une passe repose sur l'affectation efficacede chaque nouveau point aux clusters existants. Dans le cas général, où lesclusters ne peuvent être représentés par une moyenne, la détermination exhaustivedu cluster le plus proche possède une complexité quadratique avec le nombrede données. Nous proposons dans ce papier une nouvelle méthode d'affectationstochastique à chaque cluster qui minimise le nombre de comparaisons à effectuerentre la donnée et chaque cluster pour garantir, étant donné un taux d'erreuracceptable, l'affectation au cluster le plus proche. Plusieurs bornes théoriques(Bernstein, Hoeffding et Student) sont comparées dans ce papier. Les résultatssur des données artificielles et réelles montrent que la borne de Bernstein donneglobalement les meilleurs résultats (notamment lorsqu'elle est réduite) car ellepermet une accélération forte du processus de clustering, tout en conservant unnombre très faible d'erreurs.	Nicolas Labroche, Marcin Detyniecki, Thomas Baerecke	http://editions-rnti.fr/render_pdf.php?p1&p=1001959	http://editions-rnti.fr/render_pdf.php?p=1001959	clustering incrémental passer reposer laffectation efficacede poindre cluster existant Dans cas général lescluster pouvoir représenter moyenner détermination exhaustivedu cluster plaire posséder complexité quadratique nombrede donner proposer dan papier méthode daffectationstochastiqu cluster minimiser nombre comparaison effectuerentre donner cluster garantir donner taux derreuracceptabl laffectation cluster plaire borne théoriquesBernstein Hoeffding Student comparer dan papier résultatssur donnée artificiel réel montrer borner Bernstein donneglobalemer meilleur résultat lorsquelle réduire ellepermet accélération fort processus clustering conserver unnombre faible derreurs
314	Revue des Nouvelles Technologies de l'Information	EGC	2014	Comparaison des chemins de Hilbert adaptatif et des graphes de voisinage pour la caractérisation d'un parcellaire agricole	Cet article compare deux représentations de données spatiales, lesgraphes de voisinages et les chemins de Hilbert-Peano, utilisées par des algorithmesde fouille. Cette comparaison s'appuie sur la mise en oeuvre d'une méthoded'énumération de « sacs de noeuds », qui permet d'obtenir des caractérisationshomogènes à partir des deux représentations. La méthode est appliquée àla caractérisation de parcellaires agricoles et les résultats tendent à montrer quela linéarisation de l'espace capte la majorité de l'information, à l'exception deséléments rares, sur cet exemple particulier.	Thomas Guyet, Sébastien Da Silva, Claire Lavigne, Florence Le Ber	http://editions-rnti.fr/render_pdf.php?p1&p=1001974	http://editions-rnti.fr/render_pdf.php?p=1001974	article comparer représentation donnée spatial lesgraphe voisinage chemin HilbertPeano utiliser algorithmesde fouiller comparaison sappuie miser oeuvrer dune méthodedénumération « sac noeud » permettre dobtenir caractérisationshomogène partir représentation méthode appliquer àla caractérisation parcellaire agricole résultat tendre montrer quela linéarisation lespace capter majorité linformation lexception desélément exemple
315	Revue des Nouvelles Technologies de l'Information	EGC	2014	Compréhension de recettes de cuisine utilisateurs par extraction de connaissances intrinsèques	Sur les sites Web communautaires, les utilisateurs échangent des connaissances,en étant à la fois auteurs et lecteurs. Nous présentons une méthodepour construire notre propre compréhension de la sémantique de la communauté,sans recours à une base de connaissances externe. Nous effectuons une extractionde la connaissance présente dans les contributions analysées. Nous proposonsune évaluation de la confiance imputable à cette compréhension déduite,afin d'évaluer la qualité du contenu, avec application à un site Web de partagede recettes de cuisine.	Damien Leprovost, Thierry Despeyroux, Yves Lechevallier	http://editions-rnti.fr/render_pdf.php?p1&p=1001984	http://editions-rnti.fr/render_pdf.php?p=1001984	Sur site web communautaire utilisateur échanger connaissancesen auteur lecteur présenter méthodepour construire propre compréhension sémantique communautésan recours baser connaissance externe effectuer extractionde connaissance présent dan contribution analyser proposonsune évaluation confiance imputable compréhension déduiteafin dévaluer qualité contenir application site web partagede recetter cuisiner
316	Revue des Nouvelles Technologies de l'Information	EGC	2014	Construction de cube OLAP à partir d'un entrepôt de données orienté colonnes	L'optimisation de la construction de cubes OLAP 1 a été jusqu'à présentaxée sur le développement d'algorithmes de calcul performants. Ces derniersopèrent sur des données extraites de l'entrepôt de données qui est généralementimplémenté selon le modèle relationnel qui adopte l'architecture orientéelignes. Or, pour les requêtes décisionnelles, l'architecture orientée colonnes offrede meilleures performances. Cependant, les SGBDR 2 selon cette architecture nedisposent pas d'opérateurs appropriés pour le calcul de cube OLAP. Nous proposonsdans cet article une nouvelle méthode de calcul de cube OLAP. Les résultatsobtenus à partir des expérimentations que nous avons menées démontrentque notre approche optimise considérablement le temps de construction de cubeOLAP et réduit le temps de réponse relatif à l'exploitation du cube comparé àl'approche orientée lignes.	Khaled Dehdou, Fadila Bentayeb, Nadia Kabachi, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1001970	http://editions-rnti.fr/render_pdf.php?p=1001970	loptimisation construction cuber OLAP 1 jusquà présentaxer développement dalgorithme calcul performant derniersoper donnée extrait lentrepôt donnée généralementimplémenter modeler relationnel adopter larchitectur orientéelign Or requête décisionnel larchitecture orienter colonne offred meilleur performance sgbdr 2 architecturer nedisposent dopérateurs approprié calcul cuber OLAP proposonsdans article méthode calcul cuber olap résultatsobtenu partir expérimentation mener démontrentqu approcher optimiser considérablemer temps construction cubeolap réduire temps réponse relatif lexploitation cuber comparer àlapproch orienter ligne
317	Revue des Nouvelles Technologies de l'Information	EGC	2014	Construction de profils de préférences contextuelles basée sur l'extraction de motifs séquentiels	L'utilisation de préférences suscite un intérêt croissant pour personnaliserdes réponses et effectuer des recommandations. En amont, l'étape essentielleest l'élicitation des préférences qui consiste à construire un profil depréférences en sollicitant le moins possible l'utilisateur. Dans cet article, nousprésentons une méthode basée sur l'extraction de motifs séquentiels afin de générerdes règles de préférences contextuelles à partir d'une base de paires detransactions. À partir de ces règles générées, qui ont une expressivité plus richeque celle des approches existantes, nous montrons comment construire et utiliserun profil modélisant les préférences de l'utilisateur. De plus, notre approchea l'avantage de bénéficier des nombreux algorithmes efficaces d'extraction deséquences fréquentes. L'évaluation de notre méthode sur des données réellesmontre que les modèles de préférences construits permettent d'effectuer des recommandationsjustes à un utilisateur.	Arnaud Giacometti, Dominique Haoyuan Li, Arnaud Soulet	http://editions-rnti.fr/render_pdf.php?p1&p=1001954	http://editions-rnti.fr/render_pdf.php?p=1001954	lutilisation préférence susciter intérêt croître personnaliserde réponse effectuer recommandation En amont létape essentielleest lélicitation préférence consister construire profil depréférences solliciter lutilisateur Dans article nousprésenton méthode baser lextraction motif séquentiel générerde règle préférence contextuel partir dune baser paire detransaction À partir règle généré expressivité plaire richequ approche existant montrer construire utiliserun profil modéliser préférence lutilisateur De plaire approchea lavantage bénéficier algorithm efficace dextraction deséquenc fréquent lévaluation méthode donnée réellesmontre modèle préférence construit permettre deffectuer recommandationsjuste utilisateur
318	Revue des Nouvelles Technologies de l'Information	EGC	2014	De l'ombre à la lumière : plus de visibilité sur l'Eclipse	L'extraction de connaissances à partir de données issues du génie logicielest un domaine qui s'est beaucoup développé ces dix dernières années, avecnotamment la fouille de référentiels logiciels (Mining Software Repositories) etl'application de méthodes statistiques (partitionnement, détection d'outliers) àdes thématiques du processus de développement logiciel. Cet article présente ladémarche de fouille de données mise en oeuvre dans le cadre de Polarsys, ungroupe de travail de la fondation Eclipse, de la définition des exigences à la propositiond'un modèle de qualité dédié et à son implémentation sur un prototype.Les principaux concepts adoptés et les leçons tirées sont également passés enrevue.	Boris Baldassari, Flavien Huynh, Philippe Preux	http://editions-rnti.fr/render_pdf.php?p1&p=1001967	http://editions-rnti.fr/render_pdf.php?p=1001967	lextraction connaissance partir donnée issu génie logicielest domaine sest développer année avecnotammer fouiller référentiel logiciel Mining Software Repositories etlapplication méthode statistique partitionnement détection doutlier àd thématique processus développement logiciel article présent ladémarche fouiller donnée mettre oeuvrer dan cadrer Polarsys ungroup travail fondation eclips définition exigence propositiondun modèle qualité dédier implémentation prototypele principal concept adopter leçon tirer également passer enrevue
319	Revue des Nouvelles Technologies de l'Information	EGC	2014	De nouvelles pondérations adaptées à la classification de petits volumes de données textuelles	Un des défis actuels dans le domaine de la classification supervisée dedocuments est de pouvoir produire un modèle fiable à partir d'un faible volumede données. Avec un volume conséquent de données, les classifieurs fournissentdes résultats satisfaisants mais les performances sont dégradées lorsque celui-cidiminue. Nous proposons, dans cet article, de nouvelles méthodes de pondérationsrésistant à une diminution du volume de données. Leur efficacité, évaluéeen utilisant des algorithmes de classification supervisés existants (Naive Bayeset Class-Feature-Centroid) sur deux corpus différents, est supérieure à celle desautres algorithmes lorsque le nombre de descripteurs diminue. Nous avons étudiéen parallèle les paramètres influençant les différentes approches telles que lenombre de classes, de documents ou de descripteurs.	Flavien Bouillot, Pascal Poncelet, Mathieu Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1001922	http://editions-rnti.fr/render_pdf.php?p=1001922	défi actuel dan domaine classification superviser dedocument pouvoir produire modeler fiable partir dun faible volumede donner Avec volume conséquent donnée classifieur fournissentd résultat satisfaisant performance dégrader celuicidiminue proposer dan article méthode pondérationsrésister diminution volume donnée efficacité évaluéeen utiliser algorithme classification superviser existant naive Bayeset ClassFeatureCentroid corpus supérieur desautr algorithmer nombre descripteur diminuer étudiéen parallèle paramètre influencer approche lenombre classe document descripteur
320	Revue des Nouvelles Technologies de l'Information	EGC	2014	Des humanités au numérique : interdisciplinarité et réciprocité	Les Humanités Numériques, aussi contestable et critiquable que soit le terme, font maintenantpartie du paysage de la recherche en sciences humaines, institutionnalisées par la TrèsGrande Infrastructure de Recherche Huma-Num du CNRS. Elles sont généralement définiescomme la convergence de disciplines autour d'un matériau numérique, matériau inévitablementaccompagné d'un outillage tout aussi numérique. Ce matériau, suivant la discipline quil'observe pourra être considéré comme un objet éditorial, un objet analysable ou un objetcalculable. Nous tenterons de montrer que ce matériau peut aussi être perçu, voire construit,comme un dépôt voire un entrepôt de connaissances. Notre présentation s'appuiera sur diversprojets de recherche en humanités numériques auxquels nous contribuons afin de mettre enexergue le lien qui peut être fait entre extraction et gestion de connaissances d'une part ethumanités numériques d'autre part : le premier peut trouver un terrain expérimental dans lesecond tandis que le second peut tirer profit des méthodes et outils développés par le premier.Nous égrainerons par ailleurs d'autres problématiques inhérentes aux Humanités numériques :de la constitution à l'analyse du corpus en passant par la formalisation et la normalisationdes données. Enfin, nous tenterons de montrer par l'exemple que les questions posées par leshumanités numériques ne sont pas sans rappeler celles des industries de la connaissance.	Thomas Lebarbé	http://editions-rnti.fr/render_pdf.php?p1&p=1001907	http://editions-rnti.fr/render_pdf.php?p=1001907	humanité numérique contestable critiquable terme faire maintenantpartie paysage rechercher science humain institutionnaliser trèsgrande Infrastructure rechercher HumaNum cnrs généralement définiescomme convergence discipline autour dun matériau numérique matériau inévitablementaccompagner dun outillage numérique matériau discipliner quilobserve pouvoir considérer objet éditorial objet analysable objetcalculabl tenter montrer matériau pouvoir percevoir voire construitcomme dépôt voire entrepôt connaissance présentation sappuier diversprojet rechercher humanité numérique auxquel contribuer mettre enexergue lien pouvoir faire entrer extraction gestion connaissance dune partir ethumanité numérique dautr partir   pouvoir trouver terrain expérimental dan lesecond second pouvoir tirer profit méthode outil développer premiernous égrainer ailleur dautre problématique inhérent humanité numérique constitution lanalyse corpus passer formalisation normalisationde donnée tenter montrer lexemple question poser leshumanité numérique rappeler industrie connaissance
321	Revue des Nouvelles Technologies de l'Information	EGC	2014	Détection d'opinions dans des tweets	Twitter est à l'heure actuelle un des réseau sociaux les plus utilisé aumonde et analyser les opinions qui y sont contenues permet de fournir de précieusesinformations notamment aux entreprises commerciales. Dans cet article,nous décrivons une méthode permettant de déterminer l'opinion d'un tweet endétectant dans un premier temps sa subjectivité, puis sa polarité.	Caroline Collet, Alexandre Pauchet, Laurent Vercouter, Khaled Khelif	http://editions-rnti.fr/render_pdf.php?p1&p=1001964	http://editions-rnti.fr/render_pdf.php?p=1001964	twitter lheure actuel réseau social plaire utiliser aumond analyser opinion yu contenu permettre fournir précieusesinformation entreprise commercial Dans articlenou décrivon méthode permettre déterminer lopinion dun tweet endétecter dan temps subjectivité pouvoir polarité
322	Revue des Nouvelles Technologies de l'Information	EGC	2014	Détection de changements dans des flots de données qualitatives	Pour mieux analyser et extraire de la connaissance de flots de données,des approches spécifiques ont été proposées ces dernières années. L'un deschallenges auquel elles doivent faire face est la détection de changement dansles données. Alors que de plus en plus de données qualitatives sont générées,peu de travaux de recherche se sont intéressés à la détection de changement dansce contexte et les travaux existants se sont principalement focalisés sur la qualitéd'un modèle appris plutôt qu'au réel changement dans les données. Danscet article nous proposons une nouvelle méthode de détection de changementnon supervisée, appelée CDCStream (Change Detection in Categorical DataStreams), adaptée aux flux de données qualitatives.	Dino Ienco, Albert Bifet, Bernhard Pfahringer, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1001968	http://editions-rnti.fr/render_pdf.php?p=1001968	Pour mieux analyser extraire connaissance flot donnéesd approche spécifique proposer année lun deschallenge devoir faire face détection changement dansl donnée plaire plaire donnée qualitatif généréespeu travail rechercher intéresser détection changement dansc contexte travail existant principalement focaliser qualitédun modeler apprendre quau réel changement dan donnée Danscet article proposer méthode détection changementnon superviser appeler CDCStream Change Detection in Categorical DataStreams adapter flux donnée qualitatif
323	Revue des Nouvelles Technologies de l'Information	EGC	2014	Détection de situations à risque basée sur des détecteurs de mouvement à domicile pour les personnes dépendantes	Avec le vieillissement de la population dans les décennies à venir, laprise en charge de la dépendance est devenu un enjeu majeur. Les nouvellestechnologies permettent d'améliorer le confort et la sécurité des personnes dépendantesà domicile. Dans cet article nous proposons une méthode de détectionde situations à risques basée sur le seuillage automatique des intervalles d'inactivitédes capteurs de mouvement de type infrarouge passif. Notre contributionconsiste à apprendre de façon automatique la durée maximale d'inactivité, parpièce et par plage horaire. La méthode est évaluée sur des données réelles provenantde l'activité d'une personne réelle dans un appartement équipé de capteursdomotiques. Notre approche permet de réduire le temps d'appel des secours.	Alban Meffre, Nicolas Lachiche, Pierre Gançarski, Christophe Collet	http://editions-rnti.fr/render_pdf.php?p1&p=1001983	http://editions-rnti.fr/render_pdf.php?p=1001983	Avec vieillissement population dan décennie venir laprise charger dépendance devenir enjeu majeur nouvellestechnologie permettre daméliorer confort sécurité dépendantesà domicile Dans article proposer méthode détectionde situation risque baser seuillage automatique intervalle dinactivitéde capteur mouvement typer infrarouge passif contributionconsiste automatique durer maximal dinactivité parpièce plage horaire méthode évaluer donnée réel provenantde lactivité dune réel dan appartement équiper capteursdomotique approcher permettre réduire temps dappel secours
324	Revue des Nouvelles Technologies de l'Information	EGC	2014	Du texte à la base de données géographiques	Avec la prolifération des données géographiques, il y a un fort besoinde concevoir des outils automatiques pour l'exploitation des connaissances géographiquesincarnées dans les documents textuels. C'est dans ce contexte, quenous proposons une approche permettant de générer une base de données géographiques(BDG) à partir de textes. Notre approche s'articule autour de deuxgrandes phases : la génération du schéma de la BDG et la détermination desdonnées qui serviront au remplissage de cette base. L'implémentation de notreapproche a donné naissance à un outil que nous avons baptisé GDB Generatoret que nous avons intégré dans le SIG : OpenJUMP.	Nesrine Hassini, Khaoula Mahmoudi, Sami Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1001972	http://editions-rnti.fr/render_pdf.php?p=1001972	Avec prolifération donnée géographique yu fort besoinde concevoir outil automatique lexploitation connaissance géographiquesincarner dan document textuel cest dan contexte quenous proposer approcher permettre générer baser donnée géographiquesBDG partir texte approcher sarticul autour deuxgrande phase   génération schéma BDG détermination desdonner servir remplissage baser limplémentation notreapproche donner naissance outil baptiser GDB Generatoret intégrer dan sig   OpenJUMP
325	Revue des Nouvelles Technologies de l'Information	EGC	2014	Dynamique des communautés par prédiction d'interactions dans les réseaux sociaux	Dans cet article, nous proposons une approche générale de prédictiondes communautés basée sur un modèle d'apprentissage automatique pour la prédictiondes interactions. En effet, nous pensons que, si on peut prédire avec précisionla structure du réseau, alors on a juste à rechercher les communautés surle réseau prédit. Des expérimentations sur des jeux de données réels montrent lafaisabilité de cette approche.	Blaise Ngonmang, Emmanuel Viennet	http://editions-rnti.fr/render_pdf.php?p1&p=1001977	http://editions-rnti.fr/render_pdf.php?p=1001977	Dans article proposer approcher général prédictionde communauter baser modeler dapprentissage automatique prédictionde interacter En penser pouvoir prédir précisionla structurer réseau rechercher communauté surle réseau prédire expérimentation jeu donnée réel montrer lafaisabilité approcher
326	Revue des Nouvelles Technologies de l'Information	EGC	2014	Evaluation de la pertinence dans un système de recommandation sémantique de nouvelles économiques	De nos jours dans les secteurs commerciaux et financiers la veille estcruciale et complexe, car la charge d'informations est importante. Pour répondreà cette problématique, nous proposons un système novateur de recommandationd'articles basé sur une modélisation ontologique des connaissances. Nous présentonségalement une nouvelle méthode d'évaluation de la pertinence utilisantle modèle vectoriel intrinsèquement efficace et adapté afin de pallier la confusionnative de ces modèles entre les notions de similarité et de pertinence.	David Werner, Christophe Cruz, Aurélie Bertaux	http://editions-rnti.fr/render_pdf.php?p1&p=1001976	http://editions-rnti.fr/render_pdf.php?p=1001976	De jour dan secteur commercial financier veiller estcrucial complexe charger dinformation important Pour répondreà problématique proposer système novateur recommandationdarticle baser modélisation ontologique connaissance présentonségalemer méthod dévaluation pertinence utilisantl modeler vectoriel intrinsèquement efficace adapter pallier confusionnative modèle entrer notion similarité pertinence
327	Revue des Nouvelles Technologies de l'Information	EGC	2014	Exploration d'une collection de chansons à partir d'une interface de visualisation basée sur une analyse des paroles	Dans cet article, nous présentons une approche de fouille de textesainsi qu'une interface de visualisation afin d'explorer une large collection dechansons frana¸ises à partir des paroles. Dans un premier temps, nous collectonsparoles et métadonnées de différentes sources sur leWeb. Nous utilisons une approchecombinant clustering et analyse sémantique latente afin d'identifier différentesthématiques et de déterminer différents descripteurs significatifs. Noustransformons par la suite le modèle afin d'obtenir une visualisation interactivepermettant d'explorer la collection de chansons	Rémy Kessler, Audrey Laplante, Dominic Forest	http://editions-rnti.fr/render_pdf.php?p1&p=1001946	http://editions-rnti.fr/render_pdf.php?p=1001946	Dans article présenter approcher fouiller textesainsi quun interface visualisation dexplorer large collection dechanson frana¸is partir Dans temps collectonsparoler métadonné source leweb utiliser approchecombiner clustering analyser sémantique latent didentifier différentesthématiqu déterminer descripteur significatif noustransformon suite modeler dobtenir visualisation interactivepermetter dexplorer collection chanson
328	Revue des Nouvelles Technologies de l'Information	EGC	2014	Extension de l'étiquetage géographique des pixels d'une image par fouille de données	Les techniques de classification modernes permettent d'étiqueter leszones non couvertes des bases de données cartographiques, mais souffrent d'unmanque de robustesse important. Dans cet article, nous proposons une méthoderobuste d'extension d'étiquetage sur l'emprise d'une image satellite, par analysehiérarchique des données existantes. Notre approche est fondée sur une sélectiond'attributs par thème de la base de données, une sélection des pixels d'apprentissageet des classifications par objet de chaque thème. La décision finaled'étiquetage est prise après fusion des classifications par thème. Notre méthodeest appliquée avec succès et comparée à plusieurs méthodes de classification,couplant données d'occupation du sol et imagerie spatiale très haute résolution.	Adrien Gressin, Nicole Vincent, Clément Mallet, Nicolas Paparoditis	http://editions-rnti.fr/render_pdf.php?p1&p=1001966	http://editions-rnti.fr/render_pdf.php?p=1001966	technique classification moderne permettre détiqueter leszone couvrir base donnée cartographique souffrir dunmanqu robustesse importer Dans article proposer méthoderobust dextension détiquetage lemprise dune imager satellite analysehiérarchique donnée existant approcher fonder sélectiondattribut thème baser donnée sélection pixel dapprentissageet classification objet thème décision finaledétiquetage priser fusion classification thème méthodeest appliquer succès comparer méthode classificationcouplant donner doccupation sol imagerie spatial résolution
329	Revue des Nouvelles Technologies de l'Information	EGC	2014	Extraction de motifs dans des graphes orientés attribués en présence d'automorphisme	Les graphes orientés attribués sont des graphes orientés dans lesquelsles noeuds sont associés à un ensemble d'attributs. De nombreuses données, issuesdu monde réel, peuvent être représentées par ce type de structure, maisencore peu d'algorithmes sont capables de les traiter directement. La fouille desgraphes attribués est difficile, car elle nécessite de combiner l'exploration de lastructure du graphe avec l'identification d'itemsets fréquents. De plus, du fait del'explosion combinatoire des itemsets, les isomorphismes de sous-graphes, dontla présence impacte énormément les performances des algorithmes de fouille,sont beaucoup plus nombreux que dans les graphes étiquetés.Dans cet article, nous présentons une nouvelle méthode de fouille de donnéesqui permet d'extraire des motifs fréquents à partir d'un ou de plusieurs graphesorientés attribués. Nous montrons comment réduire l'explosion combinatoireprovoquée par les isomorphismes de sous-graphes en traitant de manière particulièreles motifs automorphes.	Claude Pasquier, Frédéric Flouvat, Jérémy Sanhes, Nazha Selmaoui-Folcher	http://editions-rnti.fr/render_pdf.php?p1&p=1001949	http://editions-rnti.fr/render_pdf.php?p=1001949	graphe orienter attribuer graphe orienter dan lesquelsles noeud associé ensemble dattribut donnée issuesdu monder réel pouvoir représenter typer structurer maisencor dalgorithme capable traiter fouiller desgraphes attribuer difficile nécessiter combiner lexploration lastructure graphe lidentification ditemset fréquent De plaire faire delexplosion combinatoire itemset isomorphisme sousgraphe dontla présence impacte énormémer performance algorithme fouillesont plaire dan graphe étiquetésDans article présenter méthode fouiller donnéesqui permettre dextraire motif fréquent partir dun graphesorienté attribuer montrer réduire lexplosion combinatoireprovoquer isomorphisme sousgraphe traiter manière particulièrel motif automorphe
330	Revue des Nouvelles Technologies de l'Information	EGC	2014	Extraction de règles d'épisodes minimales dans des séquences complexes	Les messages déposés quotidiennement sur les réseaux sociaux et lesblogs sont très nombreux et constituent une source d'informations précieuse.Leur fouille peut être utilisée dans un but de prédiction d'informations. Notreobjectif dans cet article est de proposer un algorithme permettant la prédictiond'informations au plus tôt et de façon fiable, par le biais de l'identification derègles d'épisodes.	Lina Fahed, Armelle Brun, Anne Boyer	http://editions-rnti.fr/render_pdf.php?p1&p=1001975	http://editions-rnti.fr/render_pdf.php?p=1001975	message déposer quotidiennement réseau social lesblogs constituer source dinformation précieuseleur fouill pouvoir utiliser dan boire prédiction dinformation Notreobjectif dan article proposer algorithme permettre prédictiondinformater plaire tôt fiable biais lidentification derègl dépisod
331	Revue des Nouvelles Technologies de l'Information	EGC	2014	Extraire les motifs minimaux efficacement et en profondeur	Les représentations condensées ont fait l'objet de nombreux travauxdepuis 15 ans. Tandis que les motifs maximaux des classes d'équivalence ontreçu beaucoup d'attention, les motifs minimaux sont restés dans l'ombre notammentà cause de la difficulté de leur extraction. Dans ce papier, nous présentonsun cadre générique concernant l'extraction de motifs minimaux en introduisantla notion de système minimisable d'ensembles. Il permet de considérer des langagesvariés comme les motifs ensemblistes ou les chaînes de caractères, maisaussi différentes métriques dont la fréquence. Ensuite, pour n'importe quel systèmeminimisable d'ensembles, nous introduisons un test de minimalité rapidepermettant d'extraire en profondeur les motifs minimaux. Nous démontrons quel'algorithme proposé est polynomial-delay et polynomial-space. Des expérimentationssur les benchmarks traditionnels complètent notre étude.	Arnaud Soulet, François Rioult	http://editions-rnti.fr/render_pdf.php?p1&p=1001950	http://editions-rnti.fr/render_pdf.php?p=1001950	représentation condenser faire lobjet travauxdepui 15 an motif maximal classe déquivalence ontreçu dattention motif minimal rester dan lombre notammentà causer difficulté extraction Dans papier présentonsun cadrer générique concerner lextraction motif minimal introduisantla notion système minimisabl densemble permettre considérer langagesvarié motif ensembliste chaîne caractère maisaussi métrique fréquence ensuite nimport systèmeminimisabl densemble introduire test minimalité rapidepermetter dextraire profondeur motif minimal démontrer quelalgorithme proposer polynomialdelay polynomialspace expérimentationssur benchmark traditionnel compléter étude
332	Revue des Nouvelles Technologies de l'Information	EGC	2014	Fouille de données par programmation visuelle structurée avec KD-Ariane	Nous présentons ici la plate-forme KD-Ariane, un déploiement d'outilspour la fouille de données dans l'environnement de programmation visuelleAriane. Ce déploiement facilite la conception de chaînes structurées de traitementspour l'extraction de connaissance dans les données	Régis Clouard, François Rioult	http://editions-rnti.fr/render_pdf.php?p1&p=1001989	http://editions-rnti.fr/render_pdf.php?p=1001989	présenter plateforme kdarian déploiement doutilspour fouiller donnée dan lenvironnement programmation visuelleAriane déploiemer faciliter conception chaîne structurer traitementspour lextraction connaissance dan donnée
333	Revue des Nouvelles Technologies de l'Information	EGC	2014	Fouille de motifs séquentiels pour l'élicitation de stratégies à partir de traces d'interactions entre agents en compétition	Pour atteindre un but, tout agent en compétition élabore inévitablementdes stratégies. Lorsque l'on dispose d'une certaine quantité de traces d'interactionsentre agents, il est naturel d'utiliser la fouille de motifs séquentielspour découvrir de manière automatique ces stratégies. Dans cet article, nous proposonsune méthodologie qui permet l'élicitation de stratégies et leur capacité àdiscriminer une réussite ou un échec. La méthodologie s'articule en trois étapes :(i) les traces brutes sont transformées en une base de séquences selon des choixqui permettent, (ii) l'extraction de stratégies fréquentes, (iii) lesquelles sont muniesd'une mesure originale d'émergence. C'est donc une méthodologie de découvertede connaissances que nous proposons. Nous montrons l'intérêt des motifsextraits et la faisabilité de l'approche à travers des expérimentations quantitativeset qualitatives sur des données réelles issues du domaine émergent dusport électronique.	Guillaume Bosc, Mehdi Kaytoue-Uberall, Chedy Raïssi, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1001948	http://editions-rnti.fr/render_pdf.php?p=1001948	Pour atteindre boire agent compétition élaborer inévitablementd stratégie Lorsque lon disposer dune quantité trace dinteractionsentre agent dutiliser fouiller motif séquentielspour découvrir manière automatique stratégier Dans article proposonsune méthodologie permettre lélicitation stratégie capacité àdiscriminer réussite échec méthodologie sarticul étape ie trace brut transformer baser séquence choixqui permettre ii lextraction stratégie fréquent iii muniesdune mesurer original démergence cest méthodologie découvertede connaissance proposer montrer lintérêt motifsextrait faisabilité lapproche travers expérimentation quantitativeset qualitatif donnée réel issu domaine émerger dusport électronique
334	Revue des Nouvelles Technologies de l'Information	EGC	2014	Généralisation des k-moyennes pour produire des recouvrements ajustables	La recherche de groupes non-disjoints à partir de données non-étiquetéesest une problématique importante en classification non-supervisée. Laclassification recouvrante (Overlapping clustering) contribue à la résolution deplusieurs problèmes réels qui nécessitent la détermination de groupes qui se chevauchent.Cependant, bien que les recouvrements entre groupes soient tolérésvoire encouragés dans ces applications, il convient de contrôler leur importance.Nous proposons dans ce papier des généralisations de k-moyennes offrant lecontrôle et le paramétrage des recouvrements. Deux principes de régulation sontmis en place, ils visent à contrôler les recouvrements relativement à leur tailleet à la dispersion des classes. Les expérimentations réalisées sur des jeux dedonnées réelles, montrent l'intérêt des principes proposés.	Chiheb-Eddine Ben N’Cir, Guillaume Cleuziou, Nadia Essoussi	http://editions-rnti.fr/render_pdf.php?p1&p=1001932	http://editions-rnti.fr/render_pdf.php?p=1001932	rechercher groupe nondisjoint partir donnée nonétiquetéesest problématique important classification nonsupervisé laclassification recouvrant Overlapping clustering contribuer résolution deplusieur problème réel nécessiter détermination groupe chevauchentcepender recouvrement entrer groupe tolérésvoir encourager dan application convier chuter importancenou proposer dan papier généralisation kmoyenne offrir lecontrôle paramétrage recouvrement Deux principe régulation sontmis placer viser chuter recouvrement tailleet dispersion classe expérimentation réaliser jeu dedonner réel montrer lintérêt principe proposer
335	Revue des Nouvelles Technologies de l'Information	EGC	2014	Génération d'un extrait textuel à partir de bases de données	Dans ce papier, nous présentons une approche dédiée à la transformationd'une base de données en un extrait textuel. L'idée sous-jacente à notreproposition est d'apporter plus de sémantique aux données de la base. Cet objectifest atteint moyennant l'utilisation des ontologies comme ressources sémantiques.Notre approche prend comme input un ensemble de bases de donnéeset associe à chacune une ontologie. Une ontologie globale est générée, à partirde laquelle des règles d'association sont proposées pour mieux expliciter sasémantique. Enfin, la génération d'un extrait textuel prend lieu.	Ghada Landoulsi, Khaoula Mahmoudi, Sami Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1001971	http://editions-rnti.fr/render_pdf.php?p=1001971	Dans papier présenter approcher dédier transformationdune baser donnée extraire textuel lidée sousjacent notreproposition dapporter plaire sémantique donnée baser objectifest atteindre moyenner lutilisation ontologie ressourcer sémantiquesnotre approcher prendre input ensemble base donnéeset associer ontologie ontologie global générer partirde règle dassociation proposer mieux expliciter sasémantique Enfin génération dun extraire textuel prendre lieu
336	Revue des Nouvelles Technologies de l'Information	EGC	2014	Granularité des motifs de co-variations dans des graphes attribués dynamiques	Découvrir des connaissances dans des graphes qui sont dynamiqueset dont les sommets sont attribués est de plus en plus étudié, par exemple dansle contexte de l'analyse d'interactions sociales. Il est souvent possible d'expliciterdes hiérarchies sur les attributs permettant de formaliser des connaissancesa priori sur les descriptions des sommets. Nous proposons d'étendre destechniques de fouille sous contraintes récemment proposées pour l'analyse degraphes attribués dynamiques lorsque l'on exploite de telles hiérarchies et doncle potentiel de généralisation/spécialisation qu'elles permettent. Nous décrivonsun algorithme qui calcule des motifs de co-évolution multi-niveaux, c'est-à-diredes ensembles de sommets qui satisfont une contrainte topologique et qui évoluentde la même façon selon un ensemble de tendances et de pas de temps. Nosexpérimentations montrent que l'utilisation d'une hiérarchie permet d'extrairedes collections de motifs plus concises sans perdre d'information.	Elise Desmier, Marc Plantevit, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1001955	http://editions-rnti.fr/render_pdf.php?p=1001955	découvrir connaissance dan graphe dynamiqueset sommet attribuer plaire plaire étudier exemple dansl contexte lanalyse dinteraction social dexpliciterdes hiérarchie attribut permettre formaliser connaissancesa priori description sommet proposer détendre destechniqu fouiller sou contraindre récemment proposer lanalyse degraphes attribué dynamique lon exploiter hiérarchie doncle potentiel généralisationspécialisation quell permettre décrivonsun algorithme calculer motif coévolution multiniveaux cestàdirede ensembl sommet satisfaire contraint topologique évoluentde ensemble tendance temps nosexpérimentation montrer lutilisation dune hiérarchie permettre dextrairedes collection motif plaire concis perdre dinformation
337	Revue des Nouvelles Technologies de l'Information	EGC	2014	Identification de classes non-disjointes ayants des densités différentes	La classification recouvrante correspond à un enjeu important en classificationnon-supervisée en permettant à une observation d'appartenir à plusieursclusters. Plusieurs méthodes ont été proposées pour faire face à cetteproblématique en utilisant plusieurs approches usuelles de classification. Cependant,malgré l'efficacité de ces méthodes à déterminer des groupes non-disjoints,elles échouent lorsque les données comportent des groupes de densités différentescar elles ignorent la densité locale de chaque groupe et ne considèrentque la distance Euclidienne entres les observations. Afin de détecter des groupesnon-disjoints de densités différentes, nous proposons deux méthodes de classificationintégrant la variation de densité des différentes classes dans le processusde classification. Des expériences réalisées sur des ensembles de données artificiellesmontrent que les méthodes proposées permettent d'obtenir de meilleuresperformances lorsque les données contiennent des groupes de densités différentes.	Hela Masmoudi, Chiheb-Eddine Ben N&#146;Cir, Nadia Essoussi	http://editions-rnti.fr/render_pdf.php?p1&p=1001936	http://editions-rnti.fr/render_pdf.php?p=1001936	classification recouvrant correspondre enjeu importer classificationnonsuperviser permettre observation dappartenir plusieurscluster méthode proposer faire face cetteproblématique utiliser approche usuel classification Cependantmalgré lefficacité méthode déterminer groupe nondisjointsell échouer donnée comporter groupe densité différentescar ignorer densité local grouper considèrentque distancer Euclidienne entr observation Afin détecter groupesnondisjoint densité proposer méthode classificationintégrer variation densité classe dan processusde classification expérience réaliser ensemble donnée artificiellesmontrent méthode proposer permettre dobtenir meilleuresperformance donnée contenir groupe densité
338	Revue des Nouvelles Technologies de l'Information	EGC	2014	Identification de rôles communautaires dans des réseaux orientés appliquée à Twitter	La notion de structure de communautés est particulièrement utile pourétudier les réseaux complexes, car elle amène un niveau d'analyse intermédiaire,par opposition aux plus classiques niveaux local (voisinage des noeuds) et global(réseau entier). Le concept de rôle communautaire permet de décrire le positionnementd'un noeud en fonction de sa connectivité communautaire. Cependant,les approches existantes sont restreintes aux réseaux non-orientés, utilisentdes mesures topologiques ne considérant pas tous les aspects de la connectivitécommunautaire, et des méthodes d'identification des rôles non-généralisables àtous les réseaux. Nous proposons de résoudre ces problèmes en généralisant lesmesures existantes, et en utilisant une méthode non-supervisée pour déterminerles rôles. Nous illustrons l'intérêt de notre méthode en l'appliquant au réseaude Twitter. Nous montrons que nos modifications mettent en évidence les rôlesspécifiques d'utilisateurs particuliers du réseau, nommés capitalistes sociaux.	Nicolas Dugué, Vincent Labatut, Anthony Perez	http://editions-rnti.fr/render_pdf.php?p1&p=1001921	http://editions-rnti.fr/render_pdf.php?p=1001921	notion structurer communauter utile pourétudier réseau complexe amener niveau danalyse intermédiairepar opposition plaire classique niveau local voisinage noeud globalréseau entier concept rôle communautaire permettre décrir positionnementdun noeud fonction connectivité communautaire cependantle approche existant restreindre réseau nonorienté utilisentd mesure topologique considérer tou aspect connectivitécommunautaire méthode didentification rôle nongénéralisabl àtous réseau proposer résoudre problème généraliser lesmesure existant utiliser méthode nonsuperviser déterminerl rôle illustrer lintérêt méthode lappliquer réseaude Twitter montrer modification mettre évidence rôlesspécifiqu dutilisateur réseau nommer capitaliste social
339	Revue des Nouvelles Technologies de l'Information	EGC	2014	Incremental learning with latent factor models for attribute prediction in social-attribute networks	Dans ce travail, nous nous intéressons au problème de la prédiction d'attributs sur lesnoeuds dans un réseau social. La plupart des techniques sont hors ligne et ne sont pas adaptéesà des situations où les données arrivent massivement en flux comme dans le cas des médiassociaux. Dans ce travail, nous utilisons les modèles de variables latentes pour prédire les attributsinconnus des noeuds dans un réseau social et proposer une méthode pour mettre à jourincrémentalement le modèle avec des nouvelles données. Des expérimentations sur un jeu dedonnées issues des médias sociaux montrent que notre méthode est moins coûteuse en tempsde calcul et peut garantir des performances acceptables en comparaison avec les techniquesnon-incrémentales de l'état de l'art.	Duc Kinh Le Tran, Cécile Bothorel, Pascal Cheung Mon Chan	http://editions-rnti.fr/render_pdf.php?p1&p=1001916	http://editions-rnti.fr/render_pdf.php?p=1001916	Dans travail intéresser problème prédiction dattributs lesnoeuds dan réseau social technique ligne adaptéesà situation donnée arriver massivement flux dan cas médiassociaux Dans travail utiliser modèle variable latent prédire attributsinconnu noeud dan réseau social proposer méthode mettre jourincrémentalement modeler donnée expérimentation jeu dedonné issu média social montrer méthode coûteux tempsde calcul pouvoir garantir performance acceptable comparaison techniquesnonincrémentale létat lart
340	Revue des Nouvelles Technologies de l'Information	EGC	2014	Intégration de plusieurs formes de représentations spatiales dans un modèle de simulation	In this paper, we focus on modeling expert knowledge for simulating complex landscapespatial dynamics. One modeling tool to do that is the Ocelet modeling language that usesinteraction graphs to describe spatial dynamics. Most present approaches impose an a priorichoice of spatial format between: (i) a vector format representing the shapes of the entities, or(ii) a gridding of space into regular elements (raster). In this paper we show how Ocelet wasextended to support the interaction semantics between these two spatial formats (vector andraster). As case study, we present a runoff model in a tropical insular environment.	Mathieu Castets, Pascal Degenne, Danny Lo Seen, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1001988	http://editions-rnti.fr/render_pdf.php?p=1001988	in this paper we focus modeling expert knowledge for simulating complex landscapespatial dynamic one modeling tool to do that is the Ocelet modeling language that usesinteraction graph to describe spatial dynamic Most present approaches impos an priorichoice of spatial format between ie vector format representing the shape of the entitier orii gridding of space into regular elements raster In this paper we show how ocelet wasextended to support the interaction semantic between these two spatial format vector andraster caser study we preser runoff model in tropical insular environmer
341	Revue des Nouvelles Technologies de l'Information	EGC	2014	Intégration et visualisation de données liées thématiques sur un référentiel géographique	De nombreuses ressources publiées sur le Web des données sont décritespar une composante qui désigne d'une manière directe ou indirecte unelocalisation géographique. Comme toute autre propriété, cette information delocalisation peut être mise à profit pour permettre l'interconnexion des donnéesavec d'autres sources. Elle permet en outre leur représentation cartographique.Cependant, les informations de localisation utilisées dans les sources de donnéeslinked data peuvent parfois s'avérer imprécises ou hétérogènes d'une source àl'autre. Ceci rend donc leur exploitation pour réaliser une interconnexion difficile,voire impossible. Dans cet article, nous proposons de pallier ces difficultésen ancrant les données linked data thématiques aux objets d'un référentielgéographique. Nous mettons à profit le référentiel géographique afin de mettreen correspondance des données thématiques dotées d'indications de localisationhétérogènes. Nous exploitons enfin les relations de correspondance créées entredonnées thématiques et référentiel géographique dans une application de visualisationcartographique des données.	Abdelfettah Feliachi, Nathalie Abadie, Fayçal Hamdi	http://editions-rnti.fr/render_pdf.php?p1&p=1001912	http://editions-rnti.fr/render_pdf.php?p=1001912	ressource publier web donnée décritespar composant désigner dune manière direct indirect unelocalisation géographique Comme propriété information delocalisation pouvoir mettre profit permettre linterconnexion donnéesavec dautr source permettre outrer représentation cartographiquecepender information localisation utiliser dan source donnéeslinked dater pouvoir savérer imprécise hétérogène dune source àlautre exploitation réaliser interconnexion difficilevoir impossible Dans article proposer pallier difficultésen ancrer donnée linked dater thématique objet dun référentielgéographiqu mettre profit référentiel géographique mettreen correspondance donnée thématique doter dindication localisationhétérogène exploiter relation correspondance créer entredonner thématique référentiel géographique dan application visualisationcartographique donnée
342	Revue des Nouvelles Technologies de l'Information	EGC	2014	Investigation visuelle d'événements dans un grand flot de liens	Nous présentons une nouvelle méthode d'analyse exploratoirede grands flots de liens que nous appliquons à la détection d'événementssignificatifs dans plus de 2 millions d'interactions (pendant 4 mois) entreutilisateurs du réseau social en ligne Github. Nous combinons une méthodestatistique de détection automatique d'événements dans une série temporelle,Outskewer, avec un système de visualisation de graphes. Outskewer identifiedes instants de l'évolution du graphe d'interactions méritant d'être étudiés, etun analyste peut valider et interpréter ces événements par la visualisation demotifs anormaux dans les sous-graphes correspondants. Nous montrons par demultiples exemples que cette approche 1) permet de détecter des événementspertinents et de rejeter ceux qui ne le sont pas, 2) est adaptée à une démarcheexploratoire car elle ne nécessite pas de connaissance a priori sur les données.	Sébastien Heymann, Bénédicte Le Grand	http://editions-rnti.fr/render_pdf.php?p1&p=1001918	http://editions-rnti.fr/render_pdf.php?p=1001918	présenter méthode danalyse exploratoired grand flot lien appliquer détection dévénementssignificatif dan plaire 2 million dinteraction pendre 4 mois entreutilisateur réseau social ligne Github combiner méthodestatistique détection automatique dévénement dan série temporelleoutskewer système visualisation graphe Outskewer identifieder instant lévolution graph dinteraction mériter dêtre étudier etun analyst pouvoir valider interpréter événement visualisation demotifs anormal dan sousgraphe correspondant montrer demultipl exemple approcher 1 permettre détecter événementspertinent rejeter 2 adapter démarcheexploratoire nécessiter connaissance priori donnée
343	Revue des Nouvelles Technologies de l'Information	EGC	2014	L'utilisation des entités nommées pour l'expansion sémantique des requêtes Web	Les entités nommées sont des éléments intéressants pour les applicationsfondées sur le Traitement du Langage Naturel. Dans le cas de la recherched'information, les entités nommées sont largement employées par les utilisateursdu web dans les requêtes de recherche, soit pour définir un concept debase, soit pour décrire un autre concept dans la requête. Du côté du modèlede recherche, les entités nommées sont des éléments riches en information quiaident à mieux cibler les documents pertinents. Dans cet article, nous étudionsl'avantage d'étendre les entités nommées dans la requête. L'idée est d'utiliserune technique d'expansion sémantique sur une ontologie générale (Yago) pourdésambiguïser les entités nommées et pour trouver leurs différentes appellationsque l'on intègre dans la requête en utilisant 3 approches : sac de mots, dépendanceséquentielle, et concept clé. Nous mesurons l'efficacité de ces expériencesen termes de précision et rappel, et nous étudions l'effet du rôle des entités nomméessur l'expansion. Nous concluons que l'expansion des entités nommées estune méthode simple qui améliore significativement la qualité de la recherchequand elle est comparée à un modèle de référence sans expansion. De plus, cetteméthode est assez compétitive par rapport à l'approche pseudo retour de pertinencesouvent utilisée pour l'expansion de la requête.	Bissan Audeh, Philippe Beaune, Michel Beigbeder	http://editions-rnti.fr/render_pdf.php?p1&p=1001910	http://editions-rnti.fr/render_pdf.php?p=1001910	entité nommer élément intéressant applicationsfondée traitement langage Dans cas recherchedinformation entité nommé largement employer utilisateursdu web dan requête rechercher définir concept debase décrire concept dan requête côté modèlede rechercher entité nommé élément richer information quiaident mieux cibler document pertinent Dans article étudionslavantage détendre entité nommer dan requête Lidée dutiliserune technique dexpansion sémantique ontologie général Yago pourdésambiguïser entité nommer trouver appellationsqu lon intègre dan requête utiliser 3 approche   sac dépendanceséquentiell concept clé mesuron lefficacité expériencesen terme précision rappel étudier leffet rôle entité nomméessur lexpansion conclure lexpansion entité nommer estune méthode simple améliorer significativement qualité recherchequand comparer modeler référence expansion De plaire cetteméthod compétitif rapport lapproche pseudo pertinencesouvent utiliser lexpansion requête
344	Revue des Nouvelles Technologies de l'Information	EGC	2014	La subjectivité dans le discours médical : sur les traces de l'incertitude et des émotions	Les acteurs et usagers du domaine médical (médecins, infirmiers, patients,internes, pharmaciens, etc.) ne sont pas issus de la même catégorie socioprofessionnelleet ne présentent pas le même niveau de maîtrise du domaine.Leurs écrits en témoignent et véhiculent, de plus, la subjectivité qui leur estpropre. Nous nous intéressons à l'étude automatisée de la subjectivité dans lediscours médical dans des textes en langue française. Nous confrontons le discoursdes médecins (articles scientifiques, rapports cliniques) à celui des patients(messages de forums de santé) en analysant contrastivement les différencesd'emploi des descripteurs tels que les marqueurs d'incertitude et de polarité,les marques émotives non lexicales (smileys, ponctuations répétées, etc.)et lexicales, et les termes médicaux relatifs aux pathologies, traitements et procédures.Nous effectuons une annotation et catégorisation automatiques des documentsafin de mieux observer les spécificités que présentent les discours médicauxciblés.	Pierre Chauveau Thoumelin, Natalia Grabar	http://editions-rnti.fr/render_pdf.php?p1&p=1001958	http://editions-rnti.fr/render_pdf.php?p=1001958	acteur usager domaine médical médecin infirmier patientsinterner pharmacien issu catégorie socioprofessionnelleet présenter niveau maîtriser domaineleur écrit témoigner véhiculer plaire subjectivité estpropre intéresser létude automatisé subjectivité dan lediscour médical dan texte langue français confronter discoursd médecin articl scientifique rapport clinique patientsmessage forum santé analyser contrastivemer différencesdemploi descripteur marqueur dincertitude polaritél marqu émotif lexical smiley ponctuation répéter etcet lexical terme médical relatif pathologier traitement procéduresnous effectuer annotation catégorisation automatique documentsafin mieux observer spécificité présenter discours médicauxcibler
345	Revue des Nouvelles Technologies de l'Information	EGC	2014	Les nouvelles théories de l'incertain	La notion d'incertitude a été longtemps un sujet de controverses. En particulier la prééminencede la théorie des probabilités dans les sciences tend à gommer les différences présentesdans les premières tentatives de formalisation, remontant au 17ème siècle, entre l'incertitudedue à la variabilité des phénomènes répétables et l'incertitude due au manque d'information(dite épistémique). L'école Bayésienne affirme que quelle que soit l'origine de l'incertitude,celle-ci peut être modélisée par une distribution de probabilité unique. Cette affirmation a étébeaucoup remise en cause dans les trente dernières années. En effet l'emploi systématiqued'une distribution unique en cas d'information partielle mène à des utilisations paradoxales dela théorie des probabilités.Dans de nombreux domaines, il est crucial de distinguer entre l'incertitude due à la variabilitéd'observations et l'incertitude due à l'ignorance partielle. Cette dernière peut être réduitepar l'obtention de nouvelles informations, mais pas la première, dont on ne se prémunit quepar des actions concrètes. Dans le cas des bases de données, il est souvent supposé qu'ellessont précises, et l'incertitude correspondante est souvent négligée. Quant elle est abordée onreste souvent dans une approche probabiliste orthodoxe.Néanmoins, les statisticiens ont développé des outils qui ne relèvent pas de la théorie deKolmogorov pour pallier le manque de données (intervalles de confiance, principe de maximumde vraisemblance...).De nouvelles théories de l'incertain ont émergé, qui offrent la possibilité de représenter lesincertitudes épistémiques et aléatoires de façon distincte, notamment l'incertitude épistémique,en remplaçant la distribution de probabilité unique par une famille de distributions possibles,cette famille étant d'autant plus grande que l'information est absente. Cette représentationcomplexe possède des cas particuliers plus simples à utiliser en pratique, comme les ensemblesaléatoires (théorie des fonctions de croyance), les distributions de possibilité (représentant desensembles flous de valeurs possibles) et les p-boxes, notamment.Le but de cet exposé est de susciter l'intérêt pour ces nouvelles théories de l'incertain,d'en donner les bases formelles, d'en discuter la philosophie sous-jacente, de faire le lien aveccertaines notions en statistique, et de les illustrer sur des exemples.	Didier Dubois	http://editions-rnti.fr/render_pdf.php?p1&p=1001906	http://editions-rnti.fr/render_pdf.php?p=1001906	notion dincertitude controverse En prééminencede théorie probabilité dan science tendre gommer différence présentesdan tentativ formalisation remonter 17ème siècle entrer lincertitudedue variabilité phénomène répétable lincertitud manqu dinformationdite épistémique Lécole Bayésienne affirmer lorigine lincertitudecelleci pouvoir modéliser distribution probabilité affirmation étébeaucoup remiser causer dan année En lemploi systématiquedune distribution cas dinformation partiel mener utilisation paradoxal dela théorie probabilitésdan domaine crucial distinguer entrer lincertitud variabilitédobservation lincertitud lignorance partiel pouvoir réduitepar lobtention information prémunir quepar action concret Dans cas base donnée supposer quellessont précis lincertitud correspondant négliger aborder onrest dan approcher probabiliste orthodoxenéanmoin statisticien développer outil relever théorie dekolmogorov pallier manquer donnée intervall confiance principe maximumde vraisemblancede théorie lincertain émerger offrir possibilité représenter lesincertitude épistémique aléatoire distinct lincertitude épistémiqueen remplacer distribution probabilité famille distributer possiblescette famille dautant plaire grand linformation absent représentationcomplexe posséder cas plaire simple utiliser pratiquer ensemblesaléatoire théorie fonction croyance distribution possibilité représenter desensembl flou pboxe notammentl boire exposer susciter lintérêt théorie lincertainden donner base formel den discuter philosophie sousjacent faire lien aveccertain notion statistique illustrer exemple
346	Revue des Nouvelles Technologies de l'Information	EGC	2014	"LOCAL-GENERATOR : ""diviser pour régner"" pour l'extraction des traverses minimales d'un hypergraphe"	Du fait qu'elles apportent des solutions dans de nombreuses applications,les traverses minimales des hypergraphes ne cessent de susciter l'intérêt dela communauté scientifique et le développement d'algorithmes pour les calculer.Dans cet article, nous présentons une nouvelle approche pour l'optimisation del'extraction des traverses minimales basée sur les notions d'hypergraphe partielet de traverses minimales locales selon une stratégie diviser pour régner. Nousintroduisons aussi un nouvel algorithme, appelé LOCAL-GENERATOR pour lecalcul des traverses minimales. Les expérimentations effectuées sur divers jeuxde données ont montré l'intérêt de notre approche, notamment sur les hypergraphesayant un nombre de transversalité élevé et renfermant un nombre trèsimportant de traverses minimales.	M. Nidhal Jelassi, Christine Largeron, Sadok Ben Yahia	http://editions-rnti.fr/render_pdf.php?p1&p=1001935	http://editions-rnti.fr/render_pdf.php?p=1001935	faire apporter solution dan applicationsle travers minimal hypergraphe cesser susciter lintérêt dela communauté scientifique développement dalgorithme calculerdan article présenter approcher loptimisation delextraction traverse minimal baser notion dhypergraph partielet traverse minimal local stratégie diviser régner nousintroduison nouvel algorithme appeler LOCALGENERATOR lecalcul traverse minimal expérimentation effectuer jeuxde donner montrer lintérêt approcher hypergraphesayer nombre transversalité élever renfermer nombre trèsimporter traverse minimal
347	Revue des Nouvelles Technologies de l'Information	EGC	2014	Méthodologie 3-way d'extraction d'un modèle articulatoire de la parole à partir des données d'un locuteur	Pour parler, le locuteur met en mouvement un ensemble complexed'articulateurs : la mâchoire qu'il ouvre plus ou moins, la langue à laquelle ilfait prendre de nombreuses formes et positions, les lèvres qui lui permettent delaisser l'air s'échapper plus ou moins brutalement, etc. Le modèle articulatoirele plus connu est celui de Maeda (1990), obtenu à partir d'Analyses en ComposantesPrincipales faites sur les tableaux de coordonnées des points des articulateursd'un locuteur en train de parler. Nous proposons ici une analyse 3-way dumême type de données, après leur transformation en tableaux de distances. Nousvalidons notre modèle par la prédiction des sons prononcés, qui s'avère presqueaussi bonne que celle du modèle acoustique, et même meilleure quand on prenden compte la co-articulation.	Martine Cadot, Yves Laprie	http://editions-rnti.fr/render_pdf.php?p1&p=1001985	http://editions-rnti.fr/render_pdf.php?p=1001985	Pour locuteur mettre mouvement ensemble complexedarticulateur   mâchoire quil ouvrir plaire langue ilfer prendre forme position lèvre luire permettre delaisser lair séchapper plaire brutalement modeler articulatoirele plaire connaître Maeda 1990 obtenir partir danalyse composantesprincipal faite tableau coordonnée point articulateursdun locuteur train proposer analyser 3way dumême typer donnée transformation tableau distance nousvalidon modeler prédiction prononcer savère presqueaussi modeler acoustique meilleur prenden compter coarticulation
348	Revue des Nouvelles Technologies de l'Information	EGC	2014	Mining the Crowd	Harnessing a crowd of Web users for data collection has recently become a wide-spreadphenomenon. A key challenge is that the human knowledge forms an open world and it is thusdifficult to know what kind of information we should be looking for. Classic databases haveaddressed this problem by data mining techniques that identify interesting data patterns. Thesetechniques, however, are not suitable for the crowd. This is mainly due to properties of thehuman memory, such as the tendency to remember simple trends and summaries rather thanexact details. Following these observations, we develop here a novel model for crowd mining.We will consider in the talk the logical, algorithmic, and methodological foundations neededfor such a mining process, as well as the applications that can benefit from the knowledgemined from crowd.	Tova Milo	http://editions-rnti.fr/render_pdf.php?p1&p=1001908	http://editions-rnti.fr/render_pdf.php?p=1001908	Harnessing crowd of Web user for dater collection has recently become widespreadphenomenon key challenge is that the human knowledge form an open world and it is thusdifficult to know what kind of information we should be looking for Classic databas haveaddressed this problem by dater mining technique that identify interesting dater patterns thesetechniques however are not suitabl for the crowd This is mainly to propertie of thehuman memory such the tendency to remember simple trend and summari rather thanexact details Following thes observation we develop her novel model for crowd miningw will consider in the talk the logical algorithmic and methodological foundation neededfor such mining process well the application that can benefit from the knowledgemined from crowd
349	Revue des Nouvelles Technologies de l'Information	EGC	2014	Modélisation de trajectoires cible/caméra : requêtes spatio-temporelles dans le cadre de la videosurveillance	Le nombre de caméras de vidéosurveillance installées dans le monde augmente chaquejour. En France, le système de la RATP déployé sur Paris comprend 9000 caméras fixes et19000 mobiles. Lors de faits particuliers (e.g., agressions, vols), les opérateurs de vidéo surveillancese basent sur les indications spatiales et temporelles de la victime et sur leur connaissancede la localisation des caméras pour sélectionner les contenus intéressants pour l'enquête.Deux grands problèmes peuvent alors survenir : (1) le temps de réponse est long (jusqu'à plusieursjours de traitement) et (2) un risque important de perte de résultats à cause d'une mauvaiseconnaissance du terrain (appel à des opérateurs extérieurs). Le but de notre recherche estde définir des outils d'assistance aux opérateurs qui puissent, à partir d'une trajectoire donnée,sélectionner de façon automatique les caméras pertinentes par rapport à la requête.	Dana Codreanu, André Péninou, Florence Sedes	http://editions-rnti.fr/render_pdf.php?p1&p=1001982	http://editions-rnti.fr/render_pdf.php?p=1001982	nombre caméra vidéosurveillance installer dan monder augmenter chaquejour En France système ratp déployer Paris comprendre 9000 caméra fixe et19000 mobile eg agression vol opérateur vidéo surveillancese baser indication spatiale temporel victime connaissancede localisation caméra sélectionner contenu intéressant lenquêtedeux grand problème pouvoir survenir   1 temps réponse long jusquà plusieursjour traitement 2 risquer importer perte résultat causer dune mauvaiseconnaissance terrain appel opérateur extérieur boire rechercher estde définir outil dassistance opérateur pouvoir partir dune trajectoire donnéesélectionner automatique caméra pertinent rapport requête
350	Revue des Nouvelles Technologies de l'Information	EGC	2014	Motifs récursifs : extraction ascendante hiérarchique d'ensembles d'items ou d'évènements pour le résumé de données transactionnelles ou séquentielles	Nous proposons une méthode originale pour extraire un résumé compact,représentatif et intelligible des motifs fréquents dans des données transactionnellesou séquentielles. Notre approche consiste à extraire un nouveau typede motifs que nous appelons motifs récursifs, i.e. des motifs de motifs, à l'aided'un algorithme hiérarchique agglomératif nommé RepaMiner. Nous généronsnon pas un simple ensemble de motifs mais une véritable structure dérivée dedendrogrammes, le RPgraph.	Julien Blanchard	http://editions-rnti.fr/render_pdf.php?p1&p=1001956	http://editions-rnti.fr/render_pdf.php?p=1001956	proposer méthode original extraire résumer compactreprésentatif intelligible motif fréquent dan donnée transactionnellesou séquentiel approcher consister extraire typede motif appeler motif récursif ie motif motif laidedun algorithm hiérarchique agglomératif nommer repaminer généronsnon simple ensemble motif véritable structurer dérivé dedendrogramme rpgraph
351	Revue des Nouvelles Technologies de l'Information	EGC	2014	Passage aux noyaux en classification recouvrante	La classification recouvrante correspond à un domaine d'étude très actifces dernières années et dont l'objectif est d'organiser un ensemble de donnéesen groupes d'individus similaires avec la particularité d'autoriser des chevauchementsentre les groupes. Parmi les approches étudiées nous nous intéressonsaux extensions recouvrantes des modèles de type moindres carrés et constatonsles difficultés théoriques et pratiques liées à leur adaptation aux noyaux. Nousformulons alors une nouvelle définition ensembliste pour caractériser un recouvrementde plusieurs classes, nous montrons que cette modélisation permet lerecours aux noyaux et nous proposons une solution algorithmique efficace pourrépondre au problème de la classification recouvrante à noyaux.	Guillaume Cleuziou	http://editions-rnti.fr/render_pdf.php?p1&p=1001931	http://editions-rnti.fr/render_pdf.php?p=1001931	classification recouvrant correspondre domaine détude actifce année lobjectif dorganiser ensemble donnéesen groupe dindividus similaire particularité dautoriser chevauchementsentre groupe Parmi approche étudier intéressonsaux extension recouvrant modèle typer moindre carré constatonsl difficulté théorique pratique lier adaptation noyau nousformulon définition ensembliste caractériser recouvrementde classe montrer modélisation permettre lerecours noyau proposer solution algorithmique efficace pourrépondre problème classification recouvrant noyau
352	Revue des Nouvelles Technologies de l'Information	EGC	2014	Pondération de blocs de variables en bi-partitionnement topologique	Dans cet article, nous proposons une nouvelle approche permettantà la fois le bi-partitionnement topologique (bi-clustering) et la pondération deblocs variables. Le modèle que nous proposons FBR-BiTM (Feature Block Relevanceusing BiTM) permet de découvrir un espace topologique d'un ensembled'observations et de variables en associant un nouveau score de pondération àchaque sous ensemble de variables. L'estimation des coefficients de pondérationest réalisée dans le même processus d'apprentissage que le bi-partitionnement.Ces pondérations sont locales et associées à chaque prototype. Elles reflètentl'importance locale de chaque bloc de variables pour le bi-partitionnement. L'évaluationmontre que l'approche proposée, comparée	Amine Chaibi, Hanane Azzag, Mustapha Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1001943	http://editions-rnti.fr/render_pdf.php?p=1001943	Dans article proposer approcher permettantà bipartitionnement topologique biclustering pondération debloc variable modeler proposer FBRBiTM Feature Block Relevanceusing BiTM permettre découvrir espacer topologique dun ensembledobservation variable associer score pondération àchaqu sou ensemble variable lestimation coefficient pondérationest réaliser dan processus dapprentissage bipartitionnementc pondération local associer prototype reflètentlimportance local bloc variable bipartitionnement Lévaluationmontre lapproche proposé comparer
353	Revue des Nouvelles Technologies de l'Information	EGC	2014	Prédiction de valeurs manquantes dans les bases de données— Une première approche fondée sur la notion de proportion analogique	Cet article présente une méthode originale de prédiction de valeursmanquantes dans les bases de données relationnelles, fondée sur la notion deproportion analogique. Nous montrons en particulier comment un algorithmeproposé dans le cadre de la classification automatique peut être adapté à cette fin.Deux cas sont considérés : celui d'une base de données transactionnelle (attributsbooléens), et celui où les valeurs manquantes peuvent être de type numérique.	William Correa Beltran, Hélène Jaudoin, Olivier Pivert	http://editions-rnti.fr/render_pdf.php?p1&p=1001961	http://editions-rnti.fr/render_pdf.php?p=1001961	article présenter méthode original prédiction valeursmanquante dan base donnée relationnel fonder notion deproportion analogique montrer algorithmeproposé dan cadrer classification automatique pouvoir adapter findeux cas considérer   dune baser donnée transactionnel attributsbooléen manquant pouvoir typer numérique
354	Revue des Nouvelles Technologies de l'Information	EGC	2014	Que ressentent les patients ?	Les forums de santé en ligne sont des espaces d'échanges où les patientspartagent leurs sentiments à propos de leurs maladies, traitements, etc.Sous couvert d'anonymat, ils expriment très librement leurs expériences personnelles.Ces forums sont donc une source d'informations très utile pour les professionnelsde santé afin de mieux identifier et comprendre les problèmes, lescomportements et les sentiments de leurs patients. Dans cet article, nous proposonsd'exploiter les messages des forums via des techniques de fouille de textespour extraire des traces d'émotions (e.g. joie, colère, surprise , etc.).	Soumia Melzi, Amine Abdaoui, Jérôme Azé, Sandra Bringay, Pascal Poncelet, Florence Galtier	http://editions-rnti.fr/render_pdf.php?p1&p=1001957	http://editions-rnti.fr/render_pdf.php?p=1001957	forum santé ligne espace déchang patientspartager sentiment propos maladie traitement etcsous couvrir danonymat exprimer librement expérience personnellesc forum source dinformation utile professionnelsde santé mieux identifier comprendre problème lescomportement sentiment patient Dans article proposonsdexploiter message forum technique fouiller textespour extraire trace démotion eg joie colèr surpris  
355	Revue des Nouvelles Technologies de l'Information	EGC	2014	Réconciliation des profils dans les réseaux sociaux	It is not uncommon that individuals create multiple profiles across several SNSs, eachcontaining partially overlapping sets of personal information. As a result, the creation of aglobal profile that gives an holistic view of the information of an individual requires methodsthat automatically match, or reconciliates, profiles across SNSs. In this paper, we focus on theproblem of identifying, or matching, the profiles of any individual across social networks.	Nacéra Bennacer, Coriane Nana Jipmo, Antonio Penta, Gianluca Quercini	http://editions-rnti.fr/render_pdf.php?p1&p=1001915	http://editions-rnti.fr/render_pdf.php?p=1001915	it is not uncommon that individual create profil acros several snss eachcontaining partially overlapping set of personal information result the creation of aglobal profil that giv an holistic view of the information of an individual requir methodsthat automatically match or reconciliat profile acros SNSs In this paper we focus theproblem of identifying or matching the profil of any individual acros social network
356	Revue des Nouvelles Technologies de l'Information	EGC	2014	Reconstruction et analyse sémantique de chronologies cybercriminelles	La reconstruction de chronologies d'évènements cybercriminels (oureconstruction d'évènements) est une étape primordiale dans une investigationnumérique. Cette phase permet aux enquêteurs d'avoir une vue des évènementssurvenus durant un incident. La reconstruction d'évènements requiert l'étuded'importants volumes de données en raison de l'omniprésence des nouvellestechnologies dans notre quotidien. De plus, les conclusions produites se doiventde respecter les critères fixés par la justice. Afin de répondre à ces challenges,nous proposons une nouvelle méthodologie basée sur une ontologie permettantd'assister les enquêteurs tout au long du processus d'enquête.	Yoan Chabot, Aurélie Bertaux, Tahar Kechadi, Christophe Nicolle	http://editions-rnti.fr/render_pdf.php?p1&p=1001969	http://editions-rnti.fr/render_pdf.php?p=1001969	reconstruction chronologie dévènement cybercriminel oureconstruction dévènement étape primordial dan investigationnumérique phase permettre enquêteur davoir évènementssurvenu durer incident reconstruction dévènement requérir létudedimportant volume donnée raison lomniprésence nouvellestechnologie dan quotidien De plaire conclusion produire doiventde respecter critère fixé justice Afin répondre challengesnous proposer méthodologie basé ontologie permettantdassister enquêteur long processus denquête
357	Revue des Nouvelles Technologies de l'Information	EGC	2014	Règles d'association inter-langues au service de la recherche d'information multilingue	Dans cet article, nous proposons de montrer l'intérêt et l'utilité de déploiementdes règles d'association inter-langues (RAILs) dans le domaine de laRecherche d'Information Multilingue (RIM). Ces règles sont des connaissancesadditionnelles résultantes d'un processus de fouille de grands corpus parallèlesalignés au niveau de la phrase. En effet, leurs conclusions exprimées dans unelangue cible représentent des traductions potentielles de leurs prémisses, expriméesdans une langue source. Nous illus trons l'utilisation des RAILs dans lecontexte de la RIM à travers deux propositions, à savoir : (i) la traduction desrequêtes et (ii) la traduction des termes de l'index. L'évaluation expérimentale aété menée sur la collection de documents MUCHMORE. Les résultats ont montréune amélioration significative de la pertinence système.	Belhaj Rhouma Sourour, Asma Ben Achour, Malek Hajjem, Chiraz Latiri	http://editions-rnti.fr/render_pdf.php?p1&p=1001927	http://editions-rnti.fr/render_pdf.php?p=1001927	Dans article proposer montrer lintérêt lutilité déploiementde règle dassociation interlangu rail dan domaine larecherche dinformation Multilingue RIM règle connaissancesadditionnelle résultant dun processus fouiller grand corpus parallèlesaligner niveau phraser En conclusion exprimer dan unelangue cibl représenter traduction potentiel prémisse expriméesdans langue source illus tron lutilisation rail dan lecontexte RIM travers proposition savoir   ie traduction desrequêt ii traduction terme lindex lévaluation expérimental aété mener collection document muchmore résultat montréune amélioration significatif pertinence système
358	Revue des Nouvelles Technologies de l'Information	EGC	2014	Representative training sets for classification and the variability of empirical distributions	We propose a novel approach for the estimation of the size of trainingsets that are needed for constructing valid models in machine learning and datamining. We aim to provide a good representation of the underlying populationwithout making any distributional assumptions.Our technique is based on the computation of the standard deviation of the 2-statistics of a series of samples. When successive statistics are relatively close,we assume that the samples produced represent adequately the true underlyingdistribution of the population, and the models learned from these samples willbehave almost as well as models learned on the entire population.We validate our results by experiments involving classifiers of various levels ofcomplexity and learning capabilities.	Saaid Baraty, Dan Simovici	http://editions-rnti.fr/render_pdf.php?p1&p=1001940	http://editions-rnti.fr/render_pdf.php?p=1001940	We proposer novel approach for the estimation of the size of trainingset that are needed for constructing valid model in machiner learning and datamining we aim to provide good representation of the underlying populationwithout making any distributional assumptionsour technique is based the computation of the standard deviation of the  2statistics of serier of sample When successif statistic are relatively closewe assumer that the sampl produced represent adequately the true underlyingdistribution of the population and the model learned from these sample willbehave almost well model learned the entir populationWe validat our results by experiment involving classifier of variou level ofcomplexity and learning capabilitier
359	Revue des Nouvelles Technologies de l'Information	EGC	2014	Requêtes skyline en présence d'exceptions	Dans cet article, nous nous intéressons à la recherche des points lesplus intéressants au sens de l'ordre de Pareto, i.e., à l'évaluation de requêtes« skyline » , dans des jeux de données présentant des anomalies. Il n'est pas rareque les données, de petites annonces par exemple, soient peuplées d'erreurs oud'exceptions qui peuvent perturber la recherche des meilleurs points car cellescisont susceptibles de dominer les autres points. L'approche présentée vise àcalculer les requêtes skyline malgré la présence de ces exceptions, sans pourautant les écarter définitivement, et à présenter graphiquement les résultats defaçon à identifier rapidement les points d'intérêt et les anomalies potentielles.	Hélène Jaudoin, Olivier Pivert, Daniel Rocacher	http://editions-rnti.fr/render_pdf.php?p1&p=1001944	http://editions-rnti.fr/render_pdf.php?p=1001944	Dans article intéresser rechercher point lesplus intéressant sens lordre Pareto ie lévaluation requête « skyline »   dan jeu donnée présenter anomalie nest rareque donnée petit annonce exemple peupler derreurs oudexception pouvoir perturber rechercher meilleur point cellescisont susceptible dominer point Lapproche présenter viser àcalculer requête skyline présence exception pourauter écarter définitivement poster graphiquement résultat defaçon identifier rapidement point dintérêt anomalie potentiel
360	Revue des Nouvelles Technologies de l'Information	EGC	2014	Sélection d'une méthode de classification multi-label pour un système interactif	L'objectif de cet article est d'évaluer la capacité de 12 algorithmesde classification multi-label à apprendre, en peu de temps, avec peu d'exemplesd'apprentissage. Les résultats expérimentaux montrent des différences importantesentre les méthodes analysées, pour les 3 mesures d'évaluation choisies:Log-Loss, Ranking-Loss et Temps d'apprentissage/prédiction, et les meilleursrésultats sont obtenus avec: multi-label k Nearest neighbours (ML-kNN), suivide Ensemble de Classifier Chains (ECC) et Ensemble de Binary Relevance (EBR).	Noureddine Yacine Nair Benrekia, Pascale Kuntz, Franck Meyer	http://editions-rnti.fr/render_pdf.php?p1&p=1001941	http://editions-rnti.fr/render_pdf.php?p=1001941	Lobjectif article dévaluer capacité 12 algorithmesde classification multilabel temps dexemplesdapprentissage résultat expérimental montrer différence importantesentre méthode analyser 3 mesure dévaluation choisiesloglos rankinglos temps dapprentissageprédiction meilleursrésultat obtenir multilabel nearest neighbours MLkNN suivid ensemble Classifier Chains ECC ensemble Binary Relevance EBR
361	Revue des Nouvelles Technologies de l'Information	EGC	2014	Sélection de prototypes en vue d'une catégorisation de textes avec les K plus proches voisins : étude comparative	La technique des K plus proches voisins (KNN) est une méthoded'apprentissage à base d'instances, elle a été appliquée dans la catégorisationde textes depuis de nombreuses années. En contraste avec ses performances declassification, il est reconnu que cet algorithme est lent pendant la classificationd'un nouveau document. Les Techniques de sélection de prototypes sont apparuescomme des méthodes très compétitives pour améliorer le KNN grâce à laréduction des données. L'étude contenue dans ce papier a pour objectif d'analyserl'impact de ces méthodes sur la performance de la classification de textesavec l'algorithme KNN.	Fatiha Barigou, Baya Naouel Barigou, Baghdad Atmani, Bouziane Beldjilali	http://editions-rnti.fr/render_pdf.php?p1&p=1001926	http://editions-rnti.fr/render_pdf.php?p=1001926	technique plaire voisin knn méthodedapprentissage baser dinstancer appliquer dan catégorisationde texter année En contraster performance declassification reconnaître algorithme lent pendre classificationdun document technique sélection prototype apparuescomme méthode compétitif améliorer knn grâce laréduction donnée Létude contenir dan papier objectif danalyserlimpact méthode performance classification textesavec lalgorithm knn
362	Revue des Nouvelles Technologies de l'Information	EGC	2014	Sous échantillonnage et machine à noyaux élastiques pour la classification de données de mouvement capturé	Dans le domaine de la reconnaissance de gestes isolés, bon nombrede travaux se sont intéressés à la réduction de dimension sur l'axe spatial pourréduire à la fois la complexité algorithmique et la variabilité des réalisationsgestuelles. Il est assez étonnant de constater que peu de ces méthodes se sontexplicitement penchées sur la réduction de dimension sur l'axe temporel. Enmatière de complexité, la réduction de dimension sur cet axe est un enjeu majeurquant à l'utilisabilité de distances élastiques en complexité quadratique. Parailleurs, la prise en compte de la variabilité sur cet axe demeure une source avéréede gain de performance. Pour tenter d'apporter un éclairage en matière deréduction de dimension sur l'axe temporel, nous présentons dans cet article uneapproche basée sur un sous échantillonnage temporel associé à l'exploitationd'un apprentissage automatique à base de noyaux élastiques. Nous montronsexpérimentalement, sur deux jeux de données très référencés dans la communautéet très opposés en matière de qualité de capture de mouvement, qu'il estpossible de réduire sensiblement le nombre de postures sur les trajectoires temporellestout en conservant, grâce à des noyaux élastiques, des performances dereconnaissance au niveau de l'état de l'art du domaine. Le gain de complexitéobtenu rend une telle approche éligible pour des applications temps-réel.	Pierre-François Marteau, Sylvie Gibet, Clément Reverdy	http://editions-rnti.fr/render_pdf.php?p1&p=1001928	http://editions-rnti.fr/render_pdf.php?p=1001928	Dans domaine reconnaissance geste isolé nombrede travail intéresser réduction dimension laxe spatial pourréduir complexité algorithmique variabilité réalisationsgestuell étonner constater méthode sontexplicitement pencher réduction dimension laxe temporel enmatière complexité réduction dimension axer enjeu majeurquer lutilisabilité distance élastiquer complexité quadratique Parailleurs priser compter variabilité axer demeurer source avéréed gain performance Pour tenter dapporter éclairage matière deréduction dimension laxe temporel présenter dan article uneapproch basé sou échantillonnage temporel associer lexploitationdun apprentissage automatique baser noyau élastique montronsexpérimentalemer jeu donnée référencer dan communautéet opposer matière qualité capturer mouvement quil estpossibl réduire sensiblemer nombre posture trajectoire temporellestout conserver grâce noyau élastique performance dereconnaissance niveau létat lart domaine gain complexitéobtenu approcher éligible application tempsréel
363	Revue des Nouvelles Technologies de l'Information	EGC	2014	Stratégies argumentatives pour la classification collaborative multicritères des connaissances cruciales	Dans cet article, nous proposons une approche argumentative visant àautomatiser la résolution des conflits entre les décideurs qui ont des préférencescontradictoires lors d'une classification multicritères collaborative des connaissancescruciales. Notre étude expérimentale a prouvé que cette approche peutrésoudre jusqu'à 81% des conflits et améliorer la qualité d'approximation dedécideurs d'un taux de 0.62 pour un récepteur et de 0.15 pour un initiateur.	Sarra Bouzayane, Inès Saad	http://editions-rnti.fr/render_pdf.php?p1&p=1001981	http://editions-rnti.fr/render_pdf.php?p=1001981	Dans article proposer approcher argumentatif viser àautomatiser résolution conflit entrer décideur préférencescontradictoire dune classification multicritère collaborative connaissancescruciale étude expérimental prouver approcher peutrésoudr jusquà 81 conflit améliorer qualité dapproximation dedécideur dun taux 062 récepteur 015 initiateur
364	Revue des Nouvelles Technologies de l'Information	EGC	2014	Symétries et Extraction de Motifs Ensemblistes	Les symétries sont des propriétés structurelles qu'on détecte dans ungrand nombre de bases de données. Dans cet article, nous étudions l'exploitationdes symétries pour élaguer l'espace de recherche dans les problèmes d'extractionde motifs ensemblistes. Notre approche est basée sur une intégrationdynamique des symétries dans les algorithmes de type Apriori permettant de réduirel'espace des motifs candidats. En effet, pour un motif donné, les symétriesnous permettent de déduire les motifs qui lui sont symétriques et vérifiant parconséquent les mêmes propriétés. Nous détaillons notre approche en utilisantl'exemple des motifs fréquents. Ensuite, nous la généralisons au cadre unificateurde Mannila et Toivonen pour l'extraction des motifs ensemblistes. Les expériencesmenées montrent la faisabilité et l'apport de notre approche d'élagagebasé sur les symétries.	Said Jabbour, Mehdi Khiari, Lakhdar Sais, Yakoub Salhi, Karim Tabia	http://editions-rnti.fr/render_pdf.php?p1&p=1001953	http://editions-rnti.fr/render_pdf.php?p=1001953	symétrie propriété structurel quon détecter dan ungrand nombre base donnée Dans article étudier lexploitationd symétrie élaguer lespace rechercher dan problème dextractionde motif ensembliste approcher baser intégrationdynamiqu symétrie dan algorithme typer Apriori permettre réduirelespace motif candidat En motif donner symétriesnou permettre déduire motif luire symétrique vérifier parconséquer propriété détailler approcher utilisantlexemple motif fréquent ensuite généraliser cadrer unificateurde Mannila Toivonen lextraction motif ensembliste expériencesmenée montrer faisabilité lapport approcher délagagebaser symétrie
365	Revue des Nouvelles Technologies de l'Information	EGC	2014	The Hitchhiker's Guide to Ontology	Artificial Intelligence has long had the dream of making computers smarter. For quite sometime, this vision has remained just that: a dream. With the development of large knowledgebases, though, we now have large amounts of semantic information at our hands. This changesthe game of AI. Computers have indeed become smarter. In this talk, we present the latestdevelopments in the field: The construction of general purpose knowledge bases (includingYAGO and DBpedia, as well as NELL and TextRunner), and their applications to tasks thatwere previously out of scope, The extraction of fine-grained information from natural languagetexts, semantic query answering, and the interpretation of newspaper texts at large scale.	Fabian Suchanek	http://editions-rnti.fr/render_pdf.php?p1&p=1001909	http://editions-rnti.fr/render_pdf.php?p=1001909	Artificial Intelligence has long had the dream of making computer smarter For quite sometim thi vision has remained just that dream with the development of large knowledgebas though we now hav large amount of semantic information at our hand This changesthe game of AI Computers hav indeed become smarter In this talk we preser the latestdevelopment in the field The construction of general purpose knowledge baser includingyago and dbpedia well nell and TextRunner and their application to task thatwer previously out of scope The extraction of finegrained information from natural languagetext semantic query answering and the interpretation of newspaper text at large scal
366	Revue des Nouvelles Technologies de l'Information	EGC	2014	Un système de détection de thématiques populaires sur Twitter	With the ever-growing amount of messages exchanged via Twitter, there is an increasinginterest in filtering this information, which is delivered under the form of a stream of messages.In this paper, we present a system for detecting popular topics in Twitter. The system can beapplied to static corpora and can also handle the live Twitter stream.	Adrien Guille, Cécile Favre	http://editions-rnti.fr/render_pdf.php?p1&p=1001990	http://editions-rnti.fr/render_pdf.php?p=1001990	With the evergrowing amount of messag exchanged twitter there is an increasinginterest in filtering thi information which is delivered under the form of stream of messagesin this paper we preser system for detecting popular topics in Twitter The system can beapplied to static corpus and can also handl the live twitter stream
367	Revue des Nouvelles Technologies de l'Information	EGC	2014	Une approche algébrique au problème du consensus de partitions	En classification non-supervisée, le consensus de partitions a pour objectifde produire une partition unique, représentant le consensus, à partir d'unensemble de partitions où chacune est engendrée indépendamment des autres,voire avec des méthodologies différentes. En complément des techniques ayantleur qualité propre en terme de robustesse ou de passage à l'échelle, nous apportonsun point de vue original sur le consensus de partitions, c'est-à-dire, par lebiais de définitions algébriques qui permettent d'établir la nature des déductionspouvant être réalisées dans une approche systématique (p.ex. un système à basede connaissances). Nous fondons notre approche sur le treillis des partitions pourlequel nous montrons comment peuvent être adjoint des opérateurs dans le butde formuler une expression caractérisant le consensus à partir d'un ensemble departitions.	Frédéric Dumonceaux, Guillaume Raschia, Marc Gelgon	http://editions-rnti.fr/render_pdf.php?p1&p=1001937	http://editions-rnti.fr/render_pdf.php?p=1001937	En classification nonsuperviser consensus partition objectifd produire partition représenter consensus partir dunensemble partition engendrer indépendammer autresvoire méthodologie différenter En complément technique ayantleur qualité propre terme robustesse passage léchelle apportonsun poindre original consensus partition cestàdir lebiai définition algébrique permettre détablir nature déductionspouvant réaliser dan approcher systématique pex système based connaissance fondre approcher treillis partition pourlequel montrer pouvoir adjoindre opérateur dan butde formuler expression caractériser consensus partir dun ensembl departition
368	Revue des Nouvelles Technologies de l'Information	EGC	2014	Une approche basée sur STATIS pour la fusion de cartes topologiques auto-organisées	Dans le cadre des cartes topologiques, nous proposons une nouvelleapproche d'ensemble clusters basée sur la méthode STATIS. Les méthodes d'ensembleclusters visent à améliorer la qualité de la partition d'un jeu de donnéesà travers la combinaison de plusieurs partitions.Les différentes partitions peuvent être obtenues en faisant varier les paramètresd'un algorithme (choix des centres initiaux, du voisinage initial et final des cellulesdans le cas des cartes topologiques auto-organisée SOM, etc). L'approcheprésentée dans cette communication repose sur la méthode d'analyse de donnéesmulti-tableaux STATIS pour déterminer une matrice compromis représentant aumieux la similarité entre les partitions issues des cartes topologiques. La fusiondes cartes topologiques est alors obtenue à travers une classification basée surcette matrice compromis. La méthode proposée est illustrée sur des donnéesréelles issues de l'UCI et sur des données simulées.	Mory Ouattara, Ndeye Niang, Rania Gasri, Fouad Badran, Corinne Mandin	http://editions-rnti.fr/render_pdf.php?p1&p=1001947	http://editions-rnti.fr/render_pdf.php?p=1001947	Dans cadrer carte topologique proposer nouvelleapproche densembl cluster baser méthode statis méthode densembleclusters viser améliorer qualité partition dun jeu donnéesà travers combinaison partitionsle partition pouvoir obtenu faire varier paramètresdun algorithm choix centre initial voisinage initial final cellulesdan cas carte topologique autoorganiser SOM Lapprocheprésentée dan communication reposer méthode danalyse donnéesmultitableaux stati déterminer matrice compromettre représenter aumieux similarité entrer partition issu carte topologique fusionde carte topologique obtenir travers classification basé surcett matrice compromettre méthode proposer illustrer donnéesréelle issu luci donnée simuler
369	Revue des Nouvelles Technologies de l'Information	EGC	2014	Une approche PPC pour la fouille de données séquentielles	Nous proposons dans cet article une nouvelle approche croisant destechniques de programmation par contraintes et de fouille pour l'extraction demotifs séquentiels. Le modèle que nous proposons offre un cadre générique etdéclaratif pour modéliser et résoudre des contraintes de nature hétérogène	Jean-Philippe Métivier, Samir Loudni, Thierry Charnois	http://editions-rnti.fr/render_pdf.php?p1&p=1001951	http://editions-rnti.fr/render_pdf.php?p=1001951	proposer dan article approcher croiser destechniqu programmation contraint fouiller lextraction demotif séquentiel modeler proposer offrir cadrer générique etdéclaratif modéliser résoudre contrainte nature hétérogène
370	Revue des Nouvelles Technologies de l'Information	EGC	2014	Une approcheWeb sémantique et combinatoire pour un système de recommandation sensible au contexte appliqué à l'apprentissage mobile	Au vu de l'émergence rapide des nouvelles technologies mobiles et lacroissance des offres et besoins d'une société en mouvement, les travaux se multiplientpour identifier de nouvelles plateformes d'apprentissage pertinentes afind'améliorer et faciliter l'apprentissage à distance. La prochaine étape de l'apprentissageà distance est naturellement le port de l'e-learning (apprentissageélectronique) vers les nouveaux systèmes mobiles. On parle de m-learning (apprentissagemobile). Nos travaux portent sur le développement d'une nouvellearchitecture pour le m-learning dont l'objectif est d'adapter et recommander desparcours de formations selon les contraintes contextuelles de l'apprenant.	Fayrouz Soualah Alila, Christophe Nicolle, Florence Mendes	http://editions-rnti.fr/render_pdf.php?p1&p=1001979	http://editions-rnti.fr/render_pdf.php?p=1001979	voir lémergence rapide technologi mobile lacroissance offre besoin dune société mouvement travail multiplientpour identifier plateforme dapprentissage pertinent afindaméliorer faciliter lapprentissage distancer prochain étape lapprentissageà distancer naturellement port lelearning apprentissageélectronique ver système mobile mlearning apprentissagemobil travail porter développement dune nouvellearchitecture mlearning lobjectif dadapter recommander desparcour formation contrainte contextuel lapprenant
371	Revue des Nouvelles Technologies de l'Information	EGC	2014	Une heuristique pour le paramétrage automatique de l'algorithme de clustering spectral	Trouver le nombre optimal de groupes dans le contexte d'un algorithmede clustering est un problème notoirement difficile. Dans cet article,nous en décrivons et évaluons une solution approchée dans le cas de l'algorithmespectral. Notre méthode présente l'avantage d'être déterministe, et peucoûteuse. Nous montrons qu'elle fonctionne de manière satisfaisante dans beaucoupde cas, même si quelques limites amènent des perspectives à ce travail.	Pierrick Bruneau, Olivier Parisot, Philippe Pinheiro	http://editions-rnti.fr/render_pdf.php?p1&p=1001933	http://editions-rnti.fr/render_pdf.php?p=1001933	trouver nombre optimal groupe dan contexte dun algorithmed clustering problème notoirement difficile Dans articlenou décrivon évaluon solution approcher dan cas lalgorithmespectral méthode présent lavantage dêtre déterministe peucoûteuse montrer fonctionner manière satisfaisant dan beaucoupde cas limite amener perspective travail
372	Revue des Nouvelles Technologies de l'Information	EGC	2014	Une méthode hybride pour la prédiction du profil des auteurs	Dans cet article, nous nous intéressons à la détection du profil desauteurs (âge, genre) à travers leurs discussions. La méthode proposée s'appuiesur la classification automatique qui utilise certaines données extraites d'une manièrestatistique à partir de corpus source. Nous présentons une méthode hybridequi combine l'analyse de surface dans les textes avec une méthode d'apprentissageautomatique. A fin d'obtenir une meilleure gestion de ces données, nousnous sommes basés sur l'utilisation des arbres de décision. Notre méthode adonné des résultats intéressants pour la détection du genre.	Seifeddine Mechti, Maher Jaoua, Lamia Hadrich Belguith	http://editions-rnti.fr/render_pdf.php?p1&p=1001973	http://editions-rnti.fr/render_pdf.php?p=1001973	Dans article intéresser détection profil desauteurs âge genre travers discussion méthode proposer sappuiesur classification automatique utiliser donnée extrait dune manièrestatistique partir corpus source présenter méthode hybridequi combin lanalys surface dan texte méthode dapprentissageautomatiqu fin dobtenir meilleur gestion donnée nousnou baser lutilisation arbre décision méthode adonner résultat intéressant détection genre
373	Revue des Nouvelles Technologies de l'Information	EGC	2014	Une méthode pour caractériser les communautés des réseaux dynamiques à attributs	De nombreux systèmes complexes sont étudiés via l'analyse de réseauxdits complexes ayant des propriétés topologiques typiques. Parmi cellesci,les structures de communautés sont particulièrement étudiées. De nombreusesméthodes permettent de les détecter, y compris dans des réseaux contenant desattributs nodaux, des liens orientés ou évoluant dans le temps. La détection prendla forme d'une partition de l'ensemble des noeuds, qu'il faut ensuite caractériserrelativement au système modélisé. Nous travaillons sur l'assistance à cettetâche de caractérisation. Nous proposons une représentation des réseaux sous laforme de séquences de descripteurs de noeuds, qui combinent les informationstemporelles, les mesures topologiques, et les valeurs des attributs nodaux. Lescommunautés sont caractérisées au moyen des motifs séquentiels émergents lesplus représentatifs issus de leurs noeuds. Ceci permet notamment la détectionde comportements inhabituels au sein d'une communauté. Nous décrivons uneétude empirique sur un réseau de collaboration scientifique.	Gu&#776;nce Keziban Orman, Vincent Labatut, Marc Plantevit, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1001919	http://editions-rnti.fr/render_pdf.php?p=1001919	système complexe étudier lanalyse réseauxdit complexe propriété topologique typiquer Parmi cellescile structure communauté étudier De nombreusesméthode permettre détecter yu dan réseau contenir desattributs nodal lien orienter évoluer dan temps détection prendler former dune partition lensemble noeud quil falloir ensuite caractériserrelativement système modéliser travailler lassistance cettetâche caractérisation proposer représentation réseau sou laforme séquence descripteur noeud combiner informationstemporell mesure topologique attribut nodal Lescommunautés caractériser moyen motif séquentiel émergent lesplus représentatif issu noeud permettre détectionde comportement inhabituel dune communauté décrivon uneétude empirique réseau collaboration scientifique
374	Revue des Nouvelles Technologies de l'Information	EGC	2014	Une méthode pour la détection de thématiques populaires sur Twitter	L'explosion du volume de messages échangés via Twitter entraîne unphénomène de surcharge informationnelle pour ses utilisateurs. Il est donc crucialde doter ces derniers de moyens les aidant à filtrer l'information brute, laquelleest délivrée sous la forme d'un flux de messages. Dans cette optique, nousproposons une méthode basée sur la modélisation de l'anomalie dans la fréquencede création de liens dynamiques entre utilisateurs pour détecter les picsde popularité et extraire une liste ordonnée de thématiques populaires. Les expérimentationsmenées sur des données réelles montrent que la méthode proposéeest capable d'identifier et localiser efficacement les thématiques populaires.	Adrien Guille, Cécile Favre	http://editions-rnti.fr/render_pdf.php?p1&p=1001917	http://editions-rnti.fr/render_pdf.php?p=1001917	lexplosion volume message échangé twitter entraîn unphénomène surcharger informationnel utilisateur crucialde doter moyen aider filtrer linformation brut laquelleest délivrer sou former dun flux message Dans optique nousproposer méthode baser modélisation lanomalie dan fréquencede création lien dynamique entrer utilisateur détecter picsde popularité extraire liste ordonner thématique populaire expérimentationsmenée donnée réel montrer méthode proposéeest capable didentifier localiser efficacement thématique populaire
375	Revue des Nouvelles Technologies de l'Information	EGC	2014	Une nouvelle approche pour la sélection de variables basée sur une métrique d'estimation de la qualité	La maximisation d'étiquetage (F-max) est une métrique non biaiséed'estimation de la qualité d'une classification non supervisée (clustering) qui favoriseles clusters ayant une valeur maximale de F-mesure d'étiquetage. Danscet article, nous montrons qu'une adaptation de cette métrique dans le cadrede la classification supervisée permet de réaliser une sélection de variables etde calculer pour chacune d'elles une fonction de contraste. La méthode est expérimentéesur différents types de données textuelles. Dans ce contexte, nousmontrons que cette technique améliore les performances des méthodes de classificationde façon très significative par rapport à l'état de l'art des techniquesde sélection de variables, notamment dans le cas de la classification de donnéestextuelles déséquilibrées, fortement multidimensionnelles et bruitées.	Jean-Charles Lamirel, Pascal Cuxac, Kafil Hajlaoui	http://editions-rnti.fr/render_pdf.php?p1&p=1001923	http://editions-rnti.fr/render_pdf.php?p=1001923	maximisation détiquetage Fmax métrique biaiséedestimation qualité dune classification superviser clustering favorisel cluster maximal fmesure détiquetage Danscet article montrer quune adaptation métrique dan cadrede classification superviser permettre réaliser sélection variable etde calculer deller fonction contraster méthode expérimentéesur type donnée textuel Dans contexte nousmontron technique améliorer performance méthode classificationde significatif rapport létat lart techniquesde sélection variable dan cas classification donnéestextuelle déséquilibrer fortement multidimensionnel bruiter
376	Revue des Nouvelles Technologies de l'Information	EGC	2014	Utilisation de relations ontologiques pour la comparaison d'images décrites par des annotations sémantiques	Face à la complexité des nouvelles générations d'images médicales, les processus de recherche d'images basés sur leurs contenus visuels peuvent s'avérer insuffisants. Cet article propose une nouvelle approche basée sur l'annotation des images via des termes sémantiques pouvant pallier ce problème. Elle repose sur la combinaison d'une distance hiérarchique permettant de comparer les images en considérant les corrélations entre les termes utilisés pour les décrire et d'une mesure de similarité permettant d'évaluer la proximité sémantique entre des termes ontologiques. Cette approche est validée dans le cadre de la recherche d'images tomodensitométriques.	Camille Kurtz, Daniel Rubin	http://editions-rnti.fr/render_pdf.php?p1&p=1001991	http://editions-rnti.fr/render_pdf.php?p=1001991	face complexité génération dimag médical processus rechercher dimages baser contenu visuel pouvoir savérer insuffisant article proposer approcher basé lannotation image terme sémantique pouvoir pallier problème reposer combinaison dune distancer hiérarchique permettre comparer image considérer corrélation entrer terme utiliser décrir dune mesurer similarité permettre dévaluer proximité sémantique entrer terme ontologique approcher valider dan cadrer rechercher dimager tomodensitométrique
377	Revue des Nouvelles Technologies de l'Information	EGC	2014	Vectorisation paramétrée des données textuelles	Automatic processing of textual data enables users to analyze semi-automatically and on alarge scale the data. This analysis is based on two successive processes: (i) representation oftexts, (ii) gathering of textual data (clustering). The software described in this paper focuses onthe first step of the process by offering expert a parameterized representation of textual data.	Célia Da Costa Pereira, Mathieu Lafourcade, Patrick Lloret, Cédric Lopez, Mathieu Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1001987	http://editions-rnti.fr/render_pdf.php?p=1001987	Automatic processing of textual dater enabl user to analyze semiautomatically and alarge scal the dater This analysis is based two successif processer ie representation oftexts ii gathering of textual dater clustering The software described in this paper focuse onthe first step of the process by offering expert parameterized representation of textual dater
378	Revue des Nouvelles Technologies de l'Information	EGC	2014	Vers une classification non supervisée adaptée pour obtenir des arbres de décision simplifiés	L'induction d'arbre de décision est une technique puissante et populairepour extraire de la connaissance. Néanmoins, les arbres de décision obtenusdepuis des données issues du monde réel peuvent être très complexes et donc difficilesà exploiter. Dans ce cadre, cet article présente une solution originale pouradapter le résultat d'une classification non supervisée quelconque afin d'obtenirdes arbres de décision simplifiés pour chaque cluster.	Olivier Parisot, Yoanne Didry, Pierrick Bruneau, Thomas Tamisier	http://editions-rnti.fr/render_pdf.php?p1&p=1001965	http://editions-rnti.fr/render_pdf.php?p=1001965	linduction darbre décision technique puissant populairepour extrair connaissance arbre décision obtenusdepuis donnée issu monder réel pouvoir complexe difficilesà exploiter Dans cadrer article présenter solution original pouradapter résultat dune classification superviser dobtenirde arbre décision simplifier cluster
379	Revue des Nouvelles Technologies de l'Information	EGC	2014	Vers une modularité pour données vectorielles	La modularité, introduite par Newman pour mesurer la qualité d'unepartition des sommets d'un graphe, ne prend pas en compte d'éventuelles valeursassociées à ces sommets. Dans cet article, nous introduisons une mesure de modularitécomplémentaire, basée sur l'inertie, et adaptée pour évaluer la qualitéd'une partition d'éléments représentés dans un espace vectoriel réel. Cette mesurese veut un pendant pour la classification non supervisée de la modularitéde Newman. Nous présentons également 2Mod-Louvain, une méthode utilisantce critère de modularité basée sur l'inertie conjointement à la modularité deNewman pour détecter des communautés dans des réseaux d'information. Lesexpérimentations que nous avons menées ont montré qu'en exploitant à la foisles données relationnelles et vectorielles, 2Mod-Louvain détectait plus efficacementles communautés que des méthodes utilisant un seul type de données etqu'elle était robuste face à des dégradations des données.	David Combe, Christine Largeron, Elod Egyed-Zsigmond, Mathias Géry	http://editions-rnti.fr/render_pdf.php?p1&p=1001914	http://editions-rnti.fr/render_pdf.php?p=1001914	modularité introduire Newman mesurer qualité dunepartition sommet dun graph prendre compter déventuell valeursassocier sommet Dans article introduire mesurer modularitécomplémentaire baser linertie adapté évaluer qualitédun partition déléments représenter dan espacer vectoriel réel mesurese vouloir pendre classification superviser modularitéde Newman présenter également 2modlouvain méthode utilisantce critèr modularité baser linertie conjointement modularité denewman détecter communauté dan réseau dinformation Lesexpérimentations mener montrer quen exploiter foisl donnée relationnel vectoriel 2ModLouvain détecter plaire efficacementl communauter méthode utiliser typer donnée etquell robuste face dégradation donnée
380	Revue des Nouvelles Technologies de l'Information	EGC	2014	Visualisation de données de prosopographie pour la reconstruction de carrières de personnages et de réseaux socio-professionnels	Dans cet article nous présentons deux approches de visualisation développéesdans le cadre d'un projet collaboratif sur l'accès et l'exploitation desdonnées prosopographiques de la Renaissance en France. L'objectif du projetest de modéliser et réaliser un portail sémantique assurant l'accès à différentesbases de données prosopographiques existantes afin de permettre une meilleureexploration et exploitation de ces données. Dans ce cadre, nous avons proposédeux interfaces de visualisation ProsoGraph et ProsoMap qui s'appuient respectivementsur la visualisation de graphes de réseaux sociaux et la visualisationde lieux géographiques et de trajectoires spatio-temporelles. Les deux interfacescommuniquent avec le portail via une couche sémantique et lui offrent des fonctionnalitésd'interrogation supplémentaires.	Nizar Messai, Thomas Devogele	http://editions-rnti.fr/render_pdf.php?p1&p=1001978	http://editions-rnti.fr/render_pdf.php?p=1001978	Dans article présenter approche visualisation développéesdans cadrer dun projet collaboratif laccè lexploitation desdonner prosopographiqu Renaissance France Lobjectif projetest modéliser réaliser portail sémantique assurer laccè différentesbase donnée prosopographiqu existant permettre meilleureexploration exploitation donnée Dans cadrer proposédeux interface visualisation ProsoGraph prosomap sappuient respectivementsur visualisation graphe réseau social visualisationde lieu géographique trajectoire spatiotemporell interfacescommuniquer portail coucher sémantique luire offrir fonctionnalitésdinterrogation supplémentaire
381	Revue des Nouvelles Technologies de l'Information	EGC	2013	20 ans de découverte de motifs : une étude bibliographique quantitative	Depuis deux décennies, la découverte de motifs a été l'un des champs de recherche les plus actifs de l'exploration de données. Cet article en établit une étude bibliographique quantitative en nous appuyant sur 1030 publications issues de 5 conférences internationales majeures : KDD, PKDD, PAKDD, ICDM et SDM. Nous avons d'abord mesuré depuis 2005 un sévère ralentissement de l'activité de recherche dédiée à la découverte de motifs. Puis, nous avons quantifié les principales contributions en terme de langages, de contraintes et de représentations condensées de sorte à comprendre ce ralentissement et à esquisser les directions actuelles.	Arnaud Giacometti, Dominique Haoyuan Li, Arnaud Soulet	http://editions-rnti.fr/render_pdf.php?p1&p=1001830	http://editions-rnti.fr/render_pdf.php?p=1001830	Depuis décennie découvrir motif lun champ rechercher plaire actif lexploration donnée article établir étude bibliographique quantitatif appuyer 1030 publication issu 5 conférence international majeure   KDD PKDD pakdd ICDM SDM dabord mesurer 2005 sévère ralentissement lactivité rechercher dédier découvrir motif Puis quantifier principal contributer terme langage contrainte représentation condenser sortir comprendre ralentissement esquisser direction actuel
382	Revue des Nouvelles Technologies de l'Information	EGC	2013	3D : de nouvelles perspectives en fouille exploratoire avec la stéréoscopie	Si la 3D est un sujet de débat dans la communauté, les expériences sur lesquelles s'appuient les discussions concernent le plus souvent des restitutions visuelles basées sur une projection classique en perspective linéaire. L'objectif de cette communication est de renouveler le cadre expérimental en étudiant l'impact de l'ajout de la disparité binoculaire. Nous nous focalisons ici sur une tâche importante en analyse de réseaux : l'identification de communautés. Et nous comparons la 3D monoscopique et la 3D stéréoscopique à la fois pour la performance de résolution de la tâche et pour le comportement exploratoire à travers l'analyse du mouvement du pointeur de la souris et de la dynamique des modifications de points de vue sur les graphes. Nos résultats expérimentaux mettent en évidence des performances significativement meilleures pour la 3D stéréoscopique et des différences comportementales dans l'exploration avec un centrage plus important sur des zones restreintes en stéréoscopie.	Nicolas Greffard, Fabien Picarougne, Pascale Kuntz	http://editions-rnti.fr/render_pdf.php?p1&p=1001831	http://editions-rnti.fr/render_pdf.php?p=1001831	Si 3D débattre dan communauté expérience sappuier discussion concerner plaire restitution visuel baser projection classique perspectif linéaire Lobjectif communication renouveler cadrer expérimental étudier limpact lajout disparité binoculaire focaliser tâcher important analyser réseau   lidentification communauté Et comparer 3d monoscopiqu 3d stéréoscopique performance résolution tâcher comportement exploratoire travers lanalyse mouvement pointeur souris dynamique modification point graphe résultat expérimental mettre évidence performance significativement meilleure 3d stéréoscopique différence comportemental dan lexploration centrage plaire importer zone restreindre stéréoscopie
383	Revue des Nouvelles Technologies de l'Information	EGC	2013	A POS Tagger analysed in collaboration environments and literary texts	Part-of-speech (POS) tagging is often used in other modules of natural language processing and therefore the results of this process should be as precise as possible. Many different types of taggers have been developed to improve the accuracy of the results in the field of literature or newspapers. Nowadays when the internet is widespread, the environments for online collaboration as chats, forums, blogs, wikis have become important means of communication. The purpose of this research is to analyse the results of tagging the words obtained from the labelling of the words from the online collaboration environments and literary texts with the corresponding parts of speech. In the case of POS tagging, the ambiguities arise due to the fact that a word may have multiple morphological values depending on context.	Dumitru-Clementin Cercel, Stefan Trausan-Matu	http://editions-rnti.fr/render_pdf.php?p1&p=1001847	http://editions-rnti.fr/render_pdf.php?p=1001847	partofspeech POS tagging is often used in other modul of natural language processing and therefore the results of this process should be precise Many type of tagger hav been developed to improve the accuracy of the results in the field of literature or newspaper Nowadays when the internet is widespread the environment for onlin collaboration chat forum blog wiki hav become importer means of communication The purpose of this research is to analyser the results of tagging the word obtained from the labelling of the words from the onlin collaboration environment and literary text with the corresponding part of speech In the caser of POS tagging the ambiguiti aris to the fact that word may hav morphological valu depending context
384	Revue des Nouvelles Technologies de l'Information	EGC	2013	Accélération de la méthode des K plus proches voisins pour la catégorisation de textes	Parmi la panoplie de classificateurs utilisés dans la catégorisation de textes, nous nous intéressons à l'algorithme des k-voisins les plus proches. Ces performances le situent parmi les meilleures méthodes de catégorisation de textes. Toutefois, il présente certaines limites: (i) coût mémoire car il faut stocker l'ensemble d'apprentissage en entier et (ii) coût élevé de calcul car il doit explorer l'ensemble d'apprentissage pour classer un nouveau document. Dans ce papier, nous proposons une nouvelle démarche pour réduire ce temps de classification sans dégrader les performances de classification.	Fatiha Barigou, Baghdad Atmani, Youcef Bouziane, Naouel Barigou	http://editions-rnti.fr/render_pdf.php?p1&p=1001841	http://editions-rnti.fr/render_pdf.php?p=1001841	Parmi panoplie classificateur utiliser dan catégorisation texte intéresser lalgorithme kvoisin plaire performance situer meilleure méthode catégorisation texte présenter limite ie coût mémoir falloir stocker lensembl dapprentissage entier ii coût élever calcul devoir explorer lensembl dapprentissage classer document Dans papier proposer démarcher réduire temps classification dégrader performance classification
385	Revue des Nouvelles Technologies de l'Information	EGC	2013	Analyse conceptuelle de données de simulation de systèmes complexes pour l'aide à la décision : Application à la conception d'une cabine d'avion	Dans cet article nous présentons une approche conceptuelle d'aide à la décision dans la conception de systèmes complexes. Cette approche s'appuie sur le formalisme de l'analyse de concepts formels par similarité (ACFS) pour la classification, la visualisation et l'exploration de données de simulation afin d'aider les concepteurs de systèmes complexes à identifier les choix de conception les plus pertinents. L'approche est illustrée sur un cas test de conception de cabine d'un avion de ligne fourni par les partenaires industriels et qui consiste à étudier les données de simulation de différentes configurations du système de ventilation de la cabine afin d'identifier celles qui assurent un confort convenable pour les passagers la cabine. La classification des données de simulation avec leurs scores de confort en utilisant l'ACFS permet d'identifier pour chaque paramètre de conception simulé la plage de valeurs possibles qui assure un confort convenable pour les passagers. Les résultats obtenus ont été confirmés et validés par de nouvelles simulations.	Nizar Messai, Cassio Melo, Mohamed Hamdaoui, Dung Bui, Marie-Aude Aufaure	http://editions-rnti.fr/render_pdf.php?p1&p=1001835	http://editions-rnti.fr/render_pdf.php?p=1001835	Dans article présenter approcher conceptuel daid décision dan conception système complexe approcher sappui formalisme lanalyse concept formel similarité acf classification visualisation lexploration donnée simulation daider concepteur système complexe identifier choix conception plaire pertinent Lapproche illustrer cas test conception cabine dun avion ligne fournir partenaire industriel consister étudier donnée simulation configuration système ventilation cabine didentifier assurer confort convenable passager cabine classification donnée simulation score confort utiliser lacfs permettre didentifier paramètre conception simuler plage assurer confort convenable passager résultat obtenir confirmer valider simulation
386	Revue des Nouvelles Technologies de l'Information	EGC	2013	Analyse de réseaux sociaux par l'analyse formelle de concepts	L'analyse formelle de concepts (AFC) est un formalisme de représentation et d'extraction de connaissance fondé sur les notions de concepts et de treillis de concepts (Galois).L'AFC a été exploitée avec succès dans plusieurs domaines en informatique tels le génie logiciel, les bases et entrepôts de données, l'extraction et la gestion de la connaissance et dans plusieurs applications du monde réel comme la médecine, la psychologie, la linguistique et la sociologie.Dans cette présentation, nous allons explorer le potentiel de l'AFC et de quelques extensions de cette théorie (ex. analyse triadique de concepts) dans l'analyse de réseaux sociaux en vue de découvrir des connaissances à partir de réseaux homogènes simples (ex. détection de communautés et d'individus influents à partir d'un réseau d'amis) ou même de réseaux hétérogènes (ex. extraction de règles d'association d'un réseau bibliographique).	Rokia Missaoui	http://editions-rnti.fr/render_pdf.php?p1&p=1001814	http://editions-rnti.fr/render_pdf.php?p=1001814	lanalyse formel concept AFC formalisme représentation dextraction connaissance fonder notion concept treillis concept galoislafc exploiter succès dan domaine informatique génie logiciel base entrepôt donnée lextraction gestion connaissance dan application monder réel médecine psychologie linguistique sociologieDans présentation aller explorer potentiel lafc extension théorie ex analyser triadique concept dan lanalyse réseau social découvrir connaissance partir réseau homogène simple ex détection communauté dindividus influent partir dun réseau damis réseau hétérogène ex extraction règle dassociation dun réseau bibliographique
387	Revue des Nouvelles Technologies de l'Information	EGC	2013	Analyse des réclamations d'allocataires de la CAF : un cas d'étude en fouille de données	La gestion des réclamations est un élément fondamental dans la relation client. C'est le cas en particulier pour la Caisse Nationale des Allocations Familiales qui veut mettre en place une politique nationale pour faciliter cette gestion. Dans cet article, nous décrivons la démarche que nous avons adoptée afin de traiter automatiquement les réclamations provenant d'allocataires de la CAF du Rhône. Les données brutes mises à notre disposition nécessitent une série importante de prétraitements pour les rendre utilisables. Une fois ces données correctement nettoyées, des techniques issues de l'analyse des données et de l'apprentissage non supervisé nous permettent d'extraire à la fois une typologie des réclamations basée sur leur contenu textuel mais aussi une typologie des allocataires réclamants. Après avoir présenté ces deux typologies, nous les mettons en correspondance afin de voir comment les allocataires se distribuent selon les différents types de réclamation.	Sabine Loudcher, Julien Velcin, Vincent Forissier, Cyril Broilliard, Philippe Simonnot	http://editions-rnti.fr/render_pdf.php?p1&p=1001866	http://editions-rnti.fr/render_pdf.php?p=1001866	gestion réclamation élément fondamental dan relation client cest cas Caisse national allocation familiale vouloir mettre placer politique national faciliter gestion Dans article décrire démarcher adopter traiter automatiquement réclamation provenir dallocataire CAF Rhône donnée brut mise disposition nécessiter série important prétraitement utilisable donnée correctement nettoyer technique issu lanalyse donnée lapprentissage superviser permettre dextraire typologie réclamation baser contenir textuel typologie allocataire réclamant Après présenter typologie metton correspondance voir allocataire distribuer type réclamation
388	Revue des Nouvelles Technologies de l'Information	EGC	2013	Analyse Relationnelle de Concepts pour l'exploration de données relationnelles	L'Analyse Relationnelle de Concepts (ARC) est une extension de l'Analyse Formelle de Concepts (AFC), une méthode de classification non supervisée d'objets sous forme de treillis de concepts. L'ARC supporte en plus la gestion de relations entre objets de différents contextes ce qui permet d'établir des liens entre les concepts des différents treillis. Cette particularité lui permet d'être plus intuitive à utiliser pour extraire des connaissances à partir de données relationnelles et de donner des résultats plus riches. Malheureusement lorsque les jeux de données présentent de nombreuses relations, les résultats obtenus sont difficilement exploitables et des problèmes de passages à l'échelle se posent. Nous proposons dans cet article une adaptation possible de l'ARC pour explorer les relations de manière supervisée pour augmenter la pertinence des résultats obtenus et réduire le temps de calcul. Nous prenons pour exemple des données hydrobiologiques ayant trait à la qualité des milieux aquatiques.	Xavier Dolques, Florence Le Ber, Marianne Huchard, Clémentine Nebut	http://editions-rnti.fr/render_pdf.php?p1&p=1001829	http://editions-rnti.fr/render_pdf.php?p=1001829	lanalys relationnel Concepts ARC extension lanalyse Formelle Concepts AFC méthode classification superviser dobjets sou former treillis concept LARC support plaire gestion relation entrer objet contexte permettre détablir lien entrer concept treillis particularité luire permettre dêtre plaire intuitif utiliser extraire connaissance partir donnée relationnel donner résultat plaire riche malheureusement jeu donnée présenter relation résultat obtenir difficilement exploitable problème passage léchelle poser proposer dan article adaptation larc explorer relation manière superviser augmenter pertinence résultat obtenu réduire temps calcul prendre exemple donnée hydrobiologiqu traire qualité milieu aquatique
389	Revue des Nouvelles Technologies de l'Information	EGC	2013	Approche orientée objet sémantique et coopérative pour la classification des images de zones urbaines à très haute résolution	La classification orientée objet (COO) prend de plus en plus de dimension dans les travaux de télédétection grâce à sa capacité d'intégrer des connaissances de haut niveau telles que la taille, la forme et les informations de voisinage. Cependant, les approches existantes restent tributaires de l'étape de construction des objets à cause de l'absence d'interaction entre celle-ci et celle de leur identification. Dans cet article, nous proposons une approche sémantique, hiérarchique et collaborative entre les algorithmes de croissances de régions et une classification orientée objet supervisée, permettant une coopération entre l'extraction et l'identification des objets de l'image. Les expériences menées sur une image de très haute résolution de la région de Strasbourg ont confirmé l'intérêt de l'approche introduite.	Aymen Sellaouti, Atef Hamouda, Aline Deruyver, Cédric Wemmert	http://editions-rnti.fr/render_pdf.php?p1&p=1001827	http://editions-rnti.fr/render_pdf.php?p=1001827	classification orienter objet COO prendre plaire plaire dimension dan travail télédétection grâce capacité dintégrer connaissance niveau tailler former information voisinage approche existant ruer tributaire létape construction objet causer labsence dinteraction entrer celleci identification Dans article proposer approcher sémantique hiérarchique collaborative entrer algorithme croissance région classification orienter objet superviser permettre coopération entrer lextraction lidentification objet limage expérience mener imager résolution région Strasbourg confirmer lintérêt lapproche introduire
390	Revue des Nouvelles Technologies de l'Information	EGC	2013	Classification multi-étiquettes pour l'alignement multiple de séquences protéiques	Cet article présente une application de classification multi-étiquettes permettant de déterminer le programme à utiliser pour construire un alignement multiple d'un ensemble de séquences protéiques donné. Dans un premier temps, nous avons réussi à améliorer le système existant, Alexsys en ajoutant des attributs. Dans un second temps, nous déterminons pour un ensemble de séquences protéiques donné le ou les aligneurs capable de produire les alignements de meilleur score, à epsilon près. Les mesures de performances propres à la classification multi-étiquette nous permettent d'analyser l'influence de epsilon et de choisir une valeur assez petite pour distinguer les meilleurs aligneurs des autres.	Lina Fahed, Gabriel Frey, Julie Dawn Thompson, Nicolas Lachiche	http://editions-rnti.fr/render_pdf.php?p1&p=1001864	http://editions-rnti.fr/render_pdf.php?p=1001864	article présenter application classification multiétiquett permettre déterminer programmer utiliser construire alignement dun ensemble séquence protéique donner Dans temps réussir améliorer système exister Alexsys ajouter attribut Dans second temps déterminer ensemble séquence protéique donner aligneur capable produire alignement meilleur score epsilon mesure performance propre classification multiétiquett permettre danalyser linfluence epsilon choisir petit distinguer meilleur aligneur
391	Revue des Nouvelles Technologies de l'Information	EGC	2013	Classifications croisées de données de trajectoires contraintes par un réseau routier	Le clustering (ou classification non supervisée) de trajectoires a fait l'objet d'un nombre considérable de travaux de recherche. La majorité de ces travaux s'est intéressée au cas où les objets mobiles engendrant ces trajectoires se déplacent librement dans un espace euclidien et ne prennent pas en compte les contraintes liées à la structure sous-jacente du réseau qu'ils parcourent (ex. réseau routier). Dans le présent article, nous proposons au contraire la prise en compte explicite de ces contraintes. Nous représenterons les relations entre trajectoires et segments routiers par un graphe biparti et nous étudierons la classification de ses sommets. Nous illustrerons, sur un jeu de données synthétiques, l'utilité d'une telle étude pour comprendre la dynamique du mouvement dans le réseau routier et analyser le comportement des véhicules qui l'empruntent.	Mohamed K. El Mahrsi, Romain Guigourès, Fabrice Rossi, Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001854	http://editions-rnti.fr/render_pdf.php?p=1001854	clustering classification superviser trajectoire faire lobjet dun nombre considérable travail rechercher majorité travail sest intéressé cas objet mobile engendrer trajectoire déplacer librement dan espacer euclidien prendre compter contrainte lier structurer sousjacent réseau quils parcourir ex réseau routier Dans présent article proposer contraire priser compter expliciter contrainte représenter relation entrer trajectoire segment routier graph biparti étudier classification sommet illustrer jeu donnée synthétique lutilité dune étude comprendre dynamique mouvement dan réseau routier analyser comportement véhicule lemprunter
392	Revue des Nouvelles Technologies de l'Information	EGC	2013	Comprendre et interpréter les données : enjeux et implantations d'un système de codage dans des gisements de données historiques	L'accès croissant à une information pléthorique et le développement de gisements de données ambitieux posent aujourd'hui deux grands types de difficultés aux historiens.Le premier consiste à mettre en relation des gisements qui ont été développés de manière indépendante. C'est par exemple le cas pour l'intégration d'un ensemble de bases de données prosopographiques développées entre 1980 et 2010 au Lamop, ou même dans le cadre d'un projet dont le seul lien est une problématique spatiale et temporelle (projet ANR-DFG, Euroscientia).Le deuxième tient en la nature des données introduites dans ces différents systèmes : elles sont souvent hétérogènes, ambiguës, floues. Pour que le chercheur puisse se les approprier, les données doivent faire l'objet d'un véritable travail, afin de comprendre comment elles ont été obtenues, structurées. L'historien doit donc les évaluer et les valider s'il souhaite les mettre en relation. Cette évaluation nécessitant, elle-même de pouvoir être commentée, partagée et critiquée par d'autres chercheurs.Dans les deux cas, il est nécessaire de développer des outils d'appropriation, qui permettent d'entrer dans le réel historique contenu dans les stocks de données. C'est là la fonction du projet Histobase, un système permettant d'entrer dans la structuration des gisements, d'en évaluer l'information, d'ajouter des couches d'interprétation (qualification de l'information historique) de les évaluer et de partager les données « obtenues ». Chacune des analyses individuelles et collectives fait l'objet d'une mémorisation. Il faut pour cela laisser une place importante aux historiens en tant qu'expert en prêtant une attention particulière aux processus métiers qu'ils mettent en oeuvre.	Stéphane Lamassé, Julien Alerini	http://editions-rnti.fr/render_pdf.php?p1&p=1001816	http://editions-rnti.fr/render_pdf.php?p=1001816	laccè croître information pléthorique développement gisement donnée ambitieux poser aujourdhui grand type difficulté historiensle consister mettre relation gisement développer manière indépendant cest exemple cas lintégration dun ensemble base donnée prosopographiqu développer entrer 1980 2010 Lamop dan cadrer dun projet lien problématique spatial temporel projet anrdfg euroscientiale nature donnée introduit dan système   hétérogène ambiguë flou Pour chercheur pouvoir approprier donnée devoir faire lobjet dun véritable travail comprendre obtenu structurer Lhistorien devoir évaluer valider sil souhaiter mettre relation évaluation nécessiter ellemême pouvoir commenter partager critiquer dautre chercheursDans cas nécessaire développer outil dappropriation permettre dentrer dan réel historique contenir dan stock donnée cest fonction projet Histobase système permettre dentrer dan structuration gisement den évaluer linformation dajouter couche dinterprétation qualification linformation historique évaluer partager donnée « obtenu » analyse individuel collective faire lobjet dune mémorisation falloir celer placer important historien quexpert prêter attention processus métier quil mettre oeuvrer
393	Revue des Nouvelles Technologies de l'Information	EGC	2013	Construction de descripteurs à partir du coclustering pour la classification supervisée de séries temporelles	Nous présentons un processus de construction de descripteurs pour la classification supervisée de séries temporelles. Ce processus est libre de tout paramétrage utilisateur et se décompose en trois étapes : (i) à partir des données originales, nous générons de multiples nouvelles représentations simples ; (ii) sur chacune de ces représentations, nous appliquons un algorithme de coclustering ; (iii) à partir des résultats de co-clustering, nous construisons de nouveaux descripteurs pour les séries temporelles. Nous obtenons une nouvelle base de données objets-attributs dont les objets (identifiant les séries temporelles) sont décrits par des attributs issus des diverses représentations générées. Nous utilisons un classifieur Bayésien sur cette nouvelle base de données. Nous montrons expérimentalement que ce processus offre de très bonnes performances prédictives comparées à l'état de l'art.	Dominique Gay, Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001855	http://editions-rnti.fr/render_pdf.php?p=1001855	présenter processus construction descripteur classification superviser série temporel processus libre paramétrage utilisateur décomposer étape   ie partir donnée original générer représentation simple   ii représentation appliquer algorithme coclustering   iii partir résultat coclustering construire descripteur série temporel obtenir baser donnée objetsattributs objet identifier série temporel décrire attribut issu représentation générer utiliser classifieur bayésien baser donnée montrer expérimentalement processus offrir performance prédictif comparer létat lart
394	Revue des Nouvelles Technologies de l'Information	EGC	2013	Découverte des soft-skypatterns avec une approche PPC	Les skypatterns sont des motifs traduisant des préférences de l'utilisateur selon une relation de dominance. Dans cet article, nous introduisons la notion de souplesse dans la problématique des skypatterns et nous montrons comment celle-ci permet de découvrir des motifs intéressants qui seraient manqués autrement. Nous proposons une méthode efficace d'extraction de skypatterns ainsi que de soft-skypatterns, méthode fondée sur la programmation par contraintes. La pertinence de notre approche est illustrée à travers une étude de cas en chémoinformatique pour la découverte de toxicophores.	Willy Ugarte, Patrice Boizumault, Samir Loudni, Bruno Crémilleux, Alban Lepailleur	http://editions-rnti.fr/render_pdf.php?p1&p=1001838	http://editions-rnti.fr/render_pdf.php?p=1001838	skypattern motif traduire préférence lutilisateur relation dominance Dans article introduire notion souplesse dan problématique skypattern montrer celleci permettre découvrir motif intéressant manquer proposer méthode efficace dextraction skypattern softskypattern méthod fonder programmation contraint pertinence approcher illustrer travers étude cas chémoinformatiqu découvrir toxicophore
395	Revue des Nouvelles Technologies de l'Information	EGC	2013	Detecting Academic Plagiarism with Graphs	In this paper, we tackle the problem of detecting academic plagiarism, which is considered as a severe problem owing to the convenience of online publishing. Typical information retrieval methods, stopword-based methods and ngerprinting methods, are commonly used to detect plagiarism by using the sequence of words as they appear in the article. As such, they fail to detect plagiarism when an author reconstructs a source article by re-ordering and recombining phrases. Because graph structure ts for representing relationships between entities, we propose a novel plagiarism detection method, in which we use graphs to represent documents by modeling grammatical relationships between words. Experimental results show that our proposed method outperforms two n-gram methods and increases recall values by 10 to 20%.	Bin-Hui Chou, Einoshin Suzuki	http://editions-rnti.fr/render_pdf.php?p1&p=1001848	http://editions-rnti.fr/render_pdf.php?p=1001848	in this paper we tackle the problem of detecting academic plagiarism which is considered severe problem owing to the convenience of online publishing Typical information retrieval method stopwordbased methods and ngerprinting method are commonly used to detect plagiarism by using the sequence of word they appear in the article such they fail to detect plagiarism when an author reconstruct source article by reordering and recombining phras Because graph structurer ts for representing relationships between entitier we proposer novel plagiarism detection method in which we user graph to represent document by modeling grammatical relationships between word Experimental results show that our proposed method outperform two ngram methods and increas recall valu by 10 to 20
396	Revue des Nouvelles Technologies de l'Information	EGC	2013	Détection efficace des traverses minimales d'un hypergraphe par élimination de la redondance	L'extraction des traverses minimales d'un hypergraphe est une problématique réputée comme particulièrement difficile et qui a fait l'objet de plusieurs travaux dans la littérature. Dans cet article, nous établissons un lien entre les concepts de la fouille de données et ceux de la théorie des hypergraphes, proposant ainsi un cadre méthodologique pour le calcul des traverses minimales. Le nombre de ces traverses minimales étant, souvent, exponentiel même pour des hypergraphes simples, nous proposons d'en représenter l'ensemble de manière concise et exacte. Pour ce faire, nous introduisons la notion de traverses minimales irrédondantes, à partir desquelles nous pouvons retrouver l'ensemble global de toutes les traverses minimales, à l'aide de l'algorithme IMT-EXTRACTOR. Une étude expérimentale de ce nouvel algorithme a confirmé l'intérêt de l'approche introduite.	Mohamed Nidhal Jelassi, Christine Largeron, Sadok Ben Yahia	http://editions-rnti.fr/render_pdf.php?p1&p=1001833	http://editions-rnti.fr/render_pdf.php?p=1001833	lextraction traverse minimal dun hypergraph problématique réputer difficile faire lobjet travail dan littérature Dans article établir lien entrer concept fouiller donnée théorie hypergraphe proposer cadrer méthodologique calcul traverse minimal nombre traverse minimal exponentiel hypergraphe simple proposer den représenter lensembl manière concis exact Pour faire introduire notion traverse minimal irrédondant partir desquelle pouvoir retrouver lensembl global traverse minimal laid lalgorithm imtextractor étude expérimental nouvel algorithme confirmer lintérêt lapproche introduire
397	Revue des Nouvelles Technologies de l'Information	EGC	2013	Détection précoce de tendances produits dans le cadre des activités commerciales de la grande distribution	"Dans ce papier, nous présentons une nouvelle approche qui permet la détection précoce de tendances ""produits"" dans le cadre des activités commerciales de la grande distribution. S'agissant d'un domaine où la concurrence est très vive entre les différentes enseignes avec des enjeux financiers colossaux, les stratégies commerciales ont pour principal objectif de fidéliser la clientèle pour limiter leur défection. C'est là qu'intervient la détection des changements de tendances produits, qui va permettre d'anticiper l'attrition de la clientèle. Déceler des tendances suffisamment tôt permettra aux décideurs de mettre en place des stratégies préventives efficaces à moindre coût. Notre objectif est donc d'analyser et de modéliser clairement les changements de tendances et leurs impacts potentiels globaux sur les achats des clients. Nous illustrerons notre approche sur des données réelles d'achats de clients d'une grande enseigne."	Gaël Bardury, Jean-Emile Symphor	http://editions-rnti.fr/render_pdf.php?p1&p=1001840	http://editions-rnti.fr/render_pdf.php?p=1001840	Dans papier présenter approcher permettre détection précoce tendance produire dan cadrer activité commercial grand distribution sagisser dun domaine concurrencer entrer enseigne enjeu financier colossal stratégie commercial principal objectif fidéliser clientèle limiter défection cest quintervient détection changement tendance produit aller permettre danticiper lattrition clientèle déceler tendance suffisamment tôt permettre décideur mettre placer stratégie préventif efficace moindre coût objectif danalyser modéliser clairement changement tendance impact potentiel global achat client illustrer approcher donnée réel dachat client dune grand enseigner
398	Revue des Nouvelles Technologies de l'Information	EGC	2013	Enrichissement d'ontologies grâce à l'annotation sémantique de pages web	Nous présentons une approche pour enrichir automatiquement une ontologie à partir d'un ensemble de pages web structurées. Cette approche s'appuie sur un noyau d'ontologie initial. Son originalité est d'exploiter conjointement la structure des documents et des annotations sémantiques produites à l'aide du noyau d'ontologie pour identifier de nouveaux concepts et des spécialisations de relations qui enrichissent l'ontologie. Nous avons implémenté et évalué ce processus en réalisant une ontologie de plantes à partir de fiches de jardinage.	Nathalie Aussenac-Gilles, Davide Buscaldi, Catherine Comparot, Mouna Kamel	http://editions-rnti.fr/render_pdf.php?p1&p=1001839	http://editions-rnti.fr/render_pdf.php?p=1001839	présenter approcher enrichir automatiquement ontologie partir dun ensemble page web structurer approcher sappui noyau dontologie initial originalité dexploiter conjointement structurer document annotation sémantique produire laid noyau dontologie identifier concept spécialisation relation enrichir lontologie implémenter évaluer processus réaliser ontologie plante partir fiche jardinage
399	Revue des Nouvelles Technologies de l'Information	EGC	2013	Étude des corrélations spatio-temporelles des appels mobiles en France	Nous proposons dans cet article de présenter une application d'analyse d'une base de données de grande taille issue du secteur des télécommunications. Le problème consiste à segmenter un territoire et caractériser les zones ainsi définies grâce au comportement des habitants en terme de téléphonie mobile. Nous disposons pour cela d'un réseau d'appels inter-antennes construit pendant une période de cinq mois sur l'ensemble de la France. Nous proposons une analyse en deux phases. La première couple les antennes émettrices dont les appels sont similairement distribués sur les antennes réceptrices et vice versa. Une projection de ces groupes d'antennes sur une carte de France permet une visualisation des corrélations entre la géographie du territoire et le comportement de ses habitants en terme de téléphonie. La seconde phase découpe l'année en périodes entre lesquelles on observe un changement de distributions d'appels sortant des groupes d'antennes. On peut ainsi caractériser l'évolution temporelle du comportement des usagers de mobiles dans chacune des zones du pays.	Romain Guigourès, Marc Boullé, Fabrice Rossi	http://editions-rnti.fr/render_pdf.php?p1&p=1001865	http://editions-rnti.fr/render_pdf.php?p=1001865	proposer dan article poster application danalyse dune baser donnée grand tailler issu secteur télécommunication problème consister segmenter territoire caractériser zone définir grâce comportemer habitant terme téléphonie mobile disposer celer dun réseau dappel interantenn construire pendre période mois lensembl France proposer analyser phase coupler antenne émettrice appel similairement distribuer antenne réceptrice vice verser projection groupe dantenne carte France permettre visualisation corrélation entrer géographie territoire comportement habitant terme téléphonie second phase découper lanner période entrer observer changement distribution dappel sortir groupe dantenne pouvoir caractériser lévolution temporel comportement usager mobile dan zone pays
400	Revue des Nouvelles Technologies de l'Information	EGC	2013	Étude des techniques d'oubli dans les moindres carrés récursifs pour l'apprentissage incrémental de systèmes d'inférence floue évolutifs : application à la reconnaissance de formes	Cet article étudie les possibilités d'utilisation d'oubli dans l'apprentissage incrémental en-ligne de classifieurs évolutifs basés sur des systèmes d'inférence floue. Pour cela, nous étudions différentes possibilités, existant dans la littérature dédiée au contrôle, pour introduire de l'oubli dans l'algorithme des moindres carrés récursifs. Nous présentons l'impact de ces différentes techniques dans le contexte de l'apprentissage incrémental de classifieurs évolutifs en environnement non stationnaire. Ces approches sont évaluées, pour l'optimisation des systèmes d'inférence floue, sur la problématique de la reconnaissance de gestes manuscrits sur surface tactile.	Manuel Bouillon, Eric Anquetil, Abdullah Almaksour	http://editions-rnti.fr/render_pdf.php?p1&p=1001818	http://editions-rnti.fr/render_pdf.php?p=1001818	article étudier possibilité dutilisation doubli dan lapprentissage incrémental enlign classifieur évolutif baser système dinférence flouer Pour celer étudier possibilité exister dan littérature dédier contrôler introduire loubli dan lalgorithme moindre carré récursif présenter limpact technique dan contexte lapprentissage incrémental classifieur évolutif environnement stationnaire approche évaluer loptimisation système dinférence flouer problématique reconnaissance geste manuscrit surface tactile
401	Revue des Nouvelles Technologies de l'Information	EGC	2013	Évolution d'une ontologie dédiée à la représentation de relations n-aires	Nous nous intéressons dans cet article à la problématique d'évolution d'une ontologie permettant de représenter des relations n-aires. Nous présentons la représentation formelle des changements applicables à notre ontologie permettant de modifier sa structure tout en maintenant sa cohérence structurelle. Nous illustrerons nos propos sur une ontologie dédiée à la représentation de relations n-aires entre des données expérimentales quantitatives.	Rim Touhami, Patrice Buche, Juliette Dibie-Barthélemy, Liliana Ibanescu	http://editions-rnti.fr/render_pdf.php?p1&p=1001862	http://editions-rnti.fr/render_pdf.php?p=1001862	intéresser dan article problématique dévolution dune ontologie permettre représenter relation nair présenter représentation formel changement applicable ontologie permettre modifier structurer maintenir cohérence structurel illustrer propos ontologie dédier représentation relation nair entrer donnée expérimental quantitatif
402	Revue des Nouvelles Technologies de l'Information	EGC	2013	Extraction de motifs condensés dans un unique graphe orienté acyclique attribué	Les graphes orientés acycliques attribués peuvent être utilisés dans beaucoup de domaines applicatif. Dans ce papier, nous étudions un nouveau domaine de motif pour permettre leur analyse : les chemins pondérés fréquents. Nous proposons en conséquence des contraintes primitives permettant d'évaluer leur pertinence (par exemple, les contraintes de fréquence et de compacité), et un algorithme extrayant ces solutions. Nous aboutissons à une représentation condensée dont l'efficacité et le passage à l'échelle sont étudiés empiriquement.	Jérémy Sanhes, Frédéric Flouvat, Claude Pasquier, Nazha Selmaoui-Folcher	http://editions-rnti.fr/render_pdf.php?p1&p=1001837	http://editions-rnti.fr/render_pdf.php?p=1001837	graphe orienté acyclique attribuer pouvoir utiliser dan domaine applicatif Dans papier étudier domaine motif permettre analyser   chemin pondéré fréquent proposer conséquence contrainte primitif permettre dévaluer pertinence exemple contrainte fréquence compacité algorithme extraire solution aboutir représentation condenser lefficacité passage léchelle étudier empiriquement
403	Revue des Nouvelles Technologies de l'Information	EGC	2013	Extraction de motifs fréquents dans des arbres attribués	L'extraction de motifs fréquents est une tâche importante en fouille de données. Initialement centrés sur la découverte d'ensembles d'items fréquents, les premiers travaux ont été étendus pour extraire des motifs structurels comme des séquences, des arbres ou des graphes. Dans cet article, nous proposons une nouvelle méthode de fouille de données qui consiste à extraire de nouveaux types de motifs à partir d'une collection d'arbres attribués. Les arbres attribués sont des arbres dans lesquels les noeuds sont associés à des ensembles d'attributs. L'extraction de ces motifs (appelés sous-arbres attribués) combine une recherche d'ensembles d'items fréquents à une recherche de sous-arbres et nécessite d'explorer un immense espace de recherche. Nous présentons plusieurs nouveaux algorithmes d'extraction d'arbres attribués et montrons que leurs implémentations peuvent efficacement extraire des motifs fréquents à partir de grands jeux de données.	Claude Pasquier, Jérémy Sanhes, Frédéric Flouvat, Nazha Selmaoui-Folcher	http://editions-rnti.fr/render_pdf.php?p1&p=1001836	http://editions-rnti.fr/render_pdf.php?p=1001836	lextraction motif fréquent tâcher important fouiller donnée initialement centrer découvrir densembler ditems fréquent travail étendre extraire motif structurel séquence arbre graphe Dans article proposer méthode fouiller donnée consister extraire type motif partir dune collection darbr attribuer arbre attribuer arbre dan lesquel noeud associé ensemble dattribut lextraction motif appeler sousarbre attribuer combin rechercher densembl ditems fréquent rechercher sousarbre nécessiter dexplorer immense espac rechercher présenter algorithme dextraction darbr attribuer montron implémentation pouvoir efficacement extraire motif fréquent partir grand jeu donnée
404	Revue des Nouvelles Technologies de l'Information	EGC	2013	Extraction des nombres de Betti avec un modèle génératif	L'analyse exploratoire de données multidimensionnelles est un problème complexe. Nous proposons d'extraire certains invariants topologiques appelés nombre de Betti, pour synthétiser la topologie de la structure sous-jacente aux données. Nous définissons un modèle génératif basé sur le complexe simplicial de Delaunay dont nous estimons les paramètres par l'optimisation du critère d'information Bayésien (BIC). Ce Complexe Simplicial Génératif nous permet d'extraire les nombres de Betti de données jouets et d'images d'objets en rotation. Comparé à la technique géométrique des Witness Complex, le CSG apparait plus robuste aux données bruitées.	Maxime Maillot, Michaël Aupetit, Gérard Govaert	http://editions-rnti.fr/render_pdf.php?p1&p=1001826	http://editions-rnti.fr/render_pdf.php?p=1001826	lanalyse exploratoire donnée multidimensionnel problème complexe proposer dextraire invariant topologique appeler nombre Betti synthétiser topologie structurer sousjacent donnée définir modeler génératif baser complexer simplicial Delaunay estimer paramètre loptimisation critère dinformation Bayésien BIC Complexe Simplicial Génératif permettre dextraire nombre Betti donnée jouet dimag dobjet rotation comparer technique géométrique Witness Complex CSG apparer plaire robuste donnée bruiter
405	Revue des Nouvelles Technologies de l'Information	EGC	2013	Extraction et filtrage de syntagmes nominaux pour la Recherche d'Information	Nous proposons dans cet article un Système de Recherche d'Information (SRI) qui se base sur des techniques d'indexation de textes en langue naturelle. Nous présentons une méthode d'indexation de documents qui repose sur une approche hybride pour la sélection de descripteurs textuels. Cette approche emploie des traitements du langage naturel pour l'extraction des syntagmes nominaux et sur un filtrage statistique basé sur l'information mutuelle pour sélectionner les syntagmes nominaux les plus informatifs pour le processus d'indexation. Nous effectuons des expérimentations en utilisant le corpus Le Monde 94 de la collection CLEF 2001 et sur le SRI Lemur pour évaluer l'approche proposée.	Chedi Bechikh Ali, Hatem Haddad	http://editions-rnti.fr/render_pdf.php?p1&p=1001842	http://editions-rnti.fr/render_pdf.php?p=1001842	proposer dan article système rechercher dinformation sri baser technique dindexation texte langue présenter méthode dindexation document reposer approcher hybride sélection descripteur textuel approcher employer traitement langage lextraction syntagme nominal filtrage statistique baser linformation mutuel sélectionner syntagme nominal plaire informatif processus dindexation effectuer expérimentation utiliser corpus Monde 94 collection CLEF 2001 SRI Lemur évaluer lapproche proposer
406	Revue des Nouvelles Technologies de l'Information	EGC	2013	Extraction optimisée de Règles d'Association Positives et Négatives (RAPN)	La littérature s'est beaucoup intéressée à l'extraction de règles d'association positives et peu à l'extraction de règles négatives en raison essentiellement du coût de calculs et du nombre prohibitif de règles extraites qui sont pour la plupart redondantes et inintéressantes. Dans cet article, nous nous sommes intéressés aux algorithmes d'extraction de RAPN (Règles d'Association Positives et Négatives) reposant sur l'algorithme fondateur Apriori. Nous avons fait une étude de ceux-ci en mettant en évidence leurs avantages et leurs inconvénients. A l'issue de cette étude, nous avons proposé un nouvel algorithme qui améliore cette extraction au niveau du nombre et de la qualité des règles extraites et au niveau du parcours de recherche des règles. L'étude s'est terminée par une évaluation de cet algorithme sur plusieurs bases de données.	Sylvie Guillaume, Pierre-Antoine Papon	http://editions-rnti.fr/render_pdf.php?p1&p=1001832	http://editions-rnti.fr/render_pdf.php?p=1001832	littérature sest intéressé lextraction règle dassociation positif lextraction règle négatif raison essentiellement coût calcul nombre prohibitif règle extrait redondant inintéressanter Dans article intéresser algorithm dextraction rapn Règles dassociation positif négative reposer lalgorithme fondateur Apriori faire étude ceuxci mettre évidence avantage inconvénient A lissue étude proposer nouvel algorithme améliorer extraction niveau nombre qualité règle extrait niveau parcours rechercher règle Létude sest terminer évaluation algorithme base donnée
407	Revue des Nouvelles Technologies de l'Information	EGC	2013	Grille bivariée pour la détection de changement dans un flux étiqueté	Nous présentons une méthode en-ligne de détection de changement de concept dans un flux étiqueté. Notre méthode de détection est basée sur un critère supervisé bivarié qui permet d'identifier si les données de deux fenêtres proviennent ou non de la même distribution. Notre méthode a l'intérêt de n'avoir aucun a priori sur la distribution des données, ni sur le type de changement et est capable de détecter des changements de différentes natures (changement dans la moyenne, dans la variance...). Les expérimentations montrent que notre méthode est plus performante et robuste que les méthodes de l'état de l'art testées. De plus, à part la taille des fenêtres, elle ne requiert aucun paramètre utilisateur.	Christophe Salperwyck, Marc Boullé, Vincent Lemaire	http://editions-rnti.fr/render_pdf.php?p1&p=1001859	http://editions-rnti.fr/render_pdf.php?p=1001859	présenter méthode enlign détection changement concept dan flux étiqueter méthode détection baser critère superviser bivarié permettre didentifier donnée fenêtre provenir distribution méthode lintérêt navoir priori distribution donnée typer changement capable détecter changement nature changement dan moyenner dan variance expérimentation montrer méthode plaire performant robuste méthode létat lart tester De plaire partir tailler fenêtre requérir paramètre utilisateur
408	Revue des Nouvelles Technologies de l'Information	EGC	2013	Identification de compatibilités entre descripteurs de lieux et apprentissage automatique	Les travaux présentés dans cet article s'inscrivent dans le paradigme des recherches visant à acquérir des relations sémantiques à partir de folksonomies (ensemble de tags attribués à des ressources par des utilisateurs). Nous expérimentons plusieurs approches issues de l'état de l'art ainsi que l'apport de l'apprentissage automatique pour l'identification de relations entre tags. Nous obtenons dans le meilleur des cas un taux d'erreur de 23,7 % (relations non reconnues ou fausses), ce qui est encourageant au vu de la difficulté de la tâche (les annotateurs humains ont un taux de désaccord de 12%).	Estelle Delpech, Laurent Candillier, Léa Laporte, Samuel Phan	http://editions-rnti.fr/render_pdf.php?p1&p=1001850	http://editions-rnti.fr/render_pdf.php?p=1001850	travail présenter dan article sinscrivent dan paradigme recherche viser acquérir relation sémantique partir folksonomie ensemble tag attribuer ressource utilisateur expérimenter approche issu létat lart lapport lapprentissage automatique lidentification relation entrer tag obtenir dan meilleur cas taux derreur 237   relation reconnu faux encourager voir difficulté tâcher annotateur humain taux désaccord 12
409	Revue des Nouvelles Technologies de l'Information	EGC	2013	Identification de complexes protéine-protéine par combinaison de classifieurs. Application à Escherichia Coli	Nous proposons une approche permettant de prédire des complexes impliquant trois protéines (appelés trimères) à partir de combinaison de classifieurs appris sur des complexes n'impliquant que deux protéines (dimères). La prédiction de ces trimères repose sur deux hypothèses biologiques : (i) deux protéines orthologues présentent des caractéristiques fonctionnelles similaires; (ii) deux protéines interagissant sous la forme d'un complexe sous-tendent une fonction biologique essentielle à l'espèce concernée. Ces deux hypothèses sont exploitées pour décrire chaque paire de protéines par l'ensemble des espèces pour lesquelles elles possèdent un orthologue. Un ensemble de mesures de qualité classiquement utilisées pour évaluer l'intérêt des règles d'association est utilisé pour évaluer la force du lien entre les deux protéines. L'organisme modèle Escherichia Coli a été utilisé pour évaluer notre approche.	Thomas Bourquard, Damien M. de Vienne, Jérôme Azé	http://editions-rnti.fr/render_pdf.php?p1&p=1001863	http://editions-rnti.fr/render_pdf.php?p=1001863	proposer approcher permettre prédire complexe impliquer protéine appeler trimère partir combinaison classifieur apprendre complexe nimpliquer protéine dimère prédiction trimère reposer hypothèse biologique   ie protéine orthologu présenter caractéristique fonctionnel similaire ii protéine interagir sou former dun complexe soustender fonction biologique essentiel lespèce concerner hypothèse exploiter décrire pair protéine lensembl espèce posséder orthologue ensemble mesure qualité classiquemer utiliser évaluer lintérêt règle dassociation utiliser évaluer forcer lien entrer protéine lorganisme modeler Escherichia Coli utiliser évaluer approcher
410	Revue des Nouvelles Technologies de l'Information	EGC	2013	Inférence de réseaux biologiques : un défi pour la fouille de données structurées	"La réponse cellulaire d'un organisme vivant à un signal donné, hormone, stress ou médicament, met en jeu des mécanismes complexes d'interaction et de régulation entre les gènes, les ARN messagers, les protéines et d'autres éléments tels que les micro-ARNs. On parle de réseau d'interaction pour décrire l'ensemble des interactions possibles entre protéines et de réseau de régulation génique pour représenter un ensemble de régulations entre gènes. Identifier ces interactions et ces régulations ouvre la porte à une meilleure compréhension du vivant et permet d'envisager de mieux soigner par le biais du ciblage thérapeutique. Puisque les techniques expérimentales de mesure à grande échelle, récemment développées, fournissent des données d'observation de ces réseaux, ce problème d'identification de réseau, généralement appelé inférence de réseau en biologie des systèmes, s'inscrit dans le cadre général de la fouille de données et plus particulièrement de l'apprentissage artificiel. Voilà maintenant quelques années que cette problématique a été posée à notre communauté et durant lesquelles les échanges entre biologistes et informaticiens ont non seulement permis aux biologistes d'étoffer leurs boîtes à outils mais aussi aux informaticiens de concevoir de nouvelles méthodes de fouille de données.En partant des deux problématiques distinctes que sont l'inférence de réseau d'interaction et l'inférence de réseau de régulation, je montrerai que ces deux tâches d'apprentissage posent, chacune de manière différente, la problématique de la prédiction de sorties structurées. L'inférence de réseau d'interaction entre protéines, vue comme un problème transductif de prédiction de liens, peut être résolue comme un problème d'apprentissage d'un noyau de sortie à partir d'un noyau d'entrée. L'inférence de réseau de régulation, impliquant la modélisation d'un système dynamique, peut être abordée par l'approximation parcimonieuse et structurée de fonctions à valeurs vectorielles. Je présenterai un ensemble de nouveaux outils de régression à sortie dans un espace de Hilbert, fondés sur des noyaux à valeur opérateur, qui fournissent d'excellents résultats en inférence de réseaux biologiques. Des expériences in silico sur des données artificielles, chez la levure du boulanger ou chez l'homme illustreront mes propos. En fin d'exposé, je tracerai quelques perspectives concernant les "" nouveaux "" défis dans le domaine de la bioinformatique et dans celui de la prédiction de sorties structurées."	Florence D'Alche-Buc	http://editions-rnti.fr/render_pdf.php?p1&p=1001815	http://editions-rnti.fr/render_pdf.php?p=1001815	réponse cellulaire dun organisme vivre signal donner hormone stres médicament mettre jeu mécanisme complexe dinteraction régulation entrer gène arn messager protéine dautr élément microarns réseau dinteraction décrire lensembl interaction entrer protéine réseau régulation génique représenter ensemble régulation entrer gène Identifier interaction régulation ouvrir porter meilleur compréhension vivre permettre denvisager mieux saler biais ciblage thérapeutique Puisque technique expérimental mesurer grand échelle récemment développer fournir donnée dobservation réseau problème didentification réseau généralement appeler inférence réseau biologie système sinscrit dan cadrer général fouiller donnée plaire lapprentissage artificiel Voilà maintenir année problématique poser communauté durer échange entrer biologist informaticien permettre biologist détoffer boîte outil informaticien concevoir méthode fouiller donnéesEn partir problématique distinct linférence réseau dinteraction linférence réseau régulation montrer tâche dapprentissage poser manière problématique prédiction sortie structurer Linférence réseau dinteraction entrer protéine voir problème transductif prédiction lien pouvoir résoudre problème dapprentissage dun noyau sortir partir dun noyau dentré Linférence réseau régulation impliquer modélisation dun système dynamique pouvoir aborder lapproximation parcimonieux structurer fonction vectoriel présenter ensemble outil régression sortir dan espacer Hilbert fonder noyau opérateur fournir dexcellent résultat inférence réseau biologique expérience in silico donnée artificiel levure boulanger lhomme illustrer propos En fin dexposé tracer perspective concerner     défi dan domaine bioinformatiqu dan prédiction sortie structurer
411	Revue des Nouvelles Technologies de l'Information	EGC	2013	Les capitalistes sociaux sur Twitter : détection via des mesures de similarité	Les réseaux sociaux tels que Twitter font partie du phénomène de Déluge des données, expression utilisée pour décrire l'apparition de données de plus en plus volumineuses et complexes. Pour représenter ces réseaux, des graphes orientés sont souvent utilisés. Dans cet article, nous nous focalisons sur deux aspects de l'analyse du réseau social de Twitter. En premier lieu, notre but est de trouver une méthode efficace et haut niveau pour stocker et manipuler le graphe du réseau social en utilisant des ressources informatiques raisonnables. Cet axe de recherche constitue un enjeu majeur puisqu'il est ainsi possible de traiter des graphes à échelle réelle sur des machines potentiellement accessibles par tous. Ensuite, nous étudions les capitalistes sociaux, un type particulier d'utilisateurs de Twitter observé par Ghosh et al. (2012). Nous proposons une méthode pour détecter et classifier efficacement ces utilisateurs.	Nicolas Dugué, Anthony Perez	http://editions-rnti.fr/render_pdf.php?p1&p=1001852	http://editions-rnti.fr/render_pdf.php?p=1001852	réseau social twitter faire partir phénomène Déluge donnée expression utiliser décrir lapparition donnée plaire plaire volumineux complexe Pour représenter réseau graphe orienter utiliser Dans article focaliser aspect lanalyse réseau social twitter En lieu boire trouver méthode efficace niveau stocker manipuler graphe réseau social utiliser ressource informatique raisonnable axer rechercher constituer enjeu majeur puisquil traiter graphe échelle réel machine potentiellement accessible ensuite étudier capitaliste social typer dutilisateur Twitter observer Ghosh al 2012 proposer méthode détecter classifier efficacement utilisateur
412	Revue des Nouvelles Technologies de l'Information	EGC	2013	Modèle de Recherche d'Information Sociale Centré Utilisateur	L'émergence des réseaux sociaux a révolutionné leWeb en permettant notamment aux individus de prolonger leur connexion virtuelle en une relation plus réelle et de partager leurs connaissances. Ce nouveau contexte de diffusion de l'information sur le Web peut constituer un moyen efficace pour cerner les besoins en information des utilisateurs du Web, et permettre à la recherche d'information (RI) de mieux répondre à ces besoins en adaptant les modèles d'indexation et d'interrogation. L'exploitation des réseaux sociaux confronte la RI à plusieurs défis dont les plus importants concernent la représentation de l'information dans ce modèle social de RI et son évaluation, en l'absence de collections de test et de compétitions dédiées. Dans cet article, nous présentons un modèle de RI sociale dans lequel nous proposons de modéliser et d'exploiter le contexte social de l'utilisateur. Nous avons évalué notre modèle à l'aide d'une collection de test de RI sociale construite à partir des annotations du réseau social de bookmarking collaboratif Delicious.	Chahrazed Bouhini, Mathias Géry, Christine Largeron	http://editions-rnti.fr/render_pdf.php?p1&p=1001846	http://editions-rnti.fr/render_pdf.php?p=1001846	lémergence réseau social révolutionner leweb permettre individu prolonger connexion virtuel relation plaire réel partager connaissance contexte diffusion linformation Web pouvoir constituer moyen efficace cerner besoin information utilisateur web permettre rechercher dinformation RI mieux répondre besoin adapter modèle dindexation dinterrogation lexploitation réseau social confront rire défi plaire important concerner représentation linformation dan modeler social RI évaluation labsence collection test compétition dédier Dans article présenter modeler RI social dan proposer modéliser dexploiter contexte social lutilisateur évaluer modeler laid dune collection test RI social construire partir annotation réseau social bookmarking collaboratif Delicious
413	Revue des Nouvelles Technologies de l'Information	EGC	2013	Non-disjoint grouping of text documents based Word Sequence Kernel	This paper deals with two issues in text clustering which are the detection of non disjoint groups and the representation of textual data. In fact, a text document can discuss several themes and then, it must belong to several groups. The learning algorithm must be able to produce non disjoint clusters and assigns documents to several clusters. The second issue concerns the data representation. Textual data are often represented as a bag of features such as terms, phrases or concepts. This representation of text avoids correlation between terms and doesn't give importance to the order of words in the text. We propose a non supervised learning method able to detect overlapping groups in text document by considering text as a sequence of words and using the Word Sequence Kernel as similarity measure. The experiments show that the proposed method outperforms existing overlapping methods using the bag of word representation in terms of clustering accuracy and detect more relevant groups in textual documents.	Chiheb-Eddine Ben N’Cir, Afef Zenned, Nadia Essoussi	http://editions-rnti.fr/render_pdf.php?p1&p=1001843	http://editions-rnti.fr/render_pdf.php?p=1001843	this paper deal with two issu in text clustering which are the detection of disjoindre group and the representation of textual dater In fact text document can discus several themer and then it must belong to several group The learning algorithm must be abl to produce disjoindre cluster and assigns document to several cluster The second issu concern the dater representation Textual dater are often represented bag of featur such term phras or concept This representation of text avoid correlation between term and doesnt giv importance to the order of words in the text We proposer supervised learning method abl to detect overlapping group in text document by considering text sequence of word and using the Word sequence Kernel similarity measure The experiment show that the proposed method outperform existing overlapping method using the bag of word representation in terms of clustering accuracy and detect more relever group in textual document
414	Revue des Nouvelles Technologies de l'Information	EGC	2013	Nouvelle approche de bi-partitionnement topologique	Dans ce papier, nous proposons une nouvelle approche topologique de bi-partitionnement (bi-clustering) appelée BiTM en utilisant les cartes autoorganisatrices. L'idée principale de l'approche est d'utiliser une seule carte pour le partitionnement simultané des lignes (observations) et des colonnes (variables). Contrairement aux approches utilisant les cartes topologiques, notre modèle ne nécessite pas de pré-traitement de la base de données. Ainsi, une nouvelle fonction de coût est proposée. De plus, BiTM fournit une visualisation topologique des blocs ou bi-clusters facilement interprétable. Les résultats obtenus sont très encourageants et prometteurs pour continuer dans cette optique.	Amine Chaibi, Mustapha Lebbah, Hanane Azzag	http://editions-rnti.fr/render_pdf.php?p1&p=1001820	http://editions-rnti.fr/render_pdf.php?p=1001820	Dans papier proposer approcher topologique bipartitionnement biclustering appeler bitm utiliser carte autoorganisatrice Lidée principal lapproche dutiliser carte partitionnement simultaner ligne observation colonne variable contrairement approche utiliser carte topologique modeler nécessiter prétraitement baser donnée fonction coût proposer De plaire BiTM fournir visualisation topologique bloc bicluster facilement interprétable résultat obtenir encourageant prometteur continuer dan optique
415	Revue des Nouvelles Technologies de l'Information	EGC	2013	Paramétrage intelligent de l'alignement d'ontologies par l'intégrale de Choquet	Le nombre croissant d'ontologies rend le processus d'alignement une composante essentielle du Web sémantique. Plusieurs outils ont été conçus dans le but de produire des alignements. La qualité des alignements fournis par ces outils est étroitement liée à certains paramètres qui régissent leurs traitements. Dans ce papier, nous proposons une nouvelle approche permettant l'adaptation automatique des paramètres d'alignement d'ontologies par l'utilisation de l'intégrale de Choquet, comme un opérateur d'agrégation. Les expérimentations montrent une nette amélioration des résultats par rapport à un paramétrage statique et figé.	Marouen Kachroudi, Sami Zghal, Sadok Ben Yahia	http://editions-rnti.fr/render_pdf.php?p1&p=1001857	http://editions-rnti.fr/render_pdf.php?p=1001857	nombre croître dontologier processus dalignemer composant essentiel web sémantique outil concevoir dan boire produire alignement qualité alignement fournir outil étroitement lier paramètre régir traitement Dans papier proposer approcher permettre ladaptation automatique paramètre dalignemer dontologier lutilisation lintégrale Choquet opérateur dagrégation expérimentation montrer net amélioration résultat rapport paramétrage statique figer
416	Revue des Nouvelles Technologies de l'Information	EGC	2013	Processus itératif d'extraction de classes en non supervisée	Nous proposons dans cet article une nouvelle approche de classification non supervisée où les classes sont obtenues les unes après les autres suivant un processus itératif. L'approche utilise une méthode d'extraction de classes basée sur la détection de limite de classe, chaque classe étant définie par son centre. Nous avons également défini des critères d'évaluation adaptés à la méthode proposée. Plusieurs expérimentations ont montré l'intérêt de l'approche dans divers problèmes.	Alexandre Blansché, Lydia Boudjeloud	http://editions-rnti.fr/render_pdf.php?p1&p=1001817	http://editions-rnti.fr/render_pdf.php?p=1001817	proposer dan article approcher classification superviser classe obtenu processus itératif Lapproche utiliser méthode dextraction classe baser détection limiter classer classer définir centrer également définir critère dévaluation adapter méthode proposer expérimentation montrer lintérêt lapproche dan problème
417	Revue des Nouvelles Technologies de l'Information	EGC	2013	Ré-écriture de requêtes dans un système d'intégration sémantique	Nous décrivons la deuxième phase de réalisation d'un système d'intégration qui minimise l'intervention humaine habituellement nécessaire. Après la phase de construction semi-automatique du schéma (ontologie) global décrite dans de précédents articles, nous présentons ici le processus de ré-écriture de requêtes globales en des requêtes adressées aux sources.	Cheikh Niang, Béatrice Bouchou, Moussa Lo, Yacine Sam	http://editions-rnti.fr/render_pdf.php?p1&p=1001858	http://editions-rnti.fr/render_pdf.php?p=1001858	décrire phase réalisation dun système dintégration minimiser lintervention humain habituellement nécessaire Après phase construction semiautomatiqu schéma ontologie global décrire dan précédent articl présenter processus réécriture requêt globale requête adresser source
418	Revue des Nouvelles Technologies de l'Information	EGC	2013	Recherche de documents similaires sur le web par segmentations hiérarchiques et extraction de mots-clés	La recherche de documents similaires est un processus qui consiste à trouver les documents présentant des similitudes, comme la copie ou la reformulation, sur des bases documentaires ou sur internet. Elle est utilisée notamment pour protéger la propriété intellectuelle de productions issues de l'enseignement, de la recherche ou de l'industrie. Dans cet article, nous définissons une approche automatique pour permettant d'extraire des mots-clés d'un document en effectuant un bouclage sur une succession de découpage de plus en plus petit. Cette approche permet d'obtenir des mots-clés impossibles à obtenir par une approche globale notamment quand la thématique, le style ou le contenu d'un document varient dans le document. L'objectif est de permettre la détection des documents présentant des similitudes en utilisant uniquement des mots-clés.	Alain Simac-Lejeune	http://editions-rnti.fr/render_pdf.php?p1&p=1001860	http://editions-rnti.fr/render_pdf.php?p=1001860	rechercher document similaire processus consister trouver document présenter similitude copier reformulation base documentaire internet utiliser protéger propriété intellectuel production issu lenseignement rechercher lindustrie Dans article définir approcher automatique permettre dextraire motsclé dun document effectuer bouclage succession découpage plaire plaire petit approcher permettre dobtenir motsclé impossible obtenir approcher global thématique styler contenir dun document varier dan document Lobjectif permettre détection document présenter similitude utiliser uniquement motsclé
419	Revue des Nouvelles Technologies de l'Information	EGC	2013	Réutiliser les connaissances d'expert pour assister l'analyse de l'activité sur simulateur pleine échelle de conduite de centrale nucléaire - Approche à base de M-Trace	Notre travail porte sur l'aide à l'observation de l'activité dans les simulateurs pleine échelle de centrale nucléaire pour assister les formateurs pendant les simulations. Notre approche consiste à représenter l'activité sous la forme de trace modélisée et à les transformer afin d'extraire et de visualiser des informations de haut niveau permettant aux formateurs de mieux retracer et analyser les simulations. Afin de valider notre approche, nous avons conçu le prototype D3KODE que nous avons évalué avec des experts formateurs d'EDF.	Olivier Champalle, Karim Sehaba	http://editions-rnti.fr/render_pdf.php?p1&p=1001828	http://editions-rnti.fr/render_pdf.php?p=1001828	travail porter laid lobservation lactivité dan simulateur échelle central nucléaire assister formateur pendre simulation approcher consister représenter lactivité sou former tracer modélisé transformer dextraire visualiser information niveau permettre formateur mieux retracer analyser simulation Afin valider approcher concevoir prototype d3kode évaluer expert formateur dedf
420	Revue des Nouvelles Technologies de l'Information	EGC	2013	Sélection de variables non supervisée sous contraintes hiérarchiques	La sélection des variables a un rôle très important dans la fouille de données lorsqu'un grand nombre de variables est disponible. Ainsi, certaines variables peuvent être peu significatives, corrélées ou non pertinentes. Une méthode de sélection a pour objectif de mesurer la pertinence d'un ensemble utilisant principalement un critère d'évaluation. Nous présentons dans cet article un critère non supervisé permettant de mesurer la pertinence d'un sous-ensemble de variables. Ce dernier repose sur l'utilisation du score Laplacien auquel nous avons ajouté des contraintes hiérarchiques. Travailler dans le cadre non supervisé est un vrai challenge dans ce domaine dû à l'absence des étiquettes de classes. Les résultats obtenus sur plusieurs bases de tests sont très encourageants et prometteurs.	Nhat-Quang Doan, Hanane Azzag, Mustapha Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1001823	http://editions-rnti.fr/render_pdf.php?p=1001823	sélection variable rôle importer dan fouiller donnée lorsquun grand nombre variable disponible variable pouvoir significatif corrélée pertinent méthode sélection objectif mesurer pertinence dun ensemble utiliser principalement critère dévaluation présenter dan article critère superviser permettre mesurer pertinence dun sousensembl variable reposer lutilisation score Laplacien ajouter contrainte hiérarchique travailler dan cadrer superviser vrai challenge dan domaine devoir labsence étiquette classe résultat obtenir base test encourageant prometteur
421	Revue des Nouvelles Technologies de l'Information	EGC	2013	SNOW, un algorithme exploratoire pour le subspace clustering	Cet article propose un nouvel algorithme pour le problème de subspace clustering dénommé SNOW. Contrairement aux approches descendantes classiques, il ne repose pas sur l'hypothèse de localité et permet l'affectation d'une donnée à plusieurs clusters dans des sous-espaces différents. Les expérimentations préliminaires montrent que notre approche obtient de meilleurs résultats que l'algorithme COPAC sur une base de référence et a été appliquée sur une base de données réelles.	Sylvain Dormieu, Nicolas Labroche	http://editions-rnti.fr/render_pdf.php?p1&p=1001868	http://editions-rnti.fr/render_pdf.php?p=1001868	article proposer nouvel algorithme problème subspace clustering dénommer SNOW contrairement approche descendanter classiquer reposer lhypothèse localité permettre laffectation dune donner cluster dan sousespace expérimentation préliminaire montrer approcher obtenir meilleur résultat lalgorithme copac baser référence appliquer baser donnée réel
422	Revue des Nouvelles Technologies de l'Information	EGC	2013	Technique de factorisation multi-biais pour des recommandations dynamiques	La factorisation de matrices offre une grande qualité de prédiction pour les systèmes de recommandation. Mais sa nature statique empêche de tenir compte des nouvelles notes que les utilisateurs produisent en continu. Ainsi, la qualité des prédictions décroît entre deux factorisations lorsque de nombreuses notes ne sont pas prises en compte. La quantité de notes écartées est d'autant plus grande que la période entre deux factorisation est longue, ce qui accentue la baisse de qualité.Nos travaux visent à améliorer la qualité des recommandations. Nous proposons une factorisation de matrices utilisant des groupes de produits et intégrant en ligne les nouvelles notes des utilisateurs. Nous attribuons à chaque utilisateur un biais pour chaque groupe de produits similaires que nous mettons à jour. Ainsi, nous améliorons significativement les prédictions entre deux factorisations. Nos expérimentations sur des jeux de données réels montrent l'efficacité de notre approche.	Modou Gueye, Talel Abdesssalem, Hubert Naacke	http://editions-rnti.fr/render_pdf.php?p1&p=1001856	http://editions-rnti.fr/render_pdf.php?p=1001856	factorisation matrice offrir grand qualité prédiction système recommandation Mais nature statique empêcher compter note utilisateur produire continu qualité prédiction décroître entrer factorisation note prendre compter quantité note écarter dautant plaire grand période entrer factorisation long accentuer baisser qualiténos travail viser améliorer qualité recommandation proposer factorisation matrice utiliser groupe produit intégrer ligne note utilisateur attribuer utilisateur biais grouper produit similaire mettre jour améliorer significativement prédiction entrer factorisation expérimentation jeu donnée réel montrer lefficacité approcher
423	Revue des Nouvelles Technologies de l'Information	EGC	2013	Text2Geo : des données textuelles aux informations géospatiales	Dans cet article, nous nous intéressons aux méthodes d'extraction d'informations spatiales dans des documents textuels. Nous présentons la méthode hybride Text2Geo qui combine une approche d'extraction d'informations, fondée sur des patrons avec une approche de classification supervisée permettant d'explorer le contexte associé. Nous discutons des résultats expérimentaux obtenus sur le jeu de données de l'étang de Thau.	Sabiha Tahrat, Eric Kergosien, Sandra Bringay, Mathieu Roche, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001861	http://editions-rnti.fr/render_pdf.php?p=1001861	Dans article intéresser méthode dextraction dinformation spatiale dan document textuel présenter méthode hybride text2geo combiner approcher dextraction dinformation fonder patron approcher classification superviser permettre dexplorer contexte associer discuter résultat expérimental obtenir jeu donnée létang Thau
424	Revue des Nouvelles Technologies de l'Information	EGC	2013	ToTeM: une méthode de détection de communautés adaptées aux réseaux d'information	Alors que les réseaux sociaux s'attachaient à représenter des entités et les relations qui existaient entre elles, les réseaux d'information intègrent également des attributs décrivant ces entités ; ce qui conduit à revisiter les méthodes d'analyse et de fouille de ces réseaux. Dans cet article, nous proposons une méthode de classification des sommets d'un graphe qui exploite d'une part leurs relations et d'autre part les attributs les caractérisant. Cette méthode reprend le principe de la méthode de Louvain en l'étendant de façon à permettre la manipulation d'attributs continus d'une manière symétrique à ce qui existe pour les relations.	David Combe, Christine Largeron, Elod Egyed-Zsigmond, Mathias Géry	http://editions-rnti.fr/render_pdf.php?p1&p=1001849	http://editions-rnti.fr/render_pdf.php?p=1001849	réseau social sattachaier représenter entité relation exister entrer réseau dinformation intégrer également attribut décrire entité   conduire revisiter méthode danalyse fouiller réseau Dans article proposer méthode classification sommet dun graph exploiter dune partir relation dautre partir attribut caractériser méthode reprendre principe méthode Louvain létender permettre manipulation dattribut continu dune manière symétrique exister relation
425	Revue des Nouvelles Technologies de l'Information	EGC	2013	Towards a New Science of Big Data Analytics, based on the Geometry and the Topology of Complex, Hierarchic Systems	"My work is concerned with pattern recognition, knowledge discovery, computer learning and statistics. I address how geometry and topology can uncover and empower the semantics of data. In addition to the semantics of data that can be explored using Correspondence Analysis and related multivariate data analyses, hierarchy is a fundamental concept in this work. I address not only low dimensional projection for display purposes, but carry out search and pattern recognition, whenever useful, in very high dimensional spaces. High dimensional spaces present very different characteristics from low dimensions, I have shown that in a particular sense very high dimensional space becomes, as dimensionality increases, hierarchical. I have also shown how in hierarchy, and hence in an ultrametric topological mapping of information space, we track change or anomaly or rupture.In this presentation, the first theme discussed is that of linear time hierarchical clustering with application to sky survey data in astronomy, and to chemo-informatics. The second theme discussed is computational text analysis. It is interesting to note that J.P. Benzécri's original motivation was in language and linguistics. In my text analysis work, I have taken the dictum of McKee (Story : Substance, Structure, Style and the Principles of Screenwriting, Methuen, 1999) that ""text is the sensory surface of a work of art"" and show just how this insight can be rendered in computational terms. This leads to demarcating, tracking, statistical modelling, visualizing, and pattern recognition of narrative. In an application to collaborative writing, I developed an interactive framework for critiquing, and assessing fit and appropriateness of content, on the basis of semantics, leading to books that were published as e-books, having been written by school children in a few days of collaborative class work. In many aspects of this work, hierarchy expresses both continuity and change in the textual narrative or in the narrative of chronological events."	Fionn Murtagh	http://editions-rnti.fr/render_pdf.php?p1&p=1001813	http://editions-rnti.fr/render_pdf.php?p=1001813	My work is concerned with pattern recognition knowledge discovery computer learning and statistic ie addres how geometry and topology can uncover and empower the semantics of dater In addition to the semantics of dater that can be explored using correspondence Analysis and related multivariate dater analys hierarchy is fundamental concept in this work ie addres not only low dimensional projection for display purposer boire carry out search and pattern recognition whenever useful in very high dimensional spaces High dimensional spaces preser very characteristics from low dimension ie hav shown that in particular sense very high dimensional space become dimensionality increas hierarchical ie hav also shown how in hierarchy and hence in an ultrametric topological mapping of information space we track changer or anomaly or rupturein thi presentation the first theme discussed is that of linear time hierarchical clustering with application to sky survey dater in astronomy and to chemoinformatic The second them discussed is computational text analysis it is interesting to noter that JP Benzécris original motivation wa in language and linguistic In my text analysis work ie hav taken the dictum of McKee Story   Substance Structure Style and the Principles of Screenwriting Methuen 1999 that text is the sensory surface of work of art and show just how this insight can be rendered in computational term this leads to demarcating tracking statistical modelling visualizing and pattern recognition of narratif In an application to collaborative writing ie developed an interactif framework for critiquing and assessing faire and appropriateness of conter the basis of semantics leading to book that were published ebooks having been written by school children in few day of collaborative class work In many aspect of this work hierarchy exprès both continuity and changer in the textual narratif or in the narratif of chronological event
426	Revue des Nouvelles Technologies de l'Information	EGC	2013	Un Critère d'évaluation pour la construction de variables à base d'itemsets pour l'apprentissage supervisé multi-tables	Dans le contexte de la fouille de données multi-tables, les données sont représentées sous un format relationnel dans lequel les individus de la table cible sont potentiellement liés à plusieurs enregistrements dans des tables secondaires en relation un-à-plusieurs. Dans cet article, nous proposons un Framework basé sur des itemsets pour la construction de variables à partir des tables secondaires. L'informativité de ces nouvelles variables est évaluée dans le cadre de la classification supervisée au moyen d'un critère régularisé qui vise à éviter le surapprentissage. Pour ce faire, nous introduisons un espace de modèles basés sur des itemsets dans la table secondaire ainsi qu'une estimation de la densité conditionnelle des variables construites correspondantes. Une distribution a priori est définie sur cet espace de modèles, pour obtenir ainsi un critère sans paramètres permettant d'évaluer la pertinence des variables construites. Des expérimentations préliminaires montrent la pertinence de l'approche.	Dhafer Lahbib, Marc Boullé, Dominique Laurent	http://editions-rnti.fr/render_pdf.php?p1&p=1001824	http://editions-rnti.fr/render_pdf.php?p=1001824	Dans contexte fouiller donnée multitabl donnée représenter sou format relationnel dan individu tabler cibl potentiellement lier enregistrement dan table secondaire relation unàplusieur Dans article proposer Framework baser itemset construction variable partir table secondaire Linformativité variable évaluer dan cadrer classification superviser moyen dun critère régulariser viser éviter surapprentissage Pour faire introduire espacer modèle baser itemset dan tabler secondaire quune estimation densité conditionnel variable construire correspondant distribution priori définir espacer modèle obtenir critère paramètre permettre dévaluer pertinence variable construire expérimentation préliminaire montrer pertinence lapproche
427	Revue des Nouvelles Technologies de l'Information	EGC	2013	Un système hybride de recherche d'information intégrant le raisonnement à partir de cas et la composition d'ontologies	La croissance des informations disponibles sur le web nécessite des outils de recherche de plus en plus performants permettant de répondre efficacement aux besoins des utilisateurs. Dans ce contexte, l'utilisation des ontologies présente des atouts importants. Cependant, la construction manuelle d'ontologies est très coûteuse, ceci a poussé à proposer des approches permettant d'automatiser cette construction. Cet article présente un système de recherche d'information hybride basée sur le Raisonnement à Partir de Cas (RàPC) et la composition d'ontologies. Ce système vise à combiner la construction automatique d'ontologies modulaires et le RàPC, qui a pour but d'améliorer les résultats de recherche d'information (RI). Des expérimentations ont été menées et les résultats obtenus montrent une amélioration de la précision dans le cas d'une recherche d'information sur le Web.	Ghada Besbes, Hajer Baazaoui-Zghal, Henda Ben Ghezela	http://editions-rnti.fr/render_pdf.php?p1&p=1001845	http://editions-rnti.fr/render_pdf.php?p=1001845	croissance information disponible web nécessiter outil rechercher plaire plaire performant permettre répondre efficacement besoin utilisateur Dans contexte lutilisation ontologie présenter atout important construction manuel dontologie coûteux pousser proposer approche permettre dautomatiser construction article présenter système rechercher dinformation hybride baser raisonnement partir cas RàPC composition dontologier système viser combiner construction automatique dontologie modulaire ràpc boire daméliorer résultat rechercher dinformation RI expérimentation mener résultat obtenir montrer amélioration précision dan cas dune rechercher dinformation web
428	Revue des Nouvelles Technologies de l'Information	EGC	2013	Une approche en programmation par contraintes pour la classification non supervisée	Dans cet article, nous abordons le problème de classification non supervisée sous contraintes fondé sur la programmation par contraintes (PPC). Nous considérons comme critère d'optimisation la minimisation du diamètre maximal des clusters. Nous proposons un modèle pour cette tâche en PPC et nous montrons aussi l'importance des stratégies de recherche pour améliorer son efficacité. Notre modèle basé sur la distance entre les objets permet de traiter des données qualitatives et quantitatives. Des contraintes supplémentaires sur les clusters et les instances peuvent directement être ajoutées. Des expériences sur des ensembles de données classiques montrent l'intérêt de notre approche.	Thi-Bich-Hanh Dao, Khanh-Chuong Duong, Christel Vrain	http://editions-rnti.fr/render_pdf.php?p1&p=1001822	http://editions-rnti.fr/render_pdf.php?p=1001822	Dans article aborder problème classification superviser sou contrainte fonder programmation contraindre PPC considérer critèr doptimisation minimisation diamètre maximal cluster proposer modeler tâcher PPC montrer limportance stratégie rechercher améliorer efficacité modeler baser distancer entrer objet permettre traiter donnée qualitatif quantitatif contrainte supplémentaire cluster instance pouvoir ajouter expérience ensemble donnée classique montrer lintérêt approcher
429	Revue des Nouvelles Technologies de l'Information	EGC	2013	Une nouvelle mesure pour l'évaluation des méthodes d'extraction de thématiques : la Vraisemblance Généralisée	Les méthodes dédiées à l'extraction automatique de thématiques sont issues de domaines variés : linguistique computationnelle, TAL, algèbre linéaire, statistique, etc. A ces méthodes spécifiques, peuvent s'ajouter des méthodes adaptées d'autres domaines, notamment de l'apprentissage automatique non supervisé. Les résultats produits par l'ensemble de ces méthodes prennent des formes hétérogènes : partitions de documents, distributions de probabilités sur les mots, matrices. Cela pose clairement un problème pour les comparer de manière uniforme. Dans cet article, nous proposons une nouvelle mesure de qualité, intitulée Vraisemblance Généralisée, pour permettre une évaluation et ainsi la comparaison de différentes méthodes d'extraction de thématiques. Les résultats, obtenus sur un corpus de documents Web autour des élections présidentielles françaises de 2012, ainsi que sur le corpus Associated Press, montrent la pertinence de la mesure proposée.	Mohamed Dermouche, Julien Velcin, Sabine Loudcher, Leila Khouas	http://editions-rnti.fr/render_pdf.php?p1&p=1001851	http://editions-rnti.fr/render_pdf.php?p=1001851	méthode dédier lextraction automatique thématique issu domaine varié   linguistique computationnelle tal algèbre linéaire statistique A méthode spécifique pouvoir sajouter méthode adapter dautr domaine lapprentissage automatique superviser résultat produire lensembl méthode prendre forme hétérogène   partition document distribution probabilité matric celer poser clairement problème comparer manière uniforme Dans article proposer mesurer qualité intitulé Vraisemblance Généralisée permettre évaluation comparaison méthode dextraction thématique résultat obtenir corpus document Web autour élection présidentiel français 2012 corpus Associated Press montrer pertinence mesurer proposer
430	Revue des Nouvelles Technologies de l'Information	EGC	2013	Unsupervised Video Tag Correction System	We present a new system for video auto tagging which aims at correcting and completing the tags provided by users for videos uploaded on the Internet. Unlike most existing systems, we do not learn any tag classifiers or use the questionable textual information to compare our videos. We propose to compare directly the visual content of the videos described by different sets of features such as Bag-of-visual-Words or frequent patterns built from them. Then, we propagate tags between visually similar videos according to the frequency of these tags in a given video neighborhood. We also propose a controlled experimental set up to evaluate such a system. Experiments show that with suitable features, we are able to correct a reasonable amount of tags in Web videos.	Hoang-Tung Tran, Elisa Fromont, François Jacquenet, Baptiste Jeudy, Adrien Martins	http://editions-rnti.fr/render_pdf.php?p1&p=1001867	http://editions-rnti.fr/render_pdf.php?p=1001867	we present new system for video auto tagging which aim at correcting and completing the tags provided by user for video uploaded the Internet Unlike most existing system we do not learn any tag classifier or us the questionabl textual information to comparer our videos We proposer to comparer directly the visual conter of the videos described by differer set of featur such BagofvisualWords or frequent pattern built from them then we propagate tag between visually similar videos according to the frequency of these tags in given video neighborhood We also proposer controlled experimental set up to evaluate such system experiment show that with suitabl featur we are abl to correct reasonabl amount of tags in Web video
431	Revue des Nouvelles Technologies de l'Information	EGC	2013	Validation d'une carte cognitive	Les cartes cognitives sont un modèle graphique représentant des influences entre des concepts. Malgré le fait qu'une carte cognitive soit relativement simple à construire, certaines influences peuvent se contredire l'une l'autre. Cet article propose différents critères pour valider une carte cognitive, c'est-àdire indiquer si la carte contient ou non des contradictions. Nous distinguons deux types de critères : les critères de vérification qui valident une carte cognitive en déterminant sa cohérence interne et les critères de test qui valident une carte à partir d'un ensemble de contraintes choisies par le concepteur.	Aymeric Le Dorze, Laurent Garcia, David Genest, Stéphane Loiseau	http://editions-rnti.fr/render_pdf.php?p1&p=1001825	http://editions-rnti.fr/render_pdf.php?p=1001825	carte cognitif modeler graphique représenter influence entrer concept Malgré faire quun carte cognitif simple construire influence pouvoir contredir lune lautre article proposer critère valider carte cognitif cestàdir indiquer carte contenir contradiction distinguer type critère   critère vérification valider carte cognitif déterminer cohérence interne critère test valider carte partir dun ensemble contraint choisie concepteur
432	Revue des Nouvelles Technologies de l'Information	EGC	2013	Vers un cadre évolutif de classification non supervisée	La classification non supervisée (clustering) évolutive surpasse généralement par celle statique en produisant des groupes de données (clusters) qui reflètent les tendances à long terme tout en étant robuste aux variations à court terme. Dans ce travail, nous présentons un cadre différent pour le clustering évolutif d'une manière incrémentale par un suivi précis des variables de proximité temporelles entre les objets suivis par un clustering statique ordinaire.	Mohamed Charouel, Minyar Sassi-Hidri, Mohamed Ali Zoghlami	http://editions-rnti.fr/render_pdf.php?p1&p=1001821	http://editions-rnti.fr/render_pdf.php?p=1001821	classification superviser clustering évolutif surpass généralement statique produire groupe donnée cluster refléter tendance long terme robuste variation courir terme Dans travail présenter cadrer clustering évolutif dune manière incrémental précis variable proximité temporel entrer objet clustering statique ordinaire
433	Revue des Nouvelles Technologies de l'Information	EGC	2013	Vers une architecture multicouche d'ontologies dédiée à la résolution mixte de problèmes	Dans cet article, nous nous intéressons à la gestion d'expériences générées au sein des processus de résolution mixte (individuelle et/ou collective) de problèmes afin d'assister la capitalisation et le partage des connaissances dans les environnements collaboratifs. Dans ce contexte, nous proposons un cadre ontologique générique par rapport au domaine dédié à la modélisation formelle et consensuelle de ces expériences en adoptant une architecture multicouche basée sur quatre strates. La première strate est basée sur la spécialisation d'ontologies fondationnelles. La deuxième strate est basée sur la conception de trois patrons conceptuels ontologiques (PCO) noyaux (le PCO organisationnel, le PCO téléologique et le PCO argumentatif modélisant respectivement les acteurs, le problème et les solutions proposées). La troisième strate est basée sur la spécialisation des PCO noyaux dans un domaine particulier et la dernière strate est basée sur l'instanciation du modèle ontologique de domaine pour la représentation d'une situation du monde réel.	Nesrine Ben Yahia, Narjès Bellamine Ben Saoud, Henda Hajjami Ben Ghezala	http://editions-rnti.fr/render_pdf.php?p1&p=1001844	http://editions-rnti.fr/render_pdf.php?p=1001844	Dans article intéresser gestion dexpérience généré processus résolution mixte individuel etou collectif problème dassister capitalisation partager connaissance dan environnement collaboratifs Dans contexte proposer cadrer ontologique générique rapport domaine dédier modélisation formel consensuel expérience adopter architecturer multicouche basé strate strate baser spécialisation dontologie fondationnel strate baser conception patron conceptuel ontologique pco noyau PCO organisationnel PCO téléologique PCO argumentatif modéliser respectivement acteur problème solution proposer strate baser spécialisation pco noyau dan domaine strate baser linstanciation modeler ontologique domaine représentation dune situation monder réel
434	Revue des Nouvelles Technologies de l'Information	EGC	2013	Vers une Automatisation de la Construction de Variables pour la Classification Supervisée	Dans cet article, nous proposons un cadre visant à automatiser la construction de variables pour l'apprentissage supervisé, en particulier dans le cadre multi-tables. La connaissance du domaine est spécifiée d'une part en structurant les données en variables, tables et liens entre tables, d'autre part en choisissant des règles de construction de variables. L'espace de construction de variables ainsi défini est potentiellement infini, ce qui pose des problèmes d'exploration combinatoire et de sur-apprentissage. Nous introduisons une distribution de probabilité a priori sur l'espace des variables constructibles, ainsi qu'un algorithme performant de tirage d'échantillons dans cette distribution. Des expérimentations intensives montrent que l'approche est robuste et performante.	Marc Boullé, Dhafer Lahbib	http://editions-rnti.fr/render_pdf.php?p1&p=1001819	http://editions-rnti.fr/render_pdf.php?p=1001819	Dans article proposer cadrer viser automatiser construction variable lapprentissage superviser dan cadrer multitabl connaissance domaine spécifier dune partir structurer donnée variable table lien entrer table dautr partir choisir règle construction variable Lespace construction variable définir potentiellement infini poser problème dexploration combinatoire surapprentissage introduire distribution probabilité priori lespace variable constructible quun algorithme performer tirage déchantillon dan distribution expérimentation intensif montrer lapproche robuste performant
435	Revue des Nouvelles Technologies de l'Information	EGC	2013	Vers une mesure de similarité pour les séquences complexes	Le calcul de similarité entre les séquences est d'une extrême importance dans de nombreuses approches d'explorations de données. Il existe une multitude de mesures de similarités de séquences dans la littérature. Or, la plupart de ces mesures sont conçues pour des séquences simples, dites séquences d'items. Dans ce travail, nous étudions d'un point de vue purement combinatoire le problème de similarité entre des séquences complexes (i.e., des séquences d'ensembles ou itemsets). Nous présentons de nouveaux résultats afin de compter efficacement toutes les sous-séquences communes à deux séquences. Ces résultats théoriques sont la base d'une mesure de similarité calculée efficacement grâce à une approche de programmation dynamique.	Elias Egho, Chedy Raïssi, Toon Calders, Thomas Bourquard, Nicolas Jay, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1001853	http://editions-rnti.fr/render_pdf.php?p=1001853	calcul similarité entrer séquence dune extrême importance dan approche dexploration donnée exister multitude mesure similarité séquence dan littérature Or mesure conçu séquence simple séquence ditems Dans travail étudier dun poindre purement combinatoire problème similarité entrer séquence complexe ie séquence densembl itemset présenter résultat compter efficacement sousséquence communer séquence résultat théorique baser dune mesurer similarité calculer efficacement grâce approcher programmation dynamique
436	Revue des Nouvelles Technologies de l'Information	EGC	2013	Visualisation radiale : approche parallèle entre CPU et GPU	Dans cet article, nous proposons une parallélisation sur CPU et GPU d'une méthode de visualisation radiale à base de points d'intérêt. Nous montrons que cette approche peut visualiser avec des temps très courts des millions de données sur des dizaines de dimensions, et nous étudions l'efficacité de la parallélisation dans différentes configurations.	Tianyang Liu, Fatma Bouali, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1001834	http://editions-rnti.fr/render_pdf.php?p=1001834	Dans article proposer parallélisation cpu gpu dune méthode visualisation radial baser point dintérêt montrer approcher pouvoir visualiser temps court million donnée dizaine dimension étudier lefficaciter parallélisation dan configuration
437	Revue des Nouvelles Technologies de l'Information	EGC	2012	Antipattern Detection inWeb Ontologies: an Experiment using SPARQL Queries	Ontology antipatterns are structures that reflect ontology modelling problems because they lead to inconsistencies, bad reasoning performance or bad formalisation of domain knowledge. We propose four methods for the detection of antipatterns using SPARQL queries. We conduct some experiments to detect antipattern in a corpus of OWL ontologies.	Catherine Roussey, Oscar Corcho, Ond&#711;rej Šváb-Zamazal, François Scharffe, Stephan Bernard	http://editions-rnti.fr/render_pdf.php?p1&p=1001177	http://editions-rnti.fr/render_pdf.php?p=1001177	ontology antipattern are structur that reflect ontology modelling problem becaus they lead to inconsistencie bad reasoning performance or bad formalisation of domain knowledge We proposer four methods for the detection of antipatterns using sparql querie We conduct some experiment to detect antipattern in corpus of OWL ontologier
438	Revue des Nouvelles Technologies de l'Information	EGC	2012	Apprentissage d'ensemble d'opérateurs de projection orthogonale pour la détection de nouveauté	Dans ce papier, nous proposons une approche de détection de nouveautéfondée sur les opérateurs de projection orthogonale et l'idée de doublebootstrap (bi- bootstrap). Notre approche appelée Random Subspace NoveltyDetection Filter (RS-NDF), combine une technique de rééchantillonnage etl'idée d'apprentissage d'ensemble. RS-NDF est un ensemble de filtres NDF(Novelty Detection Filter), induits à partir d'échantillons bootstrap des donnéesd'apprentissage, en utilisant une sélection aléatoire des variables pour l'apprentissagedes filtres. RS-NDF utilise donc un double bootstrap, c'est à dire unrééchantillonnage avec remise sur les observations et un rééchantillonnage sansremise sur les variables. La prédiction est faite par l'agrégation des prédictionsde l'ensemble des filtres. RS-NDF présente généralement une importante améliorationdes performances par rapport au modèle de base NDF unique. Grâce àson algorithme d'apprentissage en ligne, l'approche RS-NDF est également enmesure de suivre les changements dans les données au fil du temps. Plusieursmétriques de performance montrent que l'approche proposée est plus efficace,robuste et offre de meilleures performances pour la détection de nouveauté comparéeaux autres techniques existantes.	Fatma Hamdi, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1001156	http://editions-rnti.fr/render_pdf.php?p=1001156	Dans papier proposer approcher détection nouveautéfondée opérateur projection orthogonal lider doublebootstrap bi bootstrap approcher appeler Random Subspace NoveltyDetection Filter RSNDF combin technique rééchantillonnage etlider dapprentissage densembl rsndf ensemble filtre NDFNovelty detection filter induire partir déchantillon bootstrap donnéesdapprentissage utiliser sélection aléatoire variable lapprentissagede filtre RSNDF utiliser doubler bootstrap cest unrééchantillonnage remiser observation rééchantillonnage sansremise variable prédiction faire lagrégation prédictionsde lensembl filtre RSNDF présenter généralement important améliorationde performance rapport modeler baser ndf grâce àson algorithme dapprentissage ligne lapproch RSNDF également enmesur changement dan donnée fil temps plusieursmétriqu performance montrer lapproche proposer plaire efficacerobuste offrir meilleure performance détection nouveauté comparéeaux technique existant
439	Revue des Nouvelles Technologies de l'Information	EGC	2012	Apprentissage par analyse linéaire discriminante des paramètres de fusion pour la recherche d'information multimédia texte-image	Avec le développement du numérique, des quantités très importantesde documents composés de texte et d'images sont échangés, ce qui nécessite ledéveloppement demodèles permettant d'exploiter efficacement ces informationsmultimédias. Dans le contexte de la recherche d'information, unmodèle possibleconsiste à représenter séparément les informations textuelles et visuelles et àcombiner linéairement les scores issus de chaque représentation. Cette approchenécessite le paramétrage de poids afin d'équilibrer la contribution de chaquemodalité. Le but de cet article est de présenter une nouvelle méthode permettantd'apprendre ces poids, basée sur l'analyse linéaire discriminante de Fisher(ALD). Des expérimentations réalisées sur la collection ImageCLEF montrentque l'apprentissage des poids grâce à l'ALD est pertinent et que la combinaisondes scores correspondante améliore significativement les résultats par rapport àl'utilisation d'une seule modalité.	Christophe Moulin, Christine Largeron, Cécile Barat, Mathias Géry, Christophe Ducottet	http://editions-rnti.fr/render_pdf.php?p1&p=1001201	http://editions-rnti.fr/render_pdf.php?p=1001201	Avec développement numérique quantité importantesd document composer texte dimage échanger nécessiter ledéveloppement demodèl permettre dexploiter efficacement informationsmultimédia Dans contexte rechercher dinformation unmodèl possibleconsist représenter séparément information textuel visuel àcombiner linéairement score issu représentation approchenécessite paramétrage poids déquilibrer contribution chaquemodalité boire article poster méthode permettantdapprendre poids baser lanalyse linéaire discriminante FisherALD expérimentation réaliser collection ImageCLEF montrentqu lapprentissage poids grâce lald pertinent combinaisondes score correspondant améliorer significativement résultat rapport àlutilisation dune modalité
440	Revue des Nouvelles Technologies de l'Information	EGC	2012	Biological event extraction using SVM and composite kernel function	With an overwhelming of experimental and computational results inmolecular biology, there is an increasing interest to provide tools that will automaticallyextract structured biological information recorded in freely availabletext. Extraction of named entities such as protein, gene or disease names andof simple relations of these entities, such as statements of protein-protein interactionshas gained certain success, and now the new focus research has beenmoving to higher level of information extraction such as co-reference resolutionand event extraction. It is precisely the last of these tasks which will be focusedin this paper. The biological event template allows detailed representations ofcomplex natural language statements, which is specified by a trigger and argumentslabeled by semantic roles.In this paper, we have developed a biological event extraction approach whichuses Support Vector Machines (SVM) and a suitable composite kernel functionto identify triggers and to assign the corresponding arguments. Also, we makeuse of a number of features based on both syntactic and contextual informationwhich where automatically learned from the training data.We implemented our event extraction system using the state-of-the-art of NLPtools. We achieved competitive results compared to the BioNLP'09 Shared taskbenchmark.	Maha Amami, Aymen Elkhlifi, Rim Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1001150	http://editions-rnti.fr/render_pdf.php?p=1001150	with an overwhelming of experimental and computational results inmolecular biology there is an increasing interest to provide tools that will automaticallyextract structured biological information recorded in freely availabletext extraction of named entities such protein gen or diseas names andof simple relation of these entitier such statement of proteinprotein interactionsha gained succes and now the new focus research has beenmoving to higher level of information extraction such coreference resolutionand event extraction it is precisely the last of these tasks which will be focusedin this paper The biological event template allow detailed representation ofcomplex natural language statement which is specified by trigger and argumentslabeled by semantic rolesin this paper we hav developed biological event extraction approach whichus Support Vector Machines SVM and suitabl composite kernel functionto identify triggers and to assign the corresponding argument Also we makeuse of number of featur based both syntactic and contextual informationwhich where automatically learned from the training datawe implemented our event extraction system using the stateoftheart of nlptool We achieved competitiv results compared to the BioNLP09 shared taskbenchmark
441	Revue des Nouvelles Technologies de l'Information	EGC	2012	Caractérisation et extraction de biclusters de valeurs similaires avec l'analyse de concepts triadiques	Le biclustering de données numériques est devenu depuis le début desannées 2000 une tâche importante d'analyse de données, particulièrement pourl'étude de données biologiques d'expression de gènes. Un bicluster représenteune association forte entre un ensemble d'objets et un ensemble d'attributs dansune table de données numériques. Les biclusters de valeurs similaires peuventêtre vus comme des sous-tables maximales de valeurs proches. Seules quelquesméthodes se sont penchées sur une extraction complète (i.e. non heuristique),exacte et non redondante de tels motifs, qui reste toujours un problème difficile,tandis qu'aucun cadre théorique fort ne permet leur caractérisation. Dans le présentarticle, nous introduisons des liens importants avec l'analyse formelle deconcepts. Plus particulièrement, nous montrons de manière originale que l'analysede concepts triadiques (TCA) propose un cadre mathématique intéressant etpuissant pour le biclustering de données numériques. De cette manière, les algorithmesexistants de la TCA, qui s'appliquent habituellement à des données binaires,peuvent être utilisés (directement ou après quelques modifications) aprèsun prétraitement des données pour l'extraction désirée.	Mehdi Kaytoue-Uberall, Sergei O. Kuznetsov, Amedeo Napoli, Juraj Macko, Wagner Meira Jr	http://editions-rnti.fr/render_pdf.php?p1&p=1001166	http://editions-rnti.fr/render_pdf.php?p=1001166	biclustering donnée numérique devenir desanner 2000 tâcher important danalyse donnée pourlétude donnée biologique dexpression gène bicluster représenteun association fort entrer ensemble dobjet ensemble dattributs dansun tabler donnée numérique bicluster similaire peuventêtre soustabl quelquesméthode pencher extraction complet ie heuristiqueexact redondant motif rester problème difficiletandis quaucun cadrer théorique fort permettre caractérisation Dans présentarticle introduire lien important lanalyse formel deconcepts plaire montrer manière original lanalysede concept triadique TCA proposer cadrer mathématique intéresser etpuisser biclustering donnée numérique De manière algorithmesexistant TCA sappliquer habituellement donnée binairespeuvent utiliser modification aprèsun prétraitement donnée lextraction désirer
442	Revue des Nouvelles Technologies de l'Information	EGC	2012	Classification Conceptuelle avec Généralisation par Intervalles	Nous nous intéressons aux méthodes de classification hiérarchique oupyramidale, où chaque classe formée correspond à un concept, i.e. une paire (extension,intension), considérant des données décrites par des variables quantitativesà valeurs réelles ou intervalles, ordinales et/ou prenant la forme de distributionde probabilités/fréquences sur un ensemble de catégories. Les concepts sontobtenus par une correspondance de Galois avec généralisation par intervalles, cequi permet de traiter les données de différents types dans un cadre commun. Unemesure de la généralité d'un concept est alors calculée sous une forme communepour les différents types de variables. Un exemple illustre la méthode proposée.	Géraldine Polaillon, Paula Brito	http://editions-rnti.fr/render_pdf.php?p1&p=1001141	http://editions-rnti.fr/render_pdf.php?p=1001141	intéresser méthode classification hiérarchique oupyramidal classer former correspondre concept ie pair extensionintension considérer donnée décrit variable quantitativesà réel intervalle ordinal etou prendre former distributionde probabilitésfréquence ensemble catégorie concept sontobtenus correspondance Galois généralisation intervalle cequi permettre traiter donnée type dan cadrer commun Unemesure généralité dun concept calculer sou former communepour type variable exemple illustrer méthode proposer
443	Revue des Nouvelles Technologies de l'Information	EGC	2012	Classification de données EEG par algorithme évolutionnaire pour l'étude d'états de vigilance	"L'objectif de ce travail est de prédire l'état de vigilance d'un individuà partir de l'étude de son activité cérébrale (signaux d'électro-encéphalographieEEG). La variable à prédire est binaire (état de vigilance ""normal"" ou ""relaxé"").Des EEG de 44 participants dans les deux états (88 enregistrements), ont étérecueillis via un casque à 58 électrodes. Après une étape de prétraitement et devalidation des données, un critère nommé ""critère des pentes"" a été choisi. Desméthodes de classification supervisée usuelles (k plus proches voisins, arbresbinaires de décision (CART), forêts aléatoires, PLS et sparse PLS discriminante)ont été appliquées afin de fournir des prédictions de l'état des participants. Lecritère utilisé a ensuite été raffiné grâce à un algorithme génétique, ce qui apermis de construire un modèle fiable (taux de bon classement moyen par CARTégal à 86.68 ± 1.87%) et de sélectionner une électrode parmi les 58 initiales."	Laurent Vezard, Pierrick Legrand, Marie Chavent, Frédérique Faïta-Aïnseba, Julien Clauzel	http://editions-rnti.fr/render_pdf.php?p1&p=1001198	http://editions-rnti.fr/render_pdf.php?p=1001198	Lobjectif travail prédire létat vigilance dun individuà partir létude activité cérébral signal délectroencéphalographieEEG variable prédire binaire vigilance normal relaxédes eeg 44 participant dan 88 enregistrement étérecueillis casquer 58 électrode Après étape prétraitement devalidation donnée critère nommer critère pente choisir Desméthodes classification superviser usuel plaire voisin arbresbinair décision cart forêt aléatoire pl sparse pl discriminanteont appliquer fournir prédiction létat participant Lecritère utiliser ensuite raffiner grâce algorithme génétique apermis construire modeler fiable taux classement moyen cartégal 8668 ± 187 sélectionner électrode 58 initiale
444	Revue des Nouvelles Technologies de l'Information	EGC	2012	Classification des données catégorielles via la maximisation spectrale de la modularité	Ce papier présente un algorithme spectrale pour maximiser le critèrede la modularité étendu à la classification des données catégorielles. Il met enevidence la connexion formelle entre la maximisation de la modularité et la classificationspectrale, il présente en particulier le problème de maximisation de lamodularité sous forme d'un problème algèbrique de maximisation de la trace.Nous développons ensuite un algorithme efficace pour trouver la partition optimalemaximisant le critère de modularité. Les résultats expérimentaux montrentl'efficacité de notre approche	Lazhar Labiod, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1001194	http://editions-rnti.fr/render_pdf.php?p=1001194	papier présenter algorithme spectral maximiser critèrede modularité étendre classification donnée catégoriel mettre enevidence connexion formel entrer maximisation modularité classificationspectral présenter problème maximisation lamodularité sou former dun problème algèbriqu maximisation tracenous développer ensuite algorithme efficace trouver partition optimalemaximiser critère modularité résultat expérimental montrentlefficacité approcher
445	Revue des Nouvelles Technologies de l'Information	EGC	2012	Classification probabiliste non supervisée et visualisation des données séquentielles	Nous proposons dans ce papier un nouvel algorithme de classificationnon supervisée à base de modèle de mélange topologique pour des donnéesnon i.i.d (non independently and identically distributed). Ce nouveau paradigmeprobabiliste, plonge les cartes topologiques probabilistes dans une formulationsous forme de chaînes de Markov cachées. Dans cette formulation, la générationd'une observation à un instant donné du temps est conditionnée par les étatsvoisins au même instant du temps. Ainsi, une grande proximité impliquera unegrande probabilité pour la contribution à la génération. L'approche proposée estévaluée en utilisant des données séquentielles réelles issues des bases de donnéesde l'Institut Nationale de l'Audiovisuel (INA). Les résultats obtenus sonttrès encourageants et prometteurs.	Rakia Jaziri, Mustapha Lebbah, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1001187	http://editions-rnti.fr/render_pdf.php?p=1001187	proposer dan papier nouvel algorithme classificationnon superviser baser modeler mélanger topologique donnéesnon iid independently and identically distributed paradigmeprobabilist plonger carte topologique probabilister dan formulationsou former chaîne Markov cacher Dans formulation générationdune observation instant donner temps conditionner étatsvoisin instant temps grand proximité impliquer unegrande probabilité contribution génération Lapproche proposer estévaluer utiliser donnée séquentiel réel issu base donnéesde linstitut Nationale laudiovisuel ina résultat obtenir sonttrè encourageant prometteur
446	Revue des Nouvelles Technologies de l'Information	EGC	2012	Classification topologique probabiliste pour des données catégorielles	Cet article présente une carte auto-organisatrice probabiliste pour l'analyseet la classification topologique des données catégorielles. En considérant unmodèle de mélanges parcimonieux nous introduisons une nouvelle carte autoorganisatrice(SOM) probabiliste. L'estimation des paramètres de notre modèleest réalisée à l'aide de l'algorithme EM classique. Contrairement à SOM, l'algorithmed'apprentissage proposé optimise une fonction objective. Ces performancesont été évaluées sur des données réelles et les résultats obtenus sontencourageants et prometteurs à la fois pour la classification et pour la modélisation.	Nicoleta Rogovschi, Mohamed Nadif	http://editions-rnti.fr/render_pdf.php?p1&p=1001188	http://editions-rnti.fr/render_pdf.php?p=1001188	article présenter carte autoorganisatrice probabiliste lanalyseet classification topologique donnée catégoriel En considérer unmodèl mélange parcimonieux introduire carte autoorganisatricesom probabiliste lestimation paramètre modèleest réaliser laid lalgorithme em classique contrairement som lalgorithmedapprentissage proposer optimiser fonction objectif performancesont évaluer donnée réel résultat obtenu sontencourageant prometteur classification modélisation
447	Revue des Nouvelles Technologies de l'Information	EGC	2012	Clustering de séquences d'activités pour l'étude de procédures neurochirurgicales	L'utilisation de modèles de procédure chirurgicale (Surgical ProcessModel, SPM) a récemment émergé dans le domaine de la conception d'outilsd'intervention chirurgicale assistée par ordinateur. Ces modèles, qui sont utiliséspour analyser et évaluer les interventions, représentent des procédures chirurgicales(Surgical Process, SP) qui sont formalisées comme des structures symboliquesdécrivant une chirurgie à un niveau de granularité donné. Un enjeu importantréside dans la définition de métriques permettant la comparaison et l'évaluationde ces procédures. Ainsi, les relations entre ces métriques et des donnéespré-opératoires permettent de classer les chirurgies pour mettre en lumière desinformations sur la procédure elle-même, mais également sur le comportementdu chirurgien. Dans ce papier, nous étudions la classification automatique d'unensemble de procédures chirurgicales en utilisant l'algorithme Dynamic TimeWarping (DTW) pour calculer une mesure de similarité entre procédures chirurgicales.L'utilisation de DTW permet de se concentrer sur les différents typesd'activité effectués pendant la procédure, ainsi que sur leur séquencement touten réduisant les différences temporelles. Des expériences ont été menées sur 24procédures chirurgicales d'hernie discale lombaire dans le but de discriminer leniveau d'expertise des chirurgiens à partir d'une classification connue. A l'aided'un algorithme de clustering hiérarchique utilisant DTW nous avons retrouvédeux groupes de chirurgiens présentant des niveaux d'expertise différents (junioret senior).	Germain Forestier, Florent Lalys, Laurent Riffaud, Brivael Trelhu, Pierre Jannin	http://editions-rnti.fr/render_pdf.php?p1&p=1001153	http://editions-rnti.fr/render_pdf.php?p=1001153	lutilisation modèle procédure chirurgical surgical ProcessModel spm récemment émerger dan domaine conception doutilsdintervention chirurgical assister ordinateur modèle utiliséspour analyser évaluer intervention représenter procédure chirurgicalessurgical Process sp formaliser structure symboliquesdécriver chirurgie niveau granularité donner enjeu importantréside dan définition métrique permettre comparaison lévaluationd procédure relation entrer métrique donnéespréopératoire permettre classer chirurgie mettre lumière desinformater procédure ellemêm également comportementdu chirurgien Dans papier étudier classification automatique dunensembl procédure chirurgical utiliser lalgorithm Dynamic TimeWarping dtw calculer mesurer similarité entrer procédure chirurgicaleslutilisation dtw permettre concentrer typesdactivité effectuer pendre procédure séquencement touten réduire différence temporel expérience mener 24procédures chirurgical dhernie discal lombaire dan boire discriminer leniveau dexpertise chirurgien partir dune classification connaître laidedun algorithm clustering hiérarchique utiliser DTW retrouvédeux groupe chirurgien présenter niveau dexpertise junioret senior
448	Revue des Nouvelles Technologies de l'Information	EGC	2012	Clustering hiérarchique non paramétrique de données fonctionnelles	Dans cet article, il est question de clustering de courbes. Nous proposonsune méthode non paramétrique qui segmente les courbes en clusters etdiscrétise en intervalles les variables continues décrivant les points de la courbe.Le produit cartésien de ces partitions forme une grille de données qui est inféréeen utilisant une approche Bayésienne de sélection de modèle ne faisant aucunehypothèse concernant les courbes. Enfin, une technique de post-traitement, visantà réduire le nombre de clusters dans le but d'améliorer l'interprétabilitédes clusters, est proposée. Elle consiste à fusionner successivement et de façonoptimale les clusters, ce qui revient à réaliser une classification hiérarchique ascendantedont la mesure de dissimilarité correspond à la variation du critère.De manière intéressante, cette mesure est en fait une somme pondérée de divergencesde Kullback-Leibler entre les distributions des clusters avant et aprèsfusions. L'intérêt de l'approche dans le cadre de l'analyse exploratoire de donnéesfonctionnelles est illustré par un jeu de données artificiel et réel.	Marc Boullé, Romain Guigourès, Fabrice Rossi	http://editions-rnti.fr/render_pdf.php?p1&p=1001137	http://editions-rnti.fr/render_pdf.php?p=1001137	Dans article question clustering courbe proposonsune méthode paramétrique segmenter courbe cluster etdiscrétise intervalle variable continu décrire point courbele produire cartésien partition former griller donnée inféréeen utiliser approcher Bayésienne sélection modeler faire aucunehypothès concerner courbe Enfin technique posttraitement visantà réduire nombre cluster dan boire daméliorer linterprétabilitéde cluster proposer consister fusionner successivement façonoptimale cluster revenir réaliser classification hiérarchique ascendantedont mesurer dissimilarité correspondre variation critèrede manière intéressant mesurer faire sommer pondérer divergencesd KullbackLeibler entrer distribution cluster aprèsfusion Lintérêt lapproche dan cadrer lanalyse exploratoire donnéesfonctionnelle illustrer jeu donnée artificiel réel
449	Revue des Nouvelles Technologies de l'Information	EGC	2012	Clustering multi-niveaux de graphes : hiérarchique et topologique		Nhat-Quang Doan, Hanane Azzag, Mustapha Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1001205	http://editions-rnti.fr/render_pdf.php?p=1001205	
450	Revue des Nouvelles Technologies de l'Information	EGC	2012	Combinaison de classificateurs simples pour une sélection rapide de caractéristiques	La sélection de caractéristiques est une technique permettant de choisirles caractéristiques les plus pertinentes, celles adaptées à la résolution d'unproblème particulier. Les méthodes classiques présentent certains inconvénients.Par exemple, elles peuvent être trop complexes, elles peuvent faire dépendreles caractéristiques sélectionnées du classificateur utilisé, elles risquent de sélectionnerdes caractéristiques redondantes. Dans le but de limiter ces inconvénients,nous proposons dans cet article une nouvelle méthode rapide de sélectionde caractéristiques basée sur la construction et la sélection de classificateurssimples associés à chacune des caractéristiques. Une optimisation par unalgorithme génétique est proposée afin de trouver la meilleure combinaison desclassificateurs. Différentes méthodes de combinaison sont considérées et adaptéesà notre problème. Cette méthode a été appliquée sur différents ensemblesde caractéristiques de tailles variées et construite à partir de la base de chiffresmanuscrits MNIST. Les résultats obtenus montrent la robustesse de l'approcheainsi que l'efficacité de la méthode. En moyenne, le nombre de caractéristiquessélectionnées a diminué de 69,9% tout en conservant le taux de reconnaissance.	Hassan Chouaib, Florence Cloppet, Salvatore-Antoine Tabbone, Nicole Vincent	http://editions-rnti.fr/render_pdf.php?p1&p=1001196	http://editions-rnti.fr/render_pdf.php?p=1001196	sélection caractéristique technique permettre choisirle caractéristique plaire pertinent cell adapter résolution dunproblème méthode classique présenter inconvénientspar exemple pouvoir complexe pouvoir faire dépendrel caractéristique sélectionner classificateur utiliser risquer sélectionnerde caractéristique redondanter Dans boire limiter inconvénientsnou proposon dan article méthode rapide sélectionde caractéristique baser construction sélection classificateurssimple associer caractéristique optimisation unalgorithme génétique proposer trouver meilleur combinaison desclassificateur méthode combinaison considérer adaptéesà problème méthode appliquer ensemblesde caractéristique taille varier construire partir baser chiffresmanuscrit mnist résultat obtenir montrer robustesse lapprocheainsi lefficacité méthode En moyenner nombre caractéristiquessélectionnée diminuer 699 conserver taux reconnaissance
451	Revue des Nouvelles Technologies de l'Information	EGC	2012	Combinaison de classification supervisée et non-supervisée par la théorie des fonctions de croyance	Nous proposons dans cet article une nouvelle approche de classificationfondée sur la théorie des fonctions de croyance. Cette méthode repose surla fusion entre la classification supervisée et la classification non supervisée. Eneffet, nous sommes face à un problème de manque de données d'apprentissagepour des applications dont les résultats de classification supervisée et non superviséesont très variables selon les classificateurs employés. Les résultats ainsiobtenus sont par conséquent considérés comme incertains.Notre approche se propose de combiner les résultats des deux types de classificationen exploitant leur complémentarité via la théorie des fonctions de croyance.Celle-ci permet de tenir compte de l'aspect d'incertitude et d'imprécision. Aprèsavoir dresser les différentes étapes de notre nouveau schéma de classification,nous détaillons la fusion de classificateurs. Cette nouvelle approche est appliquéesur des données génériques, issues d'une vingtaine de bases de données.Les résultats obtenus ont montré l'efficacité de l'approche proposée.	Fatma Karem, Mounir Dhibi, Arnaud Martin	http://editions-rnti.fr/render_pdf.php?p1&p=1001143	http://editions-rnti.fr/render_pdf.php?p=1001143	proposer dan article approcher classificationfondée théorie fonction croyance méthode reposer surla fusion entrer classification superviser classification superviser eneffet face problème manquer donnée dapprentissagepour application résultat classification superviser superviséesont variable classificateur employer résultat ainsiobtenus conséquent considérer incertainsNotre approcher proposer combiner résultat type classificationen exploiter complémentarité théorie fonction croyancecelleci permettre compter laspect dincertitude dimprécision Aprèsavoir dresser étape schéma classificationnou détailler fusion classificateur approcher appliquéesur donnée générique issu dune vingtaine base donnéesles résultat obtenir montrer lefficacité lapproche proposer
452	Revue des Nouvelles Technologies de l'Information	EGC	2012	Community Detection in Social Networks with Attribute and Relationship Data		The Anh Dang, Emmanuel Viennet	http://editions-rnti.fr/render_pdf.php?p1&p=1001204	http://editions-rnti.fr/render_pdf.php?p=1001204	
453	Revue des Nouvelles Technologies de l'Information	EGC	2012	Découverte de règles d'association pour l'aide à la prévision des accidents maritimes	Les systèmes de surveillance maritime permettent la récupération et lafusion des informations sur les navires (position, vitesse, etc.) à des fins de suividu trafic maritime sur un dispositif d'affichage. Aujourd'hui, l'identification desrisques à partir de ces systèmes est difficilement automatisable compte-tenu del'expertise à formaliser, du nombre important de navires et de la multiplicité desrisques (collision, échouement, etc). De plus, le remplacement périodique desopérateurs de surveillance complique la reconnaissance d'événements anormauxqui sont éparses et parcellaires dans le temps et l'espace. Dans l'objectif de faireévoluer ces systèmes de surveillance maritime, nous proposons dans cet article,une approche originale fondée sur le data mining pour l'extraction de motifsfréquents. Cette approche se focalise sur des règles de prévision et de ciblagepour l'identification automatique des situations induisant ou constituant le cadredes accidents maritimes.	Bilal Idiri, Aldo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1001192	http://editions-rnti.fr/render_pdf.php?p=1001192	système surveillance maritime permettre récupération lafusion information navir position vitess fin suividu trafic maritime dispositif daffichage Aujourdhui lidentification desrisquer partir système difficilement automatisabl comptetenu delexpertise formaliser nombre importer navir multiplicité desrisqu collision échouemer De plaire remplacement périodique desopérateurs surveillance compliqu reconnaissance dévénement anormauxqui éparser parcellaire dan temps lespace Dans lobjectif faireévoluer système surveillance maritime proposer dan articleune approcher original fonder dater mining lextraction motifsfréquent approcher focaliser règle prévision ciblagepour lidentification automatique situation induire constituer cadredes accident maritime
454	Revue des Nouvelles Technologies de l'Information	EGC	2012	Détection de groupes outliers en classification non supervisée	"Nous proposons dans ce papier une nouvelle méthode de détection degroupes outliers. Notre mesure nommée GOF (Group Outlier Factor) est estiméepar l'apprentissage non-supervisé. Nous l'avons intégré dans l'apprentissage descartes topologiques. Notre approche est basée sur la densité relative de chaquegroupe de données, et fournit simultanément un partitionnement des donnéeset un indicateur quantitatif (GOF) sur ""la particularité"" de chaque cluster ougroupe. Les résultats obtenus sont très encourageants et prometteurs pour continuerdans cette optique."	Amine Chaibi, Mustapha Lebbah, Hanane Azzag	http://editions-rnti.fr/render_pdf.php?p1&p=1001184	http://editions-rnti.fr/render_pdf.php?p=1001184	proposer dan papier méthode détection degroupes outlier mesurer nommé GOF Group Outlier Factor estiméepar lapprentissage nonsuperviser laver intégré dan lapprentissage descart topologique approcher baser densité relatif chaquegroupe donnée fournir simultanément partitionnement donnéeset indicateur quantitatif gof particularité cluster ougroupe résultat obtenir encourageant prometteur continuerdans optique
455	Revue des Nouvelles Technologies de l'Information	EGC	2012	Détection non supervisée d'une sous-population par méthode d'ensemble et changement de représentation itératif	L'apprentissage non supervisé a classiquement pour objectif la détectionde sous-populations homogènes (classes) considérées de manière équivalentesans information a priori sur celles-ci. Le problème étudié dans cet articleest quelque peu distinct. On se focalise ici uniquement sur une sous-populationd'intérêt que l'on cherche à identifier avec un rappel et une précision optimales.Nous proposons, pour cela, une méthode s'appuyant sur les principes suivants :(1) travailler dans l'espace de représentation fourni par des experts faibles pourcette tâche, (2) confronter ces experts pour détecter des seuils de sélection pluspertinents, et (3) les combiner itérativement afin de converger vers l'expert idéal.Cette méthode est éprouvée et comparée sur des données synthétiques.	Christine Martin, Antoine Cornuéjols	http://editions-rnti.fr/render_pdf.php?p1&p=1001195	http://editions-rnti.fr/render_pdf.php?p=1001195	lapprentissage superviser classiquement objectif détectionde souspopulation homogène classe considérer manière équivalentesan information priori cellesci problème étudier dan articleest distinct focaliser uniquement souspopulationdintérêt lon chercher identifier rappel précision optimalesnous proposer celer méthode sappuyer principe 1 travailler dan lespace représentation fournir expert faible pourcette tâch 2 confronter expert détecter seuil sélection pluspertinent 3 combiner itérativement converger ver lexpert idéalcett méthode éprouver comparer donnée synthétique
456	Revue des Nouvelles Technologies de l'Information	EGC	2012	Development of a distributed recommender system using the Hadoop Framework	Producing high quality recommendations has become a challenge inthe recent years. Indeed, the growth in the quantity of data involved in the recommendationprocess pose some scalability and effectiveness problems. Theseissues have encouraged the research of new technologies. Instead of developinga new recommender system we improve an already existing method. A distributedframework was considered based on the known quality and simplicity ofthe MapReduce project. The Hadoop Open Source project played a fundamentalrole in this research. It undoubtedly encouraged and facilitated the constructionof our application, supplying all tools needed. Our main goal in this research wasto prove that building a distributed recommender system was not only possible,but simple and productive.	Raja Chiky, Renata Ghisloti, Zakia Kazi Aoul	http://editions-rnti.fr/render_pdf.php?p1&p=1001179	http://editions-rnti.fr/render_pdf.php?p=1001179	Producing high quality recommendation has become challenge inthe recent year Indeed the growth in the quantity of dater involved in the recommendationprocess pos some scalability and effectiveness problems theseissue hav encouraged the research of new technologi Instead of developinga new recommender system we improve an already existing method distributedframework wa considered based the known quality and simplicity ofth MapReduce project The Hadoop Open source project played fundamentalrole in this research it undoubtedly encouraged and facilitated the constructionof our application supplying all tools needed Our main goal in this research wasto prove that building distributed recommender system wa not only possiblebut simple and productif
457	Revue des Nouvelles Technologies de l'Information	EGC	2012	Evaluating Bayesian Networks by Sampling with Simplified Assumptions	The most common fitness evaluation for Bayesian networks in the presence of data is the Cooper-Herskovitz criterion. This technique involves massive amounts of data and, therefore, expansive computations. We propose a cheaper alternative evaluation method using simplified ssumptions which produces evaluations that are strongly correlated with the Cooper-Herskovitz criterion.	Saaid Baraty, Dan A. Simovici	http://editions-rnti.fr/render_pdf.php?p1&p=1001135	http://editions-rnti.fr/render_pdf.php?p=1001135	The most common fitness evaluation for Bayesian network in the presence of dater is the CooperHerskovitz criterion This technique involv massif amount of dater and therefore expansif computation We proposer cheaper alternatif evaluation method using simplified ssumption which produc evaluation that are strongly correlated with the CooperHerskovitz criterion
458	Revue des Nouvelles Technologies de l'Information	EGC	2012	Evaluation rapide du diamètre d'un graphe	"Lors de l'analyse de graphes, il est important de connaître leurs propriétésafin de pouvoir par exemple identifier leur structure et les comparer.Une des caractérisations importante de ces graphes repose sur le fait de déterminers'il s'agit ou non d'un ""petit monde"". Pour ce faire, la valeur du diamètredu graphe est essentielle. Or la mesure du diamètre est pour un très grandgraphe, une opération extrêmement longue. Nous proposons un algorithme endeux phases qui permet d'obtenir rapidement une estimation du diamètre d'ungraphe avec une proportion d'erreur faible. En réduisant cet algorithme à uneseule phase et en acceptant une marge d'erreur plus élevée, nous obtenons uneestimation très rapide du diamètre. Nous testons cet algorithme sur deux grandsgraphes de terrain (plus d'un million de noeuds) et comparons ses performancesavec celles d'un algorithme de référence BFS (Breadth-First Search). Les résultatsobtenus sont décrits et commentés."	Christian Belbeze, Max Chevalier, Chantal Soulé-Dupuy	http://editions-rnti.fr/render_pdf.php?p1&p=1001183	http://editions-rnti.fr/render_pdf.php?p=1001183	lanalyse graphe importer connaître propriétésafin pouvoir exemple identifier structurer comparerune caractérisation important graphe reposer faire déterminersil sagit dun petit monder Pour faire diamètredu graph essentiel Or mesurer diamètre grandgraphe opération extrêmement long proposer algorithme endeux phase permettre dobtenir rapidement estimation diamètre dungraph proportion derreur faible En réduire algorithme uneseule phase accepter marge derreur plaire élevé obtenir uneestimation rapide diamètre tester algorithme grandsgraphe terrain plaire dun million noeud comparon performancesavec dun algorithme référence BFS BreadthFirst Search résultatsobtenu décrit commenter
459	Revue des Nouvelles Technologies de l'Information	EGC	2012	Exploitation de l'asymétrie entre termes pour l'extraction automatique de taxonomies à partir de textes	Nous présentons dans cet article une nouvelle approche pour la générationautomatique de structures lexicales (ou taxonomies) à partir de textes.Cette tâche est fondée sur l'hypothèse forte selon laquelle l'accumulation defaits statistiques simples sur les usages en corpus permet d'approximer des informationsde niveau sémantique sur le lexique. Nous utilisons la prétopologiecomme cadre de travail afin de formaliser et de combiner plusieurs hypothèsessur les usages terminologiques et enfin de structurer le lexique sous la formed'une taxonomie. Nous considérons également le problème de l'évaluation destaxonomies résultantes et proposons un nouvel indice afin de les comparer et depositionner notre approche par rapport à la littérature.	Davide Buscaldi, Guillaume Cleuziou, Gaël Dias, Vincent Levorato	http://editions-rnti.fr/render_pdf.php?p1&p=1001200	http://editions-rnti.fr/render_pdf.php?p=1001200	présenter dan article approcher générationautomatique structure lexicale taxonomie partir textescette tâcher fonder lhypothèse fort laccumulation defait statistique simple usage corpus permettre dapproximer informationsde niveau sémantique lexique utiliser prétopologiecomm cadrer travail formaliser combiner hypothèsessur usage terminologique structurer lexiqu sou formedun taxonomie considérer également problème lévaluation destaxonomier résultante proposon nouvel indice comparer depositionner approcher rapport littérature
460	Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction d'opinions appliquée à des critères	Les technologies de l'information et le succès des services associés(e.g., blogs, forums,...) ont ouvert la voie à un mode d'expression massive d'opinionssur les sujets les plus variés. Récemment, de nouvelles techniques de détectionautomatique d'opinions (opinion mining) ont fait leur apparition et viades analyses statistiques des avis exprimés, tendent à dégager une tendance globaledes opinions exprimées par les internautes. Néanmoins une analyse plusfine de celle-ci montre que les arguments avancés par les internautes relèvent decritères de jugement distincts. Ici, un film sera décrié pour un scénario décousu,là il sera encensé pour une bande son époustouflante. Dans cet article, nous proposons,après avoir caractérisé automatiquement des critères dans un document,d'en extraire l'opinion relative. A partir d'un ensemble restreint de mots clésd'opinions, notre approche construit automatiquement une base d'apprentissagede documents issus du web et en déduit un lexique de mots ou d'expressionsd'opinions spécifiques au domaine d'application. Des expériences menées surdes jeux de données réelles illustrent l'efficacité de l'approche.	Benjamin Duthil, François Trousset, Gérard Dray, Pascal Poncelet, Jacky Montmain	http://editions-rnti.fr/render_pdf.php?p1&p=1001197	http://editions-rnti.fr/render_pdf.php?p=1001197	technologie linformation succès service associéseg blog forum ouvrir voir mode dexpression massif dopinionssur plaire varier récemment technique détectionautomatique dopinions opinion mining faire apparition viade analys statistique avis exprimer tendre dégager tendance globaled opinion exprimer internaute analyser plusfine celleci montr argument avancer internaute relever decritèr jugement distinct film décrier scénario décousulà encenser bander époustouflant Dans article proposonsaprès caractériser automatiquement critère dan documentden extraire lopinion relatif partir dun ensembl restreindre clésdopinion approcher construire automatiquement baser dapprentissagede document issu web déduire lexique dexpressionsdopinion spécifique domaine dapplication expérience mener surdes jeu donnée réel illustrer lefficacité lapproche
461	Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction de co-variations entre des propriétés de sommets et leur position topologique dans un graphe attribué	L'analyse de grands réseaux est très étudiée en fouille de données.Toutefois, les approches existantes proposent une analyse soit à un niveau macroscopique(étude des propriétés globales comme la distribution des degrés),soit à un niveau microscopique (extraction de sous-graphes fréquents ou denses).Nous proposons une nouvelle méthode qui effectue une analyse intermédiairepermettant de découvrir des motifs regroupant des propriétés microscopiques etmacroscopiques du réseau. Ces motifs capturent des co-variations entre des propriétésnumériques relatives aux sommets. Par exemple, un motif mésoscopiquedans un réseau de co-auteurs peut être plus le nombre de publications à EGC estimportant, plus la centralité des sommets correspondants dans le réseau l'estégalement. Notre contribution est multiple. D'abord, ce travail est le premierà exploiter conjointement des propriétés locales et des propriétés topologiques.De plus, nous produisons de nouvelles avancées dans le domaine de l'extractionde co-variations en revisitant les motifs émergents dans ce contexte. Enfin, nousrapportons une analyse d'un réseau bibliographique réel issu de DBLP.	Adriana Prado, Marc Plantevit, Celine Robardet, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1001171	http://editions-rnti.fr/render_pdf.php?p=1001171	lanalyse grand réseau étudier fouiller donnéestoutefoi approche existant proposer analyser niveau macroscopiqueétud propriété global distribution degréssoit niveau microscopique extraction sousgraphe fréquent densesnous proposer méthode effectuer analyser intermédiairepermetter découvrir motif regrouper propriété microscopique etmacroscopiqu réseau motif capturer covariation entrer propriétésnumérique relatif sommet Par exemple motif mésoscopiquedans réseau coauteur pouvoir plaire nombre publication egc estimporter plaire centralité sommet correspondant dan réseau lestégalement contribution dabord travail premierà exploiter conjointement propriété local propriété topologiquesde plaire produire avancer dan domaine lextractionde covariater revisiter motif émergent dan contexte nousrapporton analyser dun réseau bibliographique réel issu dblp
462	Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction de Dépendances Fonctionnelles Approximatives	La découverte de dépendances fonctionnelles (DF) à partir d'une relationexistante est une technique importante pour l'analyse de Bases de Données.L'ensemble des DF exactes ou approximatives extraites par les algorithmes existantsest valide tant que la relation n'est pas modifiée. Ceci est insuffisant pourdes situations réelles où les relations sont constamment mises à jour.Nous proposons une approche incrémentale qui maintiens à jour l'ensemble desDF valides, exactes ou approximatives selon une erreur donnée, quand des tuplessont insérés et supprimés. Les résultats expérimentaux indiquent que lors de l'extractionde DF à partir d'une relation continuellement modifiée, les algorithmesexistants sont sensiblement dépassés par notre stratégie incrémentale.	Noel Novelli, Ekaterina Simonenko	http://editions-rnti.fr/render_pdf.php?p1&p=1001503	http://editions-rnti.fr/render_pdf.php?p=1001503	découvrir dépendance fonctionnel df partir dune relationexistante technique important lanalyse Bases donnéeslensemble df exacte approximatif extrait algorithme existantsest valide relation nest modifier insuffisant pourd situation réel relation constamment mettre journous proposer approcher incrémental maintien jour lensembl desdf valide exacte approximatif erreur donner tuplessont inséré supprimé résultat expérimental indiquer lextractionde DF partir dune relation continuellemer modifier algorithmesexistant sensiblement dépasser stratégie incrémental
463	Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction de Liens Fréquents dans les Réseaux Sociaux	Cet article présente FLMin, une nouvelle méthode d'extraction de motifsfréquents dans les réseaux sociaux. Contrairement aux méthodes traditionnellesqui s'intéressent uniquement aux régularités structurelles, l'originalité denotre approche réside dans sa capacité à exploiter la structure et les attributs desnoeuds pour extraire des régularités, que nous appelons “liens fréquents”, dansles liens entre des noeuds partageant des caractéristiques communes.	Erick Stattner, Martine Collard	http://editions-rnti.fr/render_pdf.php?p1&p=1001174	http://editions-rnti.fr/render_pdf.php?p=1001174	article présent flmin méthod dextraction motifsfréquent dan réseau social contrairement méthode traditionnellesqui sintéressent uniquement régularité structurel loriginalité denotr approcher résider dan capacité exploiter structurer attribut desnoeuds extraire régularité appeler “ lien fréquent ” dansl lien entrer noeud partager caractéristique commun
464	Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction de séquences fréquentes avec intervalles d'incertitude	"Lors de l'extraction des séquences, la granularité temporelle est plusou moins importante selon les besoins des utilisateurs et les contraintes du domained'application. Nous proposons un algorithme d'extraction de séquencesfréquentes par intervalles à partir de séquences à estampilles temporelles discrètes.Nous intégrons une relaxation des contraintes temporelles en introduisantla définition de ""séquences temporelles par intervalles"" (STI). Ces intervalles reflètentune incertitude sur les occurrences précises des évènements. Nous formalisonsce nouveau concept en exhibant certaines de ses propriétés et nous menonsquelques expériences afin de comparer (qualitativement) nos résultats avec uneautre proposition assez proche de la nôtre"	Asma Ben Zakour, Sofian Maabout, Mohamed Mosbah, Marc Sistiaga	http://editions-rnti.fr/render_pdf.php?p1&p=1001160	http://editions-rnti.fr/render_pdf.php?p=1001160	lextraction séquence granularité temporel plusou important besoin utilisateur contrainte domainedapplication proposer algorithme dextraction séquencesfréquente intervalle partir séquence estampill temporel discrètesnous intégrer relaxation contrainte temporel introduisantla définition séquence temporel intervalle sti intervalle reflètentune incertitude occurrence précis évènement formalisonsce concept exhiber propriété menonsquelques expérienc comparer qualitativement résultat uneautre proposition
465	Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction de sous-parties ciblées d'une ontologie généraliste pour enrichir une ontologie particulière	Différentes ressources ontologiques généralistes de très grande tailleont été développées de façon collective et sont aujourd'hui disponibles sur leweb. Ainsi l'ontologie YAGO est une énorme base de connaissances décrivantplus de 2 millions d'entités. Afin de tirer parti de ce gigantesque travail collectif,nous montrons comment en extraire des sous-parties thématiquement focaliséespour enrichir une autre ontologie, dite cible, de taille plus limitée mais de domainecentré sur une application particulière 1.	Fayçal Hamdi, Brigitte Safar, Chantal Reynaud	http://editions-rnti.fr/render_pdf.php?p1&p=1001176	http://editions-rnti.fr/render_pdf.php?p=1001176	ressource ontologique généraliste grand tailleont développer collectif aujourdhui disponible leweb lontologie yago énorme baser connaissance décrivantplu 2 million dentiter Afin tirer partir gigantesque travail collectifnous montrer extraire souspartie thématiquement focaliséespour enrichir ontologie cibl tailler plaire limité domainecentrer application 1
466	Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction et gestion d'informations pour la construction d'une base vidéo d'apprentissage	"Indexer une vidéo consiste à rattacher un ou plusieurs concepts à dessegments de cette vidéo, un concept étant défini comme une représentation intellectuelled'une idée abstraite. L'indexation automatique se base sur l'extractionautomatique de caractéristiques fournies par un système de traitement d'images.Cependant, il est nécessaire de définir les index ou concepts. Pour cela il fautdéfinir le lien qui existe entre ces caractéristiques et ces concepts. Ce qui sépareles caractéristiques extraites sur lesquelles se base l'indexation automatique etles concepts est appelé fossé sémantique qui est le manque de concordance entreles informations que les machines peuvent extraire depuis les documents numériqueset les interprétations que les humaines en font. La définition d'un conceptpeut être faite automatiquement si l'on dispose d'une base d'apprentissage liéeau concept. Dans ce cas, il est possible ""d'apprendre"" le concept de manièrestatistique. Mais la construction de cette base d'apprentissage nécessite de faireintervenir un utilisateur ou un expert applicatif. En fait, il s'agit de s'appuyer surses connaissances pour extraire des segments vidéo représentatifs du conceptque l'on souhaite définir. On peut lui demander d'indexer manuellement la based'apprentissage, mais cette opération est longue et fastidieuse. Dans cet article,nous proposons une méthode qui permet d'extraire l'expertise pour que l'implicationde l'expert soit la plus simple et la plus limitée possible."	Alain Simac-Lejeune	http://editions-rnti.fr/render_pdf.php?p1&p=1001190	http://editions-rnti.fr/render_pdf.php?p=1001190	Indexer vidéo consister rattacher concept dessegment vidéo concept définir représentation intellectuelledune idée abstrait lindexation automatique baser lextractionautomatique caractéristique fournir système traitement dimagescepender nécessaire définir index concept Pour celer fautdéfinir lien exister entrer caractéristique concept séparele caractéristique extrait baser lindexation automatique etl concept appeler fossé sémantique manquer concordance entrel information machine pouvoir extraire document numériqueset interprétation humain faire définition dun conceptpeut faire automatiquement lon disposer dune baser dapprentissage liéeau concept Dans cas dapprendre concept manièrestatistique Mais construction baser dapprentissage nécessit faireintervenir utilisateur expert applicatif En faire sagit sappuyer surser connaissance extraire segment vidéo représentatif conceptqu lon souhaiter définir pouvoir luire demander dindexer manuellemer basedapprentissage opération long fastidieux Dans articlenou proposer méthode permettre dextraire lexpertise limplicationd lexpert plaire simple plaire limité
467	Revue des Nouvelles Technologies de l'Information	EGC	2012	Extraction incrémentale de séquences fréquentes dans un flux d'itemsets		Thomas Guyet, Rene Quiniou	http://editions-rnti.fr/render_pdf.php?p1&p=1001206	http://editions-rnti.fr/render_pdf.php?p=1001206	
468	Revue des Nouvelles Technologies de l'Information	EGC	2012	Human Detection by a Small Autonomous Mobile Robot	Nous proposons une méthode utilisant les histogrammes de gradientorienté (HOG) et les séparateurs à vaste marge (SVM) pour la détection de personnesà partir d'images prises depuis un petit robot mobile autonome. Les travauxantérieurs réalisés dans le domaine de la détection d'êtres humains à partird'images ne peuvent pas être employés pour ce type d'application car ils supposentque les images sont prises à partir d'une position élevée (au moins lahauteur d'un petit enfant) alors que la taille de notre robot n'est que de 15cm.Nous employons à la fois les HOG et les SVM car cette combinaison de méthodesest reconnue comme étant celle ayant le plus de succès pour la détectionde personnes. Pour traiter une grande variété de formes humaines, principalementen raison de la distance existant entre les personnes et le robot, nous avonsdéveloppé une nouvelleméthode de prédiction à deux étapes utilisant deux typesde classificateurs SVM qui reposent sur une estimation de la distance. L'estimationest basée sur une proportion de pixels de couleur de peau dans l'image, cequi nous permet de clairement séparer notre problème de la détection de corpsentier et de celle de corps partiel. Les essais réalisés dans un bureau ont montrédes résultats prometteurs de notre méthode avec une valeur de F de 0,93.	Kouhei Takemoto, Shigeru Takano, Einoshin Suzuki	http://editions-rnti.fr/render_pdf.php?p1&p=1001172	http://editions-rnti.fr/render_pdf.php?p=1001172	proposer méthode utiliser histogramme gradientorienté hog séparateur vaste marge svm détection personnesà partir dimage prendre petit robot mobile autonome travauxantérieur réaliser dan domaine détection dêtr humain partirdimages pouvoir employer typer dapplication supposentque image prendre partir dune position élevé lahauteur dun petit enfant tailler robot nest 15cmnou employon hog svm combinaison méthodesest reconnaître plaire succès détectiond Pour traiter grand variété forme humain principalementen raison distancer exister entrer robot avonsdévelopper nouvelleméthode prédiction étape utiliser typesd classificateur svm reposer estimation distancer Lestimationest baser proportion pixel couleur peau dan limage cequi permettre clairement séparer problème détection corpsentier corps partiel réaliser dan bureau montréd résultat prometteur méthode 093
469	Revue des Nouvelles Technologies de l'Information	EGC	2012	Identification et caractérisation de différents types de boycott par des méthodes d'Analyse de Données		Henri Ralambondrainy, Marinette Amirault-Thébault	http://editions-rnti.fr/render_pdf.php?p1&p=1001207	http://editions-rnti.fr/render_pdf.php?p=1001207	
470	Revue des Nouvelles Technologies de l'Information	EGC	2012	K-moyennes contraintes par un classifieur Application à la personnalisation de scores de campagnes	Lorsqu'on désire contacter un client pour lui proposer un produit oncalcule au préalable la probabilité qu'il achètera ce produit. Cette probabilitéest calculée à l'aide d'un modèle prédictif pour un ensemble de clients. Le servicemarketing contacte ensuite ceux ayant la plus forte probabilité d'acheter leproduit. En parallèle, et avant le contact commercial, il peut être intéressant deréaliser une typologie des clients qui seront contactés. L'idée étant de proposerdes campagnes différenciées par groupe de clients. Cet article montre commentil est possible de contraindre la typologie, réalisée à l'aide des k-moyennes, àrespecter la proximité des clients vis-à-vis de leur score d'appétence.	Vincent Lemaire, Nicolas Creff, Fabrice Clérot	http://editions-rnti.fr/render_pdf.php?p1&p=1001155	http://editions-rnti.fr/render_pdf.php?p=1001155	lorsquon désir contacter client luire proposer produire oncalcul préalable probabilité quil acheter produire probabilitéest calculer laid dun modeler prédictif ensemble client servicemarketing contact ensuite plaire fort probabilité dacheter leproduit En parallèle contact commercial pouvoir intéresser deréaliser typologie client contacter lider proposerde campagne différencier grouper client article montr commentil contraindre typologie réaliser laid kmoyenne àrespecter proximité client visàvis scor dappétence
471	Revue des Nouvelles Technologies de l'Information	EGC	2012	L'extraction de règles de dépendance bien définies entre ensembles de variables multivaluées	Cet article étudie la faisabilité et l'intérêt de l'extraction de règles dedépendance entre ensembles de variables multivaluées en comparaison du problèmebien connu de l'extraction des règles d'association fréquentes. Une règlede dépendance correspond à une dépendance fonctionnelle approximative caractériséeprincipalement par l'entropie conditionnelle associée. L'article montrecomment établir une analogie formelle entre les deux familles de règles et commentadapter à l'aide de cette analogie l'algorithme « Eclat » afin d'extraire d'unjeu de données les règles de dépendance dites bien définies. Une étude expérimentaleconclut sur les forces et inconvénients des règles de dépendance biendéfinies vis-à-vis des règles d'association fréquentes	Frédéric Pennerath	http://editions-rnti.fr/render_pdf.php?p1&p=1001168	http://editions-rnti.fr/render_pdf.php?p=1001168	article étudier faisabilité lintérêt lextraction règle dedépendance entrer ensembl variable multivaluer comparaison problèmebien connaître lextraction règle dassociation fréquent règlede dépendanc correspondre dépendanc fonctionnel approximatif caractériséeprincipalement lentropie conditionnel associer Larticle montrecomment établir analogie formel entrer famille règle commentadapter laid analogie lalgorithm « Eclat » dextraire dunjeu donnée règle dépendance définie étude expérimentaleconclut inconvénient règle dépendanc biendéfinie visàvi règle dassociation fréquent
472	Revue des Nouvelles Technologies de l'Information	EGC	2012	Méta-règles pour la génération de règles négatives	La littérature s'est beaucoup intéressée à l'extraction de règles classiques(ou positives) et peu à l'extraction des règles négatives en raison essentiellementd'une part, du coût de calculs et d'autre part, du nombre prohibitif derègles redondantes et inintéressantes extraites. La démarche que nous avons retenueest de dégager les règles négatives lors de l'extraction des règles positives,et pour cela, nous recherchons les règles négatives que l'on peut inférer ou pas àpartir de la pertinence d'une règle positive. Ces différentes inférences vont êtreformalisées par un ensemble de méta-règles.	Sylvie Guillaume, Pierre-Antoine Papon	http://editions-rnti.fr/render_pdf.php?p1&p=1001162	http://editions-rnti.fr/render_pdf.php?p=1001162	littérature sest intéressé lextraction règle classiquesou positif lextraction règle négatif raison essentiellementdun partir coût calcul dautre partir nombre prohibitif derègl redondante inintéressant extrait démarcher retenueest dégager règle négatif lextraction règle positiveset celer rechercher règle négatif lon pouvoir inférer àpartir pertinence dune régler positif inférence aller êtreformaliser ensemble métarègl
473	Revue des Nouvelles Technologies de l'Information	EGC	2012	Mining Genetic Interactions in Genome-Wide Association Study	Advanced biotechnologies have rendered feasible high-throughput data collecting in human and other model organisms. The availability of such data holds promise for dissecting complex biological processes. Making sense of the flood of biological data poses great statistical and computational challenges. I will discuss the problem of mining gene-gene interactions in high-throughput genetic data. Finding genetic interactions is an important biological problem since many common diseases are caused by joint effects of genes. Previously, it was considered intractable to find genetic interactions in the whole-genome scale due to the enormous search space. The problem was commonly addressed using heuristics which do not guarantee the optimality of the solution. I will show that by utilizing the upper bound of the test statistic and effectively indexing the data, we can dramatically prune the search space and reduce computational burden. Moreover, our algorithms guarantee to find the optimal solution. In addition to handling specific statistical tests, our algorithms can be applied to a wide range of study types by utilizing convexity, a common property of many commonly used statistics.	Wei Wang    	http://editions-rnti.fr/render_pdf.php?p1&p=1001128	http://editions-rnti.fr/render_pdf.php?p=1001128	Advanced biotechnologie hav rendered feasibl highthroughput dater collecting in human and other model organism The availability of such dater holds promettre for dissecting complex biological process Making sense of the flood of biological dater pos great statistical and computational challeng ie will discuss the problem of mining genegene interacter in highthroughput genetic dater Finding genetic interaction is an importer biological problem since many common diseaser are caused by joindre effect of gene Previously it wa considered intractabl to find genetic interaction in the wholegenome scal to the enormou search space The problem wa commonly addressed using heuristic which do not guarantee the optimality of the solution ie will show that by utilizing the upper bound of the test statistic and effectively indexing the dater we can dramatically prune the search space and reduc computational burden Moreover our algorithm guarante to find the optimal solution In addition to handling specific statistical test our algorithms can be applied to wide rang of study typer by utilizing convexity common property of many commonly used statistic
474	Revue des Nouvelles Technologies de l'Information	EGC	2012	Modèle de supervision d'interactions non-intrusif basé sur les ontologies	L'automatisation et la supervision des systèmes pervasifs est à l'heureactuelle principalement basée sur l'utilisation massive de capteurs distribuésdans l'environnement. Dans cet article, nous proposons un modèle de supervisiond'interactions basé sur l'analyse sémantique des logs domotiques (commandesémises par l'utilisateur), visant à limiter l'utilisation de ces capteurs :le principe est d'utiliser des outils d'inférences avancés, afin de déduire les informationshabituellement captées. Pour cela, une ontologie, automatiquementdérivée d'un processus dirigé par les modèles, définit les interactions utilisateursystème.L'utilisation d'un système de règles permet ensuite d'inférer des informationssur la localisation et l'intention de l'utilisateur, dans le but de réaliserdu monitoring et de proposer des services domotiques adaptés.	Willy Allègre, Thomas Burger, Pascal Berruet, Jean-Yves Antoine	http://editions-rnti.fr/render_pdf.php?p1&p=1001148	http://editions-rnti.fr/render_pdf.php?p=1001148	lautomatisation supervision système pervasif lheureactuelle principalement baser lutilisation massif capteur distribuésdan lenvironnement Dans article proposer modeler supervisiondinteraction baser lanalyse sémantique log domotiqu commandesémise lutilisateur viser limiter lutilisation capteur principe dutiliser outil dinférences avancer déduire informationshabituellement capter Pour celer ontologie automatiquementdérivé dun processus diriger modèle définir interaction utilisateursystèmelutilisation dun système règle permettre ensuite dinférer informationssur localisation lintention lutilisateur dan boire réaliserdu monitoring proposer service domotiqu adapté
475	Revue des Nouvelles Technologies de l'Information	EGC	2012	PLS path modeling and regularized generalized canonical correlation analysis for multi-block data analysis	Regularized generalized canonical correlation analysis (RGCCA) is a generalization of regularizedcanonical correlation analysis to three or more sets of variables. It constitutes a generalframework for many multi-block data analysis methods. It combines the power of multi-blockdata analysis methods (maximization of well identified criteria) and the flexibility of PLS pathmodeling (the researcher decides which blocks are connected and which are not). Searchingfor a fixed point of the stationary equations related to RGCCA, a new monotone convergentalgorithm, very similar to the PLS algorithm proposed by Herman Wold, is obtained. Finally,a practical example is discussed.	Michel Tenenhaus	http://editions-rnti.fr/render_pdf.php?p1&p=1001133	http://editions-rnti.fr/render_pdf.php?p=1001133	Regularized generalized canonical correlation analysis RGCCA is generalization of regularizedcanonical correlation analysis to three or more set of variable it constitut generalframework for many multiblock dater analysis method it combine the power of multiblockdata analysis method maximization of well identified criteria and the flexibility of PLS pathmodeling the researcher decides which block are connected and which are not Searchingfor fixed poindre of the stationary equation related to rgcca new monotone convergentalgorithm very similar to the PLS algorithm proposed by Herman Wold is obtained Finallya practical example is discussed
476	Revue des Nouvelles Technologies de l'Information	EGC	2012	Prétraitement Supervisé des Variables Numériques pour la Fouille de Données Multi-Tables	Le prétraitement des variables numériques dans le contexte de lafouille de données multi-tables diffère de celui des données classiques individuvariable.La difficulté vient principalement des relations un-à-plusieurs où lesindividus de la table cible sont potentiellement associés à plusieurs enregistrementsdans des tables secondaires. Dans cet article, nous décrivons une méthodede discrétisation des variables numériques situées dans des tables secondaires.Nous proposons un critère qui évalue les discrétisations candidates pour ce typede variables. Nous décrivons un algorithme d'optimisation simple qui permetd'obtenir la meilleure discrétisation en intervalles de fréquence égale pour lecritère proposé. L'idée est de projeter dans la table cible l'information contenuedans chaque variable secondaire à l'aide d'un vecteur d'attributs (un attributpar intervalle de discrétisation). Chaque attribut représente le nombre de valeursde la variable secondaire appartenant à l'intervalle correspondant. Ces attributsd'effectifs sont conjointement partitionnés à l'aide de modèles en grille de donnéesafin d'obtenir une meilleure séparation des valeurs de la classe. Des expérimentationssur des jeux de données réelles et artificielles révèlent que l'approchede discrétisation permet de découvrir des variables secondaires pertinentes.	Dhafer Lahbib, Marc Boullé, Dominique Laurent	http://editions-rnti.fr/render_pdf.php?p1&p=1001191	http://editions-rnti.fr/render_pdf.php?p=1001191	prétraitement variable numérique dan contexte lafouille donnée multitabl diffèr donnée classique individuvariablela difficulté venir principalement relation unàplusieur lesindividus tabler cibl potentiellement associer enregistrementsdan table secondaire Dans article décrire méthodede discrétisation variable numérique situer dan table secondairesNous proposer critère évaluer discrétisation candidat typede variable décrire algorithme doptimisation simple permetdobtenir meilleur discrétisation intervalle fréquence égal lecritère proposer Lidée projeter dan tabler cibl linformation contenuedan variable secondaire laid dun vecteur dattributs attributpar intervall discrétisation attribut représenter nombre valeursde variable secondaire appartenir lintervalle correspondre attributsdeffectif conjointement partitionner laid modèle griller donnéesafin dobtenir meilleur séparation classer expérimentationssur jeu donnée réel artificiel révéler lapprochede discrétisation permettre découvrir variable secondaire pertinent
477	Revue des Nouvelles Technologies de l'Information	EGC	2012	Raisonner sur une ontologie cartographique pour concevoir des légendes de cartes	Concevoir une carte géographique, plus particulièrement sa légende,exige des compétences spécifiques. L'objectif de ce papier est de présenter unebase de connaissances destinée à aider tout utilisateur à concevoir une ou plusieurslégendes adaptées à son besoin et conformes aux règles de cartographie.La base de connaissances est formée d'une ontologie de la cartographie nomméeOntoCarto, d'un corpus de règles : OntoCartoRules et d'un moteur de raisonnement: Corese. Dans ce papier, chaque demande de conception de légende estvue comme une instanciation particulière de l'ontologie, associée à une sélectionde règles pertinentes dans le corpus de règles, sur laquelle Corese va raisonnerpour construire des légendes adaptées à la configuration spécifique traitée. Laconception de la légende s'appuie sur la définition de deux hiérarchies d'objetsgéographiques et cartographiques. Les principes de fonctionnement de Coresesont présentés. Un prototype a été implémenté et des extraits des résultats sontmontrés.	Catherine Dominguès, Olivier Corby, Fayrouz Soualah-Alila	http://editions-rnti.fr/render_pdf.php?p1&p=1001178	http://editions-rnti.fr/render_pdf.php?p=1001178	concevoir carte géographique plaire légendeexige compétence spécifique Lobjectif papier poster unebase connaissance destiner aider utilisateur concevoir plusieurslégende adapter besoin conforme règle cartographieLa baser connaissance former dune ontologie cartographie nomméeontocarto dun corpu règle   ontocartorul dun moteur raisonnement Corese Dans papier demander conception légend estvue instanciation lontologie associer sélectionde règle pertinent dan corpus règle corese aller raisonnerpour construire légende adapter configuration spécifique traité laconception légende sappuie définition hiérarchie dobjetsgéographiqu cartographique principe fonctionnement coresesont présenter prototype implémenter extrait résultat sontmontré
478	Revue des Nouvelles Technologies de l'Information	EGC	2012	Recherche d'Information Agrégée dans des documents XML basée sur les Réseaux Bayésiens	Dans cet article, nous nous intéressons à la recherche agrégée dansdes documents XML. Pour cela, nous proposons un modèle basé sur les réseauxbayésiens. Les relations de dépendances entre requête-termes d'indexation ettermes d'indexation-éléments sont quantifiées par des mesures de probabilité.Dans ce modèle, la requête de l'utilisateur déclenche un processus de propagationpour trouver des éléments. Ainsi, au lieu de récupérer une liste des élémentsqui sont susceptibles de répondre à la requête, notre objectif est d'agréger dansun agrégat des éléments pertinents, non-redondants et complémentaires. Nousavons évalué notre approche dans le cadre de la compagne d'évaluation INEX2009 et avons présenté quelques résultats expérimentaux mettant en évidencel'impact de l'agrégation de tels éléments.	Najeh Naffakhi, Mohand Boughanem, Rim Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1001202	http://editions-rnti.fr/render_pdf.php?p=1001202	Dans article intéresser rechercher agréger dansde document xml Pour celer proposer modeler baser réseauxbayésien relation dépendance entrer requêteterme dindexation etterme dindexationélément quantifier mesure probabilitéDans modeler requête lutilisateur déclencher processus propagationpour trouver élément lieu récupérer liste élémentsqui susceptible répondre requête objectif dagréger dansun agrégat élément pertinent nonredondant complémentaire nousavon évaluer approcher dan cadrer compagnon dévaluation inex2009 présenter résultat expérimental mettre évidencelimpact lagrégation élément
479	Revue des Nouvelles Technologies de l'Information	EGC	2012	Relational Learning from Spatial Data: Retrospect and Prospect	Learning from spatial data is characterized by two main features. First, spatial objects have a locational property which implicitly defines several spatial relationships (topological, directional, distancebased) between objects. Second, attributes of spatially related units tend to be statistically correlated. These two features argue against the assumption of the independent generation of data samples (i.i.d. assumption) underlying classic machine learning algorithms, and motivate the application of relational learning algorithms, whose inferences are based on both instance properties and relations between data. This relational learning approach to spatial domains has already been investigated in the last decade, and important accomplishments in this direction have already been performed. In this talk, we retrospectively survey major achievements on relational learning from spatial data and we report open problems which still challenges researchers and prospectively suggest important topics for incorporation into a research agenda.	Donato Malerba	http://editions-rnti.fr/render_pdf.php?p1&p=1001131	http://editions-rnti.fr/render_pdf.php?p=1001131	Learning from spatial dater is characterized by two main featur First spatial object hav locational property which implicitly defines several spatial relationships topological directional distancebased between objects Second attribut of spatially related unit tendre to be statistically correlated These two featur arguer against the assumption of the independent generation of dater sampl iid assumption underlying classic machiner learning algorithm and motivate the application of relational learning algorithm whos inferenc are based both instance propertie and relation between dater This relational learning approach to spatial domains has already been investigated in the last decade and importer accomplishment in this direction hav already been performed In this talk we retrospectively survey major achievement relational learning from spatial dater and we report open problems which still challeng researchers and prospectively suggest importer topic for incorporation into research agenda
480	Revue des Nouvelles Technologies de l'Information	EGC	2012	Réorganisation hiérarchique de visualisations dans OLAP	"Dans cet article nous proposons un nouvel algorithme pour la réorganisationhiérarchique des cubes OLAP (On-Line Analytical Processing) ayantpour objectif d'améliorer leur visualisation. Cet algorithme se caractérise par lefait qu'il peut traiter des dimensions organisées hiérarchiquement et optimiserconjointement les dimensions du cube, contrairement aux autres approches. Ilutilise un algorithme génétique qui réorganise des arbres n-aires quelconques. Ila été intégré dans une interface OLAP puis testé en comparaison avec d'autresapproches de réorganisation, et fournit des résultats très positifs. A ce titre,nous avons également généralisé l'algorithme heuristique classique BEA (""bondenergy algorithm"") au cas de hiérarchies OLAP. Enfin, notre approche a été évaluéepar des utilisateurs et les résultats soulignent l'intérêt de la réorganisationdans des exemples de tâches à résoudre pour OLAP."	Sébastien Lafon, Fatma Bouali, Christiane Guinot, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1001181	http://editions-rnti.fr/render_pdf.php?p=1001181	Dans article proposer nouvel algorithme réorganisationhiérarchique cube OLAP onlin analytical Processing ayantpour objectif daméliorer visualisation algorithme caractériser lefer quil pouvoir traiter dimension organiser hiérarchiquement optimiserconjointement dimension cuber contrairement approche ilutilise algorithme génétique réorganiser arbre nair Ila intégrer dan interface OLAP pouvoir tester comparaison dautresapproche réorganisation fournir résultat positif A titrenou également généraliser lalgorithm heuristique classique bea bondenergy algorithm cas hiérarchie OLAP Enfin approcher évaluéepar utilisateur résultat souligner lintérêt réorganisationdans exemple tâche résoudre olap
481	Revue des Nouvelles Technologies de l'Information	EGC	2012	Représentations de services web : impact sur la découverte et la recommandation		Mustapha Aznag, Mohamed Quafafou, Nicolas Durand, Zahi Jarir	http://editions-rnti.fr/render_pdf.php?p1&p=1001203	http://editions-rnti.fr/render_pdf.php?p=1001203	
482	Revue des Nouvelles Technologies de l'Information	EGC	2012	RICSH : Recherche d'information contextuelle par segmentation thématique de documents	Le but principal des systèmes de recherche d'informations (SRI) classiquesest de retrouver dans un corpus de documents l'information considéréecomme pertinente pour une requête utilisateur. Cette pertinence est souvent liéeà la fréquence d'apparition des termes dans le texte par rapport au corpus sanstenir compte du contexte de la recherche. Partant de ce constat, nous proposonsdans cet article une approche pour la recherche d'information contextuelle parsegmentation thématique de documents (RICSH). Cette approche s'appuie surla méthode de pondération tf-idf que nous avons adaptée dans notre cas pourindexer le corpus. Cette adaptation se situe au niveau de l'importance du termeet de son pouvoir de discrimination par rapport aux fragments de textes et nonau corpus. Ces fragments sont obtenus grâce à un processus d'identification desunités thématiques les plus pertinentes pour chaque document.	Fadila Bentayeb, Omar Boussaid, Rachid Aknouche	http://editions-rnti.fr/render_pdf.php?p1&p=1001199	http://editions-rnti.fr/render_pdf.php?p=1001199	boire principal système rechercher dinformation SRI classiquesest retrouver dan corpus document linformation considéréecomme pertinent requête utilisateur pertinence liéeà fréquence dapparition terme dan texte rapport corpus sanstenir compter contexte rechercher partir constat proposonsdans article approcher rechercher dinformation contextuel parsegmentation thématique document RICSH approcher sappui surla méthod pondération tfidf adapter dan cas pourindexer corpus adaptation situer niveau limportance termeet pouvoir discrimination rapport fragment texte nonau corpus fragment obtenir grâce processus didentification desuniter thématique plaire pertinent document
483	Revue des Nouvelles Technologies de l'Information	EGC	2012	Sélection Bayésienne de Modèles avec Prior Dépendant des Données	Cet article analyse la consistance asymptotique des modèles en grilleappliqués à l'estimation de densité jointe de deux variables catégorielles. Lesmodèles en grille considèrent un partitionnement des valeurs de chacune des variables,le produit Cartésien des partitions formant une grille dont les cellulespermettent de résumer la table de contingence des deux variables. Le meilleurmodèle de co-partitionnement est recherché au moyen d'une approche MAP(maximum a posteriori), présentant la particularité peu orthodoxe d'exploiterune famille de modèles et une distribution a priori de ces modèles qui dépendentdes données. Ces modèles sont par nature des modèles de l'échantillon d'apprentissage,et non de la distribution sous-jacente. Nous démontrons la consistancede l'approche, qui se comporte comme un estimateur universel de densité jointeconvergeant asymptotiquement vers la vraie distribution jointe.	Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001136	http://editions-rnti.fr/render_pdf.php?p=1001136	article analyser consistance asymptotique modèle grilleappliquer lestimation densité joint variable catégoriel lesmodèl griller considérer partitionnement variablesle produire cartésien partition former griller cellulespermetter résumer tabler contingence variable meilleurmodèle copartitionnement rechercher moyen dune approcher MAPmaximum posteriori présenter particularité orthodoxe dexploiterune famille modèle distribution priori modèle dépendentde donnée modèle nature modèle léchantillon dapprentissageet distribution sousjacent démontrer consistanced lapproche comporter estimateur universel densité jointeconvergeer asymptotiquement ver vrai distribution joint
484	Revue des Nouvelles Technologies de l'Information	EGC	2012	Solving Problems with Visual Analytics: Challenges and Applications	Never before in history data is generated and collected at such high volumes as it is today. As the volumes of data available to business people, scientists, and the public increase,their effective use becomes more challenging. Keeping up to date with the flood of data,using standard tools for data analysis and exploration, is fraught with difficulty. The field ofvisual analytics seeks to provide people with better and more effective ways to understandand analyze large datasets, while also enabling them to act upon their findings immediately. Visual analytics integrates the analytic capabilities of the computer and the abilities of the human analyst, allowing novel discoveries and empowering individuals to take control of the analytical process. Visual analytics enables unexpected and hidden insights, which may lead to beneficial and profitable innovation. The talk presents the challenges of visual analytics and exemplifies them with application examples, illustrating the exiting potential of current visual analysis techniques.	Daniel Keim	http://editions-rnti.fr/render_pdf.php?p1&p=1001130	http://editions-rnti.fr/render_pdf.php?p=1001130	never before in history dater is generated and collected at such high volume it is today the volumer of dater availabl to business peopl scientists and the public increasetheir effectif us become more challenging Keeping up to dater with the flood of datausing standard tool for dater analysis and exploration is fraught with difficulty The field ofvisual analytic seeks to provide people with better and more effectif ways to understandand analyze large dataset while also enabling them to act upon their finding immediately Visual analytic integrat the analytic capabilitier of the computer and the abilitier of the human analyst allowing novel discoverie and empowering individual to take control of the analytical process Visual analytics enabl unexpected and hidden insight which may lead to beneficial and profitable innovation The talk present the challeng of visual analytic and exemplifier them with application exampl illustrating the exiting potential of current visual analysis technique
485	Revue des Nouvelles Technologies de l'Information	EGC	2012	Structuration des décisions de jurisprudence basée sur une ontologie juridique en langue arabe	L'informatique juridique, est un domaine en évolution constante. Lecontexte général de notre travail est l'élaboration d'un système de recherchede jurisprudence tunisienne en langue arabe. L'objectif opérationnel de ce systèmeest de fournir une aide aux juristes pour résoudre une situation juridiquedonnée en mettant à leur disposition une collection de situations similaires cequi améliorera leur raisonnement futur. Une ontologie du domaine juridiqueconstruite à partir des documents des décisions juridiques est nécessaire dansnotre contexte.Cette ontologie a pour but : (i) la structuration des décisions, (ii)la formulation des requêtes d'interrogation de la base des décisions, et (iii) larecherche des décisions. Dans cet article, nous présentons l'architecture de notresystème de recherche de jurisprudence. Nous nous focalisons sur l'ontologie dudomaine de jurisprudence que nous avons élaborée, aisni que sur le module destructuration des décisions.	Karima Dhouib, Sylvie Desprès, Faïez Gargouri	http://editions-rnti.fr/render_pdf.php?p1&p=1001175	http://editions-rnti.fr/render_pdf.php?p=1001175	Linformatique juridique domaine évolution constant Lecontexte général travail lélaboration dun système recherched jurisprudence tunisien langue arabe Lobjectif opérationnel systèmeest fournir aider juriste résoudre situation juridiquedonner mettre disposition collection situation similaire cequi améliorer raisonnement futur ontologie domaine juridiqueconstruite partir document décision juridique nécessaire dansnotre contextecette ontologie boire   ie structuration décision iila formulation requêt dinterrogation baser décision iii larecherche décision Dans article présenter larchitecture notresystèm rechercher jurisprudence focaliser lontologie dudomaine jurisprudence élaborer aisni moduler destructuration décision
486	Revue des Nouvelles Technologies de l'Information	EGC	2012	SweetDeki : le wiki sémantique couteau suisse du réseau social ISICIL	Le projet ANR ISICIL 1 mixe les nouvelles applications virales duweb avec des représentations formelles et des processus d'entreprise pour les intégrerdans les pratiques de veille en entreprise. Les outils développés s'appuientsur les interfaces avancées des applications du web 2.0 (blog, wiki, social bookmarking,extensions de navigateurs) pour les interactions et sur les technologiesdu web sémantique pour l'interopérabilité et le traitement de l'information. Leprésent article décrit plus précisément le wiki sémantique développé dans lecadre de ce projet et son intégration au coeur du framework ISICIL	Michel Buffa, Guillaume Husson, Nicolas Delaforge	http://editions-rnti.fr/render_pdf.php?p1&p=1001151	http://editions-rnti.fr/render_pdf.php?p=1001151	projet ANR isicil 1 mixer application viral duweb représentation formel processus dentrepris intégrerdan pratique veiller entreprendre outil développer sappuientsur interface avancer application web 20 blog wiki social bookmarkingextension navigateur interaction technologiesdu web sémantique linteropérabilité traitement linformation Leprésent article décrire plaire précisément wiki sémantique développer dan lecadre projet intégration coeur framework isicil
487	Revue des Nouvelles Technologies de l'Information	EGC	2012	TMD-MINER : Une nouvelle approche pour la détection des diffuseurs dans un système communautaire	Plusieurs méthodes ont été développées ces dernières années pour détecter,dans un réseau social, les membres qualifiés, selon les auteurs, d'influenceurs,de médiateurs, d'ambassadeurs ou encore d'experts. Dans cet article, nousproposons un nouveau cadre méthodologique permettant d'identifier des diffuseursdans le contexte où seule l'information sur l'appartenance des membres duréseau à des communautés est disponible. Ce cadre, basé sur une représentationdu réseau sous forme d'hypergraphe, nous a permis de formaliser la notion dediffuseur et d'introduire l'algorithme TMD-MINER, dédié à la détection des diffuseurset basé sur les itemsets essentiels.	Mohamed Nidhal Jelassi, Christine Largeron, Sadok Ben Yahia	http://editions-rnti.fr/render_pdf.php?p1&p=1001193	http://editions-rnti.fr/render_pdf.php?p=1001193	méthode développer année détecterdans réseau social membre qualifier auteur dinfluenceursd médiateur dambassadeur dexperts Dans article nousproposer cadrer méthodologique permettre didentifier diffuseursdan contexte linformation lappartenance membre duréseau communauté disponible cadrer baser représentationdu réseau sou former dhypergraph permettre formaliser notion dediffuseur dintroduir lalgorithm tmdminer dédier détection diffuseurset baser itemset essentiel
488	Revue des Nouvelles Technologies de l'Information	EGC	2012	Topological Decomposition and Heuristics for High Speed Clustering of Complex Networks	With the exponential growth in the size of data and networks, developmentof new and fast techniques to analyze and explore these networks isbecoming a necessity. Moreover the emergence of scale free and small worldproperties in real world networks has stimulated lots of activity in the field ofnetwork analysis and data mining. Clustering remains a fundamental techniqueto explore and organize these networks. A challenging problem is to find a clusteringalgorithm that works well in terms of clustering quality and is efficient interms of time complexity.In this paper, we propose a fast clustering algorithm which combines someheuristics with a Topological Decomposition to obtain a clustering. The algorithmwhich we call Topological Decomposition and Heuristics for Clustering(TDHC) is highly efficient in terms of asymptotic time complexity as comparedto other existing algorithms in the literature. We also introduce a number ofHeuristics to complement the clustering algorithm which increases the speed ofthe clustering process maintaining the high quality of clustering. We show theeffectiveness of the proposed clustering method on different real world data setsand compare its results with well known clustering algorithms.	Faraz Zaidi, Guy Melançon	http://editions-rnti.fr/render_pdf.php?p1&p=1001147	http://editions-rnti.fr/render_pdf.php?p=1001147	With the exponential growth in the size of dater and network developmentof new and fast technique to analyze and explorer these network isbecoming necessity Moreover the emergence of scale free and small worldpropertier in real world network has stimulated lot of activity in the field ofnetwork analysis and dater mining Clustering remain fundamental techniqueto explorer and organize these network challenging problem is to find clusteringalgorithm that work well in terms of clustering quality and is efficient interm of tim complexityin this paper we proposer fast clustering algorithm which combiner someheuristic with Topological decomposition to obtain clustering The algorithmwhich we call Topological decomposition and Heuristics for ClusteringTDHC is highly efficier in terms of asymptotic tim complexity comparedto other existing algorithm in the literatur We also introduc number ofheuristics to complemer the clustering algorithm which increas the speed ofthe clustering process maintaining the high quality of clustering We show theeffectiveness of the proposed clustering method real world dater setsand comparer it results with well known clustering algorithm
489	Revue des Nouvelles Technologies de l'Information	EGC	2012	Transformation de l'espace de description pour l'apprentissage par transfert	"Dans ce papier, nous proposons une étude sur l'utilisation de l'apprentissagetopologique pondéré et les méthodes de factorisation matricielle pourtransformer l'espace de représentation d'un jeu de données ""sparse"" afin d'augmenterla qualité de l'apprentissage, et de l'adapter au cas de l'apprentissagepar transfert. La factorisation matricielle nous permet de trouver des variableslatentes et l'apprentissage topologique pondéré est utilisé pour détecter les pluspertinentes parmi celles-ci. La représentation de nouvelles données est basée surleurs projections sur le modèle topologique pondéré.Pour l'apprentissage par transfert, nous proposons une nouvelle méthode où lareprésentation des données est faite de la même manière que dans la premièrephase, mais en utilisant un modèle topologique élagué.Les expérimentations sont présentées dans le cadre d'un Challenge Internationaloù nous avons obtenu des résultats prometteurs (5ieme rang de la compétitioninternationale).1 Introduction"	Nistor Grozavu, Younès Bennani, Lazhar Labiod	http://editions-rnti.fr/render_pdf.php?p1&p=1001185	http://editions-rnti.fr/render_pdf.php?p=1001185	Dans papier proposer étude lutilisation lapprentissagetopologique pondérer méthode factorisation matriciel pourtransformer lespace représentation dun jeu donnée sparse daugmenterla qualité lapprentissage ladapter cas lapprentissagepar transfert factorisation matriciel permettre trouver variableslatente lapprentissage topologique pondérer utiliser détecter pluspertinente cellesci représentation donnée baser surleur projection modeler topologique pondérépour lapprentissage transfert proposer méthode lareprésentation donnée faire manière dan premièrephase utiliser modeler topologique élaguél expérimentation présenter dan cadrer dun Challenge Internationaloù obtenir résultat prometteur 5ieme rang compétitioninternationale1 introduction
490	Revue des Nouvelles Technologies de l'Information	EGC	2012	Un algorithme de classification automatique pour des données relationnelles multi-vues	classification automatique (De Carvalho et al., 2012) capable de partitionnerdes objets en prenant en compte de manière simultanée plusieurs matricesde dissimilarité qui les décrivent. Ces matrices peuvent avoir été généréesen utilisant différents ensembles de variables et de fonctions de dissimilarité.Cette méthode, basée sur l'algorithme de nuées dynamiques est conçu pour fournirune partition et un prototype pour chaque classe tout en découvrant une pondérationpertinante pour chaque matrice de dissimilarité en optimisant un critèred'adéquation entre les classes et leurs représentants. Ces pondérations changentà chaque itération de l'algorithme et sont différentes pour chacune des classes.Nous présentons aussi plusieurs outils d'aide à l'interprétation des groupes et dela partition fournie par cette nouvelle méthode. Deux exemples illustrent l'interêtde la méthode. Le premier utilise des données concernant des chiffres manuscrits(0 à 9) numérisés en images binaires provenant de l'UCI. Le second utilise unensemble de rapports dont nous connaissons une classification experte donnée àpriori.	Francisco de Assis Tenório de Carvalho, Filipe M. de Melo, Yves Lechevallier, Thierry Despeyroux	http://editions-rnti.fr/render_pdf.php?p1&p=1001142	http://editions-rnti.fr/render_pdf.php?p=1001142	classification automatique De Carvalho al 2012 capable partitionnerd objet prendre compter manière simultaner matricesde dissimilarité décrire matrice pouvoir généréesen utiliser ensemble variable fonction dissimilaritécette méthode baser lalgorithme nuée dynamique concevoir fournirune partition prototype classer découvrir pondérationpertinante matrice dissimilarité optimiser critèredadéquation entrer classe représentant pondération changentà itération lalgorithme classesnou présenton outil daid linterprétation groupe dela partition fourni méthode Deux exemple illustrer linterêtde méthode utiliser donnée concerner chiffre manuscrits0 9 numériser image binaire provenir luci second utiliser unensembl rapport connaître classification expert donner àpriori
491	Revue des Nouvelles Technologies de l'Information	EGC	2012	Un assistant utilisateur pour le choix et le paramétrage des méthodes de fouille visuelle de données	Nous nous intéressons dans cet article au problème de l'automatisation du processus de choix et de paramétrage des visualisations en fouille visuelle de données. Pour résoudre ce problème, nous avons développé un assistant utilisateur qui effectue deux étapes : à partir des objectifs annoncés par l'utilisateur et des caractéristiques de ses données, le système commence par proposer à l'utilisateur différents appariements entre la base de données à visualiser et les visualisations qu'il gère. Ces appariements sont générés par une heuristique utilisant une base de connaissances sur les visualisations et la perception visuelle. Ensuite, afin d'affiner les différents paramétrages suggérés par le système, nous utilisons un algorithme génétique interactif qui permet aux utilisateurs d'évaluer et d'ajuster visuellement ces paramétrages. Nous présentons une évaluation utilisateur qui montre l'intérêt de notre système pour deux tâches.	Abdelheq Et-tahir Guettala, Fatma Bouali, Christiane Guinot, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1001180	http://editions-rnti.fr/render_pdf.php?p=1001180	intéresser dan article problème lautomatisation processus choix paramétrage visualisation fouiller visuel donnée Pour résoudre problème développer assister utilisateur effectuer étape   partir objectif annoncer lutilisateur caractéristique donnée système commencer proposer lutilisateur appariement entrer baser donnée visualiser visualisation quil gérer appariement générer heuristique utiliser baser connaissance visualisation perception visuel ensuite daffiner paramétrage suggérer système utiliser algorithme génétique interactif permettre utilisateur dévaluer dajuster visuellemer paramétrage présenter évaluation utilisateur montrer lintérêt système tâche
492	Revue des Nouvelles Technologies de l'Information	EGC	2012	Un environnement efficace pour la classification d'images à grande échelle	La plupart des processus de classification d'images comportent troisprincipales étapes : l'extraction de descripteurs de bas niveaux, la création d'unvocabulaire visuel par quantification et l'apprentissage à l'aide d'un algorithmede classification (eg.SVM). De nombreux problèmes se posent pour le passageà l'échelle comme avec l'ensemble de données ImageNet contenant 14 millionsd'images et 21,841 classes. La complexité concerne le temps d'exécution dechaque tâche et les besoins en mémoire et disque (eg. le stockage des SIFTs nécessite11To). Nous présentons une version parallèle de LibSVM pour traiter degrands ensembles de données dans un temps raisonnable. De plus, il y a beaucoupde perte d'information lors de la phase de quantification et les mots visuelsobtenus ne sont pas assez discriminants pour de grands ensembles d'images.Nous proposons d'utiliser plusieurs descripteurs simultanément pour améliorerla précision de la classification sur de grands ensembles d'images. Nous présentonsnos premiers résultats sur les 10 plus grandes classes (24,817 images)d'ImageNet.	Thanh-Nghi Doan, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1001189	http://editions-rnti.fr/render_pdf.php?p=1001189	processus classification dimager comporter troisprincipal étape   lextraction descripteur niveau création dunvocabulair visuel quantification lapprentissage laid dun algorithmede classification egsvm problème poser passageà léchell lensembl donnée imagenet contenir 14 millionsdimage 21841 classe complexité concerner temps dexécution dechaqu tâcher besoin mémoire disqu eg stockage sift nécessite11To présenter version parallèle LibSVM traiter degrands ensembl donnée dan temps raisonnable De plaire yu beaucoupde perte dinformation phase quantification visuelsobtenus discriminant grand ensemble dimagesnous proposer dutiliser descripteur simultanément améliorerla précision classification grand ensembl dimage présentonsnos résultat 10 plaire grand classe 24817 imagesdimagenet
493	Revue des Nouvelles Technologies de l'Information	EGC	2012	Une approche multidimensionnelle basée sur les comportements individuels pour la prédiction de la diffusion de l'information sur Twitter	Aujourd'hui, les réseaux sociaux en ligne sont devenus des outils trèspuissants de propagation de l'information. Ils favorisent la diffusion rapide àgrande échelle de contenu et les conséquences d'une information inexacte voirefausse peuvent alors prendre une ampleur considérable. Par conséquent il devientindispensable de proposer des moyens d'analyser le phénomène de diffusionde l'information dans ces réseaux. De nombreuses études récentes ont traitéde la modélisation du processus de diffusion de l'information, essentiellementd'un point de vue topologique et dans une perspective théorique, mais les facteursimpliqués sont encore méconnus. Nous proposons ici une solution pratiquedont l'objectif est de prédire la dynamique temporelle de la diffusion au sein deTwitter, basée sur des techniques d'apprentissage automatique. Notre approcherepose sur l'inférence de probabilités de diffusion tirées d'une analyse multidimensionnelledes comportements individuels. Les expérimentations menéesmontrent l'intérêt de la modélisation proposée.	Adrien Guille, Hakim Hacid, Cécile Favre	http://editions-rnti.fr/render_pdf.php?p1&p=1001173	http://editions-rnti.fr/render_pdf.php?p=1001173	aujourdhui réseau social ligne devenir outil trèspuissant propagation linformation favoriser diffusion rapide àgrand échelle contenir conséquence dune information inexact voirefauss pouvoir prendre ampleur considérable Par conséquent devientindispensabl proposer moyen danalyser phénomène diffusionde linformation dan réseau étude récent traitéde modélisation processus diffusion linformation essentiellementdun poindre topologique dan perspectif théorique facteursimpliqué méconnaître proposer solution pratiquedont lobjectif prédire dynamique temporel diffusion detwitter baser technique dapprentissage automatique approcherepose linférence probabilité diffusion tirer dune analyser multidimensionnelled comportement individuel expérimentation menéesmontrent lintérêt modélisation proposer
494	Revue des Nouvelles Technologies de l'Information	EGC	2012	Une distance hiérarchique basée sur la sémantique pour la comparaison d'histogrammes nominaux	La plupart des distances entre histogrammes sont définies pour comparerdes histogrammes ordonnés (dont les entités représentées sont totalementordonnées) ou des histogrammes nominaux (dont les entités représentées nepeuvent pas être comparées). Cependant, il n'existe aucune distance qui permettede comparer des histogrammes nominaux dans lesquels il est possible dequantifier des valeurs de proximité sémantique entre les entités considérées. Cetarticle propose une nouvelle distance permettant de pallier ce problème. Dans unpremier temps, une hiérarchie d'histogrammes, obtenue par le biais d'une fusionprogressive des entités considérées (prenant en compte leurs proximités sémantiques),est construite. Pour chaque étage de cette hiérarchie, une distance standardde comparaison d'histogrammes nominaux est calculée. Finalement, pourobtenir la distance proposée, ces différentes distances sont fusionnées en prenanten compte la cohérence sémantique associée aux niveaux de chaque étage de lahiérarchie. Cette distance a été validée dans le cadre de la classification de donnéesgéographiques. Les résultats obtenus sont encourageants et montrent ainsil'intérêt et l'utilité de cette dernière pour des processus de fouille de données.	Camille Kurtz	http://editions-rnti.fr/render_pdf.php?p1&p=1001144	http://editions-rnti.fr/render_pdf.php?p=1001144	distance entrer histogramme définir comparerdes histogramme ordonner entité représenter totalementordonner histogramme nominal entité représenter nepeuvent comparer nexiste distancer permettede comparer histogramme nominal dan dequantifier proximité sémantique entrer entité considérer cetarticle proposer distancer permettre pallier problème Dans unpremier temps hiérarchie dhistogramm obtenir biais dune fusionprogressive entité considérer prendre compter proximité sémantiquesest construire Pour étager hiérarchie distancer standardd comparaison dhistogramm nominal calculer finalement pourobtenir distancer proposer distance fusionner prenanten compter cohérence sémantique associer niveau étager lahiérarchie distancer valider dan cadrer classification donnéesgéographique résultat obtenir encourageant montrer ainsilintérêt lutilité processus fouiller donnée
495	Revue des Nouvelles Technologies de l'Information	EGC	2012	User Evaluation: Why?	Research in information visualisation has changed significantly in the past two decades.Once it was sufficient to simply design and implement an impressive visualisation system.Today editors and reviewers expect papers to present not only a novel system, but empiricalevidence of its worth. Why has this change come about, and what impact has it had on thoseworking in this area? This talk will discuss how a field dominated by algorithms and toolsbecame infected by human participants, and why this is a positive development in a maturingresearch discipline.	Helen Purchase	http://editions-rnti.fr/render_pdf.php?p1&p=1001132	http://editions-rnti.fr/render_pdf.php?p=1001132	Research in information visualisation has changed significantly in the past two decadesonce it wer sufficient to simply design and implemer an impressif visualisation systemtoday editors and reviewer expect paper to preser not only novel system boire empiricalevidence of it worth Why has this changer come about and what impact has it had thoseworking in this area This talk will discuss how field dominated by algorithms and toolsbecam infected by human participant and why this is positif development in maturingresearch discipliner
496	Revue des Nouvelles Technologies de l'Information	EGC	2012	Utilisation d'invariants pour une médiation inter-domaines de modèles utilisateurs : ressources invariantes et invariants sémantiques	Les services de personnalisation du Web 2.0 reposent sur l'exploitationde modèles utilisateurs. Schématiquement, plus la quantité d'informationssur les utilisateurs est grande, meilleures sont la modélisation et la qualité du service.En pratique, nombre de services rencontrent un problème de manque d'informationssur les utilisateurs. Dans cet article, nous y répondons par médiationinter-domaines de modèles utilisateurs, c'est-à-dire la complétion de modèles enexploitant des données d'un autre domaine. La médiation que nous proposonsrepose sur un transfert d'informations inter-domaines. Ce transfert consiste enl'utilisation de couples invariants ou très corrélés pouvant être des couples deressources ou de descripteurs sémantiques, identifiés après enrichissement sémantiquedes modèles. Nous montrons que le transfert sous forme de couple deressources permet une complétion de qualité et que l'exploitation de descripteurssémantiques augmente la couverture à qualité égale. Enrichir sémantiquementest donc bénéfique pour le transfert inter-domaines.	Emilien Perrin, Armelle Brun, Anne Boyer	http://editions-rnti.fr/render_pdf.php?p1&p=1001170	http://editions-rnti.fr/render_pdf.php?p=1001170	service personnalisation web 20 reposer lexploitationde modèle utilisateur schématiquemer plaire quantité dinformationssur utilisateur grand meilleure modélisation qualité serviceEn pratiquer nombre service rencontrer problème manquer dinformationssur utilisateur Dans article yu répondre médiationinterdomaine modèle utilisateur cestàdir complétion modèle enexploiter donnée dun domaine médiation proposonsrepose transfert dinformation interdomain transfert consist enlutilisation couple invariant corrélé pouvoir couple deressource descripteur sémantique identifié enrichissement sémantiqued modèle montrer transfert sou former coupler deressource permettre complétion qualité lexploitation descripteurssémantique augmenter couverture qualité égal Enrichir sémantiquementest bénéfique transfert interdomain
497	Revue des Nouvelles Technologies de l'Information	EGC	2012	Validation et optimisation d'une décomposition hiérarchique de graphes	De nombreux algorithmes de fragmentation de graphes fonctionnentpar agrégations ou divisions successives de sous-graphes menant à une décompositionhiérarchique du réseau étudié. Une question importante dans ce domaineest de savoir si cette hiérarchie reflète la structure du réseau ou si elle n'estqu'un artifice lié au déroulement de la procédure. Nous proposons un moyen devalider et, au besoin, d'optimiser la décomposition multi-échelle produite parce type de méthode. On applique notre approche sur l'algorithme proposé parBlondel et al. (2008) basé sur la maximisation de la modularité. Dans ce cadre,une généralisation de cette mesure de qualité au cas multi-niveaux est introduite.Nous testons notre méthode sur des graphes aléatoires ainsi que sur des exemplesréels issus de divers domaines.	Francois Queyroi	http://editions-rnti.fr/render_pdf.php?p1&p=1001182	http://editions-rnti.fr/render_pdf.php?p=1001182	algorithme fragmentation graphe fonctionnentpar agrégation division successif sousgraphe mener décompositionhiérarchiqu réseau étudier question important dan domaineest savoir hiérarchie refléter structurer réseau nestquun artifice lier déroulement procédure proposer moyen devalider besoin doptimiser décomposition multiéchell produire typer méthode appliquer approcher lalgorithm proposer parblondel al 2008 baser maximisation modularité Dans cadreun généralisation mesurer qualité cas multiniveaux introduitenous tester méthode graphe aléatoire exemplesréel issu domaine
498	Revue des Nouvelles Technologies de l'Information	EGC	2012	Vers la construction d'un observatoire des pratiques agricoles : gestion et propagation de l'imprécision des données agronomiques	L'un des objectifs d'Observox est de traiter et gérer l'imprécisiondes données agronomiques tant spatialement (parcelles agricoles) et quantitativement(quantités de produits disséminées) et de toujours associer une évaluationde la qualité aux données. Aussi, nous avons choisi le cadre théorique desensembles flous. A partir d'un modèle conceptuel gérant l'imperfection, nousconstruisons une base de données gérant des entités spatiotemporelles imprécisesappelées « entités agronomiques floues ». Cependant, ce choix de représentationrend possible le chevauchement des composantes spatiales entre entités.Dans ce cas, nous propageons l'imprécision du spatial vers le quantitatif àl'aide d'un opérateur de caractère additif qui prend en compte à la fois l'informationspatiale et quantitative, et qui fournit une information quantitative localeet floue. Le système ainsi construit nous permet d'obtenir une représentationfloue des quantités de produits phytosanitaires disséminés à chaque endroit duterritoire étudié.	Asma Zoghlami, Karima Zayrit, Cyril de Runz, Eric Desjardin, Herman Akdag	http://editions-rnti.fr/render_pdf.php?p1&p=1001158	http://editions-rnti.fr/render_pdf.php?p=1001158	Lun objectif dObservox traiter gérer limprécisionde donnée agronomique spatialement parcell agricole quantitativementquantiter produit disséminer associer évaluationde qualité donnée choisir cadrer théorique desensembl flou partir dun modeler conceptuel gérer limperfection nousconstruiser baser donnée gérer entité spatiotemporell imprécisesappeler « entité agronomique floue » choix représentationrend chevauchement composante spatial entrer entitésDans cas propager limprécision spatial ver quantitatif àlaid dun opérateur caractère additif prendre compter linformationspatial quantitatif fournir information quantitatif localeet flouer système construire permettre dobtenir représentationfloue quantité produit phytosanitaire disséminer endroit duterritoir étudier
499	Revue des Nouvelles Technologies de l'Information	EGC	2012	Vers une approche efficace d'extraction de motifs spatio-séquentiels	Ces dernières années, l'augmentation de la quantité d'informationsspatio-temporelles stockées dans les bases de données a fait naître de nouveauxbesoins, notamment en matière de gestion des risques naturels, sanitaires ou anthropiques(p. ex. compréhension de la dynamique d'une épidémie de Dengue).Dans cet article, nous définissons un cadre théorique pour l'extraction de motifsspatio-séquentiels, séquences de motifs spatiaux représentant l'évolution dansle temps d'une localisation et de son voisinage. Nous proposons un algorithmed'extraction efficace qui effectue un parcours en profondeur en s'appuyant surdes projections successives de la base de données. Nous introduisons égalementune mesure d'intérêt adaptée aux aspects spatio-temporels de ces motifs. Les expérimentationsréalisées sur des jeux de données réels soulignent la pertinencede l'approche proposée par rapport aux méthodes de la littérature.	Hugo Alatrista Salas, Sandra Bringay, Frédéric Flouvat, Nazha Selmaoui-Folcher, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001159	http://editions-rnti.fr/render_pdf.php?p=1001159	dernière année laugmentation quantité dinformationsspatiotemporell stocker dan base donnée faire naître nouveauxbesoin matière gestion risque sanitaire anthropiquesp ex compréhension dynamique dune épidémie denguedan article définir cadrer théorique lextraction motifsspatioséquentiel séquence motif spatial représenter lévolution dansle temps dune localisation voisinage proposer algorithmedextraction efficace effectuer parcours profondeur sappuyer surdes projection successif baser donnée introduire égalementune mesurer dintérêt adapter aspect spatiotemporel motif expérimentationsréalisée jeu donnée réel souligner pertinencede lapproche proposer rapport méthode littérature
500	Revue des Nouvelles Technologies de l'Information	EGC	2012	Vers une méthode automatique de construction de hiérarchies contextuelles	Dans de nombreux domaines (e.g., fouille de données, entrepôts dedonnées), l'existence de hiérarchies sur certains attributs peut être extrêmementutile dans le processus analytique. Toutefois, cette connaissance n'est pas toujoursdisponible ou adaptée. Il est alors nécessaire de disposer d'un processusde découverte automatique pour palier ce problème. Dans cet article, nous combinonset adaptons des techniques issues de la théorie de l'information et duclustering pour proposer une technique orientée données de construction automatiquede taxonomies. Les deux principaux avantages d'une telle approchesont son caractère totalement non-supervisé et l'absence de paramètre utilisateurà spécifier. Afin de valider notre approche, nous l'avons appliquée sur desdonnées réelles et avons conduit plusieurs types d'expérimentation. D'abord,les hiérarchies obtenues ont été expertisées pour en examiner le pouvoir informatif.Ensuite, nous avons évalué l'apport de ces taxonomies comme support àdes tâches de fouille de données nécessitant une définition hiérarchique des valeursd'attributs : l'extraction de séquences fréquentes multidimensionnelles etmulti-niveaux ainsi que la construction de résumés de tables relationnelles. Lesrésultats obtenus permettent de conclure quant à l'intérêt de notre approche	Dino Ienco, Yoann Pitarch, Pascal Poncelet, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001186	http://editions-rnti.fr/render_pdf.php?p=1001186	Dans domaine eg fouiller donnée entrepôt dedonner lexistence hiérarchie attribut pouvoir extrêmementutile dan processus analytique connaissance nest toujoursdisponibl adapter nécessaire disposer dun processusde découvrir automatique palier problème Dans article combinonset adapter technique issu théorie linformation duclustering proposer technique orienter donner construction automatiquede taxonomi principal avantage dune approchesont caractère totalement nonsuperviser labsence paramétrer utilisateurà spécifier Afin valider approcher laver appliquer desdonnée réel conduire type dexpérimentation dabordl hiérarchie obtenu expertiser examiner pouvoir informatifensuite évaluer lapport taxonomie support àd tâche fouiller donnée nécessiter définition hiérarchique valeursdattribut   lextraction séquence fréquent multidimensionnel etmultiniveaux construction résumé table relationnel lesrésultat obtenir permettre conclure lintérêt approcher
501	Revue des Nouvelles Technologies de l'Information	EGC	2012	Webmarks : Le marquage d'intérêt sur le Web de données	Depuis son apparition au sein du W3C, la définition de la ressourceWeb n'a cessé d'évoluer au delà du simple document. Lieu, service, conceptd'ontologie, représentation d'un objet réel ou non, la ressource web est complexeet il nous a semblé que les outils à disposition des internautes pour sa manipulation,comme les bookmarks par exemple, n'exploitaient pas pleinementces nouvelles dimensions. Dans cet article, nous présenterons le modèle Webmarksqui permet de préciser l'objet du marquage, la ressource, mais égalementl'intérêt de l'auteur de la marque. L'implémentation de ce modèle au sein duprojet ISICIL sera également présentée et nous discuterons de son apport encomparaison des technologies existantes	Nicolas Delaforge, Fabien Gandon	http://editions-rnti.fr/render_pdf.php?p1&p=1001152	http://editions-rnti.fr/render_pdf.php?p=1001152	Depuis apparition w3c définition ressourceweb cesser dévoluer simple document Lieu service conceptdontologie représentation dun objet réel ressource web complexeet sembler outil disposition internaute manipulationcomme bookmark exemple nexploitaient pleinementce dimension Dans article présenter modeler Webmarksqui permettre préciser lobjet marquage ressource égalementlintérêt lauteur marquer limplémentation modeler duprojet isicil également présenter discuter apport encomparaison technologie existant
502	Revue des Nouvelles Technologies de l'Information	EGC	2011	@KRex : une méthode de construction des connaissances pour la maîtrise des activités à risques - application au domaine de la sécurité nucléaire	Dans les industries à risque, comme le nucléaire, les connaissances liées au savoir et à l'expérience participent à la maîtrise des activités. Elles sont explicites, formalisables dans des documents, ou tacites, expression du savoir faire moins souvent prise en compte. AREVA développe la méthode @KRex pour valoriser le retour d'expérience existant, créer une dynamique d'extraction et de capitalisation des connaissances, faciliter leur partage et leur enrichissement. Cette communication décrit le protocole expérimental de construction des connaissances explicites et tacites du métier sécurité nucléaire.	Julien Giudici, Hervé Janiaut, Rémy Gautier	http://editions-rnti.fr/render_pdf.php?p1&p=1001010	http://editions-rnti.fr/render_pdf.php?p=1001010	Dans industrie risquer nucléaire connaissance lier savoir lexpérience participer maîtriser activité explicite formalisable dan document tacite expression savoir faire priser compter AREVA développer méthode krex valoriser dexpérience exister créer dynamique dextraction capitalisation connaissance faciliter partager enrichissement communication décrire protocole expérimental construction connaissance explicite tacite métier sécurité nucléaire
503	Revue des Nouvelles Technologies de l'Information	EGC	2011	A la recherche des tweets porteurs d'informations journalistiques		Benjamin Rosoor, Laurent Sebag, Sandra Bringay, Mathieu Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1000961	http://editions-rnti.fr/render_pdf.php?p=1000961	
504	Revue des Nouvelles Technologies de l'Information	EGC	2011	Acquisition de structures lexico-sémantiques à partir de textes : un nouveau cadre de travail fondé sur une structuration prétopologique	Les structures lexico-sémantiques jouent un rôle essentiel dans les processus de fouille de textes. En codant les relations sémantiques entre concepts du discours elles apportent une connaissance stratégiques pour enrichir les capacités de raisonnement. Le développement de telles structures étant fortement limité du fait des efforts nécessaires à leur construction, nous proposons un nouveau formalisme d'acquisition automatique d'ontologies terminologiques à partir de textes. Nous utilisons pour cela une formalisation prétopologique de l'espace des termes sur laquelle s'appuie un modèle générique de structuration. Nous présentons une étude empirique préliminaire rendant compte du potentiel de ce modèle en terme d'extraction de connaissances.	Guillaume Cleuziou, Gaël Dias, Vincent Levorato	http://editions-rnti.fr/render_pdf.php?p1&p=1000936	http://editions-rnti.fr/render_pdf.php?p=1000936	structure lexicosémantiqu jouer rôle essentiel dan processus fouiller texte En coder relation sémantique entrer concept discours apporter connaissance stratégique enrichir capacité raisonnement développement structure fortement limité faire effort nécessaire construction proposer formalisme dacquisition automatique dontologie terminologique partir texte utiliser celer formalisation prétopologiqu lespace terme sappuie modeler générique structuration présenter étude empirique préliminaire compter potentiel modeler terme dextraction connaissance
505	Revue des Nouvelles Technologies de l'Information	EGC	2011	Adaptation de l'algorithme CART pour la tarification des risques en assurance non-vie	Les développements récents en tarification de l'assurance non-vie se concentrent majoritairement sur la maîtrise et l'amélioration des Modèles Linéaires Généralisés. Performants, ces modèles imposent cependant à la fois des contraintes sur la structure du risque modélisé et sur les interactions entre variables explicatives du risque. Ces restrictions peuvent conduire, dans certaines sous-populations d'assurés, à une estimation biaisée de la prime d'assurance. Les arbres de régression permettent de s'affranchir de ces contraintes et, de plus, augmentent la lisibilité des résultats de la tarification. Nous présentons une modification de l'algorithme CART pour prendre en compte les spécificités des données d'assurance non-vie. Nous comparons alors notre proposition aux modèles linéaires généralisés sur un portefeuille réel de véhicules. Notre proposition réduit les mesures d'erreur entre le risque mesuré et le risque modélisé, et permet ainsi une meilleure tarification.	Antoine Paglia, Martial Phélippé-Guinvarc'h, Philippe Lenca	http://editions-rnti.fr/render_pdf.php?p1&p=1001028	http://editions-rnti.fr/render_pdf.php?p=1001028	développement récent tarification lassurance nonvi concentrer majoritairement maîtriser lamélioration modèle linéaire Généralisés Performants modèle imposer contrainte structurer risquer modéliser interaction entrer variable explicatif risquer restriction pouvoir conduire dan souspopulation dassurer estimation biaisé primer dassurance arbre régression permettre saffranchir contrainte plaire augmenter lisibilité résultat tarification présenter modification lalgorithme CART prendre compter spécificité donnée dassurance nonvi comparer proposition modèle linéaire généraliser portefeuille réel véhicule proposition réduire mesure derreur entrer risquer mesurer risquer modéliser permettre meilleur tarification
506	Revue des Nouvelles Technologies de l'Information	EGC	2011	Agrégation robuste de données massives à la volée : application aux compteurs électriques communicants	Dans les années à venir, plusieurs millions de compteurs électriques communicants seront déployés sur l'ensemble du territoire français. Afin d'assurer la fiabilité d'un réseau de cette envergure nous proposons une topologie de communication multi-chemins qui repose sur la duplication des données transmises. Toute exploitation des données collectées doit alors tenir compte de la présence d'éléments dupliqués. Dans cet article, nous proposons une nouvelle méthode permettant de calculer en ligne des consommations électriques agrégées (agrégation spatiale). L'idée est d'adapter l'algorithme probabiliste Summation sketch de Considine et al. au contexte des compteurs communicants. Cette approche a l'avantage d'être insensible à la duplication et permet de profiter de la structure massivement distribuée du réseau de communication des futurs compteurs électriques. L'expérimentation de cette méthode sur des données réelles montre qu'elle donne une bonne précision sur l'estimation des consommations agrégées. Cette approche est aussi complétée par une méthode basée sur la théorie des sondages : On obtient une meilleure réactivité de l'estimateur avec rapidement et donc sur des données significativement partielles une erreur inférieure à 2.5%	Yousra Chabchoub, Benoît Grossin	http://editions-rnti.fr/render_pdf.php?p1&p=1000943	http://editions-rnti.fr/render_pdf.php?p=1000943	Dans année venir million compteur électrique communicant déployer lensembl territoire français Afin dassurer fiabilité dun réseau envergure proposer topologie communication multichemin reposer duplication donnée transmise exploitation donnée collecter devoir compter présence déléments dupliquer Dans article proposer méthode permettre calculer ligne consommation électrique agréger agrégation spatial Lidée dadapter lalgorithme probabiliste Summation sketch Considine al contexte compteur communicant approcher lavantage dêtre insensible duplication permettre profiter structurer massivement distribuer réseau communication futur compteur électrique lexpérimentation méthode donnée réel montr donner précision lestimation consommation agréger approcher compléter méthode baser théorie sondage   obtenir meilleur réactivité lestimateur rapidement donnée significativement partiel erreur inférieur 25
507	Revue des Nouvelles Technologies de l'Information	EGC	2011	Aide à l'Analyse Visuelle de Réseaux Sociaux pour la Détection de Comportements Suspects	Cet article traite de l'analyse visuelle de réseaux sociaux pour la détection de comportements suspects à partir de données de communications fournies à des enquêteurs suivant deux procédures : l'interception légale et la rétention de données. Nous proposons les contributions suivantes : (i) un modèle de données et un ensemble d'opérateurs pour interroger ces données dans le but d'extraire des comportements suspects et (ii) une représentation visuelle conviviale pour une navigation simplifiée dans les données de communication accompagnée avec une implémentation.	Amyn Bennamane, Hakim Hacid, Arnaud Ansiaux, Alain Cagnati	http://editions-rnti.fr/render_pdf.php?p1&p=1000948	http://editions-rnti.fr/render_pdf.php?p=1000948	article traiter lanalyse visuel réseau social détection comportement suspect partir donnée communication fournie enquêteur procédure   linterception légal rétention donnée proposer contribution   ie modeler donnée ensemble dopérateur interroger donnée dan boire dextraire comportement suspect ii représentation visuel convivial navigation simplifier dan donnée communication accompagner implémentation
508	Revue des Nouvelles Technologies de l'Information	EGC	2011	Algorithmes de recherche exhaustif et guidé pour la recommandation d'un expert dans un réseau professionnel		Maria Malek	http://editions-rnti.fr/render_pdf.php?p1&p=1000970	http://editions-rnti.fr/render_pdf.php?p=1000970	
509	Revue des Nouvelles Technologies de l'Information	EGC	2011	Analyse comparative de méthodologies et d'outils de construction automatique d'ontologies à partir de ressources textuelles	Plusieurs méthodologies et outils de construction automatique des ontologies à partir de ressources textuelles ont été proposés ces dernières années. Dans cet article nous analysons quatre approches en les comparant à une approche de référence – Methontology. Dans leur sélection nous avons privilégié celles qui couvrent l'ensemble des étapes du processus de construction d'ontologies. Puis nous analysons et comparons la portée, les limites et les performances des implémentations logicielles associées aux approches analysées. Ces outils ont été testés sur un corpus de ressources textuelles, et nous avons comparé leurs résultats à ceux obtenus manuellement.	Toader Gherasim, Mounira Harzallah, Giuseppe Berio, Pascale Kuntz	http://editions-rnti.fr/render_pdf.php?p1&p=1000989	http://editions-rnti.fr/render_pdf.php?p=1000989	méthodologie outil construction automatique ontologie partir ressource textuel proposer année Dans article analyser approche comparer approcher référence – Methontology Dans sélection privilégier couvrir lensembl étape processus construction dontologie Puis analyser comparon porter limite performance implémentation logiciel associé approche analyser outil tester corpus ressource textuel comparer résultat obtenu manuellemer
510	Revue des Nouvelles Technologies de l'Information	EGC	2011	Analyse du comportement limite d'indices probabilistes pour une sélection discriminante	Nous étudions ici le comportement de deux types d'indices probabilistes discriminants en présence de données dont le volume va en croissant. A cet égard, un modèle spécifique de croissance de la taille des données et de liaison entre variables est mis en œuvre et celui-ci va permettre de déterminer le comportement limite des différents indices quel que soit le niveau de liaison entre la prémisse et la conclusion de la règle donnée. La clarté des résultats obtenus nous conduit à en chercher l'explication formelle. L'expérimentation a été effectuée avec la base de données UCI Wages.	Sylvie Guillaume, Israël-César Lerman	http://editions-rnti.fr/render_pdf.php?p1&p=1001036	http://editions-rnti.fr/render_pdf.php?p=1001036	étudier comportement type dindic probabiliste discriminant présence donnée volume aller croître A égard modeler spécifique croissance tailler donnée liaison entrer variable mettre œuvre celuici aller permettre déterminer comportement limiter indice niveau liaison entrer prémisse conclusion régler donner clarté résultat obtenir conduire chercher lexplication formel lexpérimentation effectuer baser donnée UCI Wages
511	Revue des Nouvelles Technologies de l'Information	EGC	2011	Analyse factorielle des correspondances hiérarchique pour la fouille d'images	Nous proposons un outil graphique interactif qui permet de visualiser et d'extraire des connaissances à partir des résultats de l'Analyse Factorielle des Correspondances (AFC) sur les images. L'AFC est une technique descriptive développée pour analyser des tableaux de contingence. L'AFC est originellement utilisée dans l'Analyse des Données Textuelles (ADT) où le corpus est représenté par un tableau de contingence croisant des documents et des mots. Dans la fouille d'images, nous définissons d'abord les « mots visuels » dans les images (analogues aux mots textuels). Ces mots visuels sont construits à partir des descripteurs locaux SIFT (Scale Invariant Feature Transform) dans l'image. Ensuite, nous appliquons l'AFC sur le tableau de contingence obtenu. Notre outil (appelé HCAViz) analyse ce tableau de contingence de façon récursive et aide l'utilisateur à interpréter et interagir avec les résultats de l'AFC. D'abord, les résultats de la première AFC sur les images sont visualisés. L'utilisateur sélectionne ensuite un groupe d'images et fait une deuxième AFC sur le nouveau tableau de contingence. Ce processus peut continuer jusqu'à ce qu'un thème « pur » se dévoile. Ceci permet de découvrir une arborescence des thèmes dans une collection d'images. Une application sur la base Caltech-4 illustre l'intérêt de HCAViz dans la fouille d'images.	Nguyen-Khang Pham, Annie Morin, François Poulet, Patrick Gros	http://editions-rnti.fr/render_pdf.php?p1&p=1000941	http://editions-rnti.fr/render_pdf.php?p=1000941	proposer outil graphique interactif permettre visualiser dextraire connaissance partir résultat lanalyse factoriel correspondance AFC image LAFC technique descriptif développer analyser tableau contingence LAFC originellement utiliser dan lanalyse donnée textuel ADT corpus représenter tableau contingence croiser document Dans fouiller dimag définir dabord « visuel » dan image analogue textuel visuel construit partir descripteur local SIFT Scale invarier Feature Transform dan limage ensuite appliquer lafc tableau contingence obtenir outil appeler hcaviz analyser tableau contingence récursif aider lutilisateur interpréter interagir résultat lafc dabord résultat AFC image visualiser Lutilisateur sélectionner ensuite grouper dimag faire AFC tableau contingence processus pouvoir continuer jusquà quun thème « » dévoiler permettre découvrir arborescence thème dan collection dimage application baser Caltech4 illustrer lintérêt HCAViz dan fouiller dimag
512	Revue des Nouvelles Technologies de l'Information	EGC	2011	Analyse spatiotemporelle des vecteurs de mouvement : application au comptage des personnes	Cet article présente une nouvelle approche qui permet de compter le nombre d'individus franchissant une ligne de comptage. L'approche proposée accumule dans le temps les vecteurs de mouvement pour chaque point de la ligne de comptage formant une carte spatiotemporelle. Une procédure de détection en ligne des blobs est ensuite utilisée afin de déterminer les régions de la carte spatiotemporelle qui correspondent à des personnes franchissant cette ligne. Le nombre d'individus associé à chaque blob est estimé grâce à un modèle de régression linéaire appliqué aux caractéristiques du blob. L'approche proposée est validée sur la base de plusieurs ensembles de données enregistrées à l'aide d'une caméra verticale ou d'une caméra oblique.	Yassine Benabbas, Tarek Yahiaoui, Thierry Urruty, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1000942	http://editions-rnti.fr/render_pdf.php?p=1000942	article présenter approcher permettre compter nombre dindividus franchir ligne comptage Lapproche proposer accumuler dan temps vecteur mouvement poindre ligne comptage former carte spatiotemporell procédure détection ligne blob ensuite utiliser déterminer région carte spatiotemporell correspondre franchir ligne nombre dindividu associer blob estimer grâce modeler régression linéaire appliquer caractéristique blob Lapproche proposer valider baser ensemble donnée enregistrer laid dune caméra vertical dune caméra oblique
513	Revue des Nouvelles Technologies de l'Information	EGC	2011	Annotation d'entités nommées par extraction de règles de transduction	La reconnaissance d'entités nommées est une problématique majoritairement traitée par des modèles spécifiés à l'aide de règles ou par apprentissage numérique. Les premiers ont le désavantage d'être coûteux à développer pour obtenir une couverture satisfaisante, les seconds sont souvent difficiles à interpréter par des experts (linguistes). Dans cet article, nous présentons une approche, dont l'objectif est d'extraire des règles symboliques discriminantes qu'un humain puisse consulter. A partir d'un corpus de référence, nous extrayons des règles de transduction, dont seules les plus informatives sont retenues. Elles sont ensuite appliquées pour effectuer une annotation : à cet effet, un algorithme recherche parmi les annotations possibles celles de meilleure qualité en termes de couverture et de probabilité. Nous présentons les résultats expérimentaux et discutons de l'intérêt et des perspectives de notre approche.	Arnaud Soulet, Damien Nouvel	http://editions-rnti.fr/render_pdf.php?p1&p=1000937	http://editions-rnti.fr/render_pdf.php?p=1000937	reconnaissance dentité nommer problématique majoritairement traiter modèle spécifier laid règle apprentissage numérique désavantager dêtre coûteux développer obtenir couverture satisfaisant second difficile interpréter expert linguist Dans article présenter approcher lobjectif dextraire règle symbolique discriminanter quun humain pouvoir consulter A partir dun corpu référence extraire règle transduction plaire informatif retenir ensuite appliquer effectuer annotation   algorithme rechercher annotation meilleur qualité terme couverture probabilité présenter résultat expérimental discuton lintérêt perspective approcher
514	Revue des Nouvelles Technologies de l'Information	EGC	2011	Apport de la catégorisation iconique pour la gestion coopérative des connaissances1		Xiaoyue Ma, Jean-Pierre Cahier, L'Hédi Zaher	http://editions-rnti.fr/render_pdf.php?p1&p=1000978	http://editions-rnti.fr/render_pdf.php?p=1000978	
515	Revue des Nouvelles Technologies de l'Information	EGC	2011	Apport des données thématiques dans les systèmes de recommandation : hybridation et démarrage à froid	"Des travaux récents (Pilaszy et al., 2009) suggèrent que les métadonnées sont quasiment inutiles pour les systèmes de recommandation, y compris en situation de cold-start : les données de logs de notation sont beaucoup plus informatives. Nous étudions, sur une base de référence de logs d'usages pour la recommandation automatique de DVD (Netflix), les performances de systèmes de recommandation basés sur des sources de données collaboratives, thématiques et hybrides en situation de démarrage à froid (cold-start). Nous exhibons des cas expérimentaux où les métadonnées apportent plus que les données de logs d'usage (collaboratives) pour la performance prédictive. Pour gérer le cold-start d'un système de recommandation, nous montrons que des approches ""en cascade"", thématiques puis hybrides, puis collaboratives, seraient plus appropriées."	Frank Meyer, Éric Gaussier, Fabrice Clérot, Julien Schluth	http://editions-rnti.fr/render_pdf.php?p1&p=1000947	http://editions-rnti.fr/render_pdf.php?p=1000947	travail récent Pilaszy al 2009 suggérer métadonnée quasiment inutile système recommandation yu situation coldstart   donnée log notation plaire informatif étudier baser référence log dusag recommandation automatique dvd Netflix performance système recommandation baser source donnée collaboratif thématique hybrider situation démarrage froid coldstart exhiber cas expérimental métadonnée apporter plaire donnée logs dusage collaboratif performance prédictif Pour gérer coldstart dun système recommandation montrer approche cascader thématique pouvoir hybride pouvoir collaborative plaire approprier
516	Revue des Nouvelles Technologies de l'Information	EGC	2011	Apprendre les contraintes topologiques dans les cartes auto-organisatrices	La Carte Auto-Organisatrice (SOM : Self-Organizing Map) est une méthode populaire pour l'analyse de la structure d'un ensemble de données. Cependant, certaines contraintes topologiques de la SOM sont fixées avant l'apprentissage et peuvent ne pas être pertinentes pour la représentation de la structure des données. Dans cet article nous nous proposons d'améliorer les performances des SOM avec un nouvel algorithme qui apprend les contraintes topologiques de la carte à partir des données. Des expériences sur des bases de données artificielles et réelles montrent que l'algorithme proposé produit de meilleurs résultats que SOM classique. Ce n'est pas le cas avec une relaxation triviale des contraintes topologiques, qui résulte en une forte augmentation de l'erreur topologique de la carte.	Guénaël Cabanes, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1000939	http://editions-rnti.fr/render_pdf.php?p=1000939	carte AutoOrganisatrice SOM   SelfOrganizing Map méthode populaire lanalyse structurer dun ensemble donnée contrainte topologique SOM fixer lapprentissage pouvoir pertinent représentation structurer donnée Dans article proposer daméliorer performance som nouvel algorithme apprendre contrainte topologique carte partir donnée expérience base donnée artificiel réel montrer lalgorithme proposer produire meilleur résultat SOM classique nest cas relaxation trivial contrainte topologique résulter fort augmentation lerreur topologique carte
517	Revue des Nouvelles Technologies de l'Information	EGC	2011	Apprentissage génératif de la structure de réseaux logiques de Markov à partir d'un graphe des prédicats	Les Réseaux Logiques de Markov (MLNs) combinent l'apport statistique des Réseaux de Markov à la logique du premier ordre. Dans cette approche, chaque clause logique se voit affectée d'un poids, l'instanciation des clauses permettant alors de produire un Réseau deMarkov. L'apprentissage d'un MLN consiste à apprendre d'une part sa structure (la liste de clauses logiques) et d'autre part les poids de celles-ci. Nous proposons ici une méthode d'apprentissage génératif de Réseau Logique de Markov. Cette méthode repose sur l'utilisation d'un graphe des prédicats, produit à partir d'un ensemble de prédicats et d'une base d'apprentissage. Une méthode heuristique de variabilisation est mise en oeuvre afin de produire le jeu de clauses candidates. Les résultats présentés montrent l'intérêt de notre approche au regard de l'état de l'art.	Quang-Thang Dinh, Matthieu Exbrayat, Christel Vrain	http://editions-rnti.fr/render_pdf.php?p1&p=1000993	http://editions-rnti.fr/render_pdf.php?p=1000993	réseau logique Markov mln combiner lapport statistique réseau Markov logique ordre Dans approcher clause logique voir affecter dun poids linstanciation clause permettre produire réseau demarkov Lapprentissage dun mln consister dune partir structurer liste clause logique dautre partir poids cellesci proposer méthode dapprentissage génératif réseau Logique Markov méthode reposer lutilisation dun graph prédicat produire partir dun ensemble prédicat dune baser dapprentissage méthode heuristique variabilisation mettre oeuvrer produire jeu clause candidat résultat présenter montrer lintérêt approcher regard létat lart
518	Revue des Nouvelles Technologies de l'Information	EGC	2011	Cartes cognitives : une exploitation à base d'échelle, vue et profil	Une carte cognitive est un réseau d'influences entre différents concepts. Le modèle des cartes cognitives permet à un utilisateur de calculer l'influence entre deux concepts. Les cartes cognitives contenant un grand nombre de concepts et d'influences sont difficiles à comprendre. Cet article introduit la notion de carte cognitive ontologique qui associe une ontologie à une carte cognitive classique pour en organiser les concepts. Afin de faciliter la compréhension d'une carte, l'utilisateur peut obtenir une vue de cette carte la simplifiant selon une échelle qu'il aura choisie. Un profil peut être créé pour construire des vues correspondant aux objectifs d'un type d'utilisateur. Si une carte est manipulée par différents utilisateurs, leurs profils combinés permettent de construire une vue partagée.	Lionel Chauvin, David Genest, Aymeric Le Dorze, Stéphane Loiseau	http://editions-rnti.fr/render_pdf.php?p1&p=1001008	http://editions-rnti.fr/render_pdf.php?p=1001008	carte cognitif réseau dinfluencer entrer concept modeler carte cognitif permettre utilisateur calculer linfluence entrer concept carte cognitif contenir grand nombre concept dinfluence difficile comprendre article introduire notion carte cognitif ontologique associer ontologie carte cognitif classique organiser concept Afin faciliter compréhension dune carte lutilisateur pouvoir obtenir carte simplifier échelle quil choisir profil pouvoir créer construire correspondre objectif dun typer dutilisateur Si carte manipuler utilisateur profil combiner permettre construire partagé
519	Revue des Nouvelles Technologies de l'Information	EGC	2011	Catégorisation des mesures d'intérêt pour l'extraction des connaissances	"La recherche de règles d'association intéressantes est un domaine de recherche important et actif en fouille de données. Les algorithmes de la famille Apriori reposent sur deux mesures pour extraire les règles, le support et la confiance. Bien que ces deux mesures possèdent des vertus algorithmiques accélératrices, elles génèrent un nombre prohibitif de règles dont la plupart sont redondantes et sans intérêt. Il est donc nécessaire de disposer d'autres mesures filtrant les règles inintéressantes. Des travaux ont été réalisés pour dégager les ""bonnes"" propriétés des mesures d'extraction des règles et ces propriétés ont été évaluées sur 61 mesures. L'objectif de cet article est de dégager des catégories de mesures afin de répondre à une préoccupation des utilisateurs : le choix d'une ou plusieurs mesures lors d'un processus d'extraction des connaissances dans le but d'éliminer les règles valides non pertinentes extraites par le couple (support, confiance). L'évaluation des propriétés sur les 61 mesures a permis de dégager 9 classes de mesures, classes obtenues grâce à deux techniques : une méthode de la classification ascendante hiérarchique et une version de la méthode de classification non-hiérarchique des k-moyennes."	Sylvie Guillaume, Dhouha Grissa, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1001014	http://editions-rnti.fr/render_pdf.php?p=1001014	rechercher règle dassociation intéressant domaine rechercher importer actif fouiller donnée algorithme famille Apriori reposer mesure extraire règle support confiance mesure posséder vertu algorithmique accélératrice générer nombre prohibitif règle redondanter intérêt nécessaire disposer dautr mesure filtrer règle inintéressant travail réaliser dégager propriété mesure dextraction règle propriété évaluer 61 mesure Lobjectif article dégager catégorie mesure répondre préoccupation utilisateur   choix dune mesure dun processus dextraction connaissance dan boire déliminer règle valide pertinent extrait coupler support confiance lévaluation propriété 61 mesure permettre dégager 9 classe mesure classe obtenu grâce technique   méthode classification ascendant hiérarchique version méthode classification nonhiérarchiqu kmoyenne
520	Revue des Nouvelles Technologies de l'Information	EGC	2011	Classificateurs aléatoires Topologiques à base de graphes de voisinage	En apprentissage supervisé, les Méthodes Ensemble (ME) ont montré leurs qualités. L'une des méthodes de référence dans ce domaine est les Forêts Aléatoires (FA). Cette dernière repose sur des partitionnements de l'espace de représentation selon des frontières parallèles aux axes ou obliques. Les conséquences de cette façon de partitionner l'espace de représentation peuvent affecter la qualité de chaque prédicteur. Il nous a semblé que cette approche pouvait être améliorée si on se libérait de cette contrainte de manière à mieux coller à la structure topologique de l'ensemble d'apprentissage. Dans cet article, nous proposons une nouvelle ME basée sur des graphes de voisinage dont les performances, sur nos premières expérimentations, sont aussi bonnes que celles des FA.	Fabien Rico, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1000933	http://editions-rnti.fr/render_pdf.php?p=1000933	En apprentissage superviser méthode ensemble ME montrer qualité Lune méthode référence dan domaine Forêts Aléatoires FA reposer partitionnement lespace représentation frontière parallèle axe oblique conséquence partitionner lespace représentation pouvoir qualité prédicteur sembler approcher pouvoir améliorer libérer contraint manière mieux coller structurer topologique lensembl dapprentissage Dans article proposer ME baser graphe voisinage performance expérimentation fa
521	Revue des Nouvelles Technologies de l'Information	EGC	2011	Classification des aéronefs par estimation de la pose	Dans le présent travail, nous proposons un outil d'aide à la reconnaissance de cibles radar basé sur la signature de forme et de la pose de la cible. La tâche principale dans le cadre de cet article consiste à établir la fonction de recherche d'images ISAR par l'exemple en exploitant l'information de pose estimée depuis les images ISAR. L'objectif est d'introduire l'information de pose dans l'indexation des images, notamment dans la phase de sélection des images candidates. Nous proposons une nouvelle méthode d'estimation de la pose basée sur l'axe le plus symétrique de la cible. La méthode proposée est ensuite comparée avec d'autres techniques connues telles que la transformée de Hough et la transformée en ondelette. Enfin, la tâche de classification est réalisée en utilisant les k-plus proches voisins incluant l'information de la pose.	Mohamed Nabil Saidi, Abdelmalek Toumi, Ali Khenchaf, Driss Aboutajdine	http://editions-rnti.fr/render_pdf.php?p1&p=1001012	http://editions-rnti.fr/render_pdf.php?p=1001012	Dans présent travail proposer outil daid reconnaissance cible radar baser signature former poser cibler tâcher principal dan cadrer article consister établir fonction rechercher dimage isar lexemple exploiter linformation poser estimer image ISAR Lobjectif dintroduir linformation poser dan lindexation image dan phase sélection image candidater proposer méthod destimation poser baser laxe plaire symétrique cibler méthode proposer ensuite comparer dautr technique connu transformer Hough transformer ondelette Enfin tâcher classification réaliser utiliser kplu voisin inclure linformation poser
522	Revue des Nouvelles Technologies de l'Information	EGC	2011	Closed-set-based Discovery of Representative Association Rules Revisited	The output of an association rule miner is often huge in practice. This is why several concise lossless representations have been proposed, such as the “essential” or “representative” rules. We revisit the algorithm given by Kryszkiewicz (Int. Symp. Intelligent Data Analysis 2001, Springer-Verlag LNCS 2189, 350–359) for mining representative rules. We show that its output is sometimes incomplete, due to an oversight in its mathematical validation, and we propose an alternative complete generator that works within only slightly larger running times.	José L Balcazar , Cristina Tîrnauca	http://editions-rnti.fr/render_pdf.php?p1&p=1001032	http://editions-rnti.fr/render_pdf.php?p=1001032	The output of an association rule miner is often hug in practice This is why several concis lossless representations hav been proposed such the “ essential ” or “ representative ” ruler we revisit the algorithm given by Kryszkiewicz Int Symp Intelligent Data Analysis 2001 springerverlag LNCS 2189 350–359 for mining representative ruler We show that it output is sometim incomplet to an oversight in it mathematical validation and we proposer an alternatif complet generator that work within only slightly larger running tim
523	Revue des Nouvelles Technologies de l'Information	EGC	2011	Comparaison entre deux indices pour l'évaluation probabiliste discriminante des règles d'association	"L'élaboration d'une échelle de probabilité discriminante pour la comparaison mutuelle entre plusieurs attributs observés sur un échantillon d'objets de ""grosse"" taille, nécessite une normalisation préalable. L'objet de cet article est l'analyse comparée entre deux approches. La première dérive de l' ""Analyse de la Vraisemblance des Liens Relationnels Normalisée"". La seconde est fondée sur la notion de ""Valeur Test"" sur un échantillon virtuel de taille 100, synthétisant l'échantillon initial."	Israël-César Lerman, Sylvie Guillaume	http://editions-rnti.fr/render_pdf.php?p1&p=1001034	http://editions-rnti.fr/render_pdf.php?p=1001034	lélaboration dune échelle probabilité discriminant comparaison mutuel entrer attribut observer échantillon dobjet gros tailler nécessiter normalisation préalable lobjet article lanalys comparer entrer approche dériver Analyse Vraisemblance lien relationnel Normalisée second fonder notion test échantillon virtuel tailler 100 synthétiser léchantillon initial
524	Revue des Nouvelles Technologies de l'Information	EGC	2011	Complex Information Processing	It is commonplace nowadays to claim that information is everywhere and that, as a result, finding the right information (mathematically : according to a set of criteria optimizing a specific goal) is very difficult. Defence applications have to cope with similar problems : communication networks, surveillance and information systems transmit and generate significant amounts of complex information which cannot be processed with low level algorithms. The challenge is to build high-level processing units (which demand a lot of computing power) so as process video streams and communication packets with little possibility of a false alarm as automatically as possible. Methods for processing, aligning, merging low-level and high-level information (from syntactic to semantic information) extracted from still images, videos, speech, text and the Internet are being considered. The framework includes theoretical approaches, algorithms as well as evaluation methods. Topics of interest are data fusion, learning techniques, data mining, HCI, even Artificial Intelligence. Defence applications are numerous, from scene understanding to weak signal detection.	Jacques Blanc-Talon	http://editions-rnti.fr/render_pdf.php?p1&p=1000922	http://editions-rnti.fr/render_pdf.php?p=1000922	it is commonplace nowadays to claim that information is everywhere and that result finding the right information mathematically   according to set of criteria optimizing specific goal is very difficult Defence application hav to cope with similar problem   communication network surveillance and information system transmettre and generat significer amount of complex information which cannot be processed with low level algorithm The challeng is to build highlevel processing unit which demand lot of computing power so process video streams and communication packet with little possibility of false alarm automatically Methods for processing aligning merging lowlevel and highlevel information from syntactic to semantic information extracted from still image video speech text and the Internet are being considered The framework include theoretical approaches algorithms well evaluation method Topics of interest are dater fusion learning technique dater mining HCI even Artificial Intelligence Defence application are numerous from scene understanding to weak signal detection
525	Revue des Nouvelles Technologies de l'Information	EGC	2011	Conception et implémentation d'une nouvelle technique cellulaire de discrétisation : intégration dans TANAGRA		Mohamed Benamina, Baghdad Atmani	http://editions-rnti.fr/render_pdf.php?p1&p=1000976	http://editions-rnti.fr/render_pdf.php?p=1000976	
526	Revue des Nouvelles Technologies de l'Information	EGC	2011	Construction d'une Ontologie d'aide au renforcement de la sécurité des systèmes de transport automatisés.		Lassaâd Mejri, Ahmed Maalel, Habib Hadj Mabrouk, Henda Ben Ghezela Hadjami	http://editions-rnti.fr/render_pdf.php?p1&p=1000967	http://editions-rnti.fr/render_pdf.php?p=1000967	
527	Revue des Nouvelles Technologies de l'Information	EGC	2011	Construction ontologique à partir de séquences d'expression de champignons		Houda Fyad, Karim Bouamrane, Baghdad Atmani, Claire Toffano-Nioche	http://editions-rnti.fr/render_pdf.php?p1&p=1000968	http://editions-rnti.fr/render_pdf.php?p=1000968	
528	Revue des Nouvelles Technologies de l'Information	EGC	2011	Data stream summarization by on-line histograms clustering		Antonio Balzanella, Lidia Rivoli, Rosanna Verde	http://editions-rnti.fr/render_pdf.php?p1&p=1000977	http://editions-rnti.fr/render_pdf.php?p=1000977	
529	Revue des Nouvelles Technologies de l'Information	EGC	2011	Découverte de motifs d'évolution significatifs dans les séries temporelles d'images satellites	Les séries temporelles d'images satellites (ou Satellite Image Time Series – SITS) sont d'importantes sources d'informations sur l'évolution du territoire. Étudier ces images permet de comprendre les changements sur des zones précises mais aussi de découvrir des schémas d'évolution à grande échelle. Toutefois, découvrir ces phénomènes impose de répondre à plusieurs défis qui sont liés aux caractéristiques des SITS et à leurs contraintes. Premièrement, chaque pixel d'une image satellite est décrit par plusieurs valeurs (les niveaux radiométriques sur différentes longueurs d'ondes). Deuxièmement, ces motifs d'évolution portent sur des périodes très longues et ne sont pas forcément synchrones selon les régions. Troisièmement, les régions qui ne sont pas concernées par des évolutions significatives sont majoritaires et leur domination rend difficile l'extraction des motifs d'évolution. Dans cet article, nous proposons une méthode qui répond à ces difficultés et nous la validons sur une série d'images satellites acquises sur une période de 20 ans.	François Petitjean, Florent Masseglia, Pierre Gançarski	http://editions-rnti.fr/render_pdf.php?p1&p=1001037	http://editions-rnti.fr/render_pdf.php?p=1001037	série temporel dimag satellite Satellite imager Time serie – SITS dimportanter source dinformation lévolution territoire Étudier image permettre comprendre changement zone précis découvrir schéma dévolution grand échelle découvrir phénomène imposer répondre défi lier caractéristique SITS contrainte pixel dune imager satellite décrire niveau radiométrique longueur donde Deuxièmement motif dévolution porter période long forcément synchroner région troisièmemer région concerner évolution significatif majoritaire domination difficile lextraction motif dévolution Dans article proposer méthode répondre difficulté validon série dimages satellite acquérir période 20 an
530	Revue des Nouvelles Technologies de l'Information	EGC	2011	Des graphes de documents aux réseaux sociaux		Michel Plantié, Michel Crampes	http://editions-rnti.fr/render_pdf.php?p1&p=1000972	http://editions-rnti.fr/render_pdf.php?p=1000972	
531	Revue des Nouvelles Technologies de l'Information	EGC	2011	Détection de changements de distribution dans un flux de données : une approche supervisée	L'analyse de flux de données traite des données massives grâce à des algorithmes en ligne qui évitent le stockage exhaustif des données. La détection de changements dans la distribution d'un flux est une question importante dont les applications potentielles sont nombreuses. Dans cet article, la détection de changement est transposée en un problème d'apprentissage supervisé. Nous avons choisi d'utiliser la méthode de discrétisation supervisée MODL car celle-ci présente des propriétés intéressantes. Notre approche est comparée favorablement à une méthode de l'état-de-l'art sur des flux de données artificiels.	Marc Boullé, Alexis Bondu	http://editions-rnti.fr/render_pdf.php?p1&p=1000944	http://editions-rnti.fr/render_pdf.php?p=1000944	lanalyse flux donnée traiter donnée massif grâce algorithme ligne éviter stockage exhaustif donnée détection changement dan distribution dun flux question important application potentiel Dans article détection changement transposer problème dapprentissage superviser choisir dutiliser méthode discrétisation superviser modl celleci présenter propriété intéressant approcher comparer favorablement méthode létatdelart flux donnée artificiel
532	Revue des Nouvelles Technologies de l'Information	EGC	2011	Détection de redondances dans les tableaux guidée par une ontologie	Nous nous intéressons dans cet article à la réconciliation d'annotations floues associées à des tableaux de données par une méthode d'annotation sémantique, qui est guidée par une ontologie de domaine. Etant donnés deux tableaux, la méthode consiste à détecter leurs instances de relation redondantes. Elle s'appuie sur les connaissances déclarées dans l'ontologie, ainsi que sur des scores de similarité entre les annotations floues représentées par des sous-ensembles flous numériques ou par des sous-ensembles flous symboliques	Rania Khefifi, Patrice Buche, Juliette Dibie-Barthélemy, Fatiha Saïs	http://editions-rnti.fr/render_pdf.php?p1&p=1001016	http://editions-rnti.fr/render_pdf.php?p=1001016	intéresser dan article réconciliation dannotations floue associer tableau donnée méthode dannotation sémantique guider ontologie domaine eter donner tableau méthode consister détecter instance relation redondant sappuie connaissance déclarer dan lontologie score similarité entrer annotation flou représenter sousensemble flou numérique sousensemble flou symbolique
533	Revue des Nouvelles Technologies de l'Information	EGC	2011	Détection des profils à long terme et à court terme dans les réseaux sociaux	La conception des profils et contextes utilisateurs se situe au coeur de l'étude et de la mise en oeuvre des mécanismes de personnalisation ou d'adaptation de contenus (recherche d'information, systèmes de recommandation, etc.). Plusieurs modèles et dimensions de profils et contextes sont décrits dans la littérature. Dans la vie réelle tout comme dans les systèmes d'information, le comportement de l'utilisateur est très souvent influencé par son environnement social. Cependant, la dimension sociale des profils et contextes utilisateurs reste très peu étudiée et évaluée. Dans cet article, nous présentons une méthode de visualisation des profils utilisateurs permettant d'évaluer la pertinence du réseau social de l'utilisateur dans l'évolution de son profil. L'expérimentation de la méthode à partir de Facebook permet d'identifier d'une part, les centres d'intérêts à court-terme et à long-terme des profils utilisateurs, et d'autre part, l'influence réelle à court-terme et à long-terme du réseau social de chaque utilisateur. Ces résultats démontrent l'intérêt de modéliser et d'intégrer une dimension sociale dans les profils et contextes utilisateurs, afin de tenter d'améliorer les mécanismes de personnalisation ou d'adaptation de contenus.	Dieudonné Tchuente, Marie-Françoise Canut, Nadine Baptiste-Jessel	http://editions-rnti.fr/render_pdf.php?p1&p=1000987	http://editions-rnti.fr/render_pdf.php?p=1000987	conception profil contexte utilisateur situer coeur létude miser oeuvrer mécanisme personnalisation dadaptation contenu rechercher dinformation systèmer recommandation modèle dimension profil contexte décrire dan littérature Dans vie réel dan système dinformation comportement lutilisateur influencer environnement social dimension social profil contexte utilisateur rester étudier évaluer Dans article présenter méthode visualisation profil utilisateur permettre dévaluer pertinence réseau social lutilisateur dan lévolution profil Lexpérimentation méthode partir Facebook permettre didentifier dune partir centre dintérêts courtterme longterme profil utilisateur dautre partir linfluenc réel courtterme longterme réseau social utilisateur résultat démontrer lintérêt modéliser dintégrer dimension social dan profil contexte utilisateur tenter daméliorer mécanisme personnalisation dadaptation contenu
534	Revue des Nouvelles Technologies de l'Information	EGC	2011	Early Classification on Temporal Sequences	Early classification of temporal sequences has applications in, for example, health informatics, intrusion detection, anomaly detection, and scientific and engineering sequence data monitoring. In early classification, instead of optimizing accuracy, our goal is to produce classification as early as possible provided that the accuracy meets some expectation. In this talk, I will advocate early classification as an exciting and challenging research problem, which has not been systematically studied in the literature. I will discuss several interesting formulations of the problem, which provide complimentary features possibly desirable in different application scenarios. I will also review some of our recent progress on this aspect.	Jian Pei	http://editions-rnti.fr/render_pdf.php?p1&p=1000918	http://editions-rnti.fr/render_pdf.php?p=1000918	Early classification of temporal sequenc has application in for example health informatic intrusion detection anomaly detection and scientific and engineering sequenc dater monitoring In early classification instead of optimizing accuracy our goal is to produce classification early provided that the accuracy meet some expectation in this talk ie will advocat early classification an exciting and challenging research problem which has not been systematically studied in the literatur ie will discus several interesting formulation of the problem which provid complimentary featur possibly desirabl in application scenario ie will also review some of our recent progres this aspect
535	Revue des Nouvelles Technologies de l'Information	EGC	2011	Entropic-Genetic Clustering	This paper addresses the clustering problem given the similarity matrix of a dataset. We define two distinct criteria with the aim of simultaneously minimizing the cut size and obtaining balanced clusters. The first criterion minimizes the similarity between objects belonging to different clusters and is an objective generally met in clustering. The second criterion is formulated with the aid of generalized entropy. The trade-off between these two objectives is explored using a multi-objective genetic algorithm with enhanced operators	Mihaela Breaban, Henri Luchian, Dan A. Simovici	http://editions-rnti.fr/render_pdf.php?p1&p=1000931	http://editions-rnti.fr/render_pdf.php?p=1000931	this paper addresse the clustering problem given the similarity matrix of dataset We define two distinct criteria with the aim of simultaneously minimizing the cut size and obtaining balanced cluster The first criterion minimize the similarity between objects belonging to cluster and is an objectif generally mettre in clustering The second criterion is formulated with the aid of generalized entropy The tradeoff between these two objective is explored using multiobjectiv genetic algorithm with enhanced operators
536	Revue des Nouvelles Technologies de l'Information	EGC	2011	Equilibrer l'analyse des motifs fréquents	Cet article propose une méthode originale d'évaluation de la qualité des motifs en anticipant la manière qui sera utilisée pour les analyser. Nous commençons par introduire le modèle de l'analyse aléatoire d'un ensemble de motifs selon une mesure d'intérêt. Avec ce modèle, nous constatons que l'étude des motifs fréquents avec le support conduit à une analyse déséquilibrée du jeu de données. Afin que chaque transaction reçoive la même attention, nous définissons le support équilibré qui corrige le support classique en pondérant les transactions. Nous proposons alors un algorithme qui calcule ces poids et nous validons expérimentalement son efficacité.	Arnaud Giacometti, Arnaud Soulet, Patrick Marcel	http://editions-rnti.fr/render_pdf.php?p1&p=1000927	http://editions-rnti.fr/render_pdf.php?p=1000927	article proposer méthode original dévaluation qualité motif anticiper manière utiliser analyser commencer introduire modeler lanalyse aléatoire dun ensemble motif mesurer dintérêt Avec modeler constater létude motif fréquent support conduire analyser déséquilibrer jeu donnée Afin transaction recevoir attention définir support équilibrer corriger support classique pondérer transaction proposer algorithme calculer poids valider expérimentalement efficacité
537	Revue des Nouvelles Technologies de l'Information	EGC	2011	Equivalence topologique entre mesures de proximité	Le choix d'une mesure de proximité entre objets a un impact direct sur les résultats de toute opération de classification, de comparaison, d'évaluation ou de structuration d'un ensemble d'objets. Pour un problème donné, l'utilisateur est amené à choisir une parmi les nombreuses mesures de proximité existantes. Or, selon la notion d'équivalence choisie, comme celle basée sur les préordonnances, certaines sont plus ou moins équivalentes. Dans cet article, nous proposons une nouvelle approche pour comparer les mesures de proximité. Celle-ci est basée sur l'équivalence topologique. A cet effet, nous introduisons un nouveau concept baptisé équivalence topologique. Ce dernier fait appel à la structure de voisinage local. Nous proposons alors de définir l'équivalence topologique entre deux mesures de proximité à travers la structure topologique induite par chaque mesure. Nous établissons ensuite des liens formels avec l'équivalence en préordonnance. Les deux approches sont comparées sur le plan théorique et sur le plan empirique. Nous illustrons le principe de cette comparaison sur un exemple simple pour une quinzaine de mesures de proximités de la littérature.	Djamel Abdelkader Zighed, Rafik Abdesselam, Ahmed Bounekkar	http://editions-rnti.fr/render_pdf.php?p1&p=1000928	http://editions-rnti.fr/render_pdf.php?p=1000928	choix dune mesurer proximité entrer objet impact direct résultat opération classification comparaison dévaluation structuration dun ensemble dobjet Pour problème donner lutilisateur amener choisir mesure proximité existant Or notion déquivalence choisi baser préordonnance plaire équivalenter Dans article proposer approcher comparer mesure proximité Celleci baser léquivalence topologique A introduire concept baptiser équivalence topologique faire appel structurer voisinage local proposer définir léquivalence topologique entrer mesure proximité travers structurer topologique induire mesurer établir ensuite lien formel léquivalence préordonnance approche comparer plan théorique plan empirique illustrer principe comparaison exemple simple quinzaine mesure proximité littérature
538	Revue des Nouvelles Technologies de l'Information	EGC	2011	Estimation de la densité d'arcs dans les graphes de grande taille: une alternative à la détection de clusters	La recherche de structures dans les graphes est un sujet étudié depuis longtemps, qui a bénéficié d'un regain d'intérêt avec la mise à disposition de graphes de grande taille sur le web, tels les réseaux sociaux. De nombreuses méthodes de recherche de clusters “naturels” dans les graphes ont été proposées, fondées notamment sur la modularité de Newman. On introduit dans cet article une nouvelle façon de résumer la structure des graphes de grande taille, en utilisant des estimateurs de densité des arcs exploitant des modèles en grille, basés sur un co-partitionnent des noeuds source et cible des arcs. Les structures identifiées par cette méthode vont au delà de la “classique” détection de clusters dans les graphes, et permettent d'estimer asymptotiquement la densité des arcs. Les expérimentations confirment le potentiel de l'approche, qui permet d'identifier des structures fortement informatives dans les graphes, sans faire l'hypothèse d'une décomposition en clusters denses.	Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1000986	http://editions-rnti.fr/render_pdf.php?p=1000986	rechercher structure dan graphe étudier bénéficier dun regain dintérêt miser disposition graphe grand tailler web réseau social méthode rechercher cluster “ ” dan graphe proposer fonder modularité Newman introduire dan article résumer structurer graphe grand tailler utiliser estimateur densité arc exploiter modèle griller baser copartitionnent noeud source cibler arc structure identifier méthode aller “ classique ” détection cluster dan graphe permettre destimer asymptotiquement densité arc expérimentation confirmer potentiel lapproche permettre didentifier structure fortement informativer dan graphe faire lhypothèse dune décomposition cluster dense
539	Revue des Nouvelles Technologies de l'Information	EGC	2011	Être ou ne pas être usager d'internet telle est la question ?		Abdoulaye Sarr, Philippe Lenca, Annabelle Boutet, Jocelyne Tremenbert	http://editions-rnti.fr/render_pdf.php?p1&p=1000963	http://editions-rnti.fr/render_pdf.php?p=1000963	
540	Revue des Nouvelles Technologies de l'Information	EGC	2011	Evaluation des outils d'extraction terminologique Quezao et Acabit	L'article décrit l'évaluation de deux outils d'extraction terminologique Acabit et Quezao. Si acabit est plus connu car librement disponible, Quezao est issu des travaux d'Orange Labs sur la recherche d'informations. Après une comparaison sur les approches théoriques des deux systèmes, une évaluation concrète va porter sur un corpus d'actualité (2424Actu) pour l'aspect qualitatif et sur un corpus de presse pour l'aspect quantitatif	Edmond Lassalle, Prem Kumar Casimir, Emilie Guimier De Neef	http://editions-rnti.fr/render_pdf.php?p1&p=1000938	http://editions-rnti.fr/render_pdf.php?p=1000938	larticle décrire lévaluation outil dextraction terminologique Acabit quezao Si acabit plaire connaître librement disponible quezao issu travail dOrange Labs rechercher dinformation Après comparaison approche théorique système évaluation concret aller porter corpus dactualité 2424Actu laspect qualitatif corpus presser laspect quantitatif
541	Revue des Nouvelles Technologies de l'Information	EGC	2011	Extraction de motifs séquentiels contextuels	Les motifs séquentiels traditionnels ne tiennent généralement pas compte des informations contextuelles fréquemment associées aux données séquentielles. Dans le cas des séquences d'achats de clients dans un magasin, l'extraction classique de motifs se focalise sur les achats des clients sans considérer leur catégorie socio-professionnelle, leur sexe, leur âge. Or, en considérant le fait qu'un motif séquentiel est spécifique à un contexte donné, un expert pourra adapter sa stratégie au type du client et prendre les décisions adéquates. Dans cet article, nous proposons d'extraire des motifs de la forme «l'achat des produits A et B suivi de l'achat du produit C est spécifique aux jeunes clients». En mettant en valeur les propriétés formelles de tels contextes, nous développons un algorithme efficace d'extraction de motifs séquentiels contextuels. Les expérimentations effectuées sur un jeu de données réelles montrent les apports et l'efficacité de l'approche proposée.	Julien Rabatel, Sandra Bringay	http://editions-rnti.fr/render_pdf.php?p1&p=1000924	http://editions-rnti.fr/render_pdf.php?p=1000924	motif séquentiel traditionnel généralement compter information contextuel fréquemment associer donnée séquentiel Dans cas séquence dachat client dan magasin lextraction classique motif focaliser achat client considérer catégorie socioprofessionnel sexe âge Or considérer faire quun motif séquentiel spécifique contexte donner expert pouvoir adapter stratégie typer client prendre décision adéquat Dans article proposer dextraire motif former « lachat produit lachat produire spécifique jeune client » En mettre propriété formel contexte développer algorithme efficace dextraction motif séquentiel contextuel expérimentation effectuer jeu donnée réel montrer apport lefficacité lapproche proposer
542	Revue des Nouvelles Technologies de l'Information	EGC	2011	Extraction de motifs temporels à partir de séquences d'événements avec intervalles temporels	La fouille de base de données séquentielles a pour objet l'extraction de motifs séquentiels représentatifs. La plupart des méthodes concernent des motifs composés d'événements liés par des relations temporelles basées sur la précédence des instants. Pourtant, dans de nombreuses situations réelles une information quantitative sur la durée des événements ou le délai inter-événements est nécessaire pour discriminer les phénomènes. Nous proposons deux algorithmes, QTIAPriori et QTIPrefixSpan, pour extraire des motifs temporels composés d'événements associés à des intervalles décrivant leur position dans le temps et leur durée. Chacun d'eux ajoute aux algorithmes GSP et PrefixSpan une étape de catégorisation d'intervalles multi-dimensionnels pour extraire les intervalles temporelles représentatifs. Les expérimentations sur des données simulées montrent la capacité des algorithmes à extraire des motifs précis en présence de bruit et montrent l'amélioration des performances en temps de calcul.	Rene Quiniou, Thomas Guyet	http://editions-rnti.fr/render_pdf.php?p1&p=1000925	http://editions-rnti.fr/render_pdf.php?p=1000925	fouiller baser donnée séquentiel objet lextraction motif séquentiel représentatif méthode concerner motif composer dévénement lier relation temporel baser précédence instant pourtant dan situation réel information quantitatif durer événement délai interévénement nécessaire discriminer phénomène proposer algorithme QTIAPriori qtiprefixspan extraire motif temporel composer dévénement associer intervalle décrire position dan temps durer Chacun ajouter algorithm GSP prefixspan étape catégorisation dintervall multidimensionnel extraire intervalle temporel représentatif expérimentation donnée simuler montrer capacité algorithme extraire motif précis présence bruire montrer lamélioration performance temps calcul
543	Revue des Nouvelles Technologies de l'Information	EGC	2011	Extraction et Analyse de réseaux sociaux issus de Bases de Données Relationnelles	Dans un contexte d'entreprise, beaucoup d'informations importantes restent stockées dans des bases de données relationnelles, constituant une source riche pour construire des réseaux sociaux. Le réseau, ainsi extrait, a souvent une taille importante ce qui rend son analyse et sa visualisation difficiles. Dans ce travail, nous proposons une étape d'extraction suivie d'une étape d'agrégation des réseaux sociaux à partir des bases de données relationnelles. L'étape d'extraction ou de construction transforme une base de données relationnelle en base de données graphe, puis le réseau social est extrait. L'étape d'agrégation, qui est basée sur l'algorithme k-SNAP, produit un graphe résumé.	Rania Soussi, Amine Louati, Marie-Aude Aufaure, Hajer Baazaoui Zghal, Yves Lechevallier, Henda Ben Ghezela Hadjami	http://editions-rnti.fr/render_pdf.php?p1&p=1000988	http://editions-rnti.fr/render_pdf.php?p=1000988	Dans contexte dentreprise dinformation important ruer stocker dan base donnée relationnel constituer source riche construire réseau social réseau extraire tailler important analyser visualisation difficile Dans travail proposer étape dextraction dune étape dagrégation réseau social partir base donnée relationnel Létape dextraction construction transform baser donnée relationnel baser donnée graph pouvoir réseau social extraire Létape dagrégation baser lalgorithme ksnap produire graph résumer
544	Revue des Nouvelles Technologies de l'Information	EGC	2011	Extraction sous contraintes d'ensembles de cliques homogènes	Nous proposons une méthode de fouille de données sur des graphes ayant un ensemble d'étiquettes associé à chaque sommet. Une application est, par exemple, d'analyser un réseau social de chercheurs co-auteurs lorsque des étiquettes précisent les conférences dans lesquelles ils publient.Nous définissons l'extraction sous contraintes d'ensembles de cliques tel que chaque sommet des cliques impliquées partage suffisamment d'étiquettes. Nous proposons une méthode pour calculer tous les Ensembles Maximaux de Cliques dits Homogènes qui satisfont une conjonction de contraintes fixée par l'analyste et concernant le nombre de cliques séparées, la taille des cliques ainsi que le nombre d'étiquettes partagées. Les expérimentations montrent que l'approche fonctionne sur de grands graphes construits à partir de données réelles et permet la mise en évidence de structures intéressantes	Pierre-Nicolas Mougel, Marc Plantevit, Christophe Rigotti, Olivier Gandrillon, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1000999	http://editions-rnti.fr/render_pdf.php?p=1000999	proposer méthode fouiller donnée graphe ensemble détiquettes associer sommet application exemple danalyser réseau social chercheur coauteur étiquette préciser conférence dan publientNous définisson lextraction sou contraint densemble clique sommet clique impliquer partager suffisamment détiquettes proposer méthode calculer tou ensemble maximal clique Homogènes satisfaire conjonction contrainte fixer lanalyste concerner nombre clique séparer tailler clique nombre détiquettes partagée expérimentation montrer lapproche fonctionner grand graphe construit partir donnée réel permettre miser évidence structure intéressant
545	Revue des Nouvelles Technologies de l'Information	EGC	2011	Heuristique pour l'extraction de motifs ensemblistes bruités	La recherche de motifs ensemblistes dans des matrices de données booléennes est une problématique importante dans un processus d'extraction de connaissances. Elle consiste à rechercher tous les rectangles de 1 dans une matrice de données à valeurs dans {0,1} dans lesquelles l'ordre des lignes et colonnes n'est pas important. Plusieurs algorithmes ont été développés pour répondre à ce problème, mais s'adaptent difficilement à des données réelles susceptibles de contenir du bruit. Un des effets du bruit est de pulvériser un motif pertinent en un ensemble de sous-motifs recouvrants et peu pertinents, entraînant une explosion du nombre de motifs résultats. Dans le cadre de ce travail, nous proposons une nouvelle approche heuristique basée sur les algorithmes de graphes pour la recherche de motifs ensemblistes dans des contextes binaires bruités. Pour évaluer notre approche, différents tests ont été réalisés sur des données synthétiques et des données réelles issues d'applications bioinformatiques.	Céline Rouveirol, Lucas Létocart, Karima Mouhoubi	http://editions-rnti.fr/render_pdf.php?p1&p=1001002	http://editions-rnti.fr/render_pdf.php?p=1001002	rechercher motif ensembliste dan matrice donnée booléen problématique important dan processus dextraction connaissance consister rechercher tou rectangle 1 dan matrice donnée dan 01 dan lordre ligne colonne nest importer algorithme développer répondre problème sadaptent difficilement donnée réel susceptible contenir bruire bruire pulvériser motif pertiner ensemble sousmotif recouvrant pertinent entraîner explosion nombre motif résultat Dans cadrer travail proposer approcher heuristique baser algorithme graphe rechercher motif ensembliste dan contexte binaire bruiter Pour évaluer approcher test réaliser donnée synthétique donnée réel issu dapplication bioinformatiqu
546	Revue des Nouvelles Technologies de l'Information	EGC	2011	Import automatique et interactif de données dans les systèmes de visualisations	La première étape du processus de visualisation d'information consiste à transformer les données d'un format brut vers une structure de données utilisable par les différents composants de visualisation. Dans les applications réelles, cette première étape représente une barrière empêchant l'accès des utilisateurs novices à une riche variété de techniques de visualisation. Par exemple, il peut être techniquement impossible pour un utilisateur lambda de transformer des données arborescentes en un modèle de graphe pouvant utiliser une représentation à base de TreeMap. Une autre barrière est aussi la multitude de transformations possible des données brutes. Il faut pouvoir explorer cet ensemble de combinaisons. Basé sur nos retours d'expériences avec des utilisateurs finaux, dans cet article, nous considérons que le format brut est sous forme tabulaire. Ce format est le plus couramment utilisé et est facilement accessible par nos utilisateurs. Nous proposons une méthode novatrice permettant de générer automatiquement des graphes valués à partir de n'importe quelle table. En analysant le contenu de chaque dimension nous identifions les interconnexions entre celles-ci. Puis nous caractérisons les entités, les attributs et les relations possibles au sein des tables. Finalement, nous intégrons l'utilisateur dans le processus de transformation en lui proposant un ensemble de transformations valides.	David Auber, Frédéric Gilbert	http://editions-rnti.fr/render_pdf.php?p1&p=1001006	http://editions-rnti.fr/render_pdf.php?p=1001006	étape processus visualisation dinformation consister transformer donnée dun format brut ver structurer donnée utilisable composant visualisation Dans application réel étape représenter barrière empêcher laccès utilisateur novice riche variété technique visualisation Par exemple pouvoir techniquement impossible utilisateur lambda transformer donnée arborescent modeler graphe pouvoir utiliser représentation baser treemap barrière multitude transformation donnée brut falloir pouvoir explorer ensemble combinaison baser dexpérience utilisateur final dan article considérer format brut sou former tabulaire format plaire couramment utiliser facilement accessible utilisateur proposer méthode novateur permettre générer automatiquement graphe valuer partir nimporte quell tabler En analyser contenir dimension identifier interconnexion entrer cellesci Puis caractériser entité attribut relation table finalement intégrer lutilisateur dan processus transformation luire proposer ensemble transformation valide
547	Revue des Nouvelles Technologies de l'Information	EGC	2011	Intégration de données haptiques brutes dans des systèmes experts de diagnostic des connaissances	Cet article a pour cadre un environnement informatique pour l'apprentissage humain (EIAH) dédié à la chirurgie orthopédique, et plus précisément sur le diagnostic des connaissances des apprenants. Pour ce faire, un réseau bayésien infère à partir d'exercices que les étudiants réalisent sur un simulateur avec bras articulé. Ce réseau résulte d'une approche centrée expert du domaine, comme très souvent dans les EIAH. Pourtant, dans un domaine comme la chirurgie où les connaissances sont tacites, le geste de l'apprenant semble intéressant à considérer. Le but de nos travaux est donc d'adopter une démarche plus centrée sur les données en incorporant au réseau bayésien les données haptiques continues issues du simulateur. Divers problèmes se posent néanmoins, d'une part sur le besoin d'étudier la nature des données pour conserver la généricité du système, et d'autre part pour trouver des méthodes de validation pertinentes concernant leur traitement	Sébastien Lallé, Vanda Luengo	http://editions-rnti.fr/render_pdf.php?p1&p=1001026	http://editions-rnti.fr/render_pdf.php?p=1001026	article cadrer environnement informatique lapprentissage humain EIAH dédier chirurgie orthopédique plaire précisément diagnostic connaissance apprenant Pour faire réseau bayésien infère partir dexercice étudiant réaliser simulateur bras articuler réseau résulter dune approcher centrer expert domaine dan eiah pourtant dan domaine chirurgie connaissance taciter geste lapprenant sembler intéresser considérer boire travail dadopter démarcher plaire centrer donnée incorporer réseau bayésien donnée haptique continu issu simulateur problème poser dune partir besoin détudier nature donnée conserver généricité système dautre partir trouver méthode validation pertinent concerner traitement
548	Revue des Nouvelles Technologies de l'Information	EGC	2011	Interprétation graphique de la courbe ROC		François Rioult	http://editions-rnti.fr/render_pdf.php?p1&p=1000969	http://editions-rnti.fr/render_pdf.php?p=1000969	
549	Revue des Nouvelles Technologies de l'Information	EGC	2011	Interprétation spectrale de la classification relationnelle	Ce papier présente une vue spectrale sur l'approche de l'analyse relationnelle pour la classification des données catégorielles. Il établit d'abord le lien théorique entre l'approche de l'analyse relationnelle et le problème de classification spectrale. En particulier, le problème de classification relationnelle est présenté comme un problème de maximisation de trace, ce problème est donc transformé par la relaxation spectrale en un problème d'optimisation sous contraintes qui peut être résolu par des multiplicateurs de Lagrange, la solution est donnée par un problème de valeurs propres.	Lazhar Labiod, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1000997	http://editions-rnti.fr/render_pdf.php?p=1000997	papier présenter spectral lapproche lanalyse relationnel classification donnée catégoriel établir dabord lien théorique entrer lapproche lanalyse relationnel problème classification spectral En problème classification relationnel présenter problème maximisation tracer problème transformer relaxation spectral problème doptimisation sou contrainte pouvoir résoudre multiplicateur Lagrange solution donner problème propre
550	Revue des Nouvelles Technologies de l'Information	EGC	2011	Introduction de l'ingénierie ontologique dans la méthodologie de développement des progiciels de gestion des collectivités territoriales		Wilfried Despagne, Thomas Burger	http://editions-rnti.fr/render_pdf.php?p1&p=1000975	http://editions-rnti.fr/render_pdf.php?p=1000975	
551	Revue des Nouvelles Technologies de l'Information	EGC	2011	ISICIL : Intégration Sémantique d'Informations à travers des Communautés d'Intelligence en Ligne		Pavel Arapov, Sébastien Comos, Olivier Corby, Nicolas Delaforge, Guillaume Erétéo, Catherine Faron-Zucker, Michel Buffa, Fabien Gandon, Guillaume Husson, Freddy Limpens	http://editions-rnti.fr/render_pdf.php?p1&p=1000960	http://editions-rnti.fr/render_pdf.php?p=1000960	
552	Revue des Nouvelles Technologies de l'Information	EGC	2011	Les moteurs de wikis sémantiques : un état de l'art	Cet article est un état de l'art sur les moteurs de wiki sémantique, en particulier sur leur utilisation des technologies du Web sémantique. Les principales notions liées aux wikis sémantiques sont d'abord présentées. Ensuite, plusieurs projets actifs de moteurs de wiki sont comparés selon différents points de vue. Finalement, des recommandations sont données pour le choix d'un moteur de wiki. En conclusion, les auteurs s'interrogent sur les perspectives des wikis sémantiques telles que la faible interopérabilité de certains moteurs.	Thomas Meilender, Nicolas Jay, Jean Lieber, Fabien Palomares	http://editions-rnti.fr/render_pdf.php?p1&p=1001020	http://editions-rnti.fr/render_pdf.php?p=1001020	article lart moteur wiki sémantique utilisation technologie web sémantique principal notion lier wiki sémantique dabord présenter ensuite projet actif moteur wiki comparer point finalement recommandation donner choix dun moteur wiki En conclusion auteur sinterroger perspective wiki sémantique faible interopérabilité moteur
553	Revue des Nouvelles Technologies de l'Information	EGC	2011	M3A : Une plateforme d'ingénierie de maintenance assistée par apprentissage automatique		Abdelkader Benameur, Baghdad Atmani	http://editions-rnti.fr/render_pdf.php?p1&p=1000954	http://editions-rnti.fr/render_pdf.php?p=1000954	
554	Revue des Nouvelles Technologies de l'Information	EGC	2011	Mesure de concordance pour les bases de données évidentielles	Dans cet article, nous proposons une mesure de concordance d'une source avec les autres sources. Cette mesure pourra servir à réduire l'importance de ses fonctions de masse avant de les combiner afin de trouver un compromis et donc réduire le conflit. Cette mesure sera illustrée par des données réelles.	Mouna Chebbah, Arnaud Martin, Boutheina Ben Yaghlane	http://editions-rnti.fr/render_pdf.php?p1&p=1000945	http://editions-rnti.fr/render_pdf.php?p=1000945	Dans article proposer mesurer concordance dune source source mesurer pouvoir servir réduire limportance fonction masser combiner trouver compromis réduire conflit mesurer illustrer donnée réel
555	Revue des Nouvelles Technologies de l'Information	EGC	2011	Mesures d'hétérogénéité sémantique des systèmes P2P non-structurés	L'autonomie des participants dans les systèmes P2P pour le partage de données peut conduire à une situation d'hétérogénéité sémantique dans le cas où les participants utilisent leurs propres ontologies pour représenter leurs données. Dans cet article nous commençons par définir des mesures de disparité entre participants en considérant leurs contextes sémantiques. En considérant la topologie du système et les disparités entre participants, nous proposons des mesures d'hétérogénéité sémantique d'un système P2P non-structuré.	Thomas Cerqueus, Sylvie Cazalens, Philippe Lamarre	http://editions-rnti.fr/render_pdf.php?p1&p=1000984	http://editions-rnti.fr/render_pdf.php?p=1000984	Lautonomie participant dan système p2p partager donnée pouvoir conduire situation dhétérogénéité sémantique dan cas participant utiliser propre ontologie représenter donnée Dans article commencer définir mesure disparité entrer participant considérer contexte sémantique En considérer topologie système disparité entrer participant proposer mesure dhétérogénéité sémantique dun système p2p nonstructuré
556	Revue des Nouvelles Technologies de l'Information	EGC	2011	Mixer les moyens pour extraire les gloses	Nous proposons d'extraire des connaissances lexicales en exploitant les « gloses » de mot, ces descriptions spontanées de sens, repérables par des marqueurs lexicaux et des configurations morpho-syntaxiques spécifiques. Ainsi dans l'extrait suivant, le mot testing est suivi d'une glose en c'est-à dire : « 10 % de ces embauches vont porter sur un métier qui monte : le «testing», c'est-à-dire la maîtrise des méthodologies rigoureuses de test des logiciels». Cette approche ouvre des perspectives pour l'acquisition lexicale et terminologique, fondamentale pour de nombreuses tâches. Dans cet article, nous comparons deux façons d'extraire les unités en relation de glose : patrons et statistiques d'associations d'unités sur le web, en les évaluant sur des données réelles.	Augusta Mela, Mathieu Roche, Mohamed el Amine Bekhtaoui	http://editions-rnti.fr/render_pdf.php?p1&p=1000935	http://editions-rnti.fr/render_pdf.php?p=1000935	proposer dextraire connaissance lexical exploiter « glose » description spontaner sens repérable marqueur lexical configuration morphosyntaxique spécifique dan lextrer testing dune gloser cestà   « 10   embauche aller porter métier mont   « testing » cestàdir maîtriser méthodologie rigoureux test logiciel » approcher ouvrir perspective lacquisition lexical terminologique fondamental tâche Dans article comparer dextraire unité relation gloser   patron statistique dassociation duniter web évaluer donnée réel
557	Revue des Nouvelles Technologies de l'Information	EGC	2011	Mobility, Data Mining and Privacy: Mining Human Movement Patterns from Trajectory Data	The technologies of mobile communications and ubiquitous computing pervade our society, and wireless networks sense the movement of people and vehicles, generating large volumes of mobility data, such as mobile phone call records and GPS tracks. This is a scenario of great opportunities and risks : on one side, mining this data can produce useful knowledge, supporting sustainable mobility and intelligent transportation systems ; on the other side, individual privacy is at risk, as the mobility data contain sensitive personal information. A new multidisciplinary research area is emerging at this crossroads of mobility, data mining, and privacy. The talk assesses this research frontier from a data mining perspective, and illustrates the results of a European-wide research project called GeoPKDD, Geographic Privacy-Aware Knowledge Discovery and Delivery. GeoPKDD has created an integrated platform named MATLAS for complex analysis of mobility data, which combines spatio-temporal querying capabilities with data mining, visual analytics and semantic technologies, thus providing a full support for the Mobility Knowledge Discovery process. In this talk, we focus on the key data mining models : trajectory patterns and trajectory clustering, and illustrate the analytical power of our system in unvealing the complexity of urban mobility in a large metropolitan area by means of a large scale experiment, based on a massive real life GPS dataset, obtained from 17,000 vehicles with on-board GPS receivers, tracked during one week of ordinary mobile activity in the urban area of the city of Milan, Italy.	Fosca Giannotti	http://editions-rnti.fr/render_pdf.php?p1&p=1000920	http://editions-rnti.fr/render_pdf.php?p=1000920	The technologi of mobile communication and ubiquitous computing pervad our society and wireless network sense the movement of people and vehicler generating large volumer of mobility dater such mobile phone call record and GPS track This is scenario of great opportunitie and risk   one sid mining this dater can produce useful knowledge supporting sustainabl mobility and intelligent transportation system   the other side individual privacy is at risk the mobility dater contain sensitif personal information A new multidisciplinary research areer is emerging at this crossroads of mobility dater mining and privacy The talk assesse this research frontier from dater mining perspectif and illustrat the results of Europeanwide research project called GeoPKDD Geographic PrivacyAware Knowledge Discovery and Delivery GeoPKDD created an integrated platform named matla for complex analysi of mobility dater which combine spatiotemporal querying capabilitie with dater mining visual analytic and semantic technologie thus providing full support for the Mobility Knowledge Discovery process In this talk we focus the key dater mining model   trajectory pattern and trajectory clustering and illustrate the analytical power of our system in unvealing the complexity of urban mobility in large metropolitan areer by means of large scal experiment based massif real lif GPS dataset obtained from 17000 vehicle with onboard GPS receiver tracked during one week of ordinary mobile activity in the urban areer of the city of Milan Italy
558	Revue des Nouvelles Technologies de l'Information	EGC	2011	Modèle pour une analyse du phénomène de linéarité de catégories sémantiques dans les énoncés en français		Bernard Decobert	http://editions-rnti.fr/render_pdf.php?p1&p=1000982	http://editions-rnti.fr/render_pdf.php?p=1000982	
559	Revue des Nouvelles Technologies de l'Information	EGC	2011	Modélisation d'une ressource termino-ontologique de domaine pour l'annotation sémantique de tableaux	Nous proposons dans cet article une modélisation d'une ressource termino-ontologique (RTO) de domaine, guidée par la tâche d'annotation sémantique de tableaux. L'annotation d'un tableau consiste à annoter ses cellules, pour pouvoir ensuite identifier les concepts représentés par ses colonnes et enfin identifier la ou les relations n-aires qu'il représente. La RTO proposée permet d'une part de modéliser dans sa composante lexicale les termes utilisés pour l'annotation des cellules en intégrant la gestion des synonymes et du multilingue, et, d'autre part, de modéliser dans sa composante conceptuelle les concepts symboliques, les concepts numériques et les relations n-aires, qui sont propres au domaine étudié.	Patrice Buche, Juliette Dibie-Barthélemy, Liliana Ibanescu, Abir Saïd	http://editions-rnti.fr/render_pdf.php?p1&p=1001021	http://editions-rnti.fr/render_pdf.php?p=1001021	proposer dan article modélisation dune ressource terminoontologiqu RTO domaine guider tâcher dannotation sémantique tableau lannotation dun tableau consister annoter cellule pouvoir ensuite identifier concept représenter colonne identifier relation nair quil représenter RTO proposer permettre dune partir modéliser dan composant lexical terme utiliser lannotation cellule intégrer gestion synonyme multilingue dautre partir modéliser dan composant conceptuel concept symbolique concept numérique relation nair propre domaine étudier
560	Revue des Nouvelles Technologies de l'Information	EGC	2011	Modélisation de la dynamique de phénomènes spatio-temporels par des séquences de motifs	Dans ce papier, nous proposons un nouveau cadre théorique permettant de modéliser la dynamique de phénomènes spatio-temporels. Nous définissons le concept de séquences spatio-temporelles de motifs afin de capturer les interactions entre des ensembles de propriétés et un phénomène à observer. Un algorithme incrémental est proposé pour extraire des séquences spatiotemporelles de motifs sous contraintes, et une nouvelle structure de données est mise en place afin d'améliorer ses performances. Un prototype a été développé et testé sur des données réelles.	Loïc Mabit, Nazha Selmaoui-Folcher, Frédéric Flouvat	http://editions-rnti.fr/render_pdf.php?p1&p=1001001	http://editions-rnti.fr/render_pdf.php?p=1001001	Dans papier proposer cadrer théorique permettre modéliser dynamique phénomène spatiotemporel définir concept séquence spatiotemporell motif capturer interaction entrer ensemble propriété phénomène observer algorithme incrémental proposer extraire séquence spatiotemporell motif sou contrainte structurer donnée mettre placer daméliorer performance prototype développer tester donnée réel
561	Revue des Nouvelles Technologies de l'Information	EGC	2011	Modélisation de la propagation de l'information sur le Web : de l'extraction des données à la simulation	Nous proposons un modèle de la propagation de l'information dans un réseau, en détaillant toutes les étapes de sa réalisation et de son utilisation dans un cadre de simulation. A partir de données réelles extraites du Web, nous identifions parmi les sources des catégories de comportements de publication distincts. Nous proposons ensuite une extension d'un modèle de diffusion de l'information existant, afin d'augmenter son pouvoir d'expression, en particulier pour reproduire ces comportements de publication, puis nous le validons sur un exemple de simulation.	François Nel, Marie-Jeanne Lesot, Philippe Capet, Thomas Delavallade	http://editions-rnti.fr/render_pdf.php?p1&p=1001023	http://editions-rnti.fr/render_pdf.php?p=1001023	proposer modeler propagation linformation dan réseau détailler étape réalisation utilisation dan cadrer simulation A partir donnée réel extraite Web identifier source catégorie comportement publication distinct proposer ensuite extension dun modeler diffusion linformation exister daugmenter pouvoir dexpression reproduire comportement publication pouvoir validon exemple simulation
562	Revue des Nouvelles Technologies de l'Information	EGC	2011	Moteur de questions réponses à partir de données du web sémantique		Michel Plu	http://editions-rnti.fr/render_pdf.php?p1&p=1000958	http://editions-rnti.fr/render_pdf.php?p=1000958	
563	Revue des Nouvelles Technologies de l'Information	EGC	2011	Moteur de questions-réponses d'une base de connaissances	Cet article présente comment la gestion et l'exploitation de connaissances issues du site web Wikipedia ont permis de développer une telle fonction qui a été intégrée depuis février 2010 dans un moteur de recherche internet français pour le grand public. Aujourd'hui cette fonction est capable de répondre à des questions formulées en langage naturelle sur environs 170 000 lieux ou personnes. La formalisation des données extraites de wikipedia en connaissances au format OWL ou RDFS a permis de déduire de nouvelles informations manquantes, de typer les entités nommées trouvées et de traiter de nouvelles formes de questions qui étaient non traitées.	Michel Plu, Johannes Heinecke	http://editions-rnti.fr/render_pdf.php?p1&p=1001024	http://editions-rnti.fr/render_pdf.php?p=1001024	article présenter gestion lexploitation connaissance issu site web Wikipedia permettre développer fonction intégrer février 2010 dan moteur rechercher internet français grand public Aujourdhui fonction capable répondre question formuler langage 170 000 lieu formalisation donnée extrait wikipedia connaissance format OWL RDFS permettre déduire information manquant typer entité nommer trouvé traiter forme question traiter
564	Revue des Nouvelles Technologies de l'Information	EGC	2011	Motifs Séquentiels delta-Libres	Bien que largement étudiée, l'extraction de motifs séquentiels reste une tâche très difficile et pose aussi le défi du grand nombre de motifs produits. Dans cet article, nous proposons une nouvelle approche extrayant les motifs séquentiels les plus généraux à fréquence similaire. Nous montrons en quoi l'extension de cette notion, déjà connue pour les motifs ensemblistes, est un problème particulièrement difficile pour les séquences. Les motifs delta-libres ainsi produits sont en nombre réduit et facilitent les usages d'un processus de fouille et nous montrons leur apport comme descripteurs dans un contexte de classification de séquences.	Marc Plantevit, Chedy Raïssi, Bruno Crémilleux	http://editions-rnti.fr/render_pdf.php?p1&p=1000926	http://editions-rnti.fr/render_pdf.php?p=1000926	largement étudier lextraction motif séquentiel rester tâcher difficile poser défi grand nombre motif produire Dans article proposer approcher extraire motif séquentiel plaire général fréquence similaire montrer lextension notion déjà connaître motif ensembliste problème difficile séquence motif deltalibr produire nombre réduire faciliter usage dun processus fouiller montrer apport descripteur dan contexte classification séquence
565	Revue des Nouvelles Technologies de l'Information	EGC	2011	MuMIe: Une Approche Automatique pour l'Interopérabilité des Métadonnées	Avec l'explosion du multimedia, l'utilisation des métadonnées est devenue cruciale pour assurer une bonne gestion des contenus. Cependant, il est nécessaire d assurer un accès uniforme aux métadonnées. Plusieurs techniques ont ainsi été développées afin de réaliser cette interopérabilité. La plupart d'entre elles sont spécifiques à un seul langage de description. Les systèmes de matching existants présentent certaines limites, en particulier dans le traitement des informations structurelles. Nous présentons dans cet article un nouveau système d'intégration qui supporte des schémas provenant de langages descriptifs différents. De plus, la méthode de matching proposée a recours à plusieurs types d'information de façon à augmenter la précision de matching	Samir Amir, Ioan Marius Bilasco, Thierry Urruty, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1000985	http://editions-rnti.fr/render_pdf.php?p=1000985	Avec lexplosion multimedia lutilisation métadonnée devenir crucial gestion contenu nécessaire accès uniforme métadonné technique développer réaliser interopérabilité dentre spécifique langage description système matching existant présenter limite dan traitement information structurel présenter dan article système dintégration supporter schéma provenir langage descriptif De plaire méthode matching proposer recourir type dinformation augmenter précision matching
566	Revue des Nouvelles Technologies de l'Information	EGC	2011	Nomao : la recherche géolocalisée personnalisée		Laurent Candillier	http://editions-rnti.fr/render_pdf.php?p1&p=1000955	http://editions-rnti.fr/render_pdf.php?p=1000955	
567	Revue des Nouvelles Technologies de l'Information	EGC	2011	Nouvelle approche de fouille de graphes AC-réduits fréquents	La fouille de graphes est devenue une piste de recherche intéressante et un défi réel en matière de fouille de données. Parmi les différentes familles de motifs de graphes, les graphes fréquents permettent une caractérisation intéressante des groupes de graphes, ainsi qu'une discrimination des différents graphes lors de la classification ou de la segmentation. A cause de la NP-complétude du test d'isomorphisme de sous-graphes et de l'immensité de l'espace de recherche, les algorithmes de fouille de graphes sont exponentiels en temps d'exécution et/ou occupation mémoire. Dans cet article, nous étudions un nouvel opérateur de projection polynomial nommé AC-projection basé sur une propriété clé du domaine de la programmation par contraintes, à savoir l'arc consistance. Cet opérateur est censé remplacer l'utilisation de l'isomorphisme de sous-graphes en établissant un biais sur la projection. Cette étude est suivie d'une évaluation expérimentale du pouvoir discriminant des patterns AC-réduits découverts.	Brahim Douar, Michel Liquiere, Cherif Chiraz Latiri, Yahya Slimani	http://editions-rnti.fr/render_pdf.php?p1&p=1001004	http://editions-rnti.fr/render_pdf.php?p=1001004	fouiller graphe devenir pister rechercher intéressant défi réel matière fouiller donnée Parmi famille motif graphe graphe fréquent permettre caractérisation intéressant groupe graphe quune discrimination graphe classification segmentation A causer npcomplétude test disomorphisme sousgraphe limmensité lespace rechercher algorithme fouiller graphe exponentiel temps dexécution etou occupation mémoir Dans article étudier nouvel opérateur projection polynomial nommer acprojection baser propriété cler domaine programmation contraindre savoir larc consistance opérateur censé remplacer lutilisation lisomorphisme sousgraphe établir biais projection étude dune évaluation expérimental pouvoir discriminer pattern acréduit découvert
568	Revue des Nouvelles Technologies de l'Information	EGC	2011	Optimisation de l'extraction de l'alignement des ontologies avec la contrainte de différence	Dans ce papier, nous proposons une approche basée sur la programmation par contraintes pour aborder efficacement le problème de l'alignement des ontologies, et plus particulièrement l'extraction des correspondances à partir des mesures de similarités. La complexité de ce problème est accentuée dans les applications à caractère dynamique où l'aspect performance est capital. Plus précisément, nous exploitons la contrainte globale de différence développée dans le domaine de la programmation par contraintes pour extraire un alignement total et injectif. Nous montrons que cette approche est efficace et se prête à une mise en oeuvre à la fois interactive et automatique.	Moussa Benaissa, Yahia Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1000991	http://editions-rnti.fr/render_pdf.php?p=1000991	Dans papier proposer approcher basé programmation contrainte aborder efficacement problème lalignement ontologie plaire lextraction correspondance partir mesure similarité complexité problème accentuer dan application caractère dynamique laspect performance capital plaire précisément exploiter contraint global différence développer dan domaine programmation contrainte extraire alignement total injectif montrer approcher efficace prêter miser oeuvrer interactif automatique
569	Revue des Nouvelles Technologies de l'Information	EGC	2011	Optimisation directe des poids de modèles dans un prédicteur Bayésien naïf moyenné	Le classifieur Bayésien naïf est un outil de classification efficace en pratique pour de nombreux problèmes réels, en dépit de l'hypothèse restrictive d'indépendance des variables conditionnellement à la classe. Récemment, de nouvelles méthodes permettant d'améliorer la performance de ce classifieur ont vu le jour, sur la base à la fois de sélection de variables et de moyennage de modèles. Dans cet article, nous proposons une extension de la sélection de variables pour le classifieur Bayésien naïf, en considérant un modèle de pondération des variables utilisées et des algorithmes d'optimisation directe de ces poids. Les expérimentations confirment la pertinence de notre approche, en permettant une diminution significative du nombre de variables utilisées, sans perte de performance prédictive.	Marc Boullé, Romain Guigourès	http://editions-rnti.fr/render_pdf.php?p1&p=1000932	http://editions-rnti.fr/render_pdf.php?p=1000932	classifieur Bayésien naïf outil classification efficace pratiquer problème réel dépit lhypothèse dindépendance variable conditionnellemer classer récemment méthode permettre daméliorer performance classifieur voir jour baser sélection variable moyennage modèle Dans article proposer extension sélection variable classifieur bayésien naïf considérer modeler pondération variable utilisée algorithme doptimisation direct poids expérimentation confirmer pertinence approcher permettre diminution significatif nombre variable utiliser perte performance prédictif
570	Revue des Nouvelles Technologies de l'Information	EGC	2011	Parameter-free association rule mining with yacaree		José L Balcazar 	http://editions-rnti.fr/render_pdf.php?p1&p=1000952	http://editions-rnti.fr/render_pdf.php?p=1000952	
571	Revue des Nouvelles Technologies de l'Information	EGC	2011	Point of View Based Clustering of Socio-Semantic Networks		Juan David Cruz, Cécile Bothorel, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1000973	http://editions-rnti.fr/render_pdf.php?p=1000973	
572	Revue des Nouvelles Technologies de l'Information	EGC	2011	Pondération et classification simultanée de données binaires et continues	Dans cet article, nous proposons une nouvelle approche de classification topologique et de pondération des variables mixtes (qualitatives et quantitatives codées en binaire) durant un processus d'apprentissage non supervisé. Cette approche est basée sur le modèle des cartes auto-organisatrices. L'apprentissage est combiné à un mécanisme de pondération des différentes variables sous forme de poids d'influence sur la pertinence des variables. L'apprentissage des pondérations et des prototypes est réalisé d'une manière simultanée en favorisant une classification optimisée des données. L'approche proposée a été validée sur des données qualitatives codées en binaire et plusieurs bases de données mixtes.	Nicoleta Rogovschi, Mustapha Lebbah, Nistor Grozavu	http://editions-rnti.fr/render_pdf.php?p1&p=1000930	http://editions-rnti.fr/render_pdf.php?p=1000930	Dans article proposer approcher classification topologique pondération variable mixte qualitatif quantitatif coder binaire durer processus dapprentissage superviser approcher baser modeler carte autoorganisatrice Lapprentissage combiner mécanisme pondération variable sou former poids dinfluence pertinence variable Lapprentissage pondération prototype réaliser dune manière simultaner favoriser classification optimiser donnée Lapproche proposer valider donnée qualitatif coder binaire base donnée mixte
573	Revue des Nouvelles Technologies de l'Information	EGC	2011	PPMI : étude formelle d'une variante à valeurs positives de la PMI		Mohamed Nadif, François Role	http://editions-rnti.fr/render_pdf.php?p1&p=1000965	http://editions-rnti.fr/render_pdf.php?p=1000965	
574	Revue des Nouvelles Technologies de l'Information	EGC	2011	Prévision de trajectoires de cyclones à l'aide de forêts aléatoires avec arbres de régression	Nous présentons une étude pour la prédiction des trajectoires de cyclones dans l'océan Atlantique Nord à partir de données issues d'images satellites. On y extrait des mesures de vitesses de vent, de vorticité, d'humidité (base JRA-25) et des mesures de latitude, de longitude et de vitesse de vent instantanée des cyclones toutes les 6 heures (base IBTrACS). Les modèles de référence à ce jour ne tiennent pas compte des corrélations entre les données et les prévisions ce qui limite leur intérêt pour certains utilisateurs. Nous proposons ainsi de prédire le déplacement en latitude et le déplacement en longitude au même instant à un horizon de 120 h toutes les 6 h à l'aide de forêts aléatoires avec arbres de régression. Sur le long terme, à partir de 18 h, la méthode proposée donne de meilleurs résultats que les méthodes existantes.	Sterenn Liberge, Sileye O. Ba, Philippe Lenca, Ronan Fablet	http://editions-rnti.fr/render_pdf.php?p1&p=1001031	http://editions-rnti.fr/render_pdf.php?p=1001031	présenter étude prédiction trajectoire cyclone dan locéan atlantique nord partir donnée issu dimag satellite yu extraire mesure vitesse vent vorticité dhumidité baser JRA25 mesure latitude longitude vitesse vent instantané cyclone 6 heure baser ibtrac modèle référence jour compter corrélation entrer donnée prévision limiter intérêt utilisateur proposer prédire déplacement latitude déplacement longitude instant horizon 120 6 laid forêt aléatoire arbre régression Sur long terme partir 18 méthode proposer donner meilleur résultat méthode existant
575	Revue des Nouvelles Technologies de l'Information	EGC	2011	Prise en compte du réseau de sources pour la fusion d'informations		Thomas Bärecke, Marie-Jeanne Lesot, Herman Akdag, Bernadette Bouchon-Meunier	http://editions-rnti.fr/render_pdf.php?p1&p=1000980	http://editions-rnti.fr/render_pdf.php?p=1000980	
576	Revue des Nouvelles Technologies de l'Information	EGC	2011	Propositionaliser des attributs numériques sans les discrétiser, ni les agréger	La fouille de données relationnelles considère des données contenues dans au moins deux tables reliées par une association un-à-plusieurs, par exemple des clients et leurs achats, ou des molécules et leurs atomes. Une façon de fouiller ces données consiste à transformer les données en une seule table attribut-valeur. Cette transformation est appelée propositionalisation. Les approches existantes gèrent principalement les attributs catégoriels. Une première solution est donc de discrétiser les attributs numériques pour les transformer en attributs catégoriels. Les approches alternatives, qui gèrent les attributs numériques, consistent à les agréger. Nous proposons une approche duale de la discrétisation, qui inverse l'ordre de traitement du nombre d'objets et du seuil, et dont la discrétisation généralise les quartiles. Nous pouvons ainsi construire des attributs que les approches existantes de propositionalisation ne peuvent pas construire, et qui ne peuvent pas non plus être obtenus par les systèmes complets de fouille de données.	Agnès Braud, Nicolas Lachiche	http://editions-rnti.fr/render_pdf.php?p1&p=1000998	http://editions-rnti.fr/render_pdf.php?p=1000998	fouiller donnée relationnel considérer donnée contenu dan table relier association unàplusieur exemple client achat molécule atome fouiller donnée consister transformer donnée tabler attributvaleur transformation appeler propositionalisation approche existant gérer principalement attribut catégoriel solution discrétiser attribut numérique transformer attribut catégoriel approche alternatif gérer attribut numérique consister agréger proposer approcher dual discrétisation inverser lordre traitement nombre dobjet seuil discrétisation généraliser quartile pouvoir construire attribut approche existant propositionalisation pouvoir construir pouvoir plaire obtenir système complet fouiller donnée
577	Revue des Nouvelles Technologies de l'Information	EGC	2011	Reasoning about the learning process	Data Mining is faced with new challenges. In emerging applications (like financial data, traffic TCP/IP, sensor networks, etc) data continuously flow eventually at high speed. The processes generating data evolve over time, and the concepts we are learning change. In this talk we present a one-pass classification algorithm able to detect and react to changes. We present a framework that identify contexts using drift detection, characterize contexts using meta-learning, and select the most appropriate base model for the incoming data using unlabeled examples. Evolving data requires that learning algorithms must be able to monitor the learning process and the ability of predictive self-diagnosis. A significant and useful characteristic is diagnostics - not only after failure has occurred, but also predictive (before failure). These aspects require monitoring the evolution of the learning process, taking into account the available resources, and the ability of reasoning and learning about it.	João Gama	http://editions-rnti.fr/render_pdf.php?p1&p=1000921	http://editions-rnti.fr/render_pdf.php?p=1000921	Data Mining is faced with new challenge in emerging application like financial dater traffic TCPIP sensor network dater continuously flow eventually at high speed The process generating dater evolve over time and the concept we are learning changer in this talk we preser onepass classification algorithm abl to detect and react to change We preser framework that identify contexts using drift detection characteriz context using metalearning and select the most appropriate baser model for the incoming dater using unlabeled exampl Evolving dater requir that learning algorithm must be abl to monitor the learning process and the ability of predictiv selfdiagnosi significer and useful characteristic is diagnostic   not only after failure has occurred boire also predictiv before failure These aspect requir monitoring the evolution of the learning process taking into account the availabl resource and the ability of reasoning and learning about it
578	Revue des Nouvelles Technologies de l'Information	EGC	2011	Reconnaissance d'Actions par Modélisation du Mouvement	Cet article propose une approche utilisant les modèles de direction et de magnitude de mouvement pour détecter les actions qui sont effectuées par des êtres humains dans des séquences vidéo. Des mélanges Gaussiens et de lois de von Mises sont estimés à partir des orientations et des magnitudes des vecteurs du flux optique calculés pour chaque bloc de la scène. Les paramètres de ces modèles sont estimés grâce à un algorithme d'apprentissage en ligne. Les actions sont reconnues grâce à une mesure qui se base sur la distance de Bhattacharyya et qui permet de comparer le modèle d'une séquence donnée avec les modèles créés à partir de séquences d'apprentissage. L'approche proposée est évaluée sur deux ensembles de vidéos contenant des actions variées exécutées aussi bien dans des environnements intérieur qu'extérieur.	Yassine Benabbas, Adel Lablack, Thierry Urruty, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1000940	http://editions-rnti.fr/render_pdf.php?p=1000940	article proposer approcher utiliser modèle direction magnitude mouvement détecter action effectuer humain dan séquence vidéo mélange gaussien loi von mise estimer partir orientation magnitude vecteur flux optique calculé bloc scène paramètre modèle estimer grâce algorithme dapprentissage ligne action reconnaître grâce mesurer baser distancer Bhattacharyya permettre comparer modeler dune séquence donner modèle créer partir séquence dapprentissage Lapproche proposer évaluer ensemble vidéo contenir action varier exécuté dan environnement intérieur quextérieur
579	Revue des Nouvelles Technologies de l'Information	EGC	2011	Résumés et interrogations de logs de requêtes OLAP	Une façon d'assister l'analyse d'entrepôt de données repose sur l'exploitation et la fouille de fichiers logs de requêtes OLAP. Mais, à notre connaissance, il n'existe pas de méthode permettant d'obtenir une représentation d'un tel log qui soit à la fois concise et exploitable. Dans ce papier, nous proposons une méthode pour résumer et interroger des logs de requêtes OLAP. L'idée de base est qu'une requête résume une autre requête et qu'un log, qui est une séquence de requêtes, résume un autre log. Notre cadre formel est composé d'une algèbre simple destinée à résumer des requêtes OLAP, et d'une mesure évaluant la qualité du résumé obtenu. Nous proposons également plusieurs stratégies pour calculer automatiquement des résumés de logs de bonne qualité, et nous montrons comment des propriétés simples sur les résumés peuvent être utilisées pour interroger un log efficacement. Des tests sur des logs de requêtes MDX ont montré l'intérêt de notre approche.	Julien Aligon, Elsa Negre, Patrick Marcel	http://editions-rnti.fr/render_pdf.php?p1&p=1000951	http://editions-rnti.fr/render_pdf.php?p=1000951	dassister lanalyse dentrepôt donnée reposer lexploitation fouiller fichier logs requête OLAP Mais connaissance nexiste méthode permettre dobtenir représentation dun log concis exploitable Dans papier proposer méthode résumer interroger log requête OLAP Lidée baser quune requête résumer requête quun log séquence requêt résumer log cadrer formel composer dune algèbre simple destiner résumer requête olap dune mesurer évaluer qualité résumer obtenir proposer également stratégie calculer automatiquement résumé log qualité montrer propriété simple résumé pouvoir utiliser interroger log efficacement test log requête mdx montrer lintérêt approcher
580	Revue des Nouvelles Technologies de l'Information	EGC	2011	Sélection des variables informatives pour l'apprentissage supervisé multi-tables	Dans la fouille de données multi-tables, les données sont représentées sous un format relationnel dans lequel les individus de la table cible sont potentiellement associés à plusieurs enregistrements dans des tables secondaires en relation un-à-plusieurs. La plupart des approches existantes opèrent en transformant la représentation multi-tables, notamment par mise à plat. Par conséquent, on perd la représentation initiale naturellement compacte mais également on risque d'introduire des biais statistiques. Notre approche a pour objectif d'évaluer l'informativité des variables explicatives originelles par rapport à la variable cible dans le contexte des relations un-à-plusieurs. Elle consiste à résumer l'information contenue dans chaque variable par un tuple d'attributs représentant les effectifs des modalités de celle-ci. Des modèles en grilles multivariées sont alors employés pour qualifier l'information apportée conjointement par les nouveaux attributs, ce qui revient à une estimation de densité conditionnelle de la variable cible connaissant la variable explicative en relation un-à-plusieurs. Les premières expérimentations sur des bases de données artificielles et réelles montrent qu'on arrive à identifier les variables explicatives potentiellement pertinentes sur tout le domaine relationnel.	Dhafer Lahbib, Marc Boullé, Dominique Laurent	http://editions-rnti.fr/render_pdf.php?p1&p=1000995	http://editions-rnti.fr/render_pdf.php?p=1000995	Dans fouiller donnée multitabl donnée représenter sou format relationnel dan individu tabler cibl potentiellement associer enregistrement dan table secondaire relation unàplusieur approche existant opérer transformer représentation multitabl miser plat Par conséquent perdre représentation initial naturellement compacter également risquer dintroduir biais statistique approcher objectif dévaluer linformativité variable explicatif originel rapport variable cibl dan contexte relation unàplusieur consister résumer linformation contenir dan variable tuple dattribut représenter effectif modalité celleci modèle grill multivariée employer qualifier linformation apporter conjointement attribut revenir estimation densité conditionnel variable cibl connaître variable explicatif relation unàplusieur expérimentation base donnée artificiel réel montrer quon arriver identifier variable explicatif potentiellement pertinent domaine relationnel
581	Revue des Nouvelles Technologies de l'Information	EGC	2011	Service de recherche Web3.0 de contenus audiovisuels		François Paulus, Jérôme Royan	http://editions-rnti.fr/render_pdf.php?p1&p=1000957	http://editions-rnti.fr/render_pdf.php?p=1000957	
582	Revue des Nouvelles Technologies de l'Information	EGC	2011	Structuration automatique des flux télévisuels par apprentissage non supervisé des répétitions		Rakia Jaziri, Mustapha Lebbah, Younès Bennani, Jean-Hugues Chenot	http://editions-rnti.fr/render_pdf.php?p1&p=1000974	http://editions-rnti.fr/render_pdf.php?p=1000974	
583	Revue des Nouvelles Technologies de l'Information	EGC	2011	Système de recherche de musique adaptable à la perception de chaque utilisateur	Dans le cadre de nos travaux sur le portage linguistique des systèmes de gestion de contenu traitant des énoncés spontanés en langue naturelle, nous présentons ici une évaluation du portage d'IMRS (système de recherche de morceau de musique en langue naturelle) Kumamoto (2007) du japonais vers le français. Cette évaluation peut se faire au niveau des représentations internes en les comparant, ou au niveau de la tâche. Ici, nous nous intéressons à une évaluation liée à la tâche en proposant un service Web qui permet de mesurer la performance globale de la nouvelle version obtenue. Nous avons par la suite cherché à améliorer et ajouter de nouvelles fonctionnalités en proposant un service de recherche de musique adaptable à la perception de chaque utilisateur. En effet, un même morceau de musique peut être jugé calme pour un premier auditeur, très calme pour un deuxième, et assez calme pour un troisième, etc. On se demande l'impression finale que porte ce dernier morceau de musique. C'est naturel que les utilisateurs évaluent différemment un même morceau de musique car ils ont des perceptions différentes. Devant cette situation, nous proposons un service de recherche de musique basé des méthodes simples et automatisées et qui sont adaptables à la perception de chaque utilisateur.	Najeh Hajlaoui	http://editions-rnti.fr/render_pdf.php?p1&p=1000962	http://editions-rnti.fr/render_pdf.php?p=1000962	Dans cadrer travail portage linguistique système gestion contenir traiter énoncé spontané langue présenter évaluation portage dimr système rechercher morceau musiquer langue Kumamoto 2007 japonais ver français évaluation pouvoir faire niveau représentation interne comparer niveau tâcher intéresser évaluation lier tâcher proposer service Web permettre mesurer performance global version obtenir suite chercher améliorer ajouter fonctionnaliter proposer service rechercher musiqu adaptable perception utilisateur En morceau musiquer pouvoir juger calmer auditeur calmer calme demander limpression final porter morceau musiquer cest utilisateur évaluer différemment morceau musiquer perception différenter Devant situation proposer service rechercher musiquer baser méthode simple automatiser adaptable perception utilisateur
584	Revue des Nouvelles Technologies de l'Information	EGC	2011	Système pour la catégorisation automatique des offres d'emploi en une typologie de fonctions	Depuis les deux dernières décennies, l'augmentation du nombre de sites d'emploi sur Internet a accentué la nécessité de proposer des outils d'aide à la décision adaptés aux besoins des recruteurs. Cet article présente un système pour la catégorisation des textes d'offres d'emploi destinées à être diffusées sur Internet. Après un pré-traitement adapté des offres, les termes descripteurs sont choisis en fonction de leur pouvoir discriminant vis-à-vis des différentes classes ce qui permet de réduire leur nombre de manière significative. Les offres sont ensuite représentées par leurs coordonnées dans l'espace factoriel obtenu par analyse des correspondances et la classification réalisée dans un cadre supervisé à l'aide de SVM.	Julie Séguéla	http://editions-rnti.fr/render_pdf.php?p1&p=1001009	http://editions-rnti.fr/render_pdf.php?p=1001009	Depuis décennie laugmentation nombre site demploi Internet accentuer nécessiter proposer outil daid décision adapter besoin recruteur article présenter système catégorisation texte doffr demploi destiner diffuser Internet Après prétraitement adapter offre terme descripteur choisir fonction pouvoir discriminer visàvis classe permettre réduire nombre manière significatif offre ensuite représenter coordonnée dan lespace factoriel obtenir analyser correspondance classification réaliser dan cadrer superviser laid svm
585	Revue des Nouvelles Technologies de l'Information	EGC	2011	Towards a DistributedWeb Search Engine	In the ocean of Web data, Web search engines are the primary way to access content. As the data is on the order of petabytes, current search engines are very large centralized systems based on replicated clusters. Web data, however, is always evolving. The number of Web sites continues to grow rapidly (230 millions at the end of 2009) and there are currently more than 20 billion indexed pages. On the other hand, Internet users are above one billion and hundreds of million of queries are issued each day. In the near future, centralized systems are likely to become less effective against such a data-query load, thus suggesting the need of fully distributed search engines. Such engines need to maintain high quality answers, fast response time, high query throughput, high availability and scalability ; in spite of network latency and scattered data. In this talk we present the main challenges behind the design of a distributed Web retrieval system and our research in all the components of a search engine : crawling, indexing, and query processing.	Ricardo Baeza-Yates	http://editions-rnti.fr/render_pdf.php?p1&p=1000919	http://editions-rnti.fr/render_pdf.php?p=1000919	In the ocean of Web dater Web search engin are the primary way to access conter the dater is the order of petabyt current search engin are very large centralized system based replicated cluster Web dater however is alway evolving The number of Web site continu to grow rapidly 230 million at the end of 2009 and there are currently more than 20 billion indexed pag the other hand Internet user are above one billion and hundreds of million of querier are issued each day In the near futur centralized systems are likely to become effectif against such dataquery load thus suggesting the need of fully distributed search engin such engin need to maintain high quality answers fast response time high query throughput high availability and scalability   in spite of network latency and scattered dater In this talk we preser the main challeng behind the design of distributed Web retrieval system and our research in all the component of search engine   crawling indexing and query processing
586	Revue des Nouvelles Technologies de l'Information	EGC	2011	Treillis des concepts SKYLINES : Analyse multidimensionnelle des SKYLINES fondée sur les ensembles en accord	Le concept de SKYLINE a été introduit pour mettre en évidence les objets « les meilleurs » selon différents critères. Une généralisation multidimensionnelle du SKYLINE a été proposée à travers le SKYCUBE qui réunit tous les SKYLINES possibles selon toutes les combinaisons de critères et permet d'analyser les liens entre objets SKYLINES. Comme le data cube, le SKYCUBE s'avère extrêmement volumineux si bien que des approches de réduction sont incontournables. Dans cet article, nous définissons une approche de matérialisation partielle du SKYCUBE. L'idée sous-jacente est d'éliminer de la représentation les Skycuboïdes facilement re-calculables. Pour atteindre cet objectif de réduction, nous caractérisons un cadre formel : le treillis des concepts ACCORDS. Cette structure combine la notion d'ensemble en accord et le treillis des concepts. À partir de cette structure, nous dérivons le treillis des concepts SKYLINES qui en est une instance contrainte. Le point fort de notre approche est d'être orientée attribut ce qui permet de borner le nombre de noeuds du treillis et d'obtenir une navigation efficace à travers les Skycuboïdes.	Sébastien Nedjar, Fabien Pesci, Lotfi Lakhal, Rosine Cicchetti	http://editions-rnti.fr/render_pdf.php?p1&p=1000949	http://editions-rnti.fr/render_pdf.php?p=1000949	concept skyline introduire mettre évidence objet « meilleur » critère généralisation multidimensionnel skyline proposer travers SKYCUBE réunir tou skyline combinaison critère permettre danalyser lien entrer objet SKYLINES Comme dater cube skycube savère extrêmement volumineux approche réduction incontournabler Dans article définir approcher matérialisation partiel SKYCUBE lider sousjacent déliminer représentation skycuboïde facilement recalculabl Pour atteindre objectif réduction caractériser cadrer formel   treillis concept accord structurer combin notion densembl accord treillis concept À partir structurer dériver treillis concept SKYLINES instance contraint poindre fort approcher dêtre orienter attribut permettre borner nombre noeud treillis dobtenir navigation efficace travers skycuboïde
587	Revue des Nouvelles Technologies de l'Information	EGC	2011	Un critère Bayésien pour évaluer la robustesse des règles de classification	L'utilisation de règles de classification dans les modèles prédictifs a été très étudiée ces dernières années. La forme simple et interprétable des règles en font des motifs très populaires. Les classifieurs combinant des règles de classification intéressantes (selon une mesure d'intérêt) offrent de bonnes performances de prédictions. Cependant, les performances de ces classifieurs dépendent de la mesure d'intérêt (e.g., confiance, taux d'accroissement, ... ) et du seuillage (non-trivial) de cette mesure pour déterminer les règles pertinentes. De plus, il est facile de montrer que les règles extraites ne sont pas individuellement robustes. Dans cet article, nous proposons un nouveau critère pour évaluer la robustesse des règles de classification dans les données Booléennes. Notre critère est issu d'une approche Bayésienne : nous proposons une expression analytique de la probabilité d'une règle connaissant les données. Ainsi, les règles les plus probables sont robustes. Le critère Bayésien nous permet alors d'identifier (sans paramètre) les règles robustes parmi un ensemble de règles données.	Marc Boullé, Dominique Gay	http://editions-rnti.fr/render_pdf.php?p1&p=1001013	http://editions-rnti.fr/render_pdf.php?p=1001013	lutilisation règle classification dan modèle prédictif étudier année former simple interprétable règle faire motif populaire classifieur combiner règle classification intéressant mesurer dintérêt offrir performance prédiction performance classifieur dépendre mesurer dintérêt eg confiance taux daccroissement    seuillage nontrivial mesurer déterminer règle pertinent De plaire facile montrer règle extraite individuellement robuster Dans article proposer critère évaluer robustesse règle classification dan donnée Booléennes critère issu dune approcher Bayésienne   proposer expression analytique probabilité dune régler connaître donnée règle plaire robuster critère Bayésien permettre didentifier paramétrer règle robuste ensemble règle donnée
588	Revue des Nouvelles Technologies de l'Information	EGC	2011	Un cycle de vie complet pour l'enrichissement sémantique des folksonomies	Les tags fournis par les utilisateurs des plateformes de tagging social ne sont pas explicitement liés sémantiquement, et ceci limite considérablement les possibilités d'exploitation de ces données. Nous présentons dans cet article notre approche pour l'enrichissement sémantiques des folksonomies qui intègre une combinaison de traitements automatiques ainsi que la capture des contributions de structuration des utilisateurs via une interface ergonomique. De plus, notre modèle supporte les points de vue qui divergent tout en permettant de les combiner en respectant leur cohérence locale. Cette approche s'adresse aux communautés de connaissances collaborant en ligne, et en intégrant leurs usages, nous sommes en mesure de proposer un cycle de vie complet pour le processus de structuration sémantique des folksonomies. La navigation dans les données de tagging est ainsi améliorée, et les folksonomies peuvent alors être directement intégrées dans la construction de thesauri.	Freddy Limpens, Fabien Gandon, Michel Buffa	http://editions-rnti.fr/render_pdf.php?p1&p=1000990	http://editions-rnti.fr/render_pdf.php?p=1000990	tag fournir utilisateur plateforme tagging social explicitement lier sémantiquemer limiter considérablemer possibilité dexploitation donnée présenter dan article approcher lenrichissement sémantique folksonomie intégrer combinaison traitement automatique capturer contribution structuration utilisateur interface ergonomique De plaire modeler supporter point diverger permettre combiner respecter cohérence local approcher sadress communauter connaissance collaborer ligne intégrer usage mesurer proposer cycle vie complet processus structuration sémantique folksonomie navigation dan donnée tagging améliorer folksonomie pouvoir intégrer dan construction thesauri
589	Revue des Nouvelles Technologies de l'Information	EGC	2011	Un outil de géolocalisation et de résumé automatique pour faciliter l'accès à l'information dans des corpus d'actualité		Emilie Guimier De Neef, Aurélien Bossard, Frédéric Gavignet, Olivier Collin	http://editions-rnti.fr/render_pdf.php?p1&p=1000956	http://editions-rnti.fr/render_pdf.php?p=1000956	
590	Revue des Nouvelles Technologies de l'Information	EGC	2011	Un outil de navigation dans un espace sémantique		Yann Vigile Hoareau, Murat Ahat, David Medernach, Marc Bui	http://editions-rnti.fr/render_pdf.php?p1&p=1000959	http://editions-rnti.fr/render_pdf.php?p=1000959	
591	Revue des Nouvelles Technologies de l'Information	EGC	2011	Un système cellulaire neuro-symbolique pour l'extraction et la gestion des connaissances	Le CNSS – Cellular Neuro-Symbolic System – est un système hybride ralliant conjointement le neuro-symbolique et le cellulaire. CNSS permet, à partir d'une base de cas pratique, de faire coopérer un réseau de neurones, un graphe d'induction et un automate cellulaire pour la construction d'un modèle de prédiction. En détectant et en éliminant les individus non applicables et les variables non pertinentes, le réseau de neurones optimise la base d'apprentissage. Le résultat ainsi obtenu est affiné par un processus d'apprentissage symbolique à base de graphe d'induction. Ce raffinement se fait par une modélisation booléenne qui va assister l'apprentissage symbolique à optimiser le graphe d'induction et va assurer, par la suite, la représentation et la génération des règles de classification sous forme conjonctives avant d'entamer la phase de déduction par un moteur d'inférence cellulaire. CNSS a été testé sur plusieurs applications en utilisant des problèmes académiques et réels. Les résultats montrent que le système CNSS a des performances supérieures et de nombreux avantages.	Baghdad Atmani, Mohamed Benamina, Bouziane Beldjilali	http://editions-rnti.fr/render_pdf.php?p1&p=1000934	http://editions-rnti.fr/render_pdf.php?p=1000934	cns – Cellular neurosymbolic System – système hybride rallier conjointement neurosymbolique cellulaire CNSS permettre partir dune baser cas pratiquer faire coopérer réseau neurone graph dinduction automate cellulaire construction dun modeler prédiction En détecter éliminer individu applicable variable pertinent réseau neurone optimiser baser dapprentissage résultat obtenir affiner processus dapprentissage symbolique baser graphe dinduction raffinement faire modélisation booléen aller assister lapprentissage symbolique optimiser graph dinduction aller suite représentation génération règle classification sou former conjonctif dentamer phase déduction moteur dinférenc cellulaire CNSS tester application utiliser problème académique réel résultat montrer système CNSS performance supérieur avantage
592	Revue des Nouvelles Technologies de l'Information	EGC	2011	Une Approche à Base de Règles Floues pour les Requêtes à Préférences Contextuelles		Olivier Pivert, Amine Mokhtari, Allel HadjAli	http://editions-rnti.fr/render_pdf.php?p1&p=1000964	http://editions-rnti.fr/render_pdf.php?p=1000964	
593	Revue des Nouvelles Technologies de l'Information	EGC	2011	Une mesure de distance dans l'espace des alignements entre parties potentiellement homologues de deux ontologies légères	Nous proposons dans cet article une méthode qui calcule la distance entre ontologies dans un but d'aide à la décision sur la pertinence ou non de leur fusion. Cette méthode calcule la distance entre parties homologues de deux ontologies par rapport à leurs niveaux de détail et leurs structures taxonomiques, et ce en exploitant les correspondances produites par un alignement préalablement effectué entre ces ontologies, et en adaptant la méthode de la distance d'édition entre arbres ordonnés. Nous limitons notre étude ici aux ontologies légères, c'est à dire des taxonomies représentées en langages OWL, le langage d'ontologies pour le Web. Notre méthode a été implémentée et testée sur des ontologies réelles, et les résultats obtenus semblent prometteurs.	Ammar Mechouche, Nathalie Abadie, Sébastien Mustière	http://editions-rnti.fr/render_pdf.php?p1&p=1001018	http://editions-rnti.fr/render_pdf.php?p=1001018	proposer dan article méthode calculer distancer entrer ontologie dan boire daid décision pertinence fusion méthode calculer distancer entrer party homologue ontologie rapport niveau détail structure taxonomique exploiter correspondance produire alignement préalablement effectuer entrer ontologie adapter méthode distancer dédition entrer arbr ordonner limiter étude ontologie léger cest taxonomie représenter langage OWL langage dontologier web méthode implémenter tester ontologie réel résultat obtenu sembler prometteur
594	Revue des Nouvelles Technologies de l'Information	EGC	2011	Une méthodologie de recommandations produits fondée sur l'actionnabilité et l'intérêt économique des clients	Dans un contexte économique difficile, la fidélisation des clients figure au premier rang des préoccupations des entreprises. En effet, selon le Gartner, fidéliser des clients existants coûterait beaucoup moins cher que prospecter de nouveaux clients. Pour y parvenir, les entreprises optimisent la marge et le cycle de vie des clients en développant une relation personnalisée aboutissant à de meilleures recommandations. Dans cet article, nous proposons une méthodologie pour les systèmes de recommandations fondée sur l'analyse des chiffres d'affaires des clients sur des familles de produits. Plus précisément, la méthodologie consiste à extraire des comportements de référence sous la forme de règles d'association et à en évaluer l'intérêt économique et l'actionnabilité. Les recommandations sont réalisées en ciblant les contre-exemples les plus actionnables sur les règles les plus rentables. Notre méthodologie est appliquée sur 12 000 clients et 100 000 produits de VMMatériaux afin d'orienter les commerciaux sur les possibilités d'accroissement de la valeur client.	Julien Blanchard, Thomas Piton, Henri Briand, Fabrice Guillet	http://editions-rnti.fr/render_pdf.php?p1&p=1000946	http://editions-rnti.fr/render_pdf.php?p=1000946	Dans contexte économique difficile fidélisation client figurer rang préoccupation entreprise En Gartner fidéliser client existant coûter prospecter client Pour yu parvenir entreprise optimiser marge cycle vie client développer relation personnaliser aboutir meilleure recommandation Dans article proposer méthodologie système recommandation fonder lanalyse chiffre daffair client famille produit plaire précisément méthodologie consister extraire comportement référence sou former règle dassociation évaluer lintérêt économique lactionnabilité recommandation réaliser cibler contreexemple plaire actionnable règle plaire rentable méthodologie appliquer 12 000 client 100 000 produit vmmatériaux dorienter commercial possibilité daccroissement client
595	Revue des Nouvelles Technologies de l'Information	EGC	2011	Une nouvelle approche pour l'extraction non supervisée de critères	Récemment de nouvelles techniques regroupées sous le vocable de détection automatique d'opinions (opinion mining) ont fait leur apparition et proposent une évaluation globale d'un document. Ainsi, elles ne permettent pas de mettre en avant le fait que les personnes expriment une opinion très positive du scénario d'un film alors qu'elles trouvent que les acteurs sont médiocres. Dans cet article, nous proposons de caractériser automatiquement les segments de textes relevant d'un critère donné sur un corpus de critiques.	Benjamin Duthil, François Trousset, Mathieu Roche, Michel Plantié, Gérard Dray, Jacky Montmain	http://editions-rnti.fr/render_pdf.php?p1&p=1000981	http://editions-rnti.fr/render_pdf.php?p=1000981	récemment technique regrouper sou vocable détection automatique dopinions opinion mining faire apparition proposer évaluation global dun document permettre mettre faire exprimer opinion positif scénario dun film quell trouver acteur médiocre Dans article proposer caractériser automatiquement segment texte relever dun critère donner corpus critique
596	Revue des Nouvelles Technologies de l'Information	EGC	2011	Une nouvelle approche visuelle pour la classification hiérarchique et topologique	"Nous proposons dans cet article une nouvelle méthode de classification hiérarchique et topologique. Notre approche consiste à construire de manière auto-organisée une partition de données représentées par un ensemble ""forêt"" d'arbres répartis sur une grille 2D. Chaque cellule de la grille est modélisée par un arbre dont les noeuds représentent les données. La partition globale obtenue est visualisée à l'aide d'une carte de TreeMap dans laquelle chaque TreeMap représente un arbre de données. Nous évaluerons les capacités et les performances de notre approche sur des données aux difficultés variables. Des résultats numériques et visuels seront présentés et discutés."	Mustapha Lebbah, Hanane Azzag	http://editions-rnti.fr/render_pdf.php?p1&p=1001038	http://editions-rnti.fr/render_pdf.php?p=1001038	proposer dan article méthode classification hiérarchique topologique approcher consister construire manière autoorganiser partition donnée représenter ensemble forêt darbr répartir griller 2D cellule griller modéliser arbre noeud représenter donnée partition global obtenir visualiser laid dune carte TreeMap dan treemap représenter arbre donnée évaluer capacité performance approcher donnée difficulté variable résultat numérique visuel présenter discuté
597	Revue des Nouvelles Technologies de l'Information	EGC	2011	Utilisation d'une ontologie du domaine pour la découverte du contenu de bases de données géographiques	L'essor récent des technologies associées à la géomatique a permis la production rapide de nombreuses données géographiques. Or, pour tirer profit de ces données, il convient de pouvoir évaluer leur pertinence et leur complexité vis à vis de l'application à laquelle on les destine. Dans cet article, nous présentons une application permettant à un utilisateur de découvrir le contenu de bases de données géographiques, à savoir, quels types d'entités géographiques sont représentés au sein de chaque base et comment. Pour accéder à ces informations l'utilisateur interroge le système via une ontologie globale du domaine qui décrit les types d'entités topographiques du monde réel. Des ontologies locales ou d'application sont utilisées pour formaliser les spécifications de chaque base de données décrite. Elles sont annotées à l'aide de concepts issus de l'ontologie globale. Ce système est implémenté sous la forme d'une interface Web et inclut un affichage cartographique d'échantillons de données	Ammar Mechouche, Nathalie Abadie, Emeric Prouteau, Sébastien Mustière	http://editions-rnti.fr/render_pdf.php?p1&p=1000983	http://editions-rnti.fr/render_pdf.php?p=1000983	Lessor récer technologie associer géomatique permettre production rapide donnée géographique Or tirer profit donnée convier pouvoir évaluer pertinence complexité voir vis lapplication destiner Dans article présenter application permettre utilisateur découvrir contenir base donnée géographique savoir type dentiter géographique représenter baser Pour accéder information lutilisateur interroger système ontologie global domaine décrire type dentiter topographique monder réel ontologie local dapplication utiliser formaliser spécification baser donnée décrire annoter laid concept issu lontologie global système implémenter sou former dune interface Web inclure affichage cartographique déchantillons donnée
598	Revue des Nouvelles Technologies de l'Information	EGC	2011	Utilisation de la Machine Cellulaire pour la Détection des Courriels Indésirables		Fatiha Barigou, Baghdad Atmani, Bouziane Beldjilali	http://editions-rnti.fr/render_pdf.php?p1&p=1000979	http://editions-rnti.fr/render_pdf.php?p=1000979	
599	Revue des Nouvelles Technologies de l'Information	EGC	2011	Utiliser des résultats d'alignement pour enrichir une ontologie	En établissant des relations entre des concepts issus de deux ontologies distinctes, les outils d'alignement peuvent être utilisés pour enrichir une des deux ontologies avec les concepts de l'autre. A partir d'une expérience menée dans le cadre du projet ANR GeOnto 1 dans le domaine de la topographie, cet article identifie des traitements complémentaires à l'alignement pour l'enrichissement et montre leur mise en oeuvre dans TaxoMap Framework.	Fayçal Hamdi, Brigitte Safar, Chantal Reynaud	http://editions-rnti.fr/render_pdf.php?p1&p=1000992	http://editions-rnti.fr/render_pdf.php?p=1000992	En établir relation entrer concept issu ontologie distinct outil dalignemer pouvoir utiliser enrichir ontologie concept lautre partir dune expérience mener dan cadrer projet ANR GeOnto 1 dan domaine topographie article identifier traitement complémentaire lalignement lenrichissement montrer miser oeuvrer dan TaxoMap Framework
600	Revue des Nouvelles Technologies de l'Information	EGC	2011	Vers la fusion d'informations hétérogènes et partielles pour l'aide au codage diagnostique		Laurent Lecornu, Clara Le Guillou, Frédéric Le Saux, Matthieu Hubert, Julien Montagner, John Puentes, Jean-Michel Cauvin	http://editions-rnti.fr/render_pdf.php?p1&p=1000971	http://editions-rnti.fr/render_pdf.php?p=1000971	
601	Revue des Nouvelles Technologies de l'Information	EGC	2011	Visualisation de l'intra et inter structure des groupes en classification non supervisée	La croissance exponentielle des données engendre des volumétries de bases de données très importantes. Une solution couramment envisagée est l'utilisation d'une description condensée des propriétés et de la structure des données. De ce fait, il devient crucial de disposer d'outils de visualisation capables de représenter la structure des données, non pas à partir des données elles mêmes, mais à partir de ces descriptions condensées. Nous proposons une méthode de description des données à partir de prototypes enrichis puis segmentés à l'aide d'un algorithme adapté de classification non supervisée. Nous introduisons ensuite un procédé de visualisation capable de mettre en valeur la structure intra et inter-groupes des données.	Guénaël Cabanes, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1001005	http://editions-rnti.fr/render_pdf.php?p=1001005	croissance exponentiel donnée engendrer volumétrie base donnée important solution couramment envisager lutilisation dune description condenser propriété structurer donnée De faire devenir crucial disposer doutil visualisation capable représenter structurer donnée partir donnée ell partir description condenser proposer méthode description donnée partir prototype enrichir pouvoir segmenter laid dun algorithme adapter classification superviser introduire ensuite procéder visualisation capable mettre structurer intra intergroupes donnée
602	Revue des Nouvelles Technologies de l'Information	EGC	2010	AbsTop-K &#945;: un algorithme d'extraction de paires abstraites hautement corrélées pour mieux recommander dans la ”longue traine	De nombreux systèmes de recommandation se focalisent sur les articles(que nous appellerons ”items”) les plus ”populaires” et ignorent souventla ”longue traîne” des produits qui le sont moins. Nous proposons l'algorithmeAbsTop-k&#945; qui améliore les recommandations en se basant sur la combinaison(pondérée par &#945;) de paires hautement corrélées entre des abstractions d'items etentre des paires d'items concrets classiquement recherchées.	Minh Thu Tran Nguyen, François Sempé, Jean-Daniel Zucker	http://editions-rnti.fr/render_pdf.php?p1&p=1001439	http://editions-rnti.fr/render_pdf.php?p=1001439	système recommandation focaliser articlesque appeler ” item ” plaire ” populaire ” ignorer souventla ” long traîner ” produit proposer lalgorithmeAbsTopk945 améliorer recommandation baser combinaisonpondérée 945 paire hautement corréler entrer abstraction ditems etentre paire ditems concret classiquement rechercher
603	Revue des Nouvelles Technologies de l'Information	EGC	2010	Action Rules and Meta-actions		Zbigniew W. Ras	http://editions-rnti.fr/render_pdf.php?p1&p=1001261	http://editions-rnti.fr/render_pdf.php?p=1001261	
604	Revue des Nouvelles Technologies de l'Information	EGC	2010	Affichage de publicités sur des portails web	Nous nous intéressons au problème de l'affichage de publicités surle web. De plus en plus d'annonceurs souhaitent maintenant payer uniquementlorsque quelqu'un clique sur leurs publicités. Dans ce modèle, l'opérateur duportail a intérêt à identifier les publicités les plus cliquées, selon ses catégoriesde visiteurs. Comme les probabilités de clic sont inconnues a priori, il s'agit d'undilemme exploration/exploitation. Ce problème a souvent été traité en ne tenantpas compte de contraintes provenant du monde réel : les campagnes de publicitésont une durée de vie et possèdent un nombre de clics à assurer et ne pas dépasser.Pour cela, nous introduisons une approche hybride (MAB+LP) entre la programmationlinéaire et les bandits. Nos algorithmes sont testés sur des modèles créésavec un important acteur du web commercial. Ces expériences montrent que cesapproches atteignent une performance très proche de l'optimum et mettent enévidence des aspects clés du problème.	Victor Gabillon, Jérémie Mary, Philippe Preux	http://editions-rnti.fr/render_pdf.php?p1&p=1001268	http://editions-rnti.fr/render_pdf.php?p=1001268	intéresser problème laffichage publicité surle web De plaire plaire dannonceur souhaiter maintenir payer uniquementlorsque quelquun clique publicité Dans modeler lopérateur duportail intérêt identifier publicité plaire cliquer catégoriesd visiteur Comme probabilité inconnu priori sagit dundilemme explorationexploitation problème traiter tenantper compter contrainte provenir monder réel   campagne publicitésont durer vie posséder nombre dépasserpour celer introduire approcher hybride MABLP entrer programmationlinéaire bandit algorithme tester modèle créésavec importer acteur web commercial expérience montrer cesapproche atteindre performance loptimum mettre enévidence aspect cler problème
605	Revue des Nouvelles Technologies de l'Information	EGC	2010	Agrégation de systèmes de fermetures: cas des hiérarchies, topologies et géométries convexes		Florent Domenach	http://editions-rnti.fr/render_pdf.php?p1&p=1001469	http://editions-rnti.fr/render_pdf.php?p=1001469	
606	Revue des Nouvelles Technologies de l'Information	EGC	2010	Aide à la décision pour la maintenance ferroviaire préventive	La maintenance de trains est un problème particulièrement délicat liéà de nombreux enjeux à la fois financiers, sécuritaires et énergétiques. Nous nousintéressons à la mise en place d'une maintenance préventive basée sur la détectionet la correction de tout comportement anormal susceptible de provoquer unproblème majeur dans un futur proche. Nous proposons ainsi un outil d'aide à ladécision afin de (i) dégager des connaissances utiles sur l'historique des trains,et (ii) détecter et étudier les anomalies comportementales, dans le but de prendredes décisions optimales en termes de maintenance ferroviaire	Julien Rabatel, Sandra Bringay, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1001319	http://editions-rnti.fr/render_pdf.php?p=1001319	maintenance train problème délicat liéà enjeu financier sécuritaire énergétique nousintéresser miser placer dune maintenance préventif baser détectionet correction comportement anormal susceptible provoquer unproblème majeur dan futur proposer outil daide ladécision ie dégager connaissance utile lhistorique trainset ii détecter étudier anomalie comportemental dan boire prendrede décision optimale terme maintenance ferroviaire
607	Revue des Nouvelles Technologies de l'Information	EGC	2010	Allier CSPs et motifs locaux pour la découverte de motifs sous contraintes n-aires	Dans cet article, nous étudions la relation entre la découverte de motifssous contraintes et les CSPs (Constraint Satisfaction Problems) afin de définirdes contraintes de plus haut niveau qui sont précieuses pour mener à bien destâches de fouille de données. Pour cela, nous proposons une approche de modélisationet d'extraction de motifs sous contraintes n-aires exploitant les motifslocaux. L'utilisateur définit un ensemble de contraintes n-aires et un solveur deCSP génère l'ensemble des solutions. Notre approche profite des progrès récentssur l'extraction de motifs locaux et permet de modéliser de manière concise etélégante tout ensemble de contraintes combinant plusieurs motifs locaux, permettantainsi la découverte de motifs répondant mieux aux buts finaux de l'utilisateur.Les expériences menées montrent la faisabilité de notre approche.	Mehdi Khiari, Patrice Boizumault, Bruno Crémilleux	http://editions-rnti.fr/render_pdf.php?p1&p=1001292	http://editions-rnti.fr/render_pdf.php?p=1001292	Dans article étudier relation entrer découvrir motifssou contraint CSPs Constraint satisfaction problem définirde contraint plaire niveau précieux mener destâche fouiller donnée Pour celer proposer approcher modélisationet dextraction motif sou contraint nair exploiter motifslocaux Lutilisateur définir ensemble contrainte nair solveur decsp génèr lensembl solution approcher profiter progrès récentssur lextraction motif local permettre modéliser manière concis etélégante ensemble contrainte combiner motif local permettantainsi découvrir motif répondre mieux but final lutilisateurle expérience mener montrer faisabilité approcher
608	Revue des Nouvelles Technologies de l'Information	EGC	2010	Analyse de documents pédagogiques en vue de leur annotation	L'utilisation des documents pédagogiques, disponibles sur le web,devient de plus en plus large tant pour l'enseignant qui a besoin de préparerson support de cours que pour l'étudiant qui désire, par exemple, s'autoformer.La description d'un document pédagogique, en l'alimentant par desmétadonnées, s'avère une solution qui confère une valeur ajoutée au documentafin d'expliciter des informations placées dans ce document. Dans cetteoptique, nous proposons une méthode d'annotation de documentspédagogiques selon différents points de vue, qui est basée sur l'analysesémantique des éléments discursifs du texte	Boutheina Smine, Rim Faiz, Jean-Pierre Desclés	http://editions-rnti.fr/render_pdf.php?p1&p=1001502	http://editions-rnti.fr/render_pdf.php?p=1001502	lutilisation document pédagogique disponible webdevient plaire plaire large lenseigner besoin préparerson support cours létudier désirer exemple sautoformerla description dun document pédagogique lalimenter desmétadonner savère solution conférer ajouter documentafin dexpliciter information placer dan document Dans cetteoptique proposer méthode dannotation documentspédagogique point baser lanalysesémantique élément discursif texte
609	Revue des Nouvelles Technologies de l'Information	EGC	2010	Analyse de séquences d'événements avec TraMineR		Nicolas S. Müller, Matthias Studer, Alexis Gabadinho, Gilbert Ritschard	http://editions-rnti.fr/render_pdf.php?p1&p=1001397	http://editions-rnti.fr/render_pdf.php?p=1001397	
610	Revue des Nouvelles Technologies de l'Information	EGC	2010	Analyse en ligne d'objets complexes avec l'analyse factorielle	Les entrepôts de données et l'analyse en ligne OLAP (On-line AnalysisProcessing) présentent des solutions reconnues et efficaces pour le processusd'aide à la décision. Notamment l'analyse en ligne, grâce aux opérateurs OLAP,permet de naviguer et de visualiser des données représentées dans un cube multidimensionnel.Mais lorsque les données ou les objets à analyser sont complexes,il est nécessaire de redéfinir et d'enrichir ces opérateurs OLAP. Dans cet article,nous proposons de combiner l'analyse OLAP et la fouille de données (data mining)afin de créer un nouvel opérateur de visualisation d'objets complexes. Cetopérateur utilise l'analyse factorielle des correspondances.	Loïc Mabit, Sabine Loudcher, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1001321	http://editions-rnti.fr/render_pdf.php?p=1001321	entrepôt donnée lanalyse ligne OLAP Online AnalysisProcessing présenter solution reconnu efficace processusdaide décision lanalyse ligne grâce opérateur olappermet naviguer visualiser donnée représenter dan cuber multidimensionnelmai donnée objet analyser complexesil nécessaire redéfinir denrichir opérateur olap Dans articlenou proposon combiner lanalyse olap fouiller donnée dater miningafin créer nouvel opérateur visualisation dobjet complexe Cetopérateur utiliser lanalys factoriel correspondance
611	Revue des Nouvelles Technologies de l'Information	EGC	2010	Analyse globale du flux optique pour la détection d'évènements dans une scène de foule	Les systèmes de vidéo-surveillance sont de plus en plus autonomesdans la détection des événements anormaux. Cet article présente une méthode dedétection des flux majeurs et des évènements qui surviennent dans une scène defoule. Ces détections sont effectuées en utilisant un modèle directionnel construità partir d'un mélange de lois de von Mises appliqué à l'orientation des vecteursde mouvement. Les flux majeurs sont alors calculés en récupérant les orientationsles plus importantes des mélanges. Divers évènements se produisant dansune foule sont aussi détectés en utilisant en plus du modèle d'orientation, unmodèle probabiliste de magnitude des vecteurs de mouvement. Les résultats del'expérimentation sur un échantillon de vidéos d'événements sont présentés.	Yassine Benabbas, Nacim Ihaddadene, Thierry Urruty, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1001315	http://editions-rnti.fr/render_pdf.php?p=1001315	système vidéosurveillance plaire plaire autonomesdans détection événement anormal article présenter méthode dedétection flux majeur évènement survenir dan scène defoul détection effectuer utiliser modeler directionnel construità partir dun mélanger loi von mise appliquer lorientation vecteursde mouvement flux majeur calculer récupérer orientationsle plaire important mélange évènement produire dansune foul détecter utiliser plaire modeler dorientation unmodèl probabiliste magnitude vecteur mouvement résultat delexpérimentation échantillon vidéo dévénement présenter
612	Revue des Nouvelles Technologies de l'Information	EGC	2010	Analyse incrémentale des usages pour le routage des requêtes dans les systèmes pairs à pairs		Taoufik Yeferny, Khedija Arour	http://editions-rnti.fr/render_pdf.php?p1&p=1001427	http://editions-rnti.fr/render_pdf.php?p=1001427	
613	Revue des Nouvelles Technologies de l'Information	EGC	2010	Applying Markov Logic to Document Annotation and Citation Deduplication	Structured learning approaches are able to take into account the relationalstructure of data, thus promising an enhancement over non-relationalapproaches. In this paper we explore two document-related tasks in relationaldomains setting, the annotation of semi-structured documents and the citationdeduplication. For both tasks, we report results of comparing relational learningapproach namely Markov logic, to non-relational one namely Support VectorMachines (SVM). We discover that increased complexity due to the relationalsetting is difficult to manage in large scale cases, where non-relational modelsmight perform better. Moreover, our experiments show that in Markov logic,the contribution of its probabilistic component decreases in large scale domains,and it tends to act like First-order logic (FOL).	Jean Baptiste Faddoul, Boris Chidlovskii	http://editions-rnti.fr/render_pdf.php?p1&p=1001329	http://editions-rnti.fr/render_pdf.php?p=1001329	structured learning approaches are abl to take into account the relationalstructur of dater thus promising an enhancement over nonrelationalapproache In this paper we explorer two documentrelated tasks in relationaldomain setting the annotation of semistructured document and the citationdeduplication For both task we report results of comparing relational learningapproach namely Markov logic to nonrelational one namely Support vectormachines SVM We discover that increased complexity to the relationalsetting is difficult to manager in large scal caser where nonrelational modelsmight perform better Moreover our experiment show that in Markov logicthe contribution of it probabilistic component decrease in large scal domainsand it to act like Firstorder logic FOL
614	Revue des Nouvelles Technologies de l'Information	EGC	2010	Apport de la technique de fouille de données spatiales dans la prédiction des risques engendrés par les changements climatiques		Hana Alouaoui, Sami Yassine Turki, Sami Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1001451	http://editions-rnti.fr/render_pdf.php?p=1001451	
615	Revue des Nouvelles Technologies de l'Information	EGC	2010	Apprentissage de patrons lexico-syntaxiques à partir de textes	Ce papier présente une approche d'apprentissage de patrons lexicosyntaxiquesà partir de textes annotés. Les patrons lexico-syntaxiques sont utiliséspour identifier des relations lexicales dans les corpus textuels. Leur constructionmanuelle est une tâche fastidieuse et des solutions permettant l'apprentissagesont souhaitables. Nous proposons une approche d'apprentissage qui reposesur l'utilisation des chemins de dépendance pour représenter les patrons et l'implémentationd'un algorithme de classification. L'approche a été appliquée dansle domaine biomédical pour identifier des patrons lexico-syntaxiques exprimantdes relations fonctionnelles.	Valentina Dragos, Marie-Christine Jaulent	http://editions-rnti.fr/render_pdf.php?p1&p=1001371	http://editions-rnti.fr/render_pdf.php?p=1001371	papier présenter approcher dapprentissage patron lexicosyntaxiquesà partir texte annoté patron lexicosyntaxiqu utiliséspour identifier relation lexical dan corpus textuel constructionmanuelle tâcher fastidieux solution permettre lapprentissagesont souhaitable proposer approcher dapprentissage reposesur lutilisation chemin dépendance représenter patron limplémentationdun algorithme classification Lapproche appliquer dansl domaine biomédical identifier patron lexicosyntaxiqu exprimantd relation fonctionnel
616	Revue des Nouvelles Technologies de l'Information	EGC	2010	Apprentissage de spécifications de CSP		Matthieu Lopez, Lionel Martin	http://editions-rnti.fr/render_pdf.php?p1&p=1001465	http://editions-rnti.fr/render_pdf.php?p=1001465	
617	Revue des Nouvelles Technologies de l'Information	EGC	2010	Apprentissage supervisé adaptatif de Concepts Formels à partir des données nominales		Mondher Maddouri, Nida Meddouri	http://editions-rnti.fr/render_pdf.php?p1&p=1001372	http://editions-rnti.fr/render_pdf.php?p=1001372	
618	Revue des Nouvelles Technologies de l'Information	EGC	2010	Approche biomimétique coopérative pour la visualisation de grands graphes multidimensionels	Face à la quantité sans cesse grandissante de données stockées, les algorithmes de fouille etde visualisation de données doivent pouvoir être capable de traiter de grandes quantités de données.Une des solutions est d'effectuer un prétraitement des données permettant la réductionde la dimension des données sans perte significative d'informations. L'idée est donc de réduirel'ensemble de descripteurs avant de faire appel à la méthode de visualisation sous forme d'ungraphe.	Lydia Boudjeloud, Hanane Azzag	http://editions-rnti.fr/render_pdf.php?p1&p=1001412	http://editions-rnti.fr/render_pdf.php?p=1001412	face quantité cesser grandissant donnée stocker algorithme fouiller etde visualisation donnée devoir pouvoir capable traiter grand quantité donnéesune solution deffectuer prétraitement donnée permettre réductionde dimension donnée perte significatif dinformation Lidée réduirelensemble descripteur faire appel méthode visualisation sou former dungraph
619	Revue des Nouvelles Technologies de l'Information	EGC	2010	Approche complexe de l'analyse de documents anciens	Cet article présente une méthode complexe pour la caractérisation etl'indexation d'images graphiques de documents anciens. A partir d'un bref étatde l'art, une méthode pour décrire ces images en tenant compte de leur complexitéest proposée. Trois étapes principales de ce traitement sont détailléesdont une méthode novatrice d'analyse, de segmentation et de description destraits. Les résultats sont issus de travaux en cours et sont encourageants	Mickaël Coustaty, Giap NGuyen, Vincent Courboulay, Jean-Marc Ogier	http://editions-rnti.fr/render_pdf.php?p1&p=1001368	http://editions-rnti.fr/render_pdf.php?p=1001368	article présenter méthode complexe caractérisation etlindexation dimag graphique document ancien partir dun bref étatde lart méthode décrire image compter complexitéest proposer Trois étape principal traitement détailléesdont méthode novateur danalyse segmentation description destrait résultat issu travail cours encourageant
620	Revue des Nouvelles Technologies de l'Information	EGC	2010	Approche Sémantique pour la Préservation de la Vie Privée dans les Médias Sociaux		Hakim Hacid, Johann Stan, Johann Daigremont	http://editions-rnti.fr/render_pdf.php?p1&p=1001446	http://editions-rnti.fr/render_pdf.php?p=1001446	
621	Revue des Nouvelles Technologies de l'Information	EGC	2010	Bien cube, les données textuelles peuvent s'agréger !	La masse des données aujourd'hui disponibles engendre des besoinscroissants de méthodes décisionnelles adaptées aux données traitées. Ainsi, récemmentde nouvelles approches fondées sur des cubes de textes sont apparuespour pouvoir analyser et extraire de la connaissance à partir de documents. L'originalitéde ces cubes est d'étendre les approches traditionnelles des entrepôts etdes technologies OLAP à des contenus textuels. Dans cet article, nous nous intéressonsà deux nouvelles fonctions d'agrégation. La première propose une nouvellemesure de TF-IDF adaptative permettant de tenir compte des hiérarchiesassociées aux dimensions. La seconde est une agrégation dynamique permettantde faire émerger des groupements correspondant à une situation réelle. Lesexpériences menées sur des données issues du serveur HAL d'une universitéconfirment l'intérêt de nos propositions.	Sandra Bringay, Anne Laurent, Pascal Poncelet, Mathieu Roche, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001367	http://editions-rnti.fr/render_pdf.php?p=1001367	masser donnée aujourdhui disponible engendrer besoinscroissant méthode décisionnel adapter donnée traiter récemmentde approche fonder cube texte apparuespour pouvoir analyser extraire connaissance partir document Loriginalitéde cube détendre approche traditionnel entrepôt etd technologi olap contenu textuel Dans article intéressonsà fonction dagrégation proposer nouvellemesure TFIDF adaptatif permettre compter hiérarchiesassociée dimension second agrégation dynamique permettantde faire émerger groupement correspondre situation réel Lesexpériences mener donnée issu serveur hal dune universitéconfirment lintérêt proposition
622	Revue des Nouvelles Technologies de l'Information	EGC	2010	Caractériser la terminologie des usagers de santé dans le domaine du cancer du sein	Internet est devenu une source importante d'informations médicalespour les patients et leurs proches : recherche d'informations sur leurs maladieset les dernières recherches cliniques, ainsi que pour y constituer des communautés“numériques” de dialogue et de partage. Cependant, accès à Internet nesignifie pas nécessairement accès à l'information. Le manque de familiarité avecle langage médical constitue un problème majeur pour les usagers de santé dansl'accès à l'information et son interprétation. Ce papier s'inscrit dans la problématiqued'étude et de caractérisation de la terminologie des usagers de santépour pouvoir proposer des services adaptés à leur langage et à leur niveau deconnaissances. Le travail réalisé est une ontologie dans le domaine du cancerdu sein orientée vers les usagers de santé. Cette ontologie est construite à partird'un ensemble de corpus de textes représentant deux catégories : les médiateurset les usagers de santé. Les éléments de cette ontologie ont été analysés en utilisantdes méthodes quantitatives et qualitatives sur plusieurs niveaux : termes,concepts et relations.	Radja Messai, Michel Simonet, Nathalie Bricon-Souf, Mireille Mousseau	http://editions-rnti.fr/render_pdf.php?p1&p=1001326	http://editions-rnti.fr/render_pdf.php?p=1001326	internet devenir source important dinformation médicalespour patient   rechercher dinformation maladieset recherche clinique yu constituer communautés“numérique ” dialoguer partager accès Internet nesignifie nécessairement accès linformation manquer familiarité avecle langage médical constituer problème majeur usager santé danslaccè linformation interprétation papier sinscrit dan problématiquedétude caractérisation terminologie usager santépour pouvoir proposer service adapter langage niveau deconnaissances travail réaliser ontologie dan domaine cancerdu orienter ver usager santé ontologie construire partirdun ensemble corpus texte représenter catégorie   médiateurset usager santé élément ontologie analyser utilisantd méthode quantitatif qualitatif niveau   termesconcept relation
623	Revue des Nouvelles Technologies de l'Information	EGC	2010	CARTOCEL : Un outil de cartographie des connaissances guidée par la machine cellulaire CASI	Nous présentons, dans ce papier, l'outil CARTOCEL (CARTOgraphiesCELlulaires) permettant une visualisation automatique et dynamique desdomaines de connaissances. Le fonctionnement de CARTOCEL est basé surune approche originale de modélisation booléenne de la cartographie des domainesde connaissances métiers/stratégiques inspirée du principe de la machinecellulaire CASI (Cellular Automata for Symbolic Induction). Le but,après une modélisation booléenne de la cartographie des domaines de connaissances,est double : d'une part affiner la cartographie par une fouille de donnéeorchestrée par CASI, et d'autre part réduire la complexité de stockage, ainsique le temps de calcul	Menaouer Brahami, Baghdad Atmani, Mostéfa Mokaddem	http://editions-rnti.fr/render_pdf.php?p1&p=1001375	http://editions-rnti.fr/render_pdf.php?p=1001375	présenter dan papier loutil cartocel cartographiescellulaires permettre visualisation automatique dynamique desdomaine connaissance fonctionnement CARTOCEL baser surune approcher original modélisation booléen cartographie domainesde connaissance métiersstratégiqu inspirer principe machinecellulaire CASI Cellular Automata for Symbolic Induction butaprè modélisation booléen cartographie domaine connaissancesest double   dune partir affiner cartographie fouiller donnéeorchestrer casi dautre partir réduire complexité stockage ainsiqu temps calcul
624	Revue des Nouvelles Technologies de l'Information	EGC	2010	ChorML : Résumés visuels de bases des données géographiques		Ibtissem Cherni, Karla Lopez, Robert Laurini, Sami Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1001443	http://editions-rnti.fr/render_pdf.php?p=1001443	
625	Revue des Nouvelles Technologies de l'Information	EGC	2010	Classer, discriminer et visualiser des séquences d'événements	Cet article 1 présente un ensemble d'outils destiné à analyser des séquencesd'événements en sciences sociales et à visualiser les résultats obtenus.Nous commençons par formaliser la notion de séquence d'événements avant dedéfinir une mesure de dissimilarité entre ces séquences afin de construire destypologies et de tester les liens entre ces séquences et d'autres variables d'intérêts.Initialement définie par Moen (2000), cette mesure se base sur la notion dedistance d'édition entre séquences et permet d'identifier les différences d'ordonnancementet de temporalité des événements. Nous proposons une extension decelle-ci afin de pouvoir prendre en compte la simultanéité des événements ainsiqu'une méthode de normalisation qui garantit le respect de l'inégalité triangulaire.Dans un deuxième temps, nous présentons un ensemble d'outils destinésà interpréter les résultats. Nous proposons ainsi deux méthodes de visualisationd'un ensemble de séquences et nous introduisons la notion de sous-séquencediscriminante qui permet d'identifier les différences d'ordonnancement des événementsles plus significatives entre groupes. L'ensemble des outils présentés estdisponible au sein de la librairie R TraMineR.	Matthias Studer, Nicolas S. Müller, Gilbert Ritschard, Alexis Gabadinho	http://editions-rnti.fr/render_pdf.php?p1&p=1001264	http://editions-rnti.fr/render_pdf.php?p=1001264	article 1 présenter ensemble doutil destiner analyser séquencesdévénement science social visualiser résultat obtenusnou commencer formaliser notion séquence dévénement dedéfinir mesurer dissimilarité entrer séquence construire destypologie tester lien entrer séquence dautr variable dintérêtsinitialement définir Moen 2000 mesurer baser notion dedistanc dédition entrer séquence permettre didentifier différence dordonnancementet temporalité événement proposer extension decelleci pouvoir prendre compter simultanéité événement ainsiquune méthode normalisation garantir respect linégalité triangulaireDans temps présenter ensemble doutil destinésà interpréter résultat proposer méthode visualisationdun ensemble séquence introduire notion sousséquencediscriminante permettre didentifier différence dordonnancemer événementsle plaire significatif entrer groupe lensembl outil présenter estdisponibl librairie traminer
626	Revue des Nouvelles Technologies de l'Information	EGC	2010	Classification de documents : calcul d'une distance structurelle	La classification des documents numériques garantit un accès rapideet ciblé à l'information. Si nous considérons qu'un document est représenté parsa ou ses structures, définir des classes de documents revient à définir desclasses de structures. Une classe structurelle représente donc des structures« proches ». Ainsi, associer la structure d'un document à sa classe structurellerevient à calculer une distance dite « structurelle ». Elle tiendra compte à lafois de l'organisation des éléments (position des noeuds, chemin), du coûtd'adaptation des représentants des classes ainsi que de la représentativité dessous-graphes. Sur un corpus de documents représentant des notices de livresissus de la bibliothèque de l'université, nous discuterons de la construction decette distance, de l'intérêt de chacun des trois paramètres utilisés	Karim Djemal, Chantal Soulé-Dupuy, Nathalie Vallès-Parlangeau	http://editions-rnti.fr/render_pdf.php?p1&p=1001369	http://editions-rnti.fr/render_pdf.php?p=1001369	classification document numérique garantir accès rapideet cibler linformation Si considérer quun document représenter parsa structure définir classe document revenir définir desclass structure classer structurel représenter structure « » associer structurer dun document classer structurellerevient calculer distancer « structurel » compter lafoi lorganisation élément position noeud chemin coûtdadaptation représentant classe représentativité dessousgraphe Sur corpus document représenter notice livresissu bibliothèque luniversité discuter construction decette distancer lintérêt paramètre utiliser
627	Revue des Nouvelles Technologies de l'Information	EGC	2010	Classification et Selection de caracteristique basees sur les concepts semantiques pour la recherche d'information multimedia	Le besoin récent de nombreuses applications multimédia basées sur le contenu a engendré une demande croissante de technologies dans le domaine de la recherche d'information multimédia. Basée sur l'état de l'art des techniques existantes, nous proposons dans cet article une approche de recherche d'information multimédia qui prend en compte les informations de scène et exploite un modèle de sélection de caractéristiques. Les principaux avantages de notre modèle de recherche par rapport aux modèles existants sont : (i) une méthode de classification basée sur des catégories de concept sémantique; (ii) un modèle de recherche par rapport aux modèles existants sont : (i) une méthode de classification basée sur des catégories de concept sémantique; (ii) un modèle de sélection de caractéristiques; (iii) un index multidimensionnel. Notre framework propose un bon compromis entre précision et rapidité de la recherche	Thierry Urruty, Ismail Elsayad, Adel Lablack, Yue Feng, Jose M. Joemon	http://editions-rnti.fr/render_pdf.php?p1&p=1001271	http://editions-rnti.fr/render_pdf.php?p=1001271	besoin récent application multimédier baser contenir engendrer demander croissant technologie dan domaine rechercher dinformation multimédier baser létat lart technique existant proposer dan article approcher rechercher dinformation multimédier prendre compter information scène exploiter modeler sélection caractéristique principal avantage modeler rechercher rapport modèle existant   ie méthode classification basé catégorie concept sémantique ii modeler rechercher rapport modèle existant   ie méthode classification basé catégorie concept sémantique ii modeler sélection caractéristique iii index multidimensionnel framework proposer compromis entrer précision rapidité rechercher
628	Revue des Nouvelles Technologies de l'Information	EGC	2010	Classification supervisée pour de grands nombres de classes à prédire : une approche par co-partitionnement des variables explicatives et à expliquer	Dans la phase de préparation des données du data mining, les méthodesde discrétisation et de groupement de valeurs supervisé possèdent denombreuses applications : interprétation, estimation de densité conditionnelle,sélection de type filtre des variables, recodage des variables en amont des classifieurs.Ces méthodes supposent habituellement un faible nombre de valeur àexpliquer (classes), typiquement moins d'une dizaine, et trouvent leur limitequand leur nombre augmente. Dans cet article, nous introduisons une extensiondes méthodes de discrétisation et groupement de valeurs, consistant à partitionnerd'une part la variable explicative, d'autre part la variable à expliquer.Le meilleur co-partitionnement est recherché au moyen d'une approche Bayesiennede la sélection de modèle. Nous présentons ensuite comment utiliser cetteméthode de prétraitement en préparation pour le classifieur Bayesien naïf. Desexpérimentations intensives démontrent l'apport de la méthode dans le cas decentaines de classes.	Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001352	http://editions-rnti.fr/render_pdf.php?p=1001352	Dans phase préparation donnée dater mining méthodesde discrétisation groupement superviser posséder denombreus application   interprétation estimation densité conditionnellesélection typer filtr variable recodage variable amont classifieursce méthode supposer habituellement faible nombre àexpliquer classe typiquement dune dizaine trouver limitequand nombre augment Dans article introduire extensionde méthode discrétisation groupement consister partitionnerdune partir variable explicatif dautre partir variable expliquerle meilleur copartitionnement rechercher moyen dune approcher Bayesiennede sélection modeler présenter ensuite utiliser cetteméthode prétraitement préparation classifieur Bayesien naïf desexpérimentation intensif démontrer lapport méthode dan cas decentain classe
629	Revue des Nouvelles Technologies de l'Information	EGC	2010	CND-Cube : Nouvelle représentation concise sans perte d'information d'un cube de données	Le calcul des cubes de données est excessivement coûteux aussi bienen temps d'exécution qu'en mémoire et son stockage sur disque peut s'avérerprohibitif. Plusieurs efforts ont été consacrés à ce problème à travers les cubesfermés, où les cellules préservant la sémantique d'agrégation sont réduites à unecellule, sans perte d'information. Dans cet article, nous introduisons le conceptdu cube de données non-dérivable fermé, nommé CND-Cube, qui généralisela notion des modèles non-dérivables fermés fréquents bidimensionnels à uncontexte multidimensionnel. Nous proposons un nouvel algorithme pour extrairele CND-Cube à partir des bases de données multidimensionnelles en se basantsur trois contraintes anti-monotones, à savoir “être fréquent”, “être non dérivable”et “être un générateur minimal”. Les expériences montrent que notreproposition fournit la représentation la plus concise d'un cube de données et elleest ainsi la plus efficace pour réduire l'espace de stockage	Hanen Brahmi, Tarek Hamrouni, Riadh Ben Messaoud, Sadok Ben Yahia	http://editions-rnti.fr/render_pdf.php?p1&p=1001303	http://editions-rnti.fr/render_pdf.php?p=1001303	calcul cube donnée excessivement coûteux bienen temps dexécution quen mémoir stockage disqu pouvoir savérerprohibitif effort consacrer problème travers cubesfermé cellule préserver sémantique dagrégation réduire unecellule perte dinformation Dans article introduire conceptdu cube donnée nondérivabl fermer nommer cndcub généralisela notion modèle nondérivabl fermer fréquent bidimensionnel uncontexte multidimensionnel proposer nouvel algorithme extrairele cndcub partir base donnée multidimensionnel basantsur contraint antimonotone savoir “ fréquent ” “ dérivable”et “ générateur minimal ” expérience montrer notreproposition fournir représentation plaire concis dun cube donnée elleest plaire efficace réduire lespace stockage
630	Revue des Nouvelles Technologies de l'Information	EGC	2010	Codage et classification non supervisée d'un corpus maya : extraire des contextes pour situer l'inconnu par rapport au connu	L'écriture logosyllabique des anciens Mayas comprend plus de 500signes et est en bonne partie déchiffrée, avec des degrés de certitude divers.Nous avons appliqué au codex de Dresde, l'un des trois seuls manuscrits quinous soient parvenus, codé sous LATEXavec le systèmemayaTEX, notre méthodede représentation graduée, par apprentissage non supervisé hybride entre clusteringet analyse factorielle oblique, sous la métrique de Hellinger, afin d'obtenirune image nuancée des thèmes traités : les individus statistiques sont les 212segments de folio du codex, et leurs attributs sont les 1687 bigrammes de signesextraits. Pour comparaison, nous avons introduit dans cette approche endogèneun élément exogène, la décomposition en éléments des signes composites, pourpréciser plus finement les contenus. La rétro-visualisation dans le texte originaldes résultats et expressions dégagées éclaire la signification de certains glyphespeu compris, en les situant dans des contextes clairement interprétables.	Mohamed Hallab, Bruno Delprat, Alain Lelu	http://editions-rnti.fr/render_pdf.php?p1&p=1001366	http://editions-rnti.fr/render_pdf.php?p=1001366	Lécriture logosyllabiqu ancien Mayas comprendre plaire 500signe partir déchiffrer degré certitude diversnous appliquer codex Dresde lun manuscrit quinou parvenir coder sou latexavec systèmemayatex méthodede représentation graduer apprentissage superviser hybride entrer clusteringet analyser factoriel oblique sou métrique hellinger dobtenirune imager nuancer thème traiter   individu statistique 212segment folio codex attribut 1687 bigramme signesextrait Pour comparaison introduire dan approcher endogèneun élément exogène décomposition élément signe compositer pourpréciser plaire finement contenu rétrovisualisation dan texte originald résultat expression dégager éclairer signification glyphespeu situer dan contexte clairement interprétable
631	Revue des Nouvelles Technologies de l'Information	EGC	2010	Combiner approche logique et numérique pour la réconciliation de données et l'alignement d'ontologies		Marie-Christine Rousset	http://editions-rnti.fr/render_pdf.php?p1&p=1001262	http://editions-rnti.fr/render_pdf.php?p=1001262	
632	Revue des Nouvelles Technologies de l'Information	EGC	2010	CombinerWeb 2.0 et Web Sémantique pour réduire les disparités d'expertise au sein de blogs d'entreprise	Avec l'avènement d'applications sociales en entreprise (blogs, wikis,etc.), il est fréquent que des individus aux niveaux d'expertise relativement distantsse réunissent au sein de communautés en ligne. Ces disparités d'expertisese traduisent entre autres par des comportements différents dans la manière detagguer les contenus créés, notamment en ce qui concerne les termes utilisés,rendant ainsi complexe la découverte d'informations pourtant publiées. Dans cetarticle, nous mettons en avant la possibilité offerte par les technologies du WebSémantique, combinées avec les paradigmes du Web Social, de résoudre cetteproblématique. Nous proposons ainsi une chaine de traitement combinant ontologies,wikis sémantiques et indexation de contenus permettant la production degraphes sémantiques interconnectés et facilitant de cette manière la découvertede contenus créés au sein de tels systèmes	Alexandre Passant, Philippe Laublet	http://editions-rnti.fr/render_pdf.php?p1&p=1001270	http://editions-rnti.fr/render_pdf.php?p=1001270	Avec lavènement dapplication social entreprendre blog wikisetc fréquent individu niveau dexpertis distantsse réunir communauter ligne disparité dexpertises traduire entrer comportement dan manière detagguer contenu créer concerner terme utilisésrendant complexe découvrir dinformation pourtant publier Dans cetarticle mettre possibilité offert technologie websémantiqu combiner paradigme web Social résoudre cetteproblématiqu proposer chaine traitement combiner ontologieswikis sémantique indexation contenu permettre production degrapher sémantique interconnecter faciliter manière découvertede contenir créer système
633	Revue des Nouvelles Technologies de l'Information	EGC	2010	Comparaison de critères de pureté pour l'intégration de connaissances en clustering semi-supervisé	L'utilisation de connaissances pour améliorer les processus de fouillede données a mobilisé un important effort de recherche ces dernières années. Ilest cependant souvent difficile de formaliser ce type de connaissances, commecelles-ci sont souvent dépendantes du domaine. Dans cet article, nous nous intéressonsà l'intégration de connaissances sous la forme d'objets étiquetés dansles algorithmes de clustering. Plusieurs critères permettant d'évaluer la puretédes clusters sont présentés et leur comportement est comparé sur des jeux dedonnées artificiels. Les avantages et les inconvénients de chaque critère sontanalysés pour aider l'utilisateur à faire un choix.	Germain Forestier, Cédric Wemmert, Pierre Gançarski	http://editions-rnti.fr/render_pdf.php?p1&p=1001278	http://editions-rnti.fr/render_pdf.php?p=1001278	lutilisation connaissance améliorer processus fouillede donner mobiliser importer effort rechercher année ilest difficile formaliser typer connaissance commecellesci dépendanter domaine Dans article intéressonsà lintégration connaissance sou former dobjet étiqueté dansl algorithm clustering critère permettre dévaluer puretéde cluster présenter comportement comparer jeu dedonner artificiel avantage inconvénient critère sontanalyser aider lutilisateur faire choix
634	Revue des Nouvelles Technologies de l'Information	EGC	2010	Comparaisons structurelles de grandes bases de données par apprentissage non-supervisé	Dans le domaine de la fouille de données, mesurer les similitudesentre différents sous-ensembles est une question importante qui a été peu étudiéejusqu'à présent. Dans cet article, nous proposons une nouvelle méthodebasée sur l'apprentissage non-supervisé. Les différents sous-ensembles à comparersont caractérisés au moyen d'un modèle à base de prototypes. Ensuite, lesdifférences entre les modèles sont détectées en utilisant une mesure de similarité	Guénaël Cabanes, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1001276	http://editions-rnti.fr/render_pdf.php?p=1001276	Dans domaine fouiller donnée mesurer similitudesentre sousensemble question important étudiéejusquà présent Dans article proposer méthodebaser lapprentissage nonsuperviser sousensembl comparersont caractériser moyen dun modeler baser prototype ensuite lesdifférence entrer modèle détecter utiliser mesurer similarité
635	Revue des Nouvelles Technologies de l'Information	EGC	2010	Composition de ServicesWeb Basée sur les Réseau Sociaux	Nous proposons dans cet article une première approche qui consisteà exploiter les réseaux sociaux afin de faciliter la composition de services parles utilisateurs finaux. Nous introduisons un Framework, nommé Social Composer(SoCo), qui implémente cette approche. SoCo fournit à l'utilisateur desrecommandations dynamiques de services basées entre autre sur le réseau socialde l'utilisateur qui est construit implicitement à partir des interactions entre lesutilisateurs, les services, les différentes compositions opérées par les membresdu réseau social, ainsi que le réseau social global.	Abderrahmane Maaradji, Hakim Hacid, Johann Daigremont, Noël Crespi	http://editions-rnti.fr/render_pdf.php?p1&p=1001290	http://editions-rnti.fr/render_pdf.php?p=1001290	proposer dan article approcher consisteà exploiter réseau social faciliter composition service parl utilisateur final introduire Framework nommer Social ComposerSoCo implémente approcher soco fournir lutilisateur desrecommandation dynamique service baser entrer réseau sociald lutilisateur construire implicitement partir interaction entrer lesutilisateur service composition opérer membresdu réseau social réseau social global
636	Revue des Nouvelles Technologies de l'Information	EGC	2010	Construction de noyaux pour l'apprentissage supervisé à partir d'arbres aléatoires	Nous montrons qu'un ensemble d'arbres de décision avec une composantealéatoire permet de construire un noyau efficace destiné à l'apprentissagesupervisé. Nous étudions théoriquement les propriétés d'un tel noyau et montronsque sous des conditions très souvent rencontrées en pratique, il existe uneséparabilité linéaire entre exemples de classes distinctes dans l'espace induit parcelui-ci. Parallèlement, nous observons également que le classique vote à la majoritéd'un ensemble d'arbres est un hyperplan (sans garantie d'optimalité) dansl'espace induit par le noyau. Enfin, comme le montrent nos expérimentations,l'utilisation conjointe d'un ensemble d'arbres et d'un séparateur à vaste marge(SVM) aboutit à des résultats extrêmement encourageants.	Vincent Pisetta, Pierre-Emmanuel Jouve, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1001351	http://editions-rnti.fr/render_pdf.php?p=1001351	montrer quun ensemble darbr décision composantealéatoire permettre construire noyau efficace destiner lapprentissagesupervisé étudier théoriquement propriété dun noyau montronsqu sou condition rencontrer pratiquer exister uneséparabiliter linéaire entrer exemple classe distincter dan lespace induire parceluici parallèlement observer également classique voter majoritédun ensembl darbr hyperplan garantir doptimalité danslespace induire noyau montrer expérimentationslutilisation conjoint dun ensemble darbr dun séparateur vaste margesvm aboutir résultat extrêmement encourageant
637	Revue des Nouvelles Technologies de l'Information	EGC	2010	Cubes Fermés / Quotients Émergents	"Le concept de Cube Émergent a été introduit afin de comparer deuxdata cubes. Dans cet article, nous introduisons deux nouvelles représentationsréduites du Cube Émergent sans perte des mesures : le Cube Fermé Émergent etle Cube Quotient Émergent. La première représentation est basée sur le conceptde fermeture cubique. C'est la plus petite représentation possible du cube dedonnées émergent. À partir du Cube Fermé Émergent et donc en stockant le minimumd'informations, il est possible de répondre efficacement aux requêtes quipeuvent être exécutées sur le Cube Émergent lui-même. La seconde représentations'appuie sur la structure du Cube Quotient qui a été proposé pour résumer uncube de données. Le Cube Quotient est revisité afin de le doter d'une sémantiquebasée sur la fermeture cubique et donc adapté au contexte du Cube Émergent. LeCube Quotient Émergent résultant est moins réduit que le Cube Fermé Émergentmais il préserve la propriété de "" spécialisation/généralisation "" du data cube quipermet la navigation au sein du Cube Émergent. Nous établissons également lelien entre les deux représentations introduites et celle basée sur les bordures classiquesen fouille de données. Des expérimentations effectuées sur divers jeux dedonnées visent à comparer la taille des différentes représentations."	Sébastien Nedjar, Alain Casali, Rosine Cicchetti, Lotfi Lakhal	http://editions-rnti.fr/render_pdf.php?p1&p=1001306	http://editions-rnti.fr/render_pdf.php?p=1001306	concept Cube Émergent introduire comparer deuxdata cuber Dans article introduire représentationsréduite Cube Émergent perte mesure   Cube Fermé Émergent etle Cube quotient Émergent représentation baser conceptde fermetur cubique cest plaire petit représentation cuber dedonner émerger À partir Cube Fermé Émergent stocker minimumdinformations répondre efficacement requêt quipeuver exécuter Cube émerger luimême second représentationsappuie structurer Cube quotient proposer résumer uncube donnée Cube quotient revisiter doter dune sémantiquebasée fermeture cubique adapter contexte Cube Émergent LeCube quotient émerger résulter réduire Cube Fermé émergentmais préserver propriété   spécialisationgénéralisation   dater cube quipermet navigation Cube Émergent établir également lelien entrer représentation introduit baser bordur classiquesen fouiller donnée expérimentation effectuer jeu dedonner viser comparer tailler représentation
638	Revue des Nouvelles Technologies de l'Information	EGC	2010	DaFOE: une plateforme pour construire des ontologies à partir de textes et de thésaurus		Jean Charlet, Sylvie Szulman, Nathalie Aussenac-Gilles, Adeline Nazarenko, Nathalie Hernandez, Nadia Nadah, Éric Sardet, Jean Delahousse, Valery Teguiak, Audrey Baneyx	http://editions-rnti.fr/render_pdf.php?p1&p=1001386	http://editions-rnti.fr/render_pdf.php?p=1001386	
639	Revue des Nouvelles Technologies de l'Information	EGC	2010	Découverte d'itemsets fréquents fermés sur architectures multicoeurs	Dans ce papier nous proposons PLCM, un algorithme parallèle dedécouverte d'itemsets fréquents fermés basé sur l'algorithme LCM, reconnucomme l'algorithme séquentiel le plus efficace pour cette tâche. Nous présentonsaussi une interface de parallélisme à la fois simple et puissante basée sur lanotion de Tuple Space, qui permet d'avoir une bonne répartition dynamique dutravail.Grâce à une étude expérimentale détaillée, nous montrons que PLCM est le seulalgorithme qui soit suffisamment générique pour calculer efficacement des itemsetsfréquents fermés à la fois sur des bases creuses et sur des bases denses,améliorant ainsi l'état de l'art.	Benjamin Négrevergne, Alexandre Termier, Jean-François Méhaut, Takeaki Uno	http://editions-rnti.fr/render_pdf.php?p1&p=1001339	http://editions-rnti.fr/render_pdf.php?p=1001339	Dans papier proposer plcm algorithme parallèle dedécouverte ditemset fréquent fermer baser lalgorithme LCM reconnucomm lalgorithm séquentiel plaire efficace tâcher présentonsaussi interface parallélisme simple puissant baser lanotion Tuple Space permettre davoir répartition dynamique dutravailgrâce étude expérimental détailler montrer plcm seulalgorithme suffisamment générique calculer efficacement itemsetsfréquent fermer base creux base densesaméliorer létat lart
640	Revue des Nouvelles Technologies de l'Information	EGC	2010	Découverte des dépendances fonctionnelles conditionnelles fréquentes	Les Dépendances Fonctionnelles Conditionnelles (DFC) ont été introduitesen 2007 pour le nettoyage des données. Elles peuvent être considéréescomme une unification de Dépendances Fonctionnelles (DF) classiques et deRègles d'Association (RA) puisqu'elles permettent de spécifier des dépendancesmixant des attributs et des couples de la forme attribut/valeur.Dans cet article, nous traitons le problème de la découverte des DFC, i.e. déterminerune couverture de l'ensemble des DFC satisfaites par une relation r. Nousmontrons comment une technique connue pour la découverte des DF (exacteset approximatives) peut être étendue aux DFC. Cette technique a été implémentéeet des expériences ont été menées pour montrer la faisabilité et le passage àl'échelle de notre proposition.	Thierno Diallo, Noel Novelli	http://editions-rnti.fr/render_pdf.php?p1&p=1001311	http://editions-rnti.fr/render_pdf.php?p=1001311	dépendance fonctionnel Conditionnelles DFC introduitesen 2007 nettoyage donnée pouvoir considéréescomme unification dépendance fonctionnel DF classique derègl dassociation ra puisquell permettre spécifier dépendancesmixer attribut couple former attributvaleurDans article traiter problème découvrir DFC ie déterminerune couverture lensemble dfc satisfaire relation nousmontrer technique connaître découvrir df exacteset approximatif pouvoir étendre DFC technique implémentéeet expérience mener montrer faisabilité passage àléchell proposition
641	Revue des Nouvelles Technologies de l'Information	EGC	2010	Density estimation on data streams : an application to Change Detection	In recent years, the amount of data to process has increased in manyapplication areas such as network monitoring, web click and sensor data analysis. Data stream mining answers to the challenge of massive data processing, this paradigm allows for treating pieces of data on the fly and overcoming data storage. The detection of changes in a data stream distribution is an important issue. This article proposes a new schema of change detection :i) the summarization of the input data stream by a set of micro-clusters;ii) the estimate of the data stream distribution exploiting micro-clusters;iii) the estimate of the divergence between the current estimated distribution and a reference distribution;iv) diagnostic step through the contribution of each predictive variable to the overall divergence between both distributions.Our schema of change detection is applied and evaluated on artificial data streams.	Marie-Luce Picard, Benoît Grossin, Alexis Bondu	http://editions-rnti.fr/render_pdf.php?p1&p=1001297	http://editions-rnti.fr/render_pdf.php?p=1001297	in recent year the amount of dater to process increased in manyapplication area such network monitoring web click and sensor dater analysis Data stream mining answers to the challenge of massif dater processing this paradigm allow for treating piece of dater the fly and overcoming dater storage The detection of change in dater stream distribution is an importer issu This article propos new schema of changer detection ie the summarization of the input dater stream by set of microclustersii the estimate of the dater stream distribution exploiting microclustersiii the estimate of the divergence between the current estimated distribution and reference distributioniv diagnostic step through the contribution of each predictiv variable to the overall divergence between both distributionsOur schema of changer detection is applied and evaluated artificial dater stream
642	Revue des Nouvelles Technologies de l'Information	EGC	2010	Detecting Anomalies in Data Streams using Statecharts	The environment around us is progressively equipped withvarious sensors, producing data continuously. The applications usingthese data face many challenges, such as data stream integration over anattribute (such as time) and knowledge extraction from raw data. In thispaper we propose one approach to face those two challenges. First, datastreams integration is performed using statecharts which represents aresume of data produced by the corresponding data producer. Second,we detect anomalous events over temporal relations among statecharts.We describe our approach in a demonstration scenario, that is using avisual tool called Patternator	Vasile-Marian Scuturici, Dan-Mircea Suciu, Romain Vuillemot, Aris Ouksel, Lionel Brunie	http://editions-rnti.fr/render_pdf.php?p1&p=1001394	http://editions-rnti.fr/render_pdf.php?p=1001394	The environmer around us is progressively equipped withvarious sensors producing dater continuously The application usingthese dater face many challenger such dater stream integration over anattribute such time and knowledge extraction from raw dater In thispaper we proposer one approach to face thos two challeng First datastreams integration is performed using statechart which represent aresum of dater produced by the corresponding dater producer Secondwe detect anomalou event over temporal relation among statechartsWe describe our approach in demonstration scenario that is using avisual tool called Patternator
643	Revue des Nouvelles Technologies de l'Information	EGC	2010	Détection des mouvements anormaux dans des vidéos		Md. Haidar Sharif, Husam Alustwani, Ioan Marius Bilasco, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1001448	http://editions-rnti.fr/render_pdf.php?p=1001448	
644	Revue des Nouvelles Technologies de l'Information	EGC	2010	Développement de méthodes de classification basées sur l'Analyse de Concepts Formels sous la plateforme WEKA		Besma Khalfi, Rahma Cherif, Nida Meddouri, Mondher Maddouri	http://editions-rnti.fr/render_pdf.php?p1&p=1001373	http://editions-rnti.fr/render_pdf.php?p=1001373	
645	Revue des Nouvelles Technologies de l'Information	EGC	2010	Differentes variantes GMM-SMOs pour l'identification du locuteur	Dans cet article, nous présentons différentes variantes GMM-SMOs pour l'identification du locuteur en mode indépendant du texte. Pour mettre en oeuvre les différents systèmes, nous avons opté une représentation multi-gaussienne de l'espace des caractéristiques basées sur l'algorithme Expectation Maximisation (EM). Ces nouvelles représentations constituent les vecteurs d'entrés pour entraîner les supports vecteurs machines (SVMs) par l'algorithme de type Optimisation par Minimisation Séquentielle (SMO).	Siwar Zribi Boujelbene, Dorra Ben Ayed Mezghanni, Noureddine Ellouze	http://editions-rnti.fr/render_pdf.php?p1&p=1001421	http://editions-rnti.fr/render_pdf.php?p=1001421	Dans article présenter variante gmmsmo lidentification locuteur mode indépender texte Pour mettre oeuvrer système opter représentation multigaussienn lespace caractéristique baser lalgorithme expectation maximisation EM représentation constituer vecteur dentré entraîner support vecteur machine svms lalgorithme typer Optimisation minimisation Séquentielle smo
646	Revue des Nouvelles Technologies de l'Information	EGC	2010	Etude comparative des langages de requêtes sémantiques pour l'extraction des liens complexes dans une base de connaissances		Thabet Slimani, Boutheina Ben Yaghlane	http://editions-rnti.fr/render_pdf.php?p1&p=1001445	http://editions-rnti.fr/render_pdf.php?p=1001445	
647	Revue des Nouvelles Technologies de l'Information	EGC	2010	Etude de stabilité de méthodes de sélection de motifs à partir des séquences protéiques		Rabie Saidi, Sabeur Aridhi, Mondher Maddouri, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1001454	http://editions-rnti.fr/render_pdf.php?p=1001454	
648	Revue des Nouvelles Technologies de l'Information	EGC	2010	Expansion de requêtes SQL par une ontologie de domaine	Cet article traite un problème dans le domaine de la gestion des basesde données classiques. Il s'agit d'exploiter une ontologie de domaine pour aiderl'utilisateur d'une base de données relationnelle dans sa recherche et de luipermettre une interrogation transparente de la base de données. Pour cela, nousproposons une approche d'expansion automatique de requêtes SQL lorsquecelles-ci n'ont pas de réponses. Notre approche est décrite par un algorithmedéfini de manière générique afin d'être utilisé pour une base de données quelconque.	Ines Fayech, Habib Ounalli	http://editions-rnti.fr/render_pdf.php?p1&p=1001345	http://editions-rnti.fr/render_pdf.php?p=1001345	article traire problème dan domaine gestion basesde donné classique sagit dexploiter ontologie domaine aiderlutilisateur dune baser donnée relationnel dan rechercher luipermettre interrogation transparent baser donnée Pour celer nousproposer approcher dexpansion automatique requête sql lorsquecellesci nont réponse approcher décrire algorithmedéfini manière générique dêtre utiliser baser donnée
649	Revue des Nouvelles Technologies de l'Information	EGC	2010	Explication de décisions de réconciliation : approche fondée sur les réseaux de Petri colorés	L'objectif des systèmes d'intégration de données est de faciliter l'exploitationet l'interprétation d'informations hétérogènes provenant de différentessources. Lorsque l'on doit intégrer de grands volumes de données, le recours àun expert n'est pas envisageable mais l'exploitation de processus d'intégrationautomatiques peut introduire des approximations ou des erreurs. Nous nous focalisonssur les résultats fournis par les méthodes de réconciliation de données.Ces dernières comparent les données entre elles et détectent celles qui réfèrent àla même entité du monde réel. Pour renforcer la confiance des utilisateurs dansles résultats retournés par ces méthodes, nous proposons dans cet article une approched'explication graphique fondée sur les réseaux de Petri colorés qui estparticulièrement adaptée aux approches de réconciliation globales, numériqueset guidées par une ontologie.	Souhir Gahbiche, Nathalie Pernelle, Fatiha Saïs	http://editions-rnti.fr/render_pdf.php?p1&p=1001313	http://editions-rnti.fr/render_pdf.php?p=1001313	Lobjectif système dintégration donnée faciliter lexploitationet linterprétation dinformation hétérogène provenir différentessource lon devoir intégrer grand volume donnée recours àun expert nest envisageable lexploitation processus dintégrationautomatiqu pouvoir introduir approximation erreur focalisonssur résultat fournir méthode réconciliation donnéesces comparer donnée entrer détecter référer àla entité monder réel Pour renforcer confiance utilisateur dansl résultat retourner méthode proposer dan article approchedexplication graphique fonder réseau Petri coloré estparticulièrement adapter approche réconciliation global numériqueset guider ontologie
650	Revue des Nouvelles Technologies de l'Information	EGC	2010	Exploration de dépendances fonctionnelles et de règles d'association avec OLAP		Pierre Allard, Sébastien Ferré	http://editions-rnti.fr/render_pdf.php?p1&p=1001408	http://editions-rnti.fr/render_pdf.php?p=1001408	
651	Revue des Nouvelles Technologies de l'Information	EGC	2010	Extraction d'itemsets distinctifs dans les flux de données	L'extraction d'itemsets distinctifs est un sujet de recherche récent quiconnait plusieurs algorithmes pour les données statiques (Knobbe et Ho, 2006;Heikinheimo et al., 2007). Ces solutions ne sont toutefois pas conçues pour lecas des flux de données, pour lesquels les temps de réponse doivent être aussifaibles que possible. Nous considérons le problème de l'extraction d'itemsetsdistinctifs dans les flux, qui peut avoir de nombreuses applications dans la sélectionde variables, la classification ou encore la recherche d'information. Nousproposons l'heuristique IDkF (Itemsets Distinctifs dans les Flux) et des résultatsd'expérimentations en comparaison d'une technique de la littérature.	Chongsheng Zhang, Florent Masseglia	http://editions-rnti.fr/render_pdf.php?p1&p=1001291	http://editions-rnti.fr/render_pdf.php?p=1001291	lextraction ditemset distinctif rechercher récent quiconnait algorithme donnée statique Knobbe Ho 2006Heikinheimo al 2007 solution conçu leca flux donnée lesquel temps réponse devoir aussifaibl considérer problème lextraction ditemsetsdistinctif dan flux pouvoir application dan sélectionde variable classification rechercher dinformation nousproposon lheuristiqu idkf itemset Distinctifs dan flux résultatsdexpérimentation comparaison dune technique littérature
652	Revue des Nouvelles Technologies de l'Information	EGC	2010	Extraction de la région d'intérêt d'une personne sur un obstacle		Adel Lablack, Thierry Urruty, Yassine Benabbas, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1001430	http://editions-rnti.fr/render_pdf.php?p=1001430	
653	Revue des Nouvelles Technologies de l'Information	EGC	2010	Extraction de motifs graduels clos	La découverte automatique de règles et motifs graduels (“plus l'âged'une personne est élevé, plus son salaire est élevé”) trouve de très nombreusesapplications sur des bases de données réelles (e.g. biologie, flots de données decapteurs). Si des algorithmes de plus en plus efficaces sont proposés dans desarticles récents, il n'en reste pas moins que ces méthodes génèrent un nombrede motifs tellement important que les experts peinent à les exploiter. Dans cetarticle, nous proposons donc une représentation condensée des motifs graduelsen introduisant les concepts théoriques associés aux opérateurs de fermeture surde tels motifs.	Sarra Ayouni, Sadok Ben Yahia, Anne Laurent, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1001293	http://editions-rnti.fr/render_pdf.php?p=1001293	découvrir automatique règle motif graduel “ plaire lâgedune élever plaire salaire élever ” trouver nombreusesapplication base donnée réel eg biologie flot donnée decapteur Si algorithme plaire plaire efficace proposer dan desarticl récent nen rester méthode générer nombrede motif tellemer importer expert peiner exploiter Dans cetarticle proposer représentation condenser motif graduelsen introduire concept théorique associé opérateur fermeture surd motif
654	Revue des Nouvelles Technologies de l'Information	EGC	2010	Extraction de règles d'association séquentielle à l'aide de modèles semi-paramétriques à risques proportionnels	La recherche de liens entre objets fréquents a été popularisée par lesméthodes d'extraction de règles d'association. Dans le cas de séquences d'événements,les méthodes de fouille permettent d'extraire des sous-séquences quipeuvent ensuite être exprimées sous la forme de règles d'association séquentielleentre événements. Cette utilisation de la fouille de séquences pour la recherchede liens entre des événements pose deux problèmes. Premièrement, lecritère principal utilisé pour sélectionner les sous-séquences d'événements estla fréquence, or les occurrences de certains événements peuvent être fortementliées entre elles même lorsqu'elles sont peu fréquentes. Deuxièmement, les mesuresactuelles utilisées pour caractériser les règles d'association ne tiennent pascompte du caractère temporel des données, comme l'importance du timing desévénements ou le problème des données censurées. Dans cet article, nous proposonsune méthode pour rechercher des liens significatifs entre des événementsà l'aide de modèles de durée. Les règles d'association sont construites à partirdes motifs séquentiels observés dans un ensemble de séquences. L'influence surle risque que l'événement « conclusion » se produise après le ou les événements« prémisse » est estimée à l'aide d'un modèle semi-paramétrique à risques proportionnels.Outre la présentation de la méthode, l'article propose une comparaisonavec d'autres mesures d'association	Nicolas S. Müller, Matthias Studer, Gilbert Ritschard, Alexis Gabadinho	http://editions-rnti.fr/render_pdf.php?p1&p=1001263	http://editions-rnti.fr/render_pdf.php?p=1001263	rechercher lien entrer objet fréquent populariser lesméthod dextraction règle dassociation Dans cas séquence dévénementsl méthode fouiller permettre dextraire sousséquence quipeuvent ensuite exprimer sou former règle dassociation séquentielleentre événement utilisation fouiller séquence recherchede lien entrer événement poser problème lecritère principal utiliser sélectionner sousséquence dévénement estla fréquence or occurrence événement pouvoir fortementlier entrer lorsquell fréquent mesuresactuell utilisée caractériser règle dassociation pascompte caractère temporel donnée limportance timing desévénement problème donnée censurer Dans article proposonsune méthode rechercher lien significatif entrer événementsà laid modèle durer règle dassociation construire partirde motif séquentiel observer dan ensemble séquence Linfluence surle risquer lévénement « conclusion » produire événement « prémisse » estimer laid dun modeler semiparamétriqu risque proportionnelsOutre présentation méthode larticle proposer comparaisonavec dautr mesure dassociation
655	Revue des Nouvelles Technologies de l'Information	EGC	2010	Extraction des séquences fermées fréquentes à partir de corpus parallèles : Application à la traduction automatique	Dans cet article, nous abordons la problématique d'extraction de séquencesfréquentes à partir de corpus de textes parallèles en prenant en comptel'ordre d'apparition des mots dans une phrase. Notre finalité est d'exploiter cesséquences dans la traduction automatique (TA). Nous introduisons ainsi la notionde règles associatives inter-langues (RAIL) et nous définissons notre modèlede traduction à base de ces associations. Nous décrivons également les différentesexpérimentations conduites sur le corpus EUROPARL afin de construire àpartir des RAIL une table de traduction bilingue qui est intégrée par la suite dansun processus complet de TA.	Cherif Chiraz Latiri, Cyrine Nasri, Kamel Smaïli, Yahya Slimani	http://editions-rnti.fr/render_pdf.php?p1&p=1001266	http://editions-rnti.fr/render_pdf.php?p=1001266	Dans article aborder problématique dextraction séquencesfréquente partir corpus texte parallèle prendre comptelordre dapparition dan phraser finalité dexploiter cesséquencer dan traduction automatique TA introduire notionde régler associatif interlangu RAIL définir modèlede traduction baser association décrire également différentesexpérimentation conduire corpus europarl construire àpartir rail tabler traduction bilingue intégrer suite dansun processus complet TA
656	Revue des Nouvelles Technologies de l'Information	EGC	2010	Fouille visuelle de données en 3D et réalité virtuelle : état de l'art	La fouille visuelle de données (ou Visual Data Mining, VDM) a pourobjectif de faciliter l'interprétation des résultats issus d'une fouille de données,grâce à l'usage de représentations graphiques. Au cours de la dernière décennie,un grand nombre de techniques de visualisation d'information ont été mises aupoint, permettant la visualisation de données multidimensionnelles dans des environnementsvirtuels. Lors des travaux antérieurs, les chercheurs ont proposédes taxonomies pour classer les techniques de VDM (Chi (2000), Herman et al.(2000)). Toutefois, ces taxonomies ne prennent en compte que partiellement lestechniques récentes relatives à l'utilisation de la 3D et de la réalité virtuelle. Lebut de cet article est de faire un état de l'art récent et spécifique à ces techniques.Celles-ci sont détaillées, classées et comparées selon différents critères : les applications,l'encodage graphique, les techniques d'interaction, les avantages etles inconvénients de chaque approche. Ces techniques sont présentées dans destableaux accompagnées d'illustrations graphiques	Zohra Ben Said, Fabrice Guillet, Paul Richard	http://editions-rnti.fr/render_pdf.php?p1&p=1001282	http://editions-rnti.fr/render_pdf.php?p=1001282	fouiller visuel donnée Visual Data Mining VDM pourobjectif faciliter linterprétation résultat issu dune fouiller donnéesgrâce lusage représentation graphique Au cours décennieun grand nombre technique visualisation dinformation mettre aupoint permettre visualisation donnée multidimensionnel dan environnementsvirtuel travail antérieur chercheur proposéd taxonomie classer technique VDM Chi 2000 Herman al2000 taxonomie prendre compter partiellemer lestechniqu récent relatif lutilisation 3d réalité virtuel Lebut article faire lart récent spécifique techniquescellesci détailler classer comparer critère   applicationslencodage graphique technique dinteraction avantage etl inconvénient approcher technique présenter dan destableaux accompagné dillustration graphique
657	Revue des Nouvelles Technologies de l'Information	EGC	2010	Gestion sémantique des droits d'accès au contenu: l'ontologie AMO	Dans cet article nous proposons une approche de la gestion des droitsd'accès pour les systèmes de gestion de contenu qui reposent sur les modèles ettechniques du web sémantique. Nous présentons l'ontologie AMO qui consiste(1) en un ensemble de classes et propriétés permettant d'annoter les ressourcesdont il s'agit de contrôler l'accès et (2) en une base de règles d'inférence modélisantla stratégie de gestion des droits à mettre en oeuvre. Appliquées sur la based'annotations des ressources, ces règles permettent de gérer les ressources selonune stratégie donnée. Cette modélisation garantit ainsi l'adaptabilité de l'ontologieà différentes stratégies de gestion des droits d'accès. Nous illustrons l'utilisationde l'ontologie AMO sur les documents du projet ANR ISICIL produitspar le wiki sémantique SweetWiki. Nous montrons comment les documents sontannotés avec AMO, quelles règles sont mises en oeuvre et quelles requêtes permettentle contrôle de l'accès aux documents.	Michel Buffa, Catherine Faron-Zucker, Anna Kolomoyskaya	http://editions-rnti.fr/render_pdf.php?p1&p=1001340	http://editions-rnti.fr/render_pdf.php?p=1001340	Dans article proposer approcher gestion droitsdaccè système gestion contenir reposer modèle ettechniqu web sémantique présenter lontologie amo consiste1 ensemble classe propriété permettre dannoter ressourcesdont sagit chuter laccè 2 baser règle dinférenc modélisantla stratégie gestion droit mettre oeuvrer Appliquées basedannotation ressource règle permettre gérer ressource selonune stratégie donner modélisation garantir ladaptabilité lontologieà stratégie gestion droit daccè illustrer lutilisationd lontologie AMO document projet ANR isicil produitspar wiki sémantique sweetwiki montrer document sontannoté amo règle mettre oeuvrer requêt permettentle contrôler laccè document
658	Revue des Nouvelles Technologies de l'Information	EGC	2010	Identifying the Presence of Communities in Complex Networks Through Topological Decomposition and Component Densities	The exponential growth of data in various fields such as Social Networksand Internet has stimulated lots of activity in the field of network analysisand data mining. Identifying Communities remains a fundamental technique toexplore and organize these networks. Few metrics are widely used to discoverthe presence of communities in a network. We argue that these metrics do nottruly reflect the presence of communities by presenting counter examples. Thisis because these metrics concentrate on local cohesiveness among nodes wherethe goal is to judge whether two nodes belong to the same community or viseversa. Thus loosing the overall perspective of the presence of communities in theentire network. In this paper, we propose a new metric to identify the presenceof communities in real world networks. This metric is based on the topologicaldecomposition of networks taking into account two important ingredients of realworld networks, the degree distribution and the density of nodes. We show theeffectiveness of the proposed metric by testing it on various real world data sets	Faraz Zaidi, Guy Melançon	http://editions-rnti.fr/render_pdf.php?p1&p=1001286	http://editions-rnti.fr/render_pdf.php?p=1001286	The exponential growth of dater in various field such Social Networksand Internet has stimulated lot of activity in the field of network analysisand dater mining Identifying communitie remain fundamental technique toexplore and organize these network Few metric are widely used to discoverthe presence of communitie in network We arguer that thes metric do nottruly reflect the presence of communitie by presenting counter exampl Thisis becaus these metric concentrate local cohesivenes among nodes wherethe goal is to judge whether two nod belong to the same community or viseverser Thus loosing the overall perspectif of the presence of communitie in theentire network In this paper we proposer new metric to identify the presenceof communities in real world network This metric is based the topologicaldecomposition of network taking into account two importer ingredients of realworld network the degree distribution and the density of nod We show theeffectiveness of the proposed metric by testing it various real world dater set
659	Revue des Nouvelles Technologies de l'Information	EGC	2010	IncFDs: un nouvel algorithme d'inférence incrémentale des dépendances fonctionnelles	L'inférence des dépendances fonctionnelles est l'une des problématiquesles plus étudiées en bases de données. Elle a fait l'objet de plusieurstravaux qui ont proposé des algorithmes afin d'inférer, efficacement, les dépendancesfonctionnelles pour les utiliser dans différents domaines : administrationde bases de données, ré-ingénierie, optimisation des requêtes,etc. Toutefois,pour les application réelles, les bases de données sont évolutives et les relationssont fréquemment augmentées ou diminuées de tuples. Par conséquent, afin des'adapter à ce cadre dynamique, une solution consiste à appliquer l'un des algorithmes,disponibles dans la littérature, pour inférer les dépendances fonctionnelles,après chaque mise à jour. Cette solution étant coûteuse, nous proposons,dans cet article, d'inférer les dépendances fonctionnelles d'une manière incrémentale.À cet effet, nous introduisons un nouvel algorithme, appelé INCFDS, etnous évaluons ses performances par rapport à l'approche classique d'inférencedes dépendances fonctionnelles à partir d'une relation dynamique.	Ghada Gasmi	http://editions-rnti.fr/render_pdf.php?p1&p=1001310	http://editions-rnti.fr/render_pdf.php?p=1001310	Linférence dépendance fonctionnel lune problématiquesle plaire étudier base donnée faire lobjet plusieurstravaux proposer algorithme dinférer efficacement dépendancesfonctionnelle utiliser dan domaine   administrationde baser donnée réingénierie optimisation requêtesetc Toutefoispour application réel base donnée évolutif relationssont fréquemment augmenter diminué tuple Par conséquent desadapter cadrer dynamique solution consister appliquer lun algorithmesdisponible dan littérature inférer dépendance fonctionnellesaprè miser jour solution coûteux proposonsdans article dinférer dépendance fonctionnel dune manière incrémentaleà introduire nouvel algorithme appeler INCFDS etnous évaluer performance rapport lapproche classique dinférenced dépendance fonctionnel partir dune relation dynamique
660	Revue des Nouvelles Technologies de l'Information	EGC	2010	Indexation et recherche d'images à très grande échelle avec une AFC incrémentale et parallèle sur GPU	Nous présentons un nouvel algorithme incrémental et parallèled'analyse factorielle des correspondances (AFC) pour la recherche d'images àgrande échelle en utilisant le processeur de la carte graphique (GPU). L'AFCest adaptée à la recherche d'images par le contenu en utilisant des descripteurslocaux des images (SIFT). L'AFC permet de réduire le nombre de dimensionset de découvrir des thèmes qui permettent de diminuer le nombre d'images àparcourir et donc le temps de réponse d'une requête. Pour traiter de trèsgrandes bases d'images, nous présentons une version incrémentale et parallèled'AFC, puis nous utilisons ses indicateurs pour construire des fichiers inverséspour retrouver les images contenant les mêmes thèmes que l'image requête.Cette étape est elle aussi parallélisée sur GPU pour obtenir des réponsesrapides. Les résultats numériques sur la base de données d'images Nistér-Stewénius plongée dans 1 million d'images de FlickR montrent que notrealgorithme incrémental et parallèle est très significativement plus rapide que saversion standard	Nguyen-Khang Pham, François Poulet, Annie Morin, Patrick Gros	http://editions-rnti.fr/render_pdf.php?p1&p=1001281	http://editions-rnti.fr/render_pdf.php?p=1001281	présenter nouvel algorithme incrémental parallèledanalyse factoriel correspondance AFC rechercher dimag àgrand échelle utiliser processeur carte graphique GPU LAFCest adapter rechercher dimager contenir utiliser descripteurslocaux image sift LAFC permettre réduire nombre dimensionset découvrir thème permettre diminuer nombre dimages àparcourir temps réponse dune requête Pour traiter trèsgrande dimager présenter version incrémental parallèledafc pouvoir utiliser indicateur construire fichier inverséspour retrouver image contenir thème limage requêtecette étape paralléliser gpu obtenir réponsesrapide résultat numérique baser donnée dimag NistérStewénius plonger dan 1 million dimage flickr montrer notrealgorithme incrémental parallèle significativement plaire rapide saversion standard
661	Revue des Nouvelles Technologies de l'Information	EGC	2010	Indice de complexité pour le tri et la comparaison de séquences catégorielles	Cet article1 propose un nouvel indice de la complexité de séquencescatégorielles. Bien que conçu pour des séquences représentant des trajectoiresbiographiques telles que celles rencontrées dans les sciences sociales, il s'appliqueà tous types de listes ordonnées d'états. L'indice prend en compte deuxaspects distincts, soit la complexité induite par l'ordonnancement des états successifsqui est mesurée par le nombre de transitions (changements d'état) et lacomplexité liée à la distribution des états dont rend compte l'entropie	Alexis Gabadinho, Gilbert Ritschard, Matthias Studer, Nicolas S. Müller	http://editions-rnti.fr/render_pdf.php?p1&p=1001267	http://editions-rnti.fr/render_pdf.php?p=1001267	article1 proposer nouvel indice complexité séquencescatégoriell concevoir séquence représenter trajectoiresbiographique cell rencontrer dan science social sappliqueà type liste ordonner détat Lindice prendre compter deuxaspect distinct complexité induire lordonnancement successifsqui mesurer nombre transition changement détat lacomplexité lier distribution compter lentropi
662	Revue des Nouvelles Technologies de l'Information	EGC	2010	Inférence Bayesienne du Maximum d'Entropie pour le Diagnostic du Cancer		Fadi Dornaika, Fadi Chakik	http://editions-rnti.fr/render_pdf.php?p1&p=1001423	http://editions-rnti.fr/render_pdf.php?p=1001423	
663	Revue des Nouvelles Technologies de l'Information	EGC	2010	Intégration de Connaissances a Priori dans le Principe du Maximum d'Entropie	Cet article montre que si l'on dispose d'une connaissance a priori surle problème en main, l'intégration de cette dernière dans le processus d'apprentissaged'une machine intelligente pour des tâches de classification peut améliorerla performance de cette machine. Nous étudions l'effet de l'intégration de laconnaissance a priori de convexité sur le processus d'apprentissage du principedu Maximum d'Entropie (MaxEnt) en utilisant des exemples virtuels. Nous testonsles idées proposées sur un problème benchmark bien connu dans la littératuredes machines d'apprentissage, le problème de formes d'ondes de Breiman.Nous avons abouti à un taux d'erreur de généralisation de 15.57% qui est trèsproche du taux d'erreur théorique estimé par Breiman (14%).	Fadi Chakik, Fadi Dornaika	http://editions-rnti.fr/render_pdf.php?p1&p=1001356	http://editions-rnti.fr/render_pdf.php?p=1001356	article montr lon disposer dune connaissance priori surl problème main lintégration dan processus dapprentissagedun machiner intelligent tâche classification pouvoir améliorerler performance machiner étudier leffet lintégration laconnaissance priori convexité processus dapprentissage principedu Maximum dentropie maxent utiliser exemple virtuel testonsl idée proposer problème benchmark connaître dan littératurede machine dapprentissage problème forme dond BreimanNous aboutir taux derreur généralisation 1557 trèsproch taux derreur théorique estimer Breiman 14
664	Revue des Nouvelles Technologies de l'Information	EGC	2010	Intégration interactive de contraintes pour la réduction de dimensions et la visualisation	"Il existe aujourd'hui de nombreuses méthodes de réduction de dimensions,que ce soit dans un cadre supervisé ou non supervisé. L'un des intérêts deces méthodes est de pouvoir visualiser les données, avec pour objectif que lesobjets qui apparaissent ""visuellement"" proches soient similaires, dans un sensqui correspond aux connaissances d'un expert du domaine ou qui soit conformeaux informations de supervision. Nous nous plaçons ici dans un contexte semisuperviséoù des connaissances sont ajoutées de façon interactive : ces informationsseront apportées sous forme de contraintes exprimant les écarts entrela représentation observée et les connaissances d'un expert. Nous pourrons parexemple spécifier que deux objets proches dans l'espace d'observation sont enfait peu similaires, ou inversement. La méthode utilisée ici dérive de l'analyseen composantes principales (ACP), à laquelle nous proposons d'intégrer deuxtypes de contraintes. Nous présentons une méthode de résolution qui a été implémentéedans un logiciel offrant une représentation 3D des données et grâceauquel l'utilisateur peut ajouter des contraintes de manière interactive, puis visualiserles modifications induites par ces contraintes. Deux types d'expérimentationsont présentés, reposant respectivement sur un jeu de données synthétiqueet sur des jeux standards : ces tests montrent qu'une représentation de bonnequalité peut être obtenue avec un nombre limité de contraintes ajoutées."	Guillaume Cleuziou, Frédéric Moal, Lionel Martin, Matthieu Exbrayat	http://editions-rnti.fr/render_pdf.php?p1&p=1001320	http://editions-rnti.fr/render_pdf.php?p=1001320	exister aujourdhui méthode réduction dimensionsqu dan cadrer superviser superviser Lun intérêt deces méthode pouvoir visualiser donnée objectif lesobjet apparaître visuellemer similaire dan sensqui correspondre connaissance dun expert domaine conformeaux information supervision placer dan contexte semisuperviséoù connaissance ajouter interactif   informationsseront apporter sou former contrainte exprimer écart entrela représentation observer connaissance dun expert pouvoir parexemple spécifier objet dan lespace dobservation enfer similaire inversement méthode utiliser dériver lanalyseen composante principal acp proposer dintégrer deuxtype contrainte présenter méthode résolution implémentéedans logiciel offrir représentation 3d donnée grâceauquel lutilisateur pouvoir ajouter contrainte manière interactif pouvoir visualiserl modification induit contrainte Deux type dexpérimentationsont présenter reposer respectivement jeu donnée synthétiqueet jeu standard   test montrer quune représentation bonnequalité pouvoir obtenir nombre limité contrainte ajouter
665	Revue des Nouvelles Technologies de l'Information	EGC	2010	Interrogation des résumés de flux de données	Les systèmes de gestion de flux de données (SGFD) ont été conçusafin de traiter une masse importante de données produites en ligne de façoncontinue. Etant donné que les ressources matérielles ne permettent pas de conservertoute cette volumétrie, seule la partie récente du flux est mémorisée dans lamémoire du SGFD. Ainsi, les requêtes évaluées par ces systèmes ne peuvent porterque sur les données les plus récentes du flux. Par conséquent, les SGFD actuelsne peuvent pas traiter des requêtes qui portent sur des périodes très longues.Nous proposons dans cet article, une approche permettant d'évaluer des requêtesqui portent sur une période plus longue que la mémoire du SGFD. Ces fenêtresfont appels à des données récentes et des données historisées. Nous présentonsle niveau logique de cette approche ainsi que son implantation sous le SGFD Esper.Une technique d'échantillonnage associée à une technique de fenêtre pointde repère est appliquée pour conserver une représentation compacte des donnéesdu flux.	Nesrine Gabsi, Fabrice Clérot, Georges Hébrail	http://editions-rnti.fr/render_pdf.php?p1&p=1001300	http://editions-rnti.fr/render_pdf.php?p=1001300	système gestion flux donnée SGFD conçusafin traiter masser important donnée produire ligne façoncontinue eter donner ressource matériel permettre conservertoute volumétrie partir récent flux mémoriser dan lamémoire sgfd requête évaluer système pouvoir porterqu donnée plaire récent flux Par conséquent sgfd actuelsn pouvoir traiter requête porter période longuesnous proposer dan article approcher permettre dévaluer requêtesqui porter période plaire long mémoire SGFD fenêtresfont appel donnée récent donnée historisé présentonsle niveau logique approcher implantation sou SGFD esperune technique déchantillonnage associer technique fenêtre pointd repérer appliquer conserver représentation compacter donnéesdu flux
666	Revue des Nouvelles Technologies de l'Information	EGC	2010	K-WORDS LAB : un outil d'analyse des mots clés permettant d'explorer les dynamiques d'un domaine scientifique.		Audrey Baneyx, Philippe Breucker	http://editions-rnti.fr/render_pdf.php?p1&p=1001376	http://editions-rnti.fr/render_pdf.php?p=1001376	
667	Revue des Nouvelles Technologies de l'Information	EGC	2010	KGRAM: une machine abstraite de graphes de connaissance	Cet article présente la machine abstraite de graphes de connaissanceKGRAM qui unifie les notions d'homomorphisme de graphe et de calcul de requêtestelles que celles du langage SPARQL sur des données RDF. KGRAMimplémente un ensemble extensible d'expressions qui définissent une famille delangages abstraits d'interrogation de graphes, GRAAL. Nous décrivons la sémantiquedynamique de GRAAL en Sémantique Naturelle et nous présentons lamachine abstraite KGRAM conçue comme l'interprète de GRAAL, qui implémenteles règles de sémantique naturelle du langage.	Olivier Corby, Catherine Faron-Zucker	http://editions-rnti.fr/render_pdf.php?p1&p=1001328	http://editions-rnti.fr/render_pdf.php?p=1001328	article présenter machiner abstrait graphe connaissancekgram unifier notion dhomomorphisme graph calcul requêtestelle langage sparql donnée RDF kgramimplémente ensemble extensible dexpression définir famille delangager abstrait dinterrogation graphe GRAAL décrire sémantiquedynamique GRAAL Sémantique Naturelle présenter lamachin abstrait kgram concevoir linterprèt GRAAL implémentel régler sémantique langage
668	Revue des Nouvelles Technologies de l'Information	EGC	2010	Le conflit dans la théorie des fonctions de croyance	Le conflit apparaît naturellement lorsque plusieurs sources d'informationsimparfaites sont en jeu. La théorie des fonctions de croyance offre unformalisme adapté à la fusion d'informations dans lequel la considération duconflit est centrale. Ce travail propose de revenir sur les différentes définitionsdu conflit dans cette théorie, tentant de les synthétiser et de montrer commentsupprimer ce conflit, ou bien comment en tenir compte lors de la combinaisondes informations.	Arnaud Martin	http://editions-rnti.fr/render_pdf.php?p1&p=1001411	http://editions-rnti.fr/render_pdf.php?p=1001411	conflit apparaître naturellement source dinformationsimparfaiter jeu théorie fonction croyance offrir unformalisme adapter fusion dinformation dan considération duconflit central travail proposer revenir définitionsdu conflit dan théorie tenter synthétiser montrer commentsupprimer conflit compter combinaisondes information
669	Revue des Nouvelles Technologies de l'Information	EGC	2010	Modèle de Langue à base de Concepts pour la Recherche d'Information	La majorité des modèles de langue appliqués à la recherched'information repose sur l'hypothèse d'indépendance des mots.Plus précisément, ces modèles sont estimés à partir des mots simplesapparaissant dans les documents sans considérer les éventuelles relationssémantiques et conceptuelles. Pour pallier ce problème, deux grandesapproches ont été explorées : la première intègre des dépendances d'ordresurfacique entre les mots, et la seconde repose sur l'utilisation des ressourcessémantiques pour capturer les dépendances entre les mots. Le modèle delangue que nous présentons dans cet article s'inscrit dans la seconde approche.Nous proposons d'intégrer les dépendances entre les mots en représentant lesdocuments et les requêtes par les concepts.	Lynda Said L'Hadj, Mohand Boughanem	http://editions-rnti.fr/render_pdf.php?p1&p=1001501	http://editions-rnti.fr/render_pdf.php?p=1001501	majorité modèle langue appliquer recherchedinformation reposer lhypothèse dindépendance motsplu précisément modèle estimer partir simplesapparaisser dan document considérer éventuel relationssémantiqu conceptuel Pour pallier problème grandesapproche explorer   intégrer dépendance dordresurfacique entrer second reposer lutilisation ressourcessémantique capturer dépendance entrer modeler delangue présenter dan article sinscrit dan second approchenous proposer dintégrer dépendance entrer représenter lesdocument requête concept
670	Revue des Nouvelles Technologies de l'Information	EGC	2010	Modélisation et interrogation de données XML multidimensionnelles	XML étant devenu omniprésent et ses techniques de stockage et d'interrogationde plus en plus efficaces, le nombre de cas d'utilisations de ces technologiesaugmente tous les jours. Un sujet prometteur est l'intégration d'XML etdes entrepôts de données, dans laquelle une base de données XML native stockeles données multidimensionnelles et exécute des requêtes OLAP écrites à l'aidedu langage d'interrogation XML XQuery. Ce papier explore les questions quipeuvent survenir lors de l'implémentation d'un tel entrepôt de données XML.	Boris Verhaegen, Esteban Zimányi, Serge Boucher	http://editions-rnti.fr/render_pdf.php?p1&p=1001327	http://editions-rnti.fr/render_pdf.php?p=1001327	xml devenir omnipréser technique stockage dinterrogationde plaire plaire efficace nombre cas dutilisation technologiesaugmente tou jour prometteur lintégration dxml etde entrepôt donnée dan baser donnée xml natif stockel donnée multidimensionnel exécuter requête olap écrit laidedu langage dinterrogation XML xquery papier explorer question quipeuvent survenir limplémentation dun entrepôt donnée xml
671	Revue des Nouvelles Technologies de l'Information	EGC	2010	Mysins : Make Your Semantic INformation System		Anthony Ventresque, Thomas Cerqueus, Louis-Alexandre Celton, Gaëtan Hervouet, Damien Levin, Philippe Lamarre, Sylvie Cazalens	http://editions-rnti.fr/render_pdf.php?p1&p=1001382	http://editions-rnti.fr/render_pdf.php?p=1001382	
672	Revue des Nouvelles Technologies de l'Information	EGC	2010	Objective Novelty of Association Rules: Measuring the Confidence Boost1	On sait bien que la confiance des régles d'association n'est pas vraimentsatisfaisant comme mésure d'interêt. Nous proposons, au lieu de la substituerpar des autres mésures (soit, en l'employant de façon conjointe a desautres mésures), évaluer la nouveauté de chaque régle par comparaison de saconfiance par rapport á des régles plus fortes qu'on trouve au même ensemblede données. C'est á dire, on considère un seuil “relative” de confiance au lieu duseuil absolute habituel. Cette idée se précise avec la magnitude du “confidenceboost”, mésurant l'increment rélative de confiance prés des régles plus fortes.Nous prouvons que nôtre proposte peut remplacer la “confidence width” et leblockage de régles employés a des publications précedentes.	José L Balcazar 	http://editions-rnti.fr/render_pdf.php?p1&p=1001308	http://editions-rnti.fr/render_pdf.php?p=1001308	savoir confiance régle dassociation nest vraimentsatisfaiser mésure dinterêt proposer lieu substituerpar mésure lemployer conjoint desautr mésur évaluer nouveauté régle comparaison saconfiance rapport á régle plaire fort quon trouver ensemblede donner Cest á considérer seuil “ relatif ” confiance lieu duseuil absolut habituel idée préciser magnitude “ confidenceboost ” mésurer lincrement rélativ confiance pré régle plaire fortesnous prouver propost pouvoir remplacer “ confidence width ” leblockage régle employé publication précedent
673	Revue des Nouvelles Technologies de l'Information	EGC	2010	OSOM : un algorithme de construction de cartes topologiques recouvrantes	Les modèles de classification recouvrante ont montré leur capacité àgénérer une organisation plus fidèle aux données tout en conservant la simplificationattendue par une structuration en classes strictes. Par ailleurs les modèlesneuronaux non-supervisés sont plébiscités lorsqu'il s'agit de visualiser la structurede classes.Nous proposons dans cette étude d'étendre les cartes auto-organisatrices traditionnellesaux cartes auto-organisatrices recouvrantes. Nous montrons que cettenouvelle structure apporte des solutions à certaines problématiques spécifiquesen classification recouvrante (nombre de classes, complexité, cohérence des recouvrements).L'algorithme OSOM s'inspire de la version recouvrante des nuées dynamiqueset de l'approche de Kohonen pour générer de telles cartes recouvrantes. Nousdiscutons du modèle proposé d'un point de vue théorique (fonction d'énergieassociée, complexité, ...). Enfin nous présentons un cadre d'évaluation généraleque nous utilisons pour valider les résultats obtenus sur des données réelles.	Guillaume Cleuziou	http://editions-rnti.fr/render_pdf.php?p1&p=1001272	http://editions-rnti.fr/render_pdf.php?p=1001272	modèle classification recouvrant montrer capacité àgénérer organisation plaire fidèle donnée conserver simplificationattendue structuration classe strict Par modèlesneuronaux nonsuperviser plébisciter lorsquil sagit visualiser structurede classesNous proposer dan étude détendre carte autoorganisatrice traditionnellesaux cart autoorganisatrice recouvranter montrer cettenouvell structurer apporter solution problématique spécifiquesen classification recouvrant nombre classe complexité cohérence recouvrementslalgorithme OSOM sinspir version recouvrante nuée dynamiqueset lapproche Kohonen générer carte recouvrant nousdiscuton modeler proposer dun poindre théorique fonction dénergieassocier complexité   présenter cadrer dévaluation généralequ utiliser valider résultat obtenir donnée réel
674	Revue des Nouvelles Technologies de l'Information	EGC	2010	Pattern Mining: The Past, Present, and Future	Pattern mining is one of the fundamental techniques in data mining. As one increases thecomplexity of the pattern types, from subsets, to subsequences, subtrees, and subgraphs, onediscovers potentially more informative patterns. In this talk I will offer a tour of the past andthe present research landscape in this area, and I'll conclude with some thoughts on directionsfor the future	Mohammed Zaki	http://editions-rnti.fr/render_pdf.php?p1&p=1001258	http://editions-rnti.fr/render_pdf.php?p=1001258	Pattern mining is one of the fundamental technique in dater mining one increas thecomplexity of the pattern type from subset to subsequence subtre and subgraph onediscover potentially more informatif pattern In this talk ie will offer tour of the past andthe present research landscape in this area and Ill conclude with some thought directionsfor the futur
675	Revue des Nouvelles Technologies de l'Information	EGC	2010	PCAR : Nouvelle Approche de Génération de Règles d'Association Cycliques	Les règles d'association cycliques vise la découverte de nouvelles relationsentre des produits qui varient d'une façon régulièrement cyclique dans letemps. Dans ce cadre, nous introduisons, un nouvel algorithme nommé PCARcaractérisé par sa performance et son aspect incrémental. L'étude empirique quenous avons menée montre la robustesse et l'efficacité de notre algorithme proposévs. ceux de la littérature	Mohamed Salah Gouider, Eya Ben Ahmed	http://editions-rnti.fr/render_pdf.php?p1&p=1001415	http://editions-rnti.fr/render_pdf.php?p=1001415	règle dassociation cyclique viser découvrir relationsentre produit varier dune régulièrement cyclique dan letemp Dans cadrer introduire nouvel algorithme nommer pcarcaractériser performance aspect incrémental Létude empirique quenous mener montrer robustesse lefficacité algorithme proposév littérature
676	Revue des Nouvelles Technologies de l'Information	EGC	2010	PGP-mc : extraction parallèle efficace de motifs graduels	Initialement utilisés pour les systèmes de commande, les règles et motifsgraduels (de la forme “plus une personne est âgée, plus son salaire est élevé”)trouvent de très nombreuses applications, par exemple dans les domainesde la biologie, des données en flots (e.g. issues de réseaux de capteurs), etc. Trèsrécemment, des algorithmes ont été proposés pour extraire automatiquementde tels motifs. Cependant, même si certains d'entre eux ont permis des gainsde performance importants, les algorithmes restent coûteux et ne permettentpas de traiter efficacement les bases de données réelles souvent très volumineuses(en nombre de lignes et/ou nombre d'attributs). Nous proposons doncdans cet article une méthode originale de recherche de ces motifs utilisant lemulti-threading pour exploiter au mieux les multiples coeurs présents dans laplupart des ordinateurs et serveurs actuels. L'efficacité de cette approche est validéepar une étude expérimentale.	Anne Laurent, Benjamin Négrevergne, Nicolas Sicard, Alexandre Termier	http://editions-rnti.fr/render_pdf.php?p1&p=1001336	http://editions-rnti.fr/render_pdf.php?p=1001336	initialement utiliser système commander règle motifsgraduel former “ plaire âger plaire salaire élevé”trouvent application exemple dan domainesde biologie donnée flot eg issu réseau capteur trèsrécemmer algorithme proposer extraire automatiquementd motif dentre permettre gainsde performance important algorithme ruer coûteux permettentpa traiter efficacement base donnée réel volumineusesen nombre ligne etou nombre dattributs proposer doncdans article méthode original rechercher motif utiliser lemultithreading exploiter mieux coeur présent dan laplupart ordinateur serveur actuel lefficacité approcher validéepar étude expérimental
677	Revue des Nouvelles Technologies de l'Information	EGC	2010	Prédiction de séries temporelles et applications à l'analyse de séquences vidéo		Rémi Auguste, Ahmed El Ghini, Ioan Marius Bilasco, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1001468	http://editions-rnti.fr/render_pdf.php?p=1001468	
678	Revue des Nouvelles Technologies de l'Information	EGC	2010	PretopoLib: la librairie JAVA de la Prétopologie	PretopoLib est une librairie JAVA implémentant les concepts de laprétopologie. Son intérêt réside dans la représentation de structures de donnéespermettant la manipulation des données par des opérations ensemblistes.Celle-ci offre un cadre de développement d'algorithmes efficaces pour la fouillede données, l'apprentissage topologique et la modélisation des systèmes complexes.	Sofiane Ben Amor, Vincent Levorato	http://editions-rnti.fr/render_pdf.php?p1&p=1001405	http://editions-rnti.fr/render_pdf.php?p=1001405	pretopolib librairie JAVA implémenter concept laprétopologie intérêt résider dan représentation structure donnéespermetter manipulation donnée opération ensemblistescelleci offrir cadrer développement dalgorithm efficace fouillede donner lapprentissage topologique modélisation système complexe
679	Revue des Nouvelles Technologies de l'Information	EGC	2010	Proposition d'opérateurs OLAP pour un modèle multidimensionnel à base d'objets complexes		Doulkifli Boukraâ, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1001407	http://editions-rnti.fr/render_pdf.php?p=1001407	
680	Revue des Nouvelles Technologies de l'Information	EGC	2010	Proposition d'une méthode de classification associative adaptative	La classification associative est une méthode de prédiction à base derègles issue de la fouille de règles d'association. Cette méthode est particulièrementintéressante car elle recherche de façon exhaustive les règles d'associationpertinentes qu'elle filtre pour ne garder que les règles d'association de classe(celles admettant pour conséquent une modalité de classe), qui sont utiliséescomme classifieur. Les connaissances produites sont ainsi directement interprétables.Des études antérieures montrent les inconvénients de cette approche,qu'il s'agisse de la génération massive de règles non utilisées ou de la mauvaiseprédiction de la classe minoritaire lorsque les classes sont déséquilibrées.Nous proposons une approche originale du type boosting de règles d'associationde classes qui utilise comme classifieur faible une base de règles significativesconstruites par un algorithme de génération d'itemsets fréquents qui se limiteà l'extraction des seules règles de classe significatives et qui prend en comptele déséquilibre des données. Des comparaisons avec d'autres méthodes de classificationassociative montrent que notre approche améliore la précision et lerappel.	Emna Bahri, Stéphane Lallich	http://editions-rnti.fr/render_pdf.php?p1&p=1001346	http://editions-rnti.fr/render_pdf.php?p=1001346	classification associatif méthode prédiction baser derègl issu fouiller règle dassociation méthode particulièrementintéressant rechercher exhaustif règle dassociationpertinent filtrer garder règle dassociation classecelle admettre conséquent modalité classer utiliséescomme classifieur connaissance produire interprétablesde étude antérieur montrer inconvénient approchequil sagiss génération massif règle utiliser mauvaiseprédiction classer minoritaire classe déséquilibréesnous proposer approcher original typer boosting règle dassociationde classe utiliser classifieur faible baser règle significativesconstruit algorithme génération ditemset fréquent limiteà lextraction règle classer significatif prendre comptele déséquilibrer donnée comparaison dautr méthode classificationassociative montrer approcher améliorer précision lerappel
681	Revue des Nouvelles Technologies de l'Information	EGC	2010	Protein Graph Repository	Protein Graph Repository (PGR) est i, outil bioinformatique sur le web permettant d'obtenir une nouvelle representation de protéines sous la forme de graphes d'acides aminés, une représentation plus simple et plus facile à étudier par les moyens informatiques et statistiques dédiés aux graphes. La génération des graphes est faite à partir d'un parseur appliqué sur des fichiers des protéines PDB extraits de la base Protein Data Bank et en precisant les parametres et la methode a utiliser. Les graphes generes sont ensuite enregistres dans un entrepot doté de moyens de recherche, de filtrage et de telechargement. PGR peut etre provisoirement consulte à l'adresse http://www.enode-edition.com/pgr/, il est spécialement dédié aux recherches intéressées à l'étude de données protéiques sous la forme de graphes et permettra donc de fournir des échantillons pour des travaux expérimentaux.	Wajdi Dhifli, Rabie Saidi	http://editions-rnti.fr/render_pdf.php?p1&p=1001403	http://editions-rnti.fr/render_pdf.php?p=1001403	Protein Graph Repository pgr ie outil bioinformatique web permettre dobtenir representation protéine sou former graphe dacide aminer représentation plaire simple plaire facile étudier moyen informatique statistique dédier graphe génération graphe faire partir dun parseur appliquer fichier protéine pdb extrait baser Protein Data Bank preciser parametre methode utiliser graphe gener ensuite enregistr dan entrepot doter moyen rechercher filtrage telechargement pgr pouvoir provisoirement consult ladresse httpwwwenodeeditioncompgr spécialement dédier recherche intéresser létude donnée protéique sou former graphe permettre fournir échantillon travail expérimental
682	Revue des Nouvelles Technologies de l'Information	EGC	2010	Recent Advances in Partitioning Clustering Algorithms for Interval-Valued Data		Francisco de Assis Tenório de Carvalho	http://editions-rnti.fr/render_pdf.php?p1&p=1001260	http://editions-rnti.fr/render_pdf.php?p=1001260	
683	Revue des Nouvelles Technologies de l'Information	EGC	2010	Recherche sémantique sur le Web basée sur l'ontologie Modulaire et le raisonnement à base de cas	Dans ce papier, nous présentons une approche de recherche sémantiquebasée sur les ontologies modulaires et le raisonnement à base de cas(RaPC). Un cas représente l'ensemble des requêtes similaires associées à leursrésultats pertinents. Les ontologies modulaires sont utilisées pour représenteret indexer les cas qui sont construits sur la base des requêtes antérieures et lesrésultats pertinents sélectionnés par les utilisateurs. La similarité à based'ontologies est utilisée pour retrouver les cas similaires à la requête utilisateuret pour fournir à celui-ci des propositions de reformulation de requêtes correspondantsà son besoin. La principale contribution de ce travail réside dans l'utilisationd'un mécanisme de RaPC et une représentation ontologique à deuxfins: l'amélioration de la recherche sémantique et l'enrichissement d'ontologiesà partir de cas. L'expérimentation de l'approche proposée montre que la précisionet le rappel des résultats se sont nettement améliorés.	Nesrine Ben Mustapha, Hajer Baazaoui Zghal, Marie-Aude Aufaure, Henda Ben Ghézala	http://editions-rnti.fr/render_pdf.php?p1&p=1001457	http://editions-rnti.fr/render_pdf.php?p=1001457	Dans papier présenter approcher rechercher sémantiquebaser ontologie modulaire raisonnement baser casrapc cas représenter lensembl requête similaire associer leursrésultat pertinent ontologie modulaire utiliser représenteret indexer cas construire baser requête antérieure lesrésultat pertinent sélectionner utilisateur similarité basedontologie utiliser retrouver cas similaire requête utilisateuret fournir celuici proposition reformulation requêt correspondantsà besoin principal contribution travail résider dan lutilisationdun mécanisme rapc représentation ontologique deuxfin lamélioration rechercher sémantique lenrichissement dontologiesà partir cas lexpérimentation lapproche proposer montrer précisionet rappel résultat nettement améliorer
684	Revue des Nouvelles Technologies de l'Information	EGC	2010	Reconnaissance de concepts basée sur l'apprentissage		Wahiba Ben Abdessalem Karaa, Bilel Bouchamia	http://editions-rnti.fr/render_pdf.php?p1&p=1001410	http://editions-rnti.fr/render_pdf.php?p=1001410	
685	Revue des Nouvelles Technologies de l'Information	EGC	2010	Réduction bi-directionnelle d'images - Vers une méthode d'extraction de caractéristiques multi-niveaux	Inspiré des performances du cerveau humain à identifier les élémentspar la vue, le problème de la réduction de la dimension dans le domaine de laperception visuelle consiste à extraire une quantité réduite des caractéristiquesd'un ensemble d'images afin de les identifier.Ce papier présente une approche innovante bi-directionnelle d'extraction de caractéristiquesd'images fondée sur l'utilisation partielle d'une méthode spatiotemporelle.Les expériences numériques appliquées sur 70000 images représentantdes chiffres écrits à la main ainsi que sur 698 images illustrant un visagesous différentes postures démontrent l'efficacité de notre approche à fortementréduire la dimension tout en conservant les relations intelligibles entre les objetsdes données, permettant même d'obtenir une meilleure classification à partir desversions réduites des images qu'à partir des versions originales	Marc Joliveau	http://editions-rnti.fr/render_pdf.php?p1&p=1001279	http://editions-rnti.fr/render_pdf.php?p=1001279	inspirer performance cerveau humain identifier élémentspar problème réduction dimension dan domaine laperception visuel consister extraire quantité réduire caractéristiquesdun ensemble dimag identifierce papier présenter approcher innovant bidirectionnel dextraction caractéristiquesdimage fonder lutilisation partiel dune méthode spatiotemporellele expérience numérique appliquer 70000 image représentantd chiffre écrit main 698 image illustrer visagesou postur démontrer lefficacité approcher fortementréduire dimension conserver relation intelligible entrer objetsde donner permettre dobtenir meilleur classification partir desversion réduire image quà partir version original
686	Revue des Nouvelles Technologies de l'Information	EGC	2010	REGLO : une nouvelle stratégie pour résumer un flux de séries temporelles	Les flux de séries temporelles sont aujourd'hui produits dans de nombreuxdomaines comme la finance (Zhu et Shasha (2002)), la surveillance deréseaux (Borgne et al. (2007); Airoldi et Faloutsos (2004)), la gestion de l'historiquedes usages fréquents (Giannella et al. (2003); Teng et al. (2003)), etc.Résumer de tels flux est devenu un domaine important qui permet de surveilleret d'enregistrer des informations fiables sur les séries observées. À ce jour, lamajorité des algorithmes de ce domaine s'est concentrée sur des résumés séparéset indépendants (Giannella et al. (2003); Zhu et Shasha (2002); Chen et al.(2002)), en accordant à chaque série le même espace en mémoire. Toutefois, lagestion de cet espace mémoire est un sujet important pour les flux de donnéeset une stratégie accordant la même quantité de mémoire à chaque série n'est pasforcément appropriée. Dans cet article, nous considérons que les séries doiventêtre en compétition vis à vis de l'espace mémoire, selon leur besoin de précision.Ainsi, nous proposons : (1) une stratégie de gestion de l'espace mémoireoptimisée et (2) une nouvelle méthode de résumé des séries temporelles par approximation.Dans ce but, nous observons à la fois l'erreur globale et les erreurslocales. La répartition de la mémoire suit les étapes suivantes : (1) recherchede la séquence la mieux représentée et (2) recherche de la partie à compresseren minimisant l'erreur. Nos expérimentations sur des données réelles montrentl'efficacité et la pertinence de notre approche.	Florent Masseglia, Alice Marascu, Yves Lechevallier	http://editions-rnti.fr/render_pdf.php?p1&p=1001294	http://editions-rnti.fr/render_pdf.php?p=1001294	flux série temporel aujourdhui produire dan nombreuxdomaine financer Zhu Shasha 2002 surveillance deréseaux Borgne al 2007 Airoldi faloutso 2004 gestion lhistoriquede usag fréquent Giannella al 2003 Teng al 2003 etcrésumer flux devenir domaine importer permettre surveilleret denregistrer information fiable série observer À jour lamajorité algorithme domaine sest concentrer résumé séparéset indépendant Giannella al 2003 zhu Shasha 2002 Chen al2002 accorder série espacer mémoire lagestion espacer mémoire importer flux donnéeset stratégie accorder quantité mémoire série nest pasforcément approprier Dans article considérer série doiventêtre compétition vis vis lespace mémoire besoin précisionainsi proposer   1 stratégie gestion lespace mémoireoptimiser 2 méthode résumer série temporel approximationDans boire observer lerreur global erreurslocale répartition mémoire étape   1 recherchede séquence mieux représenter 2 rechercher partir compresseren minimiser lerreur expérimentation donnée réel montrentlefficacité pertinence approcher
687	Revue des Nouvelles Technologies de l'Information	EGC	2010	Regrouper les données textuelles et nommer les groupes à l'aide de classes recouvrantes	Organiser les données textuelles et en tirer du sens est un défi majeuraujourd'hui. Ainsi, lorsque l'on souhaite analyser un débat en ligne ou unforum de discussion, on voudrait pouvoir rapidement voir quels sont les principauxthèmes abordés et la manière dont la discussion se structure autour d'eux.Pour cela, et parce que un même texte peut être associé à plusieurs thèmes, nousproposons une méthode originale pour regrouper les données textuelles en autorisantles chevauchements et pour nommer chaque groupe de manière lisible.La contribution principale de cet article est une méthode globale qui permet deréaliser toute la chaîne, partant des données textuelles brutes jusqu'à la caractérisationdes groupes à un niveau sémantique qui dépasse le simple ensemble demots.	Marian-Andrei Rizoiu, Julien Velcin, Jean-Hugues Chauchat	http://editions-rnti.fr/render_pdf.php?p1&p=1001361	http://editions-rnti.fr/render_pdf.php?p=1001361	organiser donnée textuel tirer sens défi majeuraujourdhui lon souhaiter analyser débattre ligne unforum discussion vouloir pouvoir rapidement voir principauxthème abordé manière discussion structurer autour deuxPour celer texte pouvoir associer thème nousproposon méthode original regrouper donnée textuel autorisantl chevauchement nommer grouper manière lisiblela contribution principal article méthode global permettre deréaliser chaîner partir donnée textuel brut jusquà caractérisationde groupe niveau sémantique dépasser simple ensemble demot
688	Revue des Nouvelles Technologies de l'Information	EGC	2010	Requêtes skyline avec prise en compte des préférences utilisateurs pour des données volumineuses	"Appréhender, parcourir des données ou des connaissances reste unetâche difficile en particulier lorsque les utilisateurs sont confrontés à de gros volumesde données. De nombreux travaux se sont intéressés à extraire des points""skylines"" comme outil de restitution. La prise en compte des préférences a retenul'attention des travaux les plus récents mais les solutions existantes restenttrès consommatrices en terme de stockage d'informations additionnelles afind'obtenir des délais raisonnables de réponse aux requêtes. Notre proposition,EC2Sky (Efficient computation of compromises), se focalise sur deux points :(1) comment répondre efficacement à des requêtes de type skyline en présencede préférences utilisateurs malgré de gros volumes de données (aussi bien enterme de dimensions que de préférences) ; (2) comment restituer les connaissancesles plus pertinentes en soulignant les compromis associés aux préférencesspécifiées."	Tassadit Bouadi, Sandra Bringay, Pascal Poncelet, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001325	http://editions-rnti.fr/render_pdf.php?p=1001325	appréhender parcourir donnée connaissance rester unetâch difficile utilisateur confronter gros volumesd donné travail intéresser extraire pointsskyline outil restitution priser compter préférence retenulattention travail plaire récent solution existant restenttrès consommatrice terme stockage dinformation additionnel afindobtenir délai raisonnable réponse requêt propositionEC2Sky efficient computation of compromis focaliser point 1 répondre efficacement requête typer skyline présencede préférenc utilisateur gros volume donnée enterme dimension préférence   2 restituer connaissancesle plaire pertinent souligner compromis associé préférencesspécifier
689	Revue des Nouvelles Technologies de l'Information	EGC	2010	Résumé généraliste de flux de données	Lorsque le volume des données est trop important pour qu'elles soient stockéesdans une base de données, ou lorsque leur fréquence de production est élevée, les Systèmesde Gestion de Flux de Données (SGFD) permettent de capturer des flux d'enregistrementsstructurés et de les interroger à la volée par des requêtes permanentes (exécutées de façoncontinue). Mais les SGFD ne conservent pas l'historique des flux qui est perdu à jamais.Cette communication propose une définition formelle de ce que devrait être un résumé généralistede flux de données. La notion de résumé généraliste est liée à la capacité de répondreà des requêtes variées et de réaliser des tâches variées de fouille de données, en utilisant lerésumé à la place du flux d'origine. Une revue de plusieurs approches de résumés est ensuiteréalisée dans le cadre de cette définition.	Christine Potier, Georges Hébrail	http://editions-rnti.fr/render_pdf.php?p1&p=1001301	http://editions-rnti.fr/render_pdf.php?p=1001301	Lorsque volume donnée importer stockéesdans baser donnée fréquence production élevé Systèmesde gestion flux donnée sgfd permettre capturer flux denregistrementsstructuré interroger voler requête permanent exécuter façoncontinue Mais sgfd conserver lhistoriqu flux perdre jamaiscette communication proposer définition formel devoir résumer généralistede flux donnée notion résumer généraliste lier capacité répondreà requête variée réaliser tâche varier fouiller donnée utiliser lerésumer placer flux dorigine revu approche résumé ensuiteréaliser dan cadrer définition
690	Revue des Nouvelles Technologies de l'Information	EGC	2010	RSS Merger		Fekade Getahun, Richard Chbeir	http://editions-rnti.fr/render_pdf.php?p1&p=1001396	http://editions-rnti.fr/render_pdf.php?p=1001396	
691	Revue des Nouvelles Technologies de l'Information	EGC	2010	SALINES : un automate au service de l'extraction de motifs séquentiels multidimensionnels	Les entrepôts de données occupent aujourd'hui une place centrale dans le processus décisionnel.Outre leur consultation, une des finalités des entrepôts est de servir de socle aux techniquesde fouilles de données. Malheureusement, les approches existantes exploitent peu les particularitésdes entrepôts (multidimensionnalité, hiérarchies et données historiques). Parmi ces méthodes, l'extractionde motifs séquentiels multidimensionnels a récemment été étudiée. Nous montrons dans cetarticle que ces dernières ne tirent pas pleinement profit des hiérarchies et ne découvrent par conséquentqu'une partie seulement des motifs qualitativement intéressants. Nous proposons alors uneméthode d'extraction de motifs séquentiels multidimensionnels basée sur un automate et extrayantde nouveaux motifs. Les différentes expérimentations menées sur des jeux de données synthétiquesattestent des bonnes performances de notre proposition.	Yoann Pitarch, Lionel Vinceslas, Anne Laurent, Pascal Poncelet, Jean-Emile Symphor	http://editions-rnti.fr/render_pdf.php?p1&p=1001265	http://editions-rnti.fr/render_pdf.php?p=1001265	entrepôt donnée occuper aujourdhui placer central dan processus décisionnelOutre consultation finalité entrepôt servir socle techniquesd fouille donnée malheureusement approche existant exploiter particularitésde entrepôt multidimensionnalité hiérarchi donnée historique Parmi méthode lextractionde motif séquentiel multidimensionnel récemment étudier montrer dan cetarticle dernière tirer pleinement profit hiérarchie découvrir conséquentquun partir motif qualitativement intéressant proposer uneméthode dextraction motif séquentiel multidimensionnel baser automate extrayantde motif expérimentation mener jeu donnée synthétiquesattestent performance proposition
692	Revue des Nouvelles Technologies de l'Information	EGC	2010	Sélection par entropie de descripteurs textuels pour la catégorisation de documents		Christophe Moulin, Christine Largeron	http://editions-rnti.fr/render_pdf.php?p1&p=1001406	http://editions-rnti.fr/render_pdf.php?p=1001406	
693	Revue des Nouvelles Technologies de l'Information	EGC	2010	Self-Clustering for Identification of Customer Purchase Behaviours	La segmentation d'une base client peut avoir différents objectifs etplusieurs segmentation peuvent être utiles pour décrire les clients ou pour s'adapteravec les stratégies commerciales d'une entreprise. Dans ce papier, nous présentonsun schéma expérimental visant à proposer un ensemble de segmentationsalternatives. Ces segmentations sont produites sur des données réelles par latransformation des données initiales, la génération et la sélection de différentessegmentations.	Guillem Lefait, Gilles Goncalves, M. Tahar Kechadi	http://editions-rnti.fr/render_pdf.php?p1&p=1001273	http://editions-rnti.fr/render_pdf.php?p=1001273	segmentation dune baser client pouvoir objectif etplusieur segmentation pouvoir utile décrire client sadapteravec stratégie commercial dune entreprendre Dans papier présentonsun schéma expérimental viser proposer ensemble segmentationsalternativ segmentation produire donnée réel latransformation donnée initial génération sélection différentessegmentation
694	Revue des Nouvelles Technologies de l'Information	EGC	2010	SequencesViewer : comment rendre accessible des motifs séquentiels de gènes trop nombreux	Les techniques d'extraction de connaissances appliquées aux gros volumesde données, issus de l'analyse de puces ADN, permettent de découvrirdes connaissances jusqu'alors inconnues. Or, ces techniques produisent de trèsnombreux résultats, difficilement exploitables par les experts. Nous proposonsun outil dédié à l'accompagnement de ces experts dans l'appropriation et l'exploitationde ces résultats. Cet outil est basé sur trois techniques de visualisation(nuages, systèmes solaire et treemap) qui permettent aux biologistes d'appréhenderde grandes quantités de motifs séquentiels (séquences ordonnées de gènes).	Arnaud Sallaberry, Nicolas Pecheur, Sandra Bringay, Mathieu Roche, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001322	http://editions-rnti.fr/render_pdf.php?p=1001322	technique dextraction connaissance appliquer gros volumesd donner issu lanalyse puce ADN permettre découvrirdes connaissance jusqualors inconnu Or technique produire trèsnombreux résultat difficilement exploitable expert proposonsun outil dédier laccompagnement expert dan lappropriation lexploitationde résultat outil baser technique visualisationnuage système solaire treemap permettre biologist dappréhenderd grand quantité motif séquentiel séquence ordonner gène
695	Revue des Nouvelles Technologies de l'Information	EGC	2010	SIAM: Système d'Indexation des Articles Médicaux		Jihen Majdoubi, Mohamed Tmar, Faïez Gargouri	http://editions-rnti.fr/render_pdf.php?p1&p=1001447	http://editions-rnti.fr/render_pdf.php?p=1001447	
696	Revue des Nouvelles Technologies de l'Information	EGC	2010	Simplification de données de vol pour un stockage optimal et une visualisation accélérée	Le projet RECORDS (collaboration entre industriels et université) apour objectif de développer une infrastructure de service sécurisée pour assurerle suivi et l'analyse des conditions d'utilisation d'aéronefs. Chaque aéronefest muni de capteurs. Au cours de chaque mission (vol) les données mesuréessont enregistrées localement. Ces dernières sont par la suite transférées dansune base de données centralisée à des fins d'analyse. Le problème rencontré estla grande quantité de données ainsi enregistrées, ce qui en rend l'exploitationdifficile. Dans cet article, nous proposons des techniques de compression et desimplification de données avec un taux de perte contrôlé. Nos expérimentationsmontrent des gains drastiques en volumétrie avec de très faibles pertes d'informations.Ceci représente une première étape avant d'appliquer des techniquesd'extraction de connaissances.	Ibrahim Chahid, Loic Martin, Sofian Maabout, Mohamed Mosbah	http://editions-rnti.fr/render_pdf.php?p1&p=1001324	http://editions-rnti.fr/render_pdf.php?p=1001324	projet record collaboration entrer industriel université apour objectif développer infrastructure service sécurisé assurerle lanalyse condition dutilisation daéronef aéronefest munir capteur cours mission vol donnée mesuréessont enregistrer localement dernière suite transférer dansun baser donnée centraliser fin danalyse problème rencontrer estla grand quantité donnée enregistrer lexploitationdifficil Dans article proposer technique compression desimplification donnée taux perte contrôler expérimentationsmontrer gain drastique volumétrie faible perte dinformationsceci représenter étape dappliquer techniquesdextraction connaissance
697	Revue des Nouvelles Technologies de l'Information	EGC	2010	SimTole	La plateforme SimTOLE est dédiee a l'evaluation d'algorithmes d'alignement d'ontologies heterogenes et reparties a travers un reseau pair a pair (P2P). Cette plateforme permet de simuler un réseau P2P dans lequel chaque pair dispose de sa propre ontologie ainsi que des outils permettant l'alignement entre l'ontologie locale et une ontologie stockée sur un pair distant. Le developpement de cette plateforme s'inscrit dans le cadre de travaux de recherche étudiant l'impact de la topologie du réseau P2P dans le processus d'inférence de correspondances sémantiques. Durant cette démonstration, la plateforme simTole est présentée puis testée pour illustrer des scénarii montrant comment affiner le processus d'alignement d'ontologies dans un réseau P2P.	Nicolas Lumineau, Lionel Médini	http://editions-rnti.fr/render_pdf.php?p1&p=1001390	http://editions-rnti.fr/render_pdf.php?p=1001390	plateforme SimTOLE dédie levaluation dalgorithmer dalignemer dontologie heterogen repartir travers reseau pair pair p2p plateforme permettre simuler réseau p2p dan pair disposer propre ontologie outil permettre lalignement entrer lontologie local ontologie stocker pair dister developpement plateforme sinscrit dan cadrer travail rechercher étudier limpact topologie réseau p2p dan processus dinférence correspondance sémantique Durant démonstration plateforme simtole présenter pouvoir tester illustrer scénario montrer affiner processus dalignemer dontologier dan réseau p2p
698	Revue des Nouvelles Technologies de l'Information	EGC	2010	SoTree : Auto-organisation topologique et hiérarchique des données	Nous proposons dans cet article d'introduire une nouvelle approche pour la classification non supervisée hiérarchique. Notre méthode nommée So-Tree consiste à construire, d'une manière autonome et simultanée, une partition topologique et hiérarchique des données. Chaque ”cluster” de la partition est associé à une cellule d'une grille 2D et est modélisé par un arbre, dont chaque noeud représente une donnée. Nous évaluerons les capacités et les performances de notre approche sur des données aux difficultés variables. Les résultats préliminaires obtenus sont encourageants et prometteurs pour continuer dans cette direction.	Mustapha Lebbah, Hanane Azzag	http://editions-rnti.fr/render_pdf.php?p1&p=1001358	http://editions-rnti.fr/render_pdf.php?p=1001358	proposer dan article dintroduir approcher classification superviser hiérarchique méthode nommé sotree consister construire dune manière autonome simultaner partition topologique hiérarchique donnée ” cluster ” partition associer cellule dune griller 2D modéliser arbre noeud représenter donner évaluer capacité performance approcher donnée difficulté variable résultat préliminaire obtenir encourageant prometteur continuer dan direction
699	Revue des Nouvelles Technologies de l'Information	EGC	2010	Sous-échantillonnage topographique par apprentissage semi-supervisé	Plusieurs aspects pourraient influencer les systèmes d'apprentissage existants.Un de ces aspects est lié au déséquilibre des classes dans lequel le nombre d'observationsappartenant à une classe, dépasse fortement celui des observations dans les autresclasses. Dans ce type de cas assez fréquent, le système d'apprentissage a des difficultésau cours de la phase d'entraînement liées au déséquilibre inter-classe. Nous proposonsune méthode de sous-échantillonnage adaptatif pour traiter ce type de bases déséquilibrées.Le processus procède par le sous-échantillonnage des données majoritaires, guidépar les données minoritaires tout au long de la phase d'un apprentissage semi-supervisée.Nous utilisons comme modèle d'apprentissage les cartes auto-organisatrices. L'approcheproposée a été validée sur plusieurs bases de données en utilisant les arbres de décisioncomme classificateur avec une validation croisée. Les résultats expérimentaux ont montrédes performances très prometteuses.	Younès Bennani, Mustapha Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1001277	http://editions-rnti.fr/render_pdf.php?p=1001277	aspect pouvoir influencer système dapprentissage existantsun aspect lier déséquilibrer classe dan nombre dobservationsappartener classer dépasser fortement observation dan autresclasse Dans typer cas fréquent système dapprentissage difficultésau cours phase dentraînement lier déséquilibrer interclass proposonsune méthode souséchantillonnage adaptatif traiter typer base déséquilibréesLe processus procéder souséchantillonnage donnée majoritaire guidépar donnée minoritaire long phase dun apprentissage semisuperviséenous utiliser modeler dapprentissage carte autoorganisatrice lapprocheproposer valider base donnée utiliser arbre décisioncomme classificateur validation croiser résultat expérimental montréd performance prometteur
700	Revue des Nouvelles Technologies de l'Information	EGC	2010	Suivi d'Automobiles par Classification Hiérarchique Ascendante		Abdelmalek Toumi, Christophe Osswald, Ali Khenchaf	http://editions-rnti.fr/render_pdf.php?p1&p=1001429	http://editions-rnti.fr/render_pdf.php?p=1001429	
701	Revue des Nouvelles Technologies de l'Information	EGC	2010	Système d'extraction des connaissances à partir des données temporelles basé sur les Réseaux Bayésiens Dynamiques	Un grand nombre d'informations qui ont une structure complexeproviennent de diverses sources. Ces informations contiennent des connaissancestrès utiles pour l'aide à la décision. L'Extraction des Connaissances àpartir des Données (ECD), permet d'acquérir des informations pertinentes pourles systèmes interactifs d'aide à la décision (SIAD). Mais, dans plusieurs domaines,les données évoluent d'une manière dynamique et finissent par dépendrede plusieurs dimensions. Les Réseaux Bayésiens dynamiques (RBD)sont des modèles représentant des connaissances incertaines sur des phénomènescomplexes de processus dynamiques. Notre objectif revient à fixer lesmeilleures modèles de connaissances extraites par les RBD et à les utiliserpour la prise de décision dynamique. Ainsi, Nous proposons dans cet articleune démarche pour la mise en place d'un processus d'extraction des connaissancesà partir des données multidimensionnelles et temporelles.	Ghada Trabelsi, Mounir Ben Ayed, Adel M. Alimi	http://editions-rnti.fr/render_pdf.php?p1&p=1001299	http://editions-rnti.fr/render_pdf.php?p=1001299	grand nombre dinformation structurer complexeprovienner source information contenir connaissancestrè utile laid décision lextraction connaissance àpartir donnée ecd permettre dacquérir information pertinent pourl système interactif daid décision SIAD Mais dan domainesl donnée évoluer dune manière dynamique finir dépendrede dimension réseau Bayésiens dynamique rbdsont modèle représenter connaissance incertaine phénomènescomplexe processus dynamique objectif revenir fixer lesmeilleur modèle connaissance extraire RBD utiliserpour priser décision dynamique proposer dan articleune démarcher miser placer dun processus dextraction connaissancesà partir donnée multidimensionnel temporel
702	Revue des Nouvelles Technologies de l'Information	EGC	2010	Tulip: a Scalable Graph Visualization Framework	The Graph Visualization Framework Tulip now enjoys 10 years ofuser experience, and has matured its architecture and development cycle. Originallydesigned to interactively navigate large graphs, the framework integratesstate-of-the-art software engineering concepts and good practices. It offers alarge panel of graphical representations (traditional graph drawing as well asalternate representations). Tulip is most useful in a data mining and knowledgediscovery context, allowing users to easily add their own data analysis and computingroutines through its plug-in architecture.	David Auber, Patrick Mary, Morgan Mathiaut, Jonathan Dubois, Antoine Lambert, Dan Archambault, Romain Bourqui, Bruno Pinaud, Maylis Delest, Guy Melançon	http://editions-rnti.fr/render_pdf.php?p1&p=1001374	http://editions-rnti.fr/render_pdf.php?p=1001374	The Graph Visualization Framework Tulip now enjoy 10 year ofuser experience and has matured its architecturer and developmer cycle Originallydesigned to interactively navigate large graph the framework integratesstateoftheart softwar engineering concept and good practice it offer alarge panel of graphical representation traditional graph drawing well asalternate representater Tulip is most useful in dater mining and knowledgediscovery context allowing user to easily add their own dater analysis and computingroutiner through it plugin architecturer
703	Revue des Nouvelles Technologies de l'Information	EGC	2010	Un modèle d'extraction de masses de croyance à partir de probabilités a posteriori pour une amélioration des performances en classification supervisée	L'objectif de cet article est de montrer que l'utilisation de la règle dedécision du maximum de masse de croyance en lieu et place de celle du maximumde probabilité a posteriori peut permettre de réduire le taux d'erreur en classificationsupervisée. Nous proposons une technique efficace pour extraire, à partird'un vecteur de probabilités a posteriori, un vecteur de masses de croyance surlequel baser la décision par le maximum de masse de croyance. L'applicationde notre méthode dans le domaine de la classification automatique en stades desommeil montre une amélioration des performances pouvant atteindre 80% deréduction du taux d'erreur de classification.	Teh Amouh, Monique Noirhomme-Fraiture, Benoît Macq	http://editions-rnti.fr/render_pdf.php?p1&p=1001349	http://editions-rnti.fr/render_pdf.php?p=1001349	Lobjectif article montrer lutilisation régler dedécision maximum masser croyance lieu placer maximumde probabilité posteriori pouvoir permettre réduire taux derreur classificationsuperviser proposer technique efficace extraire partirdun vecteur probabilité posteriori vecteur masse croyance surlequel baser décision maximum masser croyance Lapplicationde méthode dan domaine classification automatique stade desommeil montrer amélioration performance pouvoir atteindre 80 deréduction taux derreur classification
704	Revue des Nouvelles Technologies de l'Information	EGC	2010	Un système d'aide à l'extraction de relations sémantiques pour la construction d'ontologies à partir de textes	Cet article présente une méthode d'extraction de relations sémantiquespour la construction d'ontologies à partir de corpus de textes. Notre objectif estde proposer une méthode générique, qui soit indépendante du domaine et de lalangue. Elle repose sur une analyse distributionnelle des unités sémantiques ducorpus pour faire émerger des relations sémantiques candidates. Cette méthodene fait aucune hypothèse sur les types de relations recherchées ni sur leur formelinguistique. Il s'agit de regrouper les associations de termes dans des classesqui représentent des relations sémantiques candidates. L'hypothèse sous-jacenteest que les occurrences de ces associations réunies sur la base des éléments decontexte qu'elles partagent ont des chances de relever d'une même relation sémantiqueet que les relations candidates ainsi proposées peuvent aider le travailde conceptualisation de l'ontologue	Rim Bentebibel, Adeline Nazarenko, Sylvie Szulman	http://editions-rnti.fr/render_pdf.php?p1&p=1001343	http://editions-rnti.fr/render_pdf.php?p=1001343	article présenter méthode dextraction relation sémantiquespour construction dontologier partir corpus texte objectif estde proposer méthode générique indépendant domaine lalangue reposer analyser distributionnel unité sémantique ducorpu faire émerger relation sémantique candidater méthodene faire hypothèse type relation rechercher formelinguistiqu sagit regrouper association terme dan classesqui représenter relation sémantique candidat Lhypothèse sousjacenteest occurrence association réunir baser élément decontext partager chance relever dune relation sémantiqueet relation candidat proposer pouvoir aider travailde conceptualisation lontologue
705	Revue des Nouvelles Technologies de l'Information	EGC	2010	Une approche fondée sur la corrélation entre prédicats pour le traitement des réponses pléthoriques	L'interrogation de bases de données, dont les dimensions ne cessentde croître, se heurte fréquemment au problème de la gestion des réponses pléthoriques.Une des approches envisageables pour réduire l'ensemble des résultatsretournés et le rendre exploitable est de contraindre la requête initiale parl'ajout de nouvelles conditions. L'approche présentée dans cet article s'appuiesur l'identification de liens de corrélation entre prédicats associés aux attributsde la relation concernée. La requête initiale peut ainsi être intensifiée automatiquementou par validation de l'utilisateur à travers l'ajout de prédicats prochessémantiquement de ceux spécifiés.	Patrick Bosc, Allel HadjAli, Olivier Pivert, Grégory Smits	http://editions-rnti.fr/render_pdf.php?p1&p=1001305	http://editions-rnti.fr/render_pdf.php?p=1001305	linterrogation base donnée dimension cessentde croîtr heurter fréquemment problème gestion réponse pléthoriquesun approche envisageable réduire lensembl résultatsretourné exploitable contraindre requête initial parlajout condition Lapproche présenter dan article sappuiesur lidentification lien corrélation entrer prédicat associé attributsde relation concerner requête initial pouvoir intensifier automatiquementou validation lutilisateur travers lajout prédicat prochessémantiquemer spécifié
706	Revue des Nouvelles Technologies de l'Information	EGC	2010	Une approche probabiliste pour l'identification de structures de communautés	Dans cet article, nous valorisons et défendons l'idée que les modèles génératifs sont une approche prometteuse pour l'identification de structure de communautés (ISC). Nous proposons un nouveau modèle probabiliste pour l'idenditification de structures de communautés qui utilise le lissage afin de pallier le petit nombre de liens entre les noeuds. Notre modèle étant très sensible aux paramètres de lissage, nous proposons également une méthode basée sur la modularité pour leur estimation. Les résultats expérimentaux obtenus sur 3 jeux de données montrent que notre modèle SPCE est largement meilleur que le modèle PHITS	Nacim Fateh Chikhi, Bernard Rothenburger, Nathalie Aussenac-Gilles	http://editions-rnti.fr/render_pdf.php?p1&p=1001289	http://editions-rnti.fr/render_pdf.php?p=1001289	Dans article valoriser défendon lider modèle génératif approcher prometteur lidentification structurer communauté isc proposer modeler probabiliste lidenditification structure communauté utiliser lissage pallier petit nombre lien entrer noeud modeler sensible paramètre lissage proposer également méthode baser modularité estimation résultat expérimental obtenir 3 jeu donnée montrer modeler SPCE largement meilleur modeler PHITS
707	Revue des Nouvelles Technologies de l'Information	EGC	2010	Une méthode d'aide au management de connaissances pour améliorer le processus de suivi et d'évaluation de la prise en charge précoce des enfants IMC : application de l'ASHMS		Mohamed Turki, Inès Saad, Gilles Kassel, Faïez Gargouri	http://editions-rnti.fr/render_pdf.php?p1&p=1001431	http://editions-rnti.fr/render_pdf.php?p=1001431	
708	Revue des Nouvelles Technologies de l'Information	EGC	2010	Une nouvelle approche de découverte des correspondances complexes entre ontologies	Les correspondances complexes ont été étudiées à plusieurs reprisesdans le domaine d'alignement de schémas de bases de données. Par contre,dans le domaine d'alignement des ontologies, elles ont été peu étudiées. Nousproposons, dans ce papier, une nouvelle approche de découverte de correspondancescomplexes entre deux ontologies. L'approche proposée est extensionnelle,terminologique et implicative. Dans cette approche, nous utilisons le modèledes règles d'association afin de découvrir des correspondances de typex &#8658; y1 &#8743; ... &#8743; yn entre deux ontologies.	Fatma Kaâbi, Faïez Gargouri	http://editions-rnti.fr/render_pdf.php?p1&p=1001414	http://editions-rnti.fr/render_pdf.php?p=1001414	correspondance complexe étudier reprisesdan domaine dalignemer schéma base donnée Par contredans domaine dalignemer ontologie étudier nousproposon dan papier approcher découvrir correspondancescomplexe entrer ontologie Lapproche proposer extensionnelleterminologiqu implicatif Dans approcher utiliser modèlede règle dassociation découvrir correspondance typex 8658 y1 8743   8743 yn entrer ontologie
709	Revue des Nouvelles Technologies de l'Information	EGC	2010	Une nouvelle stratégie d'Apprentissage Bayésienne	Dans cet article, une nouvelle stratégie d'apprentissage actif est proposée. Cette stratégie est fondée sur une méthode de discrétisation Bayésienne semi-supervisée. Des expériences comparatives sont menées sur des données unidimensionnelles, l'objectif étant d'estimer la position d'un échelon à partir de données bruitées.	Alexis Bondu, Vincent Lemaire, Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001463	http://editions-rnti.fr/render_pdf.php?p=1001463	Dans article stratégie dapprentissage actif proposer stratégie fonder méthode discrétisation Bayésienne semisupervisée expérience comparatif mener donnée unidimensionnel lobjectif destimer position dun échelon partir donnée bruiter
710	Revue des Nouvelles Technologies de l'Information	EGC	2010	Une Ontologie pour l'Acquisition et l'Exploitation des Connaissances en Conception Inventive	L'acquisition des connaissances en vue de résoudre des problèmesconcernant l'évolution des artefacts, comme elle se doit d'être pratiquée enconception inventive, a des caractéristiques spécifiques. Elle nécessite lasélection de certaines des connaissances qui peuvent induire des évolutions,elle amène à reformuler le problème initial afin de construire un modèleabstrait de l'artefact concerné. La méthode de conception inventive induite parla théorie de la Résolution des Problèmes Inventifs (aussi connue sousl'acronyme TRIZ) n'a pas encore fait l'objet d'une véritable formalisation.Nous proposons ici une ontologie des notions principales des concepts liés àl'acquisition des connaissances dans ce cadre. Cette ontologie, outre laclarification des notions en jeu, est utilisée comme support d'un environnementinformatique d'aide à la mise en oeuvre d'une méthode pour acquérir lesconnaissances et formuler les problèmes.	François Rousselot, Cecilia Zanni, Denis Cavallucci	http://editions-rnti.fr/render_pdf.php?p1&p=1001470	http://editions-rnti.fr/render_pdf.php?p=1001470	lacquisition connaissance résoudre problèmesconcernant lévolution artefact devoir dêtre pratiquer enconception inventif caractéristique spécifique nécessit lasélection connaissance pouvoir induire évolutionselle amène reformuler problème initial construire modèleabstrait lartefact concerner méthode conception inventif induire théorie résolution problème Inventifs connaître souslacronym TRIZ faire lobjet dune véritable formalisationnou proposer ontologie notion principal concept lier àlacquisition connaissance dan cadrer ontologie outrer laclarification notion jeu utiliser support dun environnementinformatiqu daid miser oeuvrer dune méthode acquérir lesconnaissance formuler problème
711	Revue des Nouvelles Technologies de l'Information	EGC	2010	Une structure basée sur les hiérarchies pour synthétiser les itemsets fréquents extraits dans des fenêtres temporelles	Le paradigme des flots de données rend impossible la conservation de l'intégralitéde l'historique d'un flot qu'il faut alors résumer. L'extraction d'itemsets fréquentssur des fenêtres temporelles semble tout à fait adaptée mais l'amoncellement des résultatsindépendants rend impossible l'exploitation de ces résultats. Nous proposons une structurebasée sur les hiérarchies des données afin d'unifiant ces résultats. De plus, puisque laplupart des données d'un flot présentent un caractère multidimensionnel, nous intégronsla prise en compte d'itemsets multidimensionnels. Enfin, nous pallions une faiblesse majeuredes Tilted TimeWindows (TTW) en prenant en compte la distribution des données.	Yoann Pitarch, Anne Laurent, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1001467	http://editions-rnti.fr/render_pdf.php?p=1001467	paradigme flot donnée impossible conservation lintégralitéde lhistoriqu dun flot quil falloir résumer lextraction ditemset fréquentssur fenêtre temporel sembler faire adapté lamoncellemer résultatsindépendant impossible lexploitation résultat proposer structurebasée hiérarchie donnée dunifier résultat De plaire laplupart donnée dun flot présenter caractère multidimensionnel intégronsler priser compter ditemset multidimensionnel pallier faiblesse majeurede Tilted TimeWindows ttw prendre compter distribution donnée
712	Revue des Nouvelles Technologies de l'Information	EGC	2010	Utilisation de graphes sémantiques pour l'extraction et la traduction des idées essentielles d'un texte		Romain André-Lovichi, Kamel Smaïli, David Langlois	http://editions-rnti.fr/render_pdf.php?p1&p=1001435	http://editions-rnti.fr/render_pdf.php?p=1001435	
713	Revue des Nouvelles Technologies de l'Information	EGC	2010	Vers une extraction et une visualisation des co-localisations adaptées aux experts	Une des tâches classiques en fouille de données spatiales est l'extractionde co-localisations intéressantes dans des données géo-référencées. L'objectifest de trouver des sous-ensembles de caractéristiques booléennes apparaissantfréquemment dans des objets spatiaux voisins. Toutefois, les relations découvertespeuvent ne pas être pertinentes pour les experts, et leur interprétation sousforme textuelle peut être difficile. Nous proposons, dans ce contexte, une nouvelleapproche pour intégrer la connaissance des experts dans la découverte desco-localisations, ainsi qu'une nouvelle représentation visuelle de ces motifs. Unprototype a été développé et intégré dans un SIG. Des expérimentations on étémenées sur des données géologiques réelles, et les résultats validés par un expertdu domaine.	Frédéric Flouvat, Nazha Selmaoui-Folcher, Dominique Gay	http://editions-rnti.fr/render_pdf.php?p1&p=1001335	http://editions-rnti.fr/render_pdf.php?p=1001335	tâche classique fouiller donnée spatial lextractionde colocalisations intéressant dan donnée géoréférencer Lobjectifest trouver sousensemble caractéristique booléen apparaissantfréquemmer dan objet spatial voisin relation découvertespeuvent pertinenter expert interprétation sousform textuel pouvoir difficile proposer dan contexte nouvelleapproche intégrer connaissance expert dan découvrir descolocalisation quune représentation visuel motif unprototype développer intégré dan sig expérimentation étémener donnée géologique réel résultat valider expertdu domaine
714	Revue des Nouvelles Technologies de l'Information	EGC	2010	Visual Sentence-Phrase-Based Document Representation for Effective and Efficient Content-Based Image Retrieval	Having effective and efficient methods to get access to desired imagesis essential nowadays with the huge amount of digital images. This paperpresents an analogy between content-based image retrieval and text retrieval.We make this analogy from pixels to letters, patches to words, sets of patchesto phrases, and groups of sets of patches to sentences. To achieve a more accuratedocument matching, more informative features including phrases and sentencesare needed to improve these scenarios. The proposed approach is basedfirst on constructing different visual words using local patch extraction and description.After that, we study different association rules between frequent visualwords in the context of local regions in the image to construct visual phrases,which will be grouped to different sentences.	Ismail Elsayad, Jean Martinet, Thierry Urruty, Chabane Djeraba	http://editions-rnti.fr/render_pdf.php?p1&p=1001284	http://editions-rnti.fr/render_pdf.php?p=1001284	Having effectif and efficient methods to get access to desired imagesi essential nowadays with the huge amount of digital image This paperpresent an analogy between contentbased imager retrieval and text retrievalwe mak this analogy from pixel to letter patche to word set of patchesto phras and group of set of patche to sentence To achieve more accuratedocument matching more informatif featur including phras and sentencesar needed to improve these scenarios The proposed approach is basedfirst constructing visual word using local patch extraction and descriptionafter that we study association ruler between frequent visualword in the context of local region in the imager to construct visual phraseswhich will be grouped to sentence
715	Revue des Nouvelles Technologies de l'Information	EGC	2010	Visualisation de mesures agrégées pour l'estimation de la qualité des articlesWikipedia	Wikipedia, devenue l'une des bases de connaissances les plus populaires,pose le problème de la fiabilité de l'information qu'elle dissémine. Nousproposons WikipediaViz, un ensemble de visualisations basé sur un mecanismede collecte et d'agrégation de données d'édition Wikipedia pour aider le lecteurà appréhender la maturité d'un article. Nous listons cinq métriques importantes,déterminées lors de sessions de conception participative avec des experts Wikipediapour juger de la qualité, que nous présentons au lecteur sous forme devisualisations compactes et expressives, dépeignant le profil d'évolution d'un article.Nos études utilisateur ont montré queWikipediaViz réduisait significativementle temps requis pour évaluer la qualité en maintenant une bonne précision	Fanny Chevalier, Stéphane Huot, Jean-Daniel Fekete	http://editions-rnti.fr/render_pdf.php?p1&p=1001316	http://editions-rnti.fr/render_pdf.php?p=1001316	Wikipedia devenir lune base connaissance plaire populairespose problème fiabilité linformation disséminer nousproposon wikipediaviz ensemble visualisation baser mecanismede collecter dagrégation donnée dédition Wikipedia aider lecteurà appréhender maturité dun article lister métrique importantesdéterminer session conception participatif expert Wikipediapour juger qualité présenter lecteur sou former devisualisation compact expressive dépeindre profil dévolution dun articlenos étude utilisateur montrer queWikipediaViz réduire significativementle temps requérir évaluer qualité maintenir précision
716	Revue des Nouvelles Technologies de l'Information	EGC	2010	WCUM pour l'analyse d'un site web	Dans ce papier, nous proposons une approche WCUM (Web Contentand Usage based Approach) permettant de relier l'analyse du contenu d'un siteWeb à l'analyse de l'usage afin de mieux comprendre les comportements de navigationsur le site. L'apport de ce travail réside d'une part dans la propositiond'une approche reliant l'analyse du contenu à l'analyse de l'usage et d'autre partdans l'extension de l'application des méthodes de block clustering, appliquéesgénéralement en bioinformatique, au contexte Web mining afin de profiter deleur pouvoir classificatoire dans la découverte de biclasses homogènes à partird'une partition des instances et une partition des attributs recherchées simultanément.	Malika Charrad, Yves Lechevallier, Mohamed Ben Ahmed, Gilbert Saporta	http://editions-rnti.fr/render_pdf.php?p1&p=1001413	http://editions-rnti.fr/render_pdf.php?p=1001413	Dans papier proposer approcher WCUM Web Contentand usage based Approach permettre relier lanalyse contenir dun siteweb lanalyse lusage mieux comprendre comportement navigationsur site Lapport travail résider dune partir dan propositiondune approcher relier lanalyse contenir lanalyse lusage dautre partdan lextension lapplication méthode block clustering appliquéesgénéralement bioinformatique contexte Web mining profiter deleur pouvoir classificatoire dan découvrir biclass homogène partirdune partition instance partition attribut rechercher simultanément
717	Revue des Nouvelles Technologies de l'Information	EGC	2009	A Contextualization Service for a Personalized Access Model	Personalization paradigm aims at providing users with the most rel-evant content and services according to many factors such as interest center orlocation at the querying time. All this knowledge and requirements are orga-nized into user profiles and contexts. A user profile encompasses metadata de-scribing the user whereas a context groups information about the environmentof interaction between the user and the system. An interesting problem is there-fore to identify which part of the profile is significant in a given context. Thispaper proposes a contextualization service which allows defining relationshipsbetween user preferences and contexts. Further, we propose an approach forthe automatic discovery of these mappings by analyzing user behavior extractedfrom log files.	Sofiane Abbar, Mokrane Bouzeghoub, Dimitre Kostadinov, Stéphane Lopes	http://editions-rnti.fr/render_pdf.php?p1&p=1000771	http://editions-rnti.fr/render_pdf.php?p=1000771	personalization paradigm aim at providing user with the most relever conter and service according to many factors such interest center orlocation at the querying time All this knowledge and requirement are organized into user profile and context A user profiler encompasser metadata describing the user wherea context group information about the environmentof interaction between the user and the system an interesting problem is therefore to identify which partir of the profil is significant in given context thispaper propos contextualization service which allow defining relationshipsbetween user preference and context Further we proposer an approach forthe automatic discovery of these mapping by analyzing user behavior extractedfrom log fil
718	Revue des Nouvelles Technologies de l'Information	EGC	2009	Accompagner au début du 21ème siècle les organisations dans la mise en place d'une gestion des connaissances : retour d'expérience	Cet article présente succinctement le retour d'expérience d'Ardansdans l'implantation de systèmes de gestion de connaissances dans des organisationstrès variées au début de ce 21ème siècle.	Alain Berger, Jean-Pierre Cotton, Pierre Mariot	http://editions-rnti.fr/render_pdf.php?p1&p=1000811	http://editions-rnti.fr/render_pdf.php?p=1000811	article présent succinctement dexpérience dArdansdans limplantation système gestion connaissance dan organisationstrè varier 21èm siècle
719	Revue des Nouvelles Technologies de l'Information	EGC	2009	Acquisition de la théorie ontologique d'un système d'extraction d'information	La conception de systèmes d'Extraction d'Information (EI) destinésà extraire les réseaux d'interactions géniques décrits dans la littérature scientifiqueest un enjeu important. De tels systèmes nécessitent des représentationssophistiquées, s'appuyant sur des ontologies, afin de définir différentes relationsbiologiques, ainsi que les dépendances récursives qu'elles présentent entre elles.Cependant, l'acquisition de ces dépendances n'est pas possible avec les techniquesd'apprentissage automatique actuellement employées en EI, car ces dernièresne gèrent pas la récursivité. Afin de palier ces limitations, nous présentonsune application à l'EI de la Programmation Logique Inductive, en mode multipredicats.Nos expérimentations, effectuées sur un corpus bactérien, conduisentà un rappel global de 67.7% pour une précision de 75.5%.	Alain-Pierre Manine	http://editions-rnti.fr/render_pdf.php?p1&p=1000788	http://editions-rnti.fr/render_pdf.php?p=1000788	conception système dextraction dinformation ei destinésà extraire réseau dinteraction génique décrire dan littérature scientifiqueest enjeu importer De système nécessiter représentationssophistiquée sappuyer ontologie définir relationsbiologique dépendance récursif quell présenter entrer ellescepender lacquisition dépendance nest techniquesdapprentissage automatique actuellement employer EI dernièresne gérer récursivité Afin palier limitation présentonsune application leu programmation Logique inductif mode multipredicatsnos expérimentation effectuer corpus bactérien conduisentà rappel global 677 précision 755
720	Revue des Nouvelles Technologies de l'Information	EGC	2009	Acquisition, annotation et exploration interactive d'images stéréoscopiques en réalité virtuelle : application en dermatologie	Nous présentons dans cet article le système Skin3D qui implémentetous les composants matériels et logiciels nécessaires pour extraire desinformations dans des images 3D de peau. Il s'agit à la fois du matérield'éclairage et d'acquisition à base d'appareils photographiquesstéréoscopiques, d'une méthode de calibration de caméras utilisant lesalgorithmes génétiques, de matériel de réalité virtuelle pour restituer lesimages en stéréoscopie et interagir avec elles, et enfin d'un ensemble defonctionnalités interactives pour annoter les images, partager ces annotations etconstruire un hypermédia 3D. Nous présentons une étude comparativeconcernant la calibration et une application réelle de Skin3D sur des images devisages.	Mohammed Haouach, Karim Benzeroual, Christiane Guinot, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1000774	http://editions-rnti.fr/render_pdf.php?p=1000774	présenter dan article système skin3d implémentetou composant matériel logiciel nécessaire extraire desinformater dan image 3d peau sagit matérieldéclairage dacquisition baser dappareils photographiquesstéréoscopiqu dune méthode calibration caméra utiliser lesalgorithm génétique matériel réalité virtuel restituer lesimage stéréoscopie interagir dun ensemble defonctionnalités interactif annoter image partager annotation etconstruire hypermédia 3D présenter étude comparativeconcerner calibration application réel skin3d image devisag
721	Revue des Nouvelles Technologies de l'Information	EGC	2009	Aggregative and Neighboring Approximations to Query Semi-Structured Documents	Structures heterogeneity in Web resources is a constant concern inelement retrieval (i.e. tag retrieval in semi-structured documents). In this paperwe present the SHIRI 1 querying approach which allows to reach more or lessstructured document parts without an a priori knowledge on their structuring.	Yassine Mrabet, Nathalie Pernelle, Nacéra Bennacer, Mouhamadou Thiam	http://editions-rnti.fr/render_pdf.php?p1&p=1000808	http://editions-rnti.fr/render_pdf.php?p=1000808	structur heterogeneity in Web resource is constant concern inelement retrieval ie tag retrieval in semistructured document In this paperwe preser the SHIRI 1 querying approach which allow to reach more or lessstructured document part without an priori knowledge their structuring
722	Revue des Nouvelles Technologies de l'Information	EGC	2009	An approach for handling risk and uncertainty in multiarmed bandit problems	An approach is presented to deal with risk in multiarmed bandit prob-lems. Specifically, the well known exploration-exploitation dilemma is solvedfrom the point of view of maximizing an utility function which measures thedecision maker's attitude towards risk and uncertain outcomes. A link withthe preference theory is thus established. Simulations results are provided forin order to support the main ideas and to compare the approach with existingmethods, with emphasis on the short term (small sample size) behavior of theproposed method.	Fabrice Clérot, Stefano Perabò	http://editions-rnti.fr/render_pdf.php?p1&p=1000744	http://editions-rnti.fr/render_pdf.php?p=1000744	an approach is presented to deal with risk in multiarmed bandit problems Specifically the well known explorationexploitation dilemma is solvedfrom the poindre of view of maximizing an utility function which measur thedecision maker attitude towards risk and uncertain outcom link withthe preference theory is thus established Simulations results are provided forin order to support the main ideer and to comparer the approach with existingmethods with emphasi the short term small sampl size behavior of theproposed method
723	Revue des Nouvelles Technologies de l'Information	EGC	2009	Analyse de dissimilarités par arbre d'induction	Dans cet article1, nous considérons des objets pour lesquels nous dis-posons d'une matrice des dissimilarités et nous nous intéressons à leurs liensavec des attributs. Nous nous centrons sur l'analyse de séquences d'états pourlesquelles les dissimilarités sont données par la distance d'édition. Toutefois, lesméthodes développées peuvent être étendues à tout type d'objets et de mesurede dissimilarités. Nous présentons dans un premier temps une généralisation del'analyse de variance (ANOVA) pour évaluer le lien entre des objets non mesu-rables (p. ex. des séquences) avec une variable catégorielle. La clef de l'approcheest d'exprimer la variabilité en termes des seules dissimilarités ce qui nous per-met d'identifier les facteurs qui réduisent le plus la variabilité. Nous présentonsun test statistique général qui peut en être déduit et introduisons une méthodeoriginale de visualisation des résultats pour les séquences d'états. Nous présen-tons ensuite une généralisation de cette analyse au cas de facteurs multiples et endiscutons les apports et les limites, notamment en terme d'interprétation. Fina-lement, nous introduisons une nouvelle méthode de type arbre d'induction quiutilise le test précédent comme critère d'éclatement. La portée des méthodesprésentées est illustrée à l'aide d'une analyse des facteurs discriminant le plusles trajectoires occupationnelles .	Gilbert Ritschard, Matthias Studer, Nicolas S. Müller, Alexis Gabadinho	http://editions-rnti.fr/render_pdf.php?p1&p=1000733	http://editions-rnti.fr/render_pdf.php?p=1000733	Dans article1 considérer objet disposer dune matrice dissimilarité intéresser liensavec attribut centrer lanalyse séquence détat pourlesquell dissimilarité donner distancer dédition lesméthod développée pouvoir étendre typer dobjet mesurede dissimilarités présenter dan temps généralisation delanalys variance ANOVA évaluer lien entrer objet mesurable ex séquence variable catégoriel clef lapprocheest dexprimer variabilité terme dissimilariter permettre didentifier facteur réduire plaire variabilité présentonsun test statistique général pouvoir déduire introduison méthodeoriginal visualisation résultat séquence détat présenter ensuite généralisation analyser cas facteur endiscuton apport limite terme dinterprétation finalement introduire méthode typer arbre dinduction quiutilise test précédent critèr déclatement porter méthodesprésentée illustrer laid dune analyser facteur discriminer plusl trajectoire occupationnel
724	Revue des Nouvelles Technologies de l'Information	EGC	2009	Analyse de données pour la construction de modèles de procédures neurochirurgicales	Dans cet article, nous appliquons une méthode d'analyse sur desdescriptions de procédures de neurochirurgie dans le but d'en améliorer lacompréhension. La base de données XML utilisée dans cette étude estconstituée de la description de 157 chirurgies de tumeurs. Trois cent vingtdeux variables ont été identifiées et décomposées en variables prédictives(connues avant l'opération) et variables à prédire (décrivant des gesteschirurgicaux). Une analyse factorielle des correspondances (AFC) a étéréalisée sur les variables prédictives, ainsi qu'un arbre de décision basé sur undendrogramme préalablement établi. Six classes principales de variablesprédictives ont ainsi été identifiées. Puis, pour chacune de ces classes, uneanalyse AFC a été réalisée sur les variables à prédire, ainsi qu'un arbre dedécision. Bien que le nombre de cas et le choix des variables constituent unelimite à cette étude, nous avons réussi à prédire certaines caractéristiques liéesaux procédures en partant de données prédictives.	Brivael Trelhu, Florent Lalys, Laurent Riffaud, Xavier Morandi, Pierre Jannin	http://editions-rnti.fr/render_pdf.php?p1&p=1000789	http://editions-rnti.fr/render_pdf.php?p=1000789	Dans article appliquer méthode danalyse desdescription procédure neurochirurgie dan boire den améliorer lacompréhension baser donnée XML utiliser dan étude estconstituer description 157 chirurgie tumeur Trois vingtdeux variable identifier décomposer variable prédictivesconnu lopération variable prédire décrire gesteschirurgicaux analyser factoriel correspondance AFC étéréaliser variable prédictive quun arbre décision baser undendrogramme préalablement établir classe principal variablesprédictive identifier Puis classe uneanalys AFC réaliser variable prédire quun arbre dedécision nombre cas choix variable constituer unelimite étude réussir prédire caractéristique liéesaux procédure partir donnée prédictif
725	Revue des Nouvelles Technologies de l'Information	EGC	2009	Analyse et application de modèles de régression pour optimiser le retour sur investissement d'opérations commerciales		Thomas Piton, Julien Blanchard, Henri Briand, Laurent Tessier, Gaëtan Blain	http://editions-rnti.fr/render_pdf.php?p1&p=1000810	http://editions-rnti.fr/render_pdf.php?p=1000810	
726	Revue des Nouvelles Technologies de l'Information	EGC	2009	Analyse et application de modèles de régression pour optimiser le retour sur investissement d'opérations commerciales	Les activités de négoce de matériaux sont un marché extrêmementcompétitif. Pour les acteurs de ce marché, les méthodes de fouille de donnéespeuvent s'avérer intéressantes en permettant de dégager des gains de rentabilitéimportants. Dans cet article, nous présenterons le retour d'expérience du projetde fouille de données mené chez VM Matériaux pour améliorer le retour surinvestissement d'opérations commerciales. La synergie des informaticiens, dumarketing et des experts métier a permis d'améliorer l'extraction des connais-sances à partir des données de manière à aboutir à la connaissance actionnable laplus pertinente possible et ainsi aider les experts métier à prendre des décisions.	Thomas Piton, Julien Blanchard, Henri Briand, Laurent Tessier, Gaëtan Blain	http://editions-rnti.fr/render_pdf.php?p1&p=1000735	http://editions-rnti.fr/render_pdf.php?p=1000735	activité négoce matériau marcher extrêmementcompétitif Pour acteur marcher méthode fouiller donnéespeuvent savérer intéressant permettre dégager gain rentabilitéimportant Dans article présenter dexpérience projetde fouiller donnée mener VM matériau améliorer surinvestissement dopération commercial synergie informaticien dumarketing expert métier permettre daméliorer lextraction connaissance partir donnée manière aboutir connaissance actionnable laplus pertinent aider expert métier prendre décision
727	Revue des Nouvelles Technologies de l'Information	EGC	2009	Analyse multigraduelle OLAP	Les systèmes décisionnels reposent sur des bases de données multidimensionnellesqui offrent un cadre adéquat aux analyses OLAP. L'articleprésente un nouvel opérateur OLAP nommé « BLEND » rendant possible desanalyses multigraduelles. Il s'agit de transformer la structuration multidimensionnellelors des interrogations pour analyser les mesures selon des niveauxde granularité différents recombinées comme un même paramètre. Nous menonsune étude des combinaisons valides de l'opération dans le contexte deshiérarchies strictes. Enfin, une première série d'expérimentations implantel'opération dans le contexte R-OLAP en montrant le faible coût de l'opération.	Gilles Hubert, Olivier Teste	http://editions-rnti.fr/render_pdf.php?p1&p=1000768	http://editions-rnti.fr/render_pdf.php?p=1000768	système décisionnel reposer base donnée multidimensionnellesqui offrir cadrer adéquat analyser OLAP larticleprésent nouvel opérateur OLAP nommer « BLEND » desanalys multigraduell sagit transformer structuration multidimensionnellelors interrogation analyser mesure niveauxde granularité recombinée paramètre menonsune étude combinaison valide lopération dan contexte deshiérarchier strict série dexpérimentation implantelopération dan contexte ROLAP montrer faible coût lopération
728	Revue des Nouvelles Technologies de l'Information	EGC	2009	Analyse sémantique spatio-temporelle pour les ontologies OWL-DL	L'analyse sémantique est un nouveau paradigmed'interrogation du Web Sémantique qui a pour objectif d'identifier lesassociations sémantiques reliant des individus décrits dans desontologies OWL-DL. Pour déduire davantage d'associationssémantiques et augmenter la précision de l'analyse, l'informationspatio-temporelle attachée aux ressources doit être prise en compte. Aces fins - et pour combler l'absence actuelle de raisonneurs spatiotemporeldéfini pour les ontologies RDF(S) et OWL-, nous proposonsle système de représentation et d'interrogation d'ontologies spatiotemporellesONTOAST, compatible avec le langage OWL-DL. Nousprésentons les principes de base de l'algorithme de découverted'associations sémantiques entre individus intégré dans ONTOAST.Cet algorithme utilise deux contextes, l'un spatial et l'autre temporelqui permettent d'affiner la recherche. Nous décrivons enfin l'approchemise en oeuvre pour la déduction de connexions spatiales entreindividus.	Alina Dia Miron, Jérôme Gensel, Marlène Villanova-Oliver	http://editions-rnti.fr/render_pdf.php?p1&p=1000784	http://editions-rnti.fr/render_pdf.php?p=1000784	lanalyse sémantique paradigmedinterrogation web Sémantique objectif didentifier lesassociation sémantique relier individu décrire dan desontologie owldl Pour déduir davantage dassociationssémantiqu augmenter précision lanalyse linformationspatiotemporell attacher ressource devoir priser compter Aces fin   combler labsence actuel raisonneur spatiotemporeldéfini ontologie RDFS OWL proposonsl système représentation dinterrogation dontologier spatiotemporellesontoast compatible langage OWLDL nousprésenton principe baser lalgorithme découvertedassociation sémantique entrer individu intégré dan ONTOASTCet algorithme utilis contexte lun spatial lautre temporelqui permettre daffiner rechercher décrire lapprochemise oeuvrer déduction connexion spatial entreindividus
729	Revue des Nouvelles Technologies de l'Information	EGC	2009	Assessing the uncertainty in knn Data Fusion		Tomàs Aluja-Banet, Josep Daunis-i-Estadella, Enric Ripoll	http://editions-rnti.fr/render_pdf.php?p1&p=1000794	http://editions-rnti.fr/render_pdf.php?p=1000794	
730	Revue des Nouvelles Technologies de l'Information	EGC	2009	Binary Sequences and Association Graphs for Fast Detection of Sequential Patterns	We develop an efficient algorithm for detecting frequent patterns thatoccur in sequence databases under certain constraints. By combining the useof bit vector representations of sequence databases with association graphs weachieve superior time and low memory usage based on a considerable reductionof the number of candidate patterns.	Dan A. Simovici, Selim Mimaroglu	http://editions-rnti.fr/render_pdf.php?p1&p=1000781	http://editions-rnti.fr/render_pdf.php?p=1000781	We develop an efficient algorithm for detecting frequent pattern thatoccur in sequence databaser under constraint by combining the useof bit vector representation of sequence databaser with association graph weachieve superior time and low memory usage based considerabl reductionof the number of candidat patterns
731	Revue des Nouvelles Technologies de l'Information	EGC	2009	Caractérisation automatique des classes découvertes en classification non supervisée	Dans cet article, nous proposons une nouvelle approche de classifi- cation et de pondération des variables durant un processus d'apprentissage non supervisé. Cette approche est basée sur le modèle des cartes auto-organisatrices. L'apprentissage de ces cartes topologiques est combiné à un mécanisme d'esti- mation de pertinences des différentes variables sous forme de poids d'influence sur la qualité de la classification. Nous proposons deux types de pondérations adaptatives : une pondération des observations et une pondération des distances entre observations. L'apprentissage simultané des pondérations et des prototypes utilisés pour la partition des observations permet d'obtenir une classification op- timisée des données. Un test statistique est ensuite utilisé sur ces pondérations pour élaguer les variables non pertinentes. Ce processus de sélection de variables permet enfin, grâce à la localité des pondérations, d'exhiber un sous ensemble de variables propre à chaque groupe (cluster) offrant ainsi sa caractérisation. L'approche proposée a été validé sur plusieurs bases de données et les résultats expérimentaux ont montré des performances très prometteuses.	Nistor Grozavu, Younès Bennani, Mustapha Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1000737	http://editions-rnti.fr/render_pdf.php?p=1000737	Dans article proposer approcher classifi cation pondération variable durer processus dapprentissage superviser approcher baser modeler carte autoorganisatrice Lapprentissage carte topologique combiner mécanisme desti mation pertinence variable sou former poids dinfluence qualité classification proposer type pondération adaptatif   pondération observation pondération distance entrer observation Lapprentissage simultaner pondération prototype utiliser partition observation permettre dobtenir classification op timiser donnée test statistique ensuite utiliser pondération élaguer variable pertinent processus sélection variable permettre grâce localité pondération dexhiber sou ensemble variable propre grouper cluster offrir caractérisation Lapproche proposer valider base donnée résultat expérimental montrer performance prometteur
732	Revue des Nouvelles Technologies de l'Information	EGC	2009	Ciblage des règles d'association intéressantes guidé par les connaissances du décideur	L'usage du modèle des règles d'association en fouille de données estlimité par la quantité prohibitive de règles qu'il fournit et nécessite la mise enplace d'une phase de post-traitement efficace afin de cibler les règles les plusutiles. Cet article propose une nouvelle approche intégrant explicitement lesconnaissances du décideur afin de filtrer et cibler les règles intéressantes.	Claudia Marinica, Fabrice Guillet	http://editions-rnti.fr/render_pdf.php?p1&p=1000805	http://editions-rnti.fr/render_pdf.php?p=1000805	lusage modeler règle dassociation fouiller donnée estlimiter quantité prohibitif règle quil fournir nécessiter miser enplace dune phase posttraitement efficace cibler règle plusutil article proposer approcher intégrer explicitement lesconnaissance décideur filtrer cibler règle intéressant
733	Revue des Nouvelles Technologies de l'Information	EGC	2009	CISNA un système hybride LD+Règles pour gérer des connaissances		Alexis Bultey, François Rousselot, Cecilia Zanni, Denis Cavallucci	http://editions-rnti.fr/render_pdf.php?p1&p=1000821	http://editions-rnti.fr/render_pdf.php?p=1000821	
734	Revue des Nouvelles Technologies de l'Information	EGC	2009	Classification des Images de Télédétection avec ENVI FX	D'importants volumes d'images satellites et aériennes de tout type(panchromatiques, multispectrales, hyperspectrales) sont généréesquotidiennement, et leur classification par des méthodes semi-automatiquesdevient nécessaire. Le logiciel ENVI Feature eXtractionTM (ENVI FXTM) sebase sur une approche « objet » -par opposition à une approche pixelsclassique- et sur des algorithmes innovants, pour la segmentation et laclassification des images de télédétection avec un haut niveau de précision.	Franck Le Gall, Damien Barache, Ahmed Belaidi	http://editions-rnti.fr/render_pdf.php?p1&p=1000813	http://editions-rnti.fr/render_pdf.php?p=1000813	dimportants volum dimage satellite aérienner typepanchromatiqu multispectrale hyperspectral généréesquotidiennement classification méthode semiautomatiquesdevient nécessaire logiciel envi Feature extractiontm envi FXTM sebase approcher « objet » opposition approcher pixelsclassiqu algorithme innovant segmentation laclassification image télédétection niveau précision
735	Revue des Nouvelles Technologies de l'Information	EGC	2009	Collaborative Outlier Mining for Intrusion Detection	Intrusion detection is an important topic dealing with security of in-formation systems. Most successful Intrusion Detection Systems (IDS) rely onsignature detection and need to update their signature as fast as new attacks areemerging. On the other hand, anomaly detection may be utilized for this purpose,but it suffers from a high number of false alarms. Actually, any behaviour whichis significantly different from the usual ones will be considered as dangerousby an anomaly based IDS. Therefore, isolating true intrusions in a set of alarmsis a very challenging task for anomaly based intrusion detection. In this paper,we consider to add a new feature to such isolated behaviours before they can beconsidered as malicious. This feature is based on their possible repetition fromone information system to another. We propose a new outlier mining principleand validate it through a set of experiments.	Goverdhan Singh, Florent Masseglia, Céline Fiot, Alice Marascu, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1000776	http://editions-rnti.fr/render_pdf.php?p=1000776	intrusion detection is an importer topic dealing with security of information system Most successful Intrusion Detection Systems ids rely onsignature detection and need to update their signature fast new attack areemerging the other hand anomaly detection may be utilized for this purposebut it suffer from high number of false alarms Actually any behaviour whichi significantly from the usual will be considered dangerousby an anomaly based id Therefore isolating true intrusion in set of alarmsi very challenging task for anomaly based intrusion detection In this paperwe consider to add new feature to such isolated behaviour before they can beconsidered maliciou This featur is based their repetition fromone information system to another We proposer new outlier mining principleand validate it through set of experiment
736	Revue des Nouvelles Technologies de l'Information	EGC	2009	Comment valider automatiquement des relations syntaxiques induites	Nous présentons dans cet article des approches visant à valider desrelations syntaxiques induites de type Verbe-Objet. Ainsi, nous proposons d'u-tiliser dans un premier temps une approche s'appuyant sur des vecteurs séman-tiques déterminés à l'aide d'un thésaurus. La seconde approche emploie unevalidation Web. Nous effectuons des requêtes sur un moteur de recherche asso-ciées à des mesures statistiques afin de déterminer la pertinence d'une relationsyntaxique. Nous proposons enfin de combiner ces deux méthodes. La qualitéde nos approches de validation de relations syntaxiques a été évaluée en utilisantdes courbes ROC.	Nicolas Béchet, Mathieu Roche, Jacques Chauché	http://editions-rnti.fr/render_pdf.php?p1&p=1000749	http://editions-rnti.fr/render_pdf.php?p=1000749	présenter dan article approche viser valider desrelation syntaxique induit typer VerbeObjet proposer dutiliser dan temps approcher sappuyer vecteur sémantique déterminé laid dun thésauru second approcher employer unevalidation Web effectuer requête moteur rechercher associer mesure statistique déterminer pertinence dune relationsyntaxique proposer combiner méthode qualitéde approche validation relation syntaxique évaluer utilisantd courbe roc
737	Revue des Nouvelles Technologies de l'Information	EGC	2009	Comparaison de distances et noyaux classiques par degré d'équivalence des ordres induits	Le choix d'une mesure pour comparer les données est au coeur destâches de recherche d'information et d'apprentissage automatique. Nous considéronsici ce problème dans le cas où seul l'ordre induit par la mesure importe,et non les valeurs numériques qu'elle fournit : cette situation est caractéristiquedes moteurs de recherche de documents par exemple. Nous étudions dans cecadre les mesures de comparaison classiques pour données numériques, tellesque les distances et les noyaux les plus courants. Nous identifions les mesureséquivalentes, qui induisent toujours le même ordre ; pour les mesures non équivalentes,nous quantifions leur désaccord par des degrés d'équivalence basés surle coefficient de Kendall généralisé. Nous étudions les équivalences et quasiéquivalencesà la fois sur les plans théorique et expérimental.	Maria Rifqi, Marcin Detyniecki, Marie-Jeanne Lesot	http://editions-rnti.fr/render_pdf.php?p1&p=1000738	http://editions-rnti.fr/render_pdf.php?p=1000738	choix dune mesurer comparer donnée coeur destâcher rechercher dinformation dapprentissage automatique considéronsici problème dan cas lordre induire mesurer importeet numérique fournir   situation caractéristiqueder moteur rechercher document exemple étudier dan cecadre mesure comparaison classique donnée numérique tellesqu distance noyau plaire courant identifier mesureséquivalente induire ordre   mesure équivalentesnous quantifier désaccord degré déquivalence baser surle coefficient Kendall généraliser étudier équivalence quasiéquivalencesà plan théorique expérimental
738	Revue des Nouvelles Technologies de l'Information	EGC	2009	Constraint Programming for Data Mining		Luc De Raedt	http://editions-rnti.fr/render_pdf.php?p1&p=1000731	http://editions-rnti.fr/render_pdf.php?p=1000731	
739	Revue des Nouvelles Technologies de l'Information	EGC	2009	Construction de descripteurs pour classer à partir d'exemples bruités	En classification supervisée, la présence de bruit sur les valeurs desdescripteurs peut avoir des effets désastreux sur la performance des classifieurset donc sur la pertinence des décisions prises au moyen de ces modèles. Traiterce problème lorsque le bruit affecte un attribut classe a été très étudié. Il estplus rare de s'intéresser au bruit sur les autres attributs. C'est notre contextede travail et nous proposons la construction de nouveaux descripteurs robusteslorsque ceux des exemples originaux sont bruités. Les résultats expérimentauxmontrent la valeur ajoutée de cette construction par la comparaison des qualitésobtenues (e.g., précision) lorsque l'on utilise les méthodes de classification àpartir de différentes collections de descripteurs.	Nazha Selmaoui-Folcher, Jean-François Boulicaut, Dominique Gay	http://editions-rnti.fr/render_pdf.php?p1&p=1000742	http://editions-rnti.fr/render_pdf.php?p=1000742	En classification superviser présence bruire desdescripteur pouvoir désastreux performance classifieurset pertinence décision prendre moyen modèle Traiterce problème bruire affecter attribut classer étudier estplus sintéresser bruir attribut cest contextede travail proposer construction descripteur robusteslorsqu exemple original bruiter résultat expérimentauxmontrer ajouter construction comparaison qualitésobtenue eg précision lon utiliser méthode classification àpartir collection descripteur
740	Revue des Nouvelles Technologies de l'Information	EGC	2009	Contrôle des observations pour la gestion des systèmes de flux de données.	Les systèmes d'analyse de flux de données prennent de plus en plusd'importance dans un contexte où les données circulant sur les réseaux sont deplus en plus volumineuses et où la volonté de réagir au plus vite, en temps réel,devient un besoin nécessaire. Afin de permettre des analyses aussi rapides etefficaces que possible, il convient de pouvoir contrôler les flots de données et defocaliser les traitements sur les données pertinentes. Le protocole présenté dansce papier donne au module de traitement des capacités d'action et de contrôle surles observations remontantes en fonction de l'état de l'analyse. La diminutiondes flux résultant de telles focalisations permet des traitements beaucoup plusefficaces, plus pertinents et moins consommateurs de ressources. Les premiersrésultats montrent un réel gain de performances sur nos applications (facteur100).	Pierre Le Maigat, Christophe Dousson	http://editions-rnti.fr/render_pdf.php?p1&p=1000798	http://editions-rnti.fr/render_pdf.php?p=1000798	système danalyse flux donnée prendre plaire plusdimportance dan contexte donnée circuler réseau deplus plaire volumineux volonté réagir plaire vite temps réeldevier besoin nécessaire Afin permettre analyse rapider etefficace convier pouvoir chuter flot donnée defocaliser traitement donnée pertinent protocole présenter dansce papier donner moduler traitement capacité daction contrôler surl observation remontant fonction létat lanalyse diminutionde flux résulter focalisation permettre traitement plusefficace plaire pertinent consommateur ressource premiersrésultat montrer réel gain performance application facteur100
741	Revue des Nouvelles Technologies de l'Information	EGC	2009	Correspondances de Galois pour la manipulation de contextes flous multi-valués	L'analyse formelle de concepts est une méthode fondée sur la correspondancede Galois et qui permet de construire des hiérarchies de conceptsformels à partir de tableaux de données binaires. Cependant de nombreux problèmesréels abordés en fouille de données comportent des données plus complexes.Afin de traiter de tels problèmes, nous proposons une conversion de donnéesfloues multi-valuées en attributs histogrammes et une correspondance deGalois adaptée à ce format. Notre propos est illustré avec un jeu de donnéessimples. Enfin, nous évaluons brièvement les résultats et les apports de cettecorrespondance de Galois par rapport à l'approche classique	Aurélie Bertaux, Florence Le Ber, Agnès Braud	http://editions-rnti.fr/render_pdf.php?p1&p=1000763	http://editions-rnti.fr/render_pdf.php?p=1000763	lanalyse formel concept méthode fonder correspondancede Galois permettre construire hiérarchie conceptsformel partir tableau donnée binaire problèmesréel aborder fouiller donnée comporter donnée plaire complexesafin traiter problème proposer conversion donnéesfloue multivaluer attribut histogramm correspondance degalois adapter format propos illustrer jeu donnéessimple évaluer brièvement résultat apport cettecorrespondance Galois rapport lapproche classique
742	Revue des Nouvelles Technologies de l'Information	EGC	2009	DBFrequentQueries : Extraction de requêtes fréquentes		Lucie Copin, Nicolas Pecheur, Anne Laurent, Yudi Augusta, Budi Sentana, Dominique Laurent, Tao-Yuan Jen	http://editions-rnti.fr/render_pdf.php?p1&p=1000822	http://editions-rnti.fr/render_pdf.php?p=1000822	
743	Revue des Nouvelles Technologies de l'Information	EGC	2009	De l'utilisation de l'Analyse de Données Symboliques dans les Systèmes multi-agents	L'exploitation en temps réel de connaissances complexes est un défidans de nombreux domaines, tels que le web sémantique, la simulation ou lessystèmes multi-agents (SMA). Dans le paradigme multi-agents, des travaux ré-cents montrent que les communications multi-parties (CMP) offrent des oppor-tunités intéressantes en termes de réalisme des communications, diffusion desconnaissances et sémantique des actes de langage. Cependant, ces travaux seheurtent à la difficulté de mise en oeuvre des CMP, pour lesquelles les supportsde communications classiques sont insuffisants. Dans cet article, nous propo-sons d'utiliser le formalisme de l'Analyse de Données Symboliques (ADS) pourmodéliser les informations et les besoins des agents. Nous appuyons le routagedes messages sur cette modélisation dans le cadre d'un environnement de com-munication pour les systèmes multi-agents. Afin d'illustrer notre propos, nousutiliserons l'exemple de la gestion des communications dans un poste d'appelsd'urgence. Nous présentons ensuite notre retour d'expérience, et discutons lesperspectives ouvertes par la fertilisation croisée de l'ADS et des SMA.	Flavien Balbo, Julien Saunier, Edwin Diday, Suzanne Pinson	http://editions-rnti.fr/render_pdf.php?p1&p=1000746	http://editions-rnti.fr/render_pdf.php?p=1000746	lexploitation temps réel connaissance complexe défidan domaine web sémantique simulation lessystèm multiagent SMA Dans paradigme multiagent travail récent montrer communication multiparti cmp offrir opportunité intéressant terme réalisme communication diffusion desconnaissance sémantique acte langage Cependant travail seheurter difficulté miser oeuvrer cmp supportsd communication classique insuffisant Dans article proposer dutiliser formalisme lanalyse donnée symbolique ads pourmodéliser information besoin agent appuyer routagede message modélisation dan cadrer dun environnement communication système multiagent Afin dillustrer propos nousutiliseron lexemple gestion communication dan poster dappelsdurgence présenter ensuite dexpérience discuton lesperspective fertilisation croiser lad sma
744	Revue des Nouvelles Technologies de l'Information	EGC	2009	Définition d'une stratégie de résolution de problèmes pour un robot humanoïde	Nous avons développé un système dont le but est d'obtenir le logicielde commande d'un robot capable de simuler le comportement d'un humainplacé en situation de résolution de problèmes. Nous avons résolu ce problèmedans un environnement psychologique particulier où les comportements humainspeuvent être interprétés comme des ‘observables' de leurs stratégies derésolution de problèmes. Notre solution contient de plus celle d'un autre problème,celui de construire une boucle complète commençant avec le comportementd'un groupe d'humains, son analyse et son interprétation en termesd'observables humaines, la définition des stratégies utilisées par les humains (ycompris celles qui sont inefficaces), l'interprétation des observables humainesen terme de mouvements du robot, la définition de ce qu'est une “stratégie derobot ” en terme de stratégies humaines. La boucle est bouclée avec un langagede programmation capable de programmer ces stratégies robotiques, qui deviennentainsi à leur tour des observables, tout comme l'ont été les stratégieshumaines du début de la boucle. Nous expliquons comment nous avons été capablesdéfinir de façon objective ce que nous appelons une stratégie de robot.Notre solution assemble deux facteurs différents. L'un permet d'éviter lescomportements ‘inhumains' et se fonde sur la moyenne des comportementsdes humains que nous avons observés. L'autre fournit une sorte ‘d'humanité'au robot en lui permettant de dévier de cette moyenne par n fois l'écart typeobservé chez les humains qu'il doit simuler. Il devient alors possible de programmerdes comportements complètements humains.	Mary Felkin, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1000806	http://editions-rnti.fr/render_pdf.php?p=1000806	développer système boire dobtenir logicielde command dun robot capable simuler comportement dun humainplacé situation résolution problème résoudre problèmedans environnement psychologique comportement humainspeuver interpréter ‘ observable stratégie derésolution problème solution contenir plaire dun problèmecelui construire boucler complet commencer comportementdun grouper dhumains analyser interprétation termesdobservabl humain définition stratégie utiliser humain ycompri inefficace linterprétation observable humainesen terme mouvement robot définition quest “ stratégie derobot ” terme stratégie humain boucler boucler langagede programmation capable programmer stratégie robotique deviennentainsi tour observable lont stratégieshumaine boucler expliquer capablesdéfinir objectif appeler stratégie robotnotre solution assembl facteur lun permettre déviter lescomportement ‘ inhumain fondre moyenner comportementsde humain observer Lautre fournir sortir ‘ dhumanitéau robot luire permettre dévier moyenner lécart typeobservé humain quil devoir simuler devenir programmerde comportement complètement humain
745	Revue des Nouvelles Technologies de l'Information	EGC	2009	DEMON : Découverte de motifs séquentiels pour les puces adn	Prometteuses en terme de prévention, de dépistage, de diagnostic etd'actions thérapeutiques, les puces à ADN mesurent l'intensité des expressionsde plusieurs milliers de gènes. Dans cet article, nous proposons une nouvelleapproche appelée DEMON, pour extraire des motifs séquentiels à partir de don-nées issues des puces ADN et qui utilise des connaissances du domaine.	Paola Salle, Sandra Bringay, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000803	http://editions-rnti.fr/render_pdf.php?p=1000803	prometteuser terme prévention dépistage diagnostic etdaction thérapeutique puce ADN mesurer lintensité expressionsde millier gène Dans article proposer nouvelleapproche appeler demon extraire motif séquentiel partir donnée issu puce ADN utiliser connaissance domaine
746	Revue des Nouvelles Technologies de l'Information	EGC	2009	DEMON-Visualisation : un outil pour la visualisation des motifs séquentiels extraits à partir de données biologiques		Wei Xing, Paola Salle, Sandra Bringay, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000818	http://editions-rnti.fr/render_pdf.php?p=1000818	
747	Revue des Nouvelles Technologies de l'Information	EGC	2009	DesEsper: un logiciel de pré-traitement de flux appliqué à la surveillance des centrales hydrauliques		Frédéric Flouvat, Sébastien Gassmann, Jean-Marc Petit, Alain Ribière	http://editions-rnti.fr/render_pdf.php?p1&p=1000819	http://editions-rnti.fr/render_pdf.php?p=1000819	
748	Revue des Nouvelles Technologies de l'Information	EGC	2009	Détection d'intrusions dans un environnement collaboratif sécurisé	Pour pallier le problème des attaques sur les réseaux de nouvelles ap-proches de détection d'anomalies ou d'abus ont été proposées ces dernières an-nées et utilisent des signatures d'attaques pour comparer une nouvelle requêteet ainsi déterminer s'il s'agit d'une attaque ou pas. Cependant ces systèmes sontmis à défaut quand la requête n'existe pas dans la base de signature. Généra-lement, ce problème est résolu via une expertise humaine afin de mettre à jourla base de signatures. Toutefois, il arrive fréquemment qu'une attaque ait déjàété détectée dans une autre organisation et il serait utile de pouvoir bénéficier decette connaissance pour enrichir la base de signatures mais cette information estdifficile à obtenir car les organisations ne souhaitent pas forcément indiquer lesattaques qui ont eu lieu sur le site. Dans cet article nous proposons une nouvelleapproche de détection d'intrusion dans un environnement collaboratif sécurisé.Notre approche permet de considérer toute signature décrite sous la forme d'ex-pressions régulières et de garantir qu'aucune information n'est divulguée sur lecontenu des différents sites.	Nischal Verma, François Trousset, Pascal Poncelet, Florent Masseglia	http://editions-rnti.fr/render_pdf.php?p1&p=1000775	http://editions-rnti.fr/render_pdf.php?p=1000775	Pour pallier problème attaque réseau approche détection danomalier dabu proposer année utiliser signature dattaqu comparer requêteet déterminer sil sagit dune attaquer système sontmis défaut requête nexist dan baser signature généralement problème résoudre expertiser humain mettre jourla baser signature arriver fréquemment quune attaquer déjàété détecter dan organisation utile pouvoir bénéficier decette connaissance enrichir baser signature information estdifficil obtenir organisation souhaiter forcément indiquer lesattaqu lieu site Dans article proposer nouvelleapproche détection dintrusion dan environnement collaboratif sécurisénotre approcher permettre considérer signature décrire sou former dexpression régulier garantir quaucune information nest divulguer lecontenu site
749	Revue des Nouvelles Technologies de l'Information	EGC	2009	Détection d'objets atypiques dans un flot de données : une approche multi-résolution		Florent Masseglia, Alice Marascu	http://editions-rnti.fr/render_pdf.php?p1&p=1000801	http://editions-rnti.fr/render_pdf.php?p=1000801	
750	Revue des Nouvelles Technologies de l'Information	EGC	2009	Détection de séquences atypiques basée sur un modèle de Markov d'ordre variable	Récemment, le nombre et le volume des bases de données séquentiellesbiologiques ont augmenté de manière considérable. Dans ce contexte, l'identificationdes anomalies est essentielle. La plupart des approches pour lesextraire se fondent sur une base d'apprentissage ne contenant pas d'outlier. Or,dans de très nombreuses applications, les experts ne disposent pas d'une tellebase. De plus, les méthodes existantes demeurent exigeantes en mémoire, cequi les rend souvent impossibles à utiliser. Nous présentons dans cet article unenouvelle approche, basée sur un modèle de Markov d'ordre variable et sur unemesure de similarité entre objets séquentiels. Nous ajoutons aux méthodes existantesun critère d'élagage pour contrôler la taille de l'espace de rechercheet sa qualité, ainsi qu'une inégalité de concentration précise pour la mesure desimilarité, conduisant à une meilleure détection des outliers. Nous démontronsexpérimentalement la validité de notre approche.	Cécile Low-Kam, Anne Laurent, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000766	http://editions-rnti.fr/render_pdf.php?p=1000766	récemment nombre volume base donnée séquentiellesbiologiqu augmenter manière considérable Dans contexte lidentificationd anomalie essentiel approche lesextraire fondre baser dapprentissage contenir doutlier Ordans application expert disposer dune tellebase De plaire méthode existant demeurer exigeant mémoire cequi impossible utiliser présenter dan article unenouvell approcher basé modeler Markov dordre variable unemesure similarité entrer objet séquentiel ajouter méthode existantesun critèr délagage chuter tailler lespace rechercheet qualité quune inégalité concentration précis mesurer desimilarité conduire meilleur détection outlier démontronsexpérimentalemer validité approcher
751	Revue des Nouvelles Technologies de l'Information	EGC	2009	Détermination du nombre des classes dans l'algorithme CROKI2 de classification croisée	Un des problèmes majeurs de la classification non supervisée est ladétermination ou la validation du nombre de classes dans la population. Ce problèmes'étend aux méthodes de bipartitionnement ou block clustering. Dans cepapier, nous nous intéressons à l'algorithme CROKI2 de classification croiséedes tableaux de contingence proposé par Govaert (1983). Notre objectif est dedéterminer le nombre de classes optimal sur les lignes et les colonnes à traversun ensemble de techniques de validation de classes proposés dans la littératurepour les méthodes classiques de classification.	Malika Charrad, Yves Lechevallier, Gilbert Saporta, Mohamed Ben Ahmed	http://editions-rnti.fr/render_pdf.php?p1&p=1000797	http://editions-rnti.fr/render_pdf.php?p=1000797	problème majeur classification superviser ladétermination validation nombre classe dan population problèmesétend méthode bipartitionnement block clustering Dans cepapier intéresser lalgorithme croki2 classification croiséed tableau contingence proposer govaert 1983 objectif dedéterminer nombre classe optimal ligne colonne traversun ensembl technique validation classe proposer dan littératurepour méthode classique classification
752	Revue des Nouvelles Technologies de l'Information	EGC	2009	Diagnostic multi-sources adaptatif Application à la détection d'intrusion dans des serveursWeb	Le but d'un système adaptatif de diagnostic est de surveiller et diagnostiquerun système tout en s'adaptant à son évolution. Ceci passe par l'adaptationdes diagnostiqueurs qui précisent ou enrichissent leur propre modèle poursuivre au mieux le système au fil du temps. Pour détecter les besoins d'adaptation,nous proposons un cadre de diagnostic multi-sources s'inspirant de lafusion d'information. Des connaissances fournies par le concepteur sur des relationsattendues entre les diagnostiqueurs mono-source forment un méta-modèledu diagnostic. La compatibilité des résultats du diagnostic avec le méta-modèleest vérifiée en ligne. Lorsqu'une de ces relations n'est pas vérifiée, les diagnostiqueursconcernés sont modifiés.Nous appliquons cette approche à la conception d'un système adaptatif de détectiond'intrusion à partir d'un flux de connexions à un serveur Web. Les évaluationsdu système mettent en évidence sa capacité à améliorer la détection desintrusions connues et à découvrir de nouveaux types d'attaque.	Thomas Guyet, Wei Wang    , Rene Quiniou, Marie-Odile Cordier	http://editions-rnti.fr/render_pdf.php?p1&p=1000777	http://editions-rnti.fr/render_pdf.php?p=1000777	boire dun système adaptatif diagnostic surveiller diagnostiquerun système sadapter évolution passer ladaptationdes diagnostiqueur préciser enrichir propre modeler poursuivre mieux système fil temps Pour détecter besoin dadaptationnou proposer cadrer diagnostic multisource sinspirer lafusion dinformation connaissance fournir concepteur relationsattendue entrer diagnostiqueur monosourc former métamodèledu diagnostic compatibilité résultat diagnostic métamodèleest vérifier ligne Lorsquune relation nest vérifier diagnostiqueursconcerné modifiésnous appliquer approcher conception dun système adaptatif détectiondintrusion partir dun flux connexion serveur Web évaluationsdu système mettre évidence capacité améliorer détection desintrusion connu découvrir type dattaqu
753	Revue des Nouvelles Technologies de l'Information	EGC	2009	Empreintes conceptuelles et spatiales pour la caractérisation des réseaux sociaux	Cet article propose une méthode reposant sur l'utilisation del'Analyse Formelle de Concepts et des treillis de Galois pour l'analyse desystèmes complexes. Des statistiques reposant sur ces treillis permettent decalculer la distribution conceptuelle des objets classifiés par le treillis.L'expérimentation sur des échantillons de trois réseaux sociaux en ligneillustre l'utilisation de ces statistiques pour la caractérisation globale et pour lefiltrage automatique de ces systèmes.	Bénédicte Le Grand, Marie-Aude Aufaure, Michel Soto	http://editions-rnti.fr/render_pdf.php?p1&p=1000779	http://editions-rnti.fr/render_pdf.php?p=1000779	article proposer méthode reposer lutilisation delanalyse Formelle Concepts treillis Galois lanalyse desystèm complexe statistique reposer treillis permettre decalculer distribution conceptuel objet classifier treillislexpérimentation échantillon réseau social ligneillustre lutilisation statistique caractérisation global lefiltrage automatique système
754	Revue des Nouvelles Technologies de l'Information	EGC	2009	Exploration de données de traçabilité issues de la RFID par apprentissage non-supervisé	La RFID (Radio Frequency IDentification) est une technologie avancée d'enregistrementde données spatio-temporelles de traçabilité. L'objectif de ce travail est de transformer cesdonnées spatio-temporelles en connaissances exploitables par les utilisateurs par l'intermé-diaire d'une méthode de classification automatique des données. Les systèmes RFID peuventêtre utilisés pour étudier les sociétés animales, qui sont des systèmes dynamiques complexescaractérisés par beaucoup d'interactions entre les individus (Fresneau et al., 1989). Le cadreapplicatif choisi pour ce travail est l'étude de la structure d'un groupe d'individus en interactionsociale et en particulier la division du travail au sein d'une colonie de fourmis1.La RFID générant d'importants volumes de données, il est nécessaire de développer desméthodes appropriées afin d'en comprendre le sens. Nous proposons pour cela un algorithmede classification topographique non-supervisée pour l'exploration de ce type de données, ca-pable de détecter les groupes d'individus exprimant le même comportement. L'algorithmeDS2L-SOM (Density-based Simultaneous Two-Level - SOM, Cabanes et Bennani (2008)) estcapable de détecter non seulement les groupes définis par une zone vide de donnée, grâce àune estimation de la pertinence des connexions entre référents, mais aussi les groupes défi-nis seulement par une diminution de densité, grâce à une estimation de la densité autour desréférents pendant l'apprentissage.	Guénaël Cabanes, Younès Bennani, Dominique Fresneau	http://editions-rnti.fr/render_pdf.php?p1&p=1000799	http://editions-rnti.fr/render_pdf.php?p=1000799	rfid Radio Frequency identification technologie avancer denregistrementde donner spatiotemporell traçabilité Lobjectif travail transformer cesdonner spatiotemporell connaissance exploitable utilisateur lintermédiair dune méthode classification automatique donnée système RFID peuventêtr utiliser étudier société animal système dynamique complexescaractériser dinteraction entrer individu Fresneau al 1989 cadreapplicatif choisir travail létude structurer dun grouper dindividus interactionsocial division travail dune colonie fourmis1la rfid générer dimportant volume donnée nécessaire développer desméthode approprier den comprendre sens proposer celer algorithmed classification topographique nonsuperviser lexploration typer donnée capable détecter groupe dindividus exprimer comportement LalgorithmeDS2LSOM Densitybased simultaneou twolevel   SOM Cabanes Bennani 2008 estcapable détecter groupe définir zone vide donner grâce àune estimation pertinence connexion entrer référent groupe définir diminution densité grâce estimation densité autour desréférent pendre lapprentissage
755	Revue des Nouvelles Technologies de l'Information	EGC	2009	Exploration des corrélations dans un classifieur Application au placement d'offres commerciales	Cet article présente une nouvelle méthode permettant d'explorer lesprobabilités délivrées par un modèle prédictif de classification. L'augmentationde la probabilité d'occurrence de l'une des classes du problème étudié est analyséeen fonction des variables explicatives prises isolément. La méthode proposéeest posée et illustrée dans un cadre général, puis explicitement dédiée au classifieurBayesien naïf. Son illustration sur les données du challenge PAKDD 2007montre que ce type d'exploration permet de créer des indicateurs performantsd'aide à la vente.	Vincent Lemaire, Carine Hue	http://editions-rnti.fr/render_pdf.php?p1&p=1000739	http://editions-rnti.fr/render_pdf.php?p=1000739	article présenter méthode permettre dexplorer lesprobabilités délivrer modeler prédictif classification Laugmentationde probabilité doccurrence lune classe problème étudier analyséeen fonction variable explicatif prendre isolément méthode proposéeest poser illustrer dan cadrer général pouvoir explicitement dédier classifieurbayesien naïf illustration donnée challenge pakdd 2007montre typer dexploration permettre créer indicateur performantsdaid venter
756	Revue des Nouvelles Technologies de l'Information	EGC	2009	Explorer3D : classification et visualisation de données		Matthieu Exbrayat, Lionel Martin	http://editions-rnti.fr/render_pdf.php?p1&p=1000817	http://editions-rnti.fr/render_pdf.php?p=1000817	
757	Revue des Nouvelles Technologies de l'Information	EGC	2009	Extraction de motifs fermés dans des relations n-aires bruitées	L'extraction de motifs fermés dans des relations binaires a été trèsétudiée. Cependant, de nombreuses relations intéressantes sont n-aires avec n >2 et bruitées (nécessité d'une tolérance aux exceptions). Récemment, ces deuxproblèmes ont été traités indépendamment. Nous introduisons notre propositionpour combiner de telles fonctionnalités au sein d'un même algorithme.	Loïc Cerf, Jérémy Besson, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1000748	http://editions-rnti.fr/render_pdf.php?p=1000748	lextraction motif fermer dan relation binaire trèsétudier relation intéressant nair 2 bruiter nécessiter dune tolérance exception récemment deuxproblème traiter indépendamment introduire propositionpour combiner fonctionnalité dun algorithme
758	Revue des Nouvelles Technologies de l'Information	EGC	2009	Extraction de Règles de Corrélation Décisionnelles	Dans cet article, nous introduisons deux nouveaux concepts : les règlesde corrélation décisionnelles et les vecteurs de contingence. Le premier résulted'un couplage entre les règles de corrélation et les règles de décision. Il permetde mettre en évidence des liens pertinents entre certains ensembles de motifsd'une relation binaire et les valeurs d'un attribut cible (appartenant à cette mêmerelation) en se basant à la fois sur la mesure du Khi-carré et sur le support desmotifs extraits. De par la nature du problème, les algorithmes par niveaux fontque l'extraction des résultats a lieu avec des temps de réponse élevés et uneoccupation mémoire importante. Afin de palier à ces deux inconvénients, nousproposons un algorithme basé sur l'ordre lectique et les vecteurs de contingence.	Alain Casali, Christian Ernst	http://editions-rnti.fr/render_pdf.php?p1&p=1000761	http://editions-rnti.fr/render_pdf.php?p=1000761	Dans article introduire concept   règlesde corrélation décisionnel vecteur contingence résultedun couplage entrer règle corrélation règle décision permetde mettre évidence lien pertinent entrer ensemble motifsdune relation binaire dun attribut cibl appartenir mêmerelation baser mesurer Khicarré support desmotif extrait De nature problème algorithme niveau fontqu lextraction résultat lieu temps réponse élevé uneoccupation mémoir important Afin palier inconvénient nousproposon algorithme baser lordre lectiqu vecteur contingence
759	Revue des Nouvelles Technologies de l'Information	EGC	2009	Extraction efficace de règles graduelles	Les règles graduelles suscitent depuis quelques années un intérêt croissant.De telles règles, de la forme “Plus (moins) A1 et ... plus (moins) An alorsplus (moins) B1 et ... plus (moins) Bn” trouvent application dans de nombreuxdomaines tels que la bioinformatique, les contrôleurs flous, les relevés de capteursou encore les flots de données. Ces bases, souvent composées d'un grandnombre d'attributs, restent un verrou pour l'extraction automatique de connaissances,car elles rendent inefficaces les techniques de fouille habituelles (règlesd'association, clustering...). Dans cet article, nous proposons un algorithme efficaced'extraction d'itemset graduels basé sur l'utilisation des treillis. Nous définissonsformellement les notions de gradualité, ainsi que les algorithmes associés.Des expérimentations menées sur jeux de données synthétiques et réelsmontrent l'intérêt de notre méthode	Lisa Di-Jorio, Anne Laurent, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000764	http://editions-rnti.fr/render_pdf.php?p=1000764	règle graduelle susciter année intérêt croissantde règle former “ plaire a1   plaire an alorsplu b1   plaire Bn ” trouver application dan nombreuxdomaine bioinformatique contrôleur flou relevé capteursou flot donnée base composer dun grandnombre dattribut ruer verrou lextraction automatique connaissancescar inefficace technique fouiller habituel règlesdassociation clustering Dans article proposer algorithme efficacedextraction ditemset graduel baser lutilisation treillis définissonsformellemer notion gradualité algorithme associésd expérimentation mener jeu donnée synthétique réelsmontrent lintérêt méthode
760	Revue des Nouvelles Technologies de l'Information	EGC	2009	FCP-Growth, une adaptation de FP-Growth pour générer des règles d'association de classe		Emna Bahri, Stéphane Lallich	http://editions-rnti.fr/render_pdf.php?p1&p=1000804	http://editions-rnti.fr/render_pdf.php?p=1000804	
761	Revue des Nouvelles Technologies de l'Information	EGC	2009	Fouille de données dans les bases relationnelles pour l'acquisition d'ontologies riches en hiérarchies de classes	De par leur caractère structuré, les bases de données relationnellessont des sources précieuses pour la construction automatisée d'ontologies. Ce-pendant, une limite persistante des approches existantes est la production d'onto-logies de structure calquée sur celles des schémas relationnels sources. Dans cetarticle, nous décrivons la méthode RTAXON dont la particularité est d'identifierdes motifs de catégorisation dans les données afin de produire des ontologiesplus structurées, riches en hiérarchies. La méthode formalisée combine analyseclassique du schéma relationnel et fouille des données pour l'identification destructures hiérarchiques.	Farid Cerbah	http://editions-rnti.fr/render_pdf.php?p1&p=1000786	http://editions-rnti.fr/render_pdf.php?p=1000786	De caractère structurer base donnée relationnellessont source précieux construction automatisé dontologie limiter persistant approche existant production dontologier structurer calquer schéma relationnel source Dans cetarticle décrire méthode rtaxon particularité didentifierder motif catégorisation dan donnée produire ontologiesplu structurer riche hiérarchie méthode formalisé combiner analyseclassiqu schéma relationnel fouiller donnée lidentification destructur hiérarchique
762	Revue des Nouvelles Technologies de l'Information	EGC	2009	Fusion Symbolique pour la Recommandation de Programmes Télévisées	Nous proposons une approche générique pour la fusion d'informa-tions qui repose sur l'utilisation du modèle des Graphes Conceptuels et l'opé-ration de jointure maximale. Nous validons notre approche par le biais d'ex-périmentations. Ces expérimentations soulignent l'importance des heuristiquesmises en place.	Claire Laudy, Jean-Gabriel Ganascia	http://editions-rnti.fr/render_pdf.php?p1&p=1000796	http://editions-rnti.fr/render_pdf.php?p=1000796	proposer approcher générique fusion dinformation reposer lutilisation modeler Graphes Conceptuels lopération jointure maximal valider approcher biais dexpérimentation expérimentation souligner limportance heuristiquesmise placer
763	Revue des Nouvelles Technologies de l'Information	EGC	2009	Générer des règles de classification par Dopage de Concepts Formels	La classification supervisée est une tâche de fouille de données (DataMining), qui consiste à construire un classifieur à partir d'un ensemble d'exemplesétiquetés par des classes (phase d'apprentissage) et ensuite prédire les classesdes nouveaux exemples avec ce classifieur (phase de classification). En classi-fication supervisée, plusieurs approches ont été proposées dont l'approche ba-sée sur l'Analyse de Concepts Formels. L'apprentissage de Concepts Formelsest basé généralement sur la structure mathématique du treillis de Galois (outreillis de concepts). Cependant, la complexité exponentielle de génération d'untreillis de Galois a limité les champs d'application de ces systèmes. Dans cetarticle, nous présentons plusieurs méthodes de classification supervisée baséessur l'Analyse de Concepts Formels. Nous présentons aussi le boosting (dopage)de classifieurs, une technique de classification innovante. Enfin, nous proposonsle boosting de concepts formels, une nouvelle méthode adaptative qui construitseulement une partie du treillis englobant les meilleurs concepts. Ces conceptssont utilisés comme étant des règles de classification. Les résultats expérimen-taux réalisés ont prouvé l'intérêt de la méthode proposée par rapport à cellesexistantes.	Mondher Maddouri, Nida Meddouri	http://editions-rnti.fr/render_pdf.php?p1&p=1000760	http://editions-rnti.fr/render_pdf.php?p=1000760	classification superviser tâcher fouiller donnée DataMining consister construire classifieur partir dun ensemble dexemplesétiqueter classe phase dapprentissage ensuite prédir classesde exemple classifieur phase classification En classification superviser approche proposer lapproche basé lanalyse Concepts Formels Lapprentissage Concepts formelsest baser généralement structurer mathématique treillis Galois outreillis concept complexité exponentiel génération duntreillis Galois limiter champ dapplication système Dans cetarticle présenter méthode classification superviser baséessur lanalyse Concepts Formels présenter boosting dopaged classifieur technique classification innovant proposonsle boosting concept formel méthode adaptatif construitseulemer partir treillis englober meilleur concept conceptssont utiliser règle classification résultat expérimental réaliser prouver lintérêt méthode proposer rapport cellesexistante
764	Revue des Nouvelles Technologies de l'Information	EGC	2009	Graphes des liens et anti liens statistiquement valides entre les mots d'un corpus textuel : test de randomisation TourneBool sur le corpus Reuters	"La définition du voisinage est un élément central en fouille de données, et de nombreuses définitions ont été avancées. Nous en proposons ici une version statistique issue de notre test de randomisation TourneBool, qui permet, à partir d'un tableau de relations binaires objets décrits/descripteurs, d'établir quelles relations entre descripteurs sont dues au hasard, et lesquelles ne le sont pas, sans faire d'hypothèse sur les lois de répartitions sous-jacentes, c'est à dire en tenant compte de lois de tous types sans avoir besoin de les spécifier. Ce test est basé sur la génération et l'exploitation d'un ensemble de matrices randomisées ayant les mêmes sommes marginales en lignes et colonnes que la matrice d'origine. Après une première application encourageante à un corpus textuel réduit, nous avons opéré le passage à l'échelle adéquat pour traiter des corpus textuels de taille réelle, comme celui des dépêches Reuters. Nous caractérisons le graphe des mots de ce corpus au moyen d'indicateurs classiques comme le coefficient de clustering, la distribution des degrés et de la taille des communautés, etc. Une autre caractéristique de TourneBool est qu'il permet aussi de dégager les ""anti liens"" entre mots, à savoir les mots qui s'évitent plus qu'attendu du fait du hasard. Le graphe des liens et celui des anti-liens seront caractérisés de la même façon."	Martine Cadot, Alain Lelu	http://editions-rnti.fr/render_pdf.php?p1&p=1000783	http://editions-rnti.fr/render_pdf.php?p=1000783	définition voisinage élément central fouiller donnée définition avancer proposer version statistique issu test randomisation tournebool permettre partir dun tableau relation binaire objet décritsdescripteur détablir relation entrer descripteur hasard faire dhypothèse loi répartition sousjacent cest compter loi type besoin spécifier test baser génération lexploitation dun ensemble matrice randomiser somme marginal ligne colonne matrice dorigine Après application encourageant corpus textuel réduire opérer passage léchelle adéquat traiter corpus textuel tailler réel dépêche Reuters caractériser graphe corpus moyen dindicateurs classique coefficient clustering distribution degré tailler communauté caractéristique TourneBool quil permettre dégager anti lien entrer savoir séviter plaire quattendu faire hasard graphe lien antilien caractériser
765	Revue des Nouvelles Technologies de l'Information	EGC	2009	Handling Texts ? A Challenge for Data Mining	The amount of data in free form by far surpasses the structured records in databases in theirnumber. However, standard learning algorithms require observations in the form of vectorsgiven a fixed set of attributes. For texts, there is no such fixed set of attributes. The bag ofwords representation yields vectors with as many components as there are words in a language.Hence, the classification of documents represented as bag of word vectors demands efficientlearning algorithms. The TCat model for the support vector machine (Joachims 2002) offers asound performance estimation for text classification.The huge mass of documents, in principle, offers answers to many questions and is oneof the most important sources of knowledge. However, information retrieval and text classi-fication deliver merely the document, in which the answer can be found by a human reader ?not the answer itself. Hence, information extraction has become an important topic: if we canextract information from text, we can apply standard machine learning to the extracted facts(Craven et al. 1998). First, information extraction has to recognize Named Entities (see, e.g.,Roessler, Morik 2005). Second, relations between these become the nucleus of events. Ex-tracting events from a complex web site with long documents allows to automatically discoverregularities which are otherwise hidden in the mass of sentences (see, e.g., Jungermann, Morik2008).	Katharina Morik	http://editions-rnti.fr/render_pdf.php?p1&p=1000732	http://editions-rnti.fr/render_pdf.php?p=1000732	The amount of dater in free form by far surpasse the structured record in databaser in theirnumber However standard learning algorithm requir observation in the form of vectorsgiven fixed set of attribut For text there is no such fixed set of attribut The bag ofwords representation yields vectors with many component there are words in languagehence the classification of document represented bag of word vectors demands efficientlearning algorithm The TCat model for the support vector machiner Joachims 2002 offer asound performance estimation for text classificationthe huge mas of document in principle offer answers to many question and is oneof the most importer source of knowledge However information retrieval and text classification deliver merely the document in which the answer can be found by human reader not the answer itself hence information extraction has become an importer topic if we canextract information from text we can apply standard machiner learning to the extracted factscraven al 1998 First information extraction has to recognize Named entitier see egroessler morik 2005 second relation between these become the nucleus of event Extracting event from complex web site with long document allow to automatically discoverregularities which are otherwise hidden in the mass of sentence see eg Jungermann morik2008
766	Revue des Nouvelles Technologies de l'Information	EGC	2009	L'Analyse Formelle de Concepts pour l'Extraction de Connaissances dans les Données d'Expression de Gènes	L'analyse formelle de concepts (AFC, Ganter etWille (1999)) est uneméthode pertinente d'extraction de connaissances à partir de données complexesd'expression de gènes (Blachon et al. (2007), Motameny et al. (2008)). Dans cepapier, nous proposons d'extraire des groupes de gènes partageant un compor-tement similaire montrant des changements “significatifs” à travers divers envi-ronnements biologiques, servant d'hypothèses à la fonction des gènes.	Mehdi Kaytoue-Uberall, Sébastien Duplessis, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1000793	http://editions-rnti.fr/render_pdf.php?p=1000793	lanalyse formel concept AFC ganter etwille 1999 uneméthode pertinent dextraction connaissance partir donnée complexesdexpression gène Blachon al 2007 Motameny al 2008 Dans cepapier proposer dextraire groupe gène partager comportement similaire montrer changement “ significatif ” travers environnement biologique servir dhypothès fonction gène
767	Revue des Nouvelles Technologies de l'Information	EGC	2009	La carte GHSOM comme alternative à la SOM pour l'analyse exploratoire de données	L'objecif de cet article est de faire de la carte auto-organisatrice hiérarchique(GHSOM) un outil utilisable dans le cadre d'une démarche d'analyseexploratoire de données. La visualisation globale est un outil indispensable pourrendre les résultats d'une segmentation intelligibles pour un utilisateur. Nousproposons donc différents outils de visualisation pour la GHSOM équivalents àceux de la SOM.	Françoise Fessant, Fabrice Clérot, Pascal Gouzien	http://editions-rnti.fr/render_pdf.php?p1&p=1000734	http://editions-rnti.fr/render_pdf.php?p=1000734	Lobjecif article faire carte autoorganisatrice hiérarchiqueGHSOM outil utilisable dan cadrer dune démarcher danalyseexploratoir donnée visualisation global outil indispensable pourrendre résultat dune segmentation intelligible utilisateur nousproposon outil visualisation GHSOM équivalent àceux SOM
768	Revue des Nouvelles Technologies de l'Information	EGC	2009	La « créativité calculatoire » et les heuristiques créatives en synthèse de prédicats multiples	Nous présentons une approche à ce que nous appelons la « créativitécalculatoire », c'est-à-dire les procédés par lesquels une machine peut fairemontre d'une certaine créativité. Dans cet article, nous montronsessentiellement que la synthèse de prédicats multiples en programmationlogique inductive (ILP) et la synthèse de programmes à partir de spécificationsformelles (SPSF), deux domaines de l'informatique qui s'attaquent à desproblèmes où la notion de créativité est centrale, ont été amenés à ajouter àleur formalisme de base (l'ILP pour l'un, les tableaux de Beth pour l'autre)toute une série d'heuristiques. Cet article présente une collectiond'heuristiques qui sont destinées à fournir au programme une forme decréativité calculatoire. Dans cette présentation, l'accent est plutôt mis sur lesheuristiques de l'ILP mais lorsque cela était possible sans de trop longsdéveloppements, nous avons aussi présenté quelques heuristiques de la SPSF.L'outil indispensable de la créativité calculatoire est ce que nous appelons un‘générateur d'atouts' dont une spécification (forcément informelle commenous le verrons) est fournie comme première conclusion aux exemples décritsdans le corps de l'article.	Marta Franová, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1000747	http://editions-rnti.fr/render_pdf.php?p=1000747	présenter approcher appeler « créativitécalculatoir » cestàdir procédé machiner pouvoir fairemontre dune créativité Dans article montronsessentiellemer synthèse prédicat programmationlogiqu inductif ilp synthèse programme partir spécificationsformelle spsf domaine linformatique sattaquer desproblème notion créativité central amener ajouter àleur formalisme baser lilp lun tableau Beth lautretoute série dheuristiqu article présenter collectiondheuristique destiner fournir programmer former decréativité calculatoir Dans présentation laccent mettre lesheuristique lilp celer longsdéveloppement présenter heuristique spsfloutil indispensable créativité calculatoir appeler un‘générateur datout spécification forcément informel commenous verron fournir conclusion exemple décritsdans corps larticle
769	Revue des Nouvelles Technologies de l'Information	EGC	2009	Le logiciel SYR pour l'Analyse de Données Symboliques		Filipe Afonso, Edwin Diday, Wassim Khaskhoussi	http://editions-rnti.fr/render_pdf.php?p1&p=1000823	http://editions-rnti.fr/render_pdf.php?p=1000823	
770	Revue des Nouvelles Technologies de l'Information	EGC	2009	Logiciel « DtmVic » Data and Text Mining: Visualisation, Inférence, Classification		Ludovic Lebart	http://editions-rnti.fr/render_pdf.php?p1&p=1000815	http://editions-rnti.fr/render_pdf.php?p=1000815	
771	Revue des Nouvelles Technologies de l'Information	EGC	2009	Management des connaissances dans le domaine du patrimoine culturel		Stefan du Château, Danielle Boulanger, Eunika Mercier-Laurent	http://editions-rnti.fr/render_pdf.php?p1&p=1000792	http://editions-rnti.fr/render_pdf.php?p=1000792	
772	Revue des Nouvelles Technologies de l'Information	EGC	2009	Méthode de regroupement par graphe de voisinage	Ce travail s'inscrit dans la problématique de l'apprentissage non su-pervisé. Dans ce cadre se retrouvent les méthodes de classification automatiquenon paramétriques qui reposent sur l'hypothèse que plus des individus sontproches dans l'espace de représentation, plus ils ont de chances de faire par-tie de la même classe. Cet article propose une nouvelle méthode de ce type quiconsidère la proximité à travers la structure fournie par un graphe de voisinage.	Fabrice Muhlenbach	http://editions-rnti.fr/render_pdf.php?p1&p=1000782	http://editions-rnti.fr/render_pdf.php?p=1000782	travail sinscrit dan problématique lapprentissage superviser Dans cadrer retrouver méthode classification automatiquenon paramétrique reposer lhypothèse plaire individu sontproche dan lespace représentation plaire chance faire partir classer article proposer méthode typer quiconsidèr proximité travers structurer fourni graphe voisinage
773	Revue des Nouvelles Technologies de l'Information	EGC	2009	Modèle de préférences contextuelles pour les analyses OLAP	Cet article présente un environnement pour la personnalisation desanalyses OLAP afin de réduire la charge de navigation de l'utilisateur. Nousproposons un modèle de préférences contextuelles qui permet de restituer lesdonnées en fonction des préférences de l'utilisateur et de son contexted'analyse.	Houssem Jerbi, Franck Ravat, Olivier Teste, Gilles Zurfluh	http://editions-rnti.fr/render_pdf.php?p1&p=1000769	http://editions-rnti.fr/render_pdf.php?p=1000769	article présenter environnement personnalisation desanalys olap réduire charger navigation lutilisateur nousproposon modeler préférence contextuel permettre restituer lesdonner fonction préférence lutilisateur contextedanalyse
774	Revue des Nouvelles Technologies de l'Information	EGC	2009	Modélisation des connaissances dans le cadre de bibliothèques numériques spécialisées	Nous présentons une application innovante de la modélisation desconnaissances au domaine des bibliothèques numériques spécialisées. Nous utilisonsla spécification experte de la TEI (Text Encoding Initiative) pour modéliserla connaissance apportée par les chercheurs qui travaillent sur des archivesmanuscrites. Nous montrons les limites de la TEI dans le cas d'une approchediachronique du document, cette dernière impliquant la construction simultanéede structures de données concurrentes. Nous décrivons un modèle qui présentele problème et permet d'envisager des solutions. Enfin, nous justifions les structuresarborescentes sur lesquelles se base ce modèle.	Sylvie Calabretto, Pierre-Edouard Portier	http://editions-rnti.fr/render_pdf.php?p1&p=1000785	http://editions-rnti.fr/render_pdf.php?p=1000785	présenter application innovant modélisation desconnaissances domaine bibliothèqu numérique spécialiser utilisonsla spécification expert tei Text Encoding initiative modéliserla connaissance apporter chercheur travailler archivesmanuscrite montrer limite TEI dan cas dune approchediachronique document impliquer construction simultanéed structur donnée concurrenter décrire modeler présentele problèm permettre denvisager solution justifier structuresarborescente baser modeler
775	Revue des Nouvelles Technologies de l'Information	EGC	2009	Okmed et Wokm	Cet article traite de la problématique de la classification recouvrante(overlapping clustering) et propose deux variantes de l'approche OKM : OKMEDet WOKM. OKMED généralise k-médoïdes au cas recouvrant, il permet d'organiserun ensemble d'individus en classes non-disjointes, à partir d'une matricede distances. La méthode WOKM (Weighted-OKM) étend OKM par une pondérationlocale des classes ; cette variante autorise chaque individu à appartenir àplusieurs classes sur la base de critères différents. Des expérimentations sont réaliséessur une application cible : la classification de textes. Nous montrons alorsque OKMED présente un comportement similaire à OKM pour la métrique euclidienne,et offre la possibilité d'utiliser des métriques plus adaptées et d'obtenirde meilleures performances. Enfin, les résultats obtenus avec WOKM montrentun apport significatif de la pondération locale des classes	Guillaume Cleuziou	http://editions-rnti.fr/render_pdf.php?p1&p=1000736	http://editions-rnti.fr/render_pdf.php?p=1000736	article traiter problématique classification recouvranteoverlapping clustering proposer variante lapproche OKM   okmedet wokm OKMED généraliser kmédoïd cas recouvrir permettre dorganiserun ensembl dindividus classe nondisjointer partir dune matricede distance méthode wokm weightedokm étendre OKM pondérationlocale classe   variant autoriser individu appartenir àplusieurs classe baser critère expérimentation réaliséessur application cibl   classification texte montrer alorsqu OKMED présenter comportement similaire OKM métrique euclidienneet offrir possibilité dutiliser métrique plaire adapter dobtenirde meilleur performance résultat obtenir wokm montrentun apport significatif pondération local classe
776	Revue des Nouvelles Technologies de l'Information	EGC	2009	Online and Adaptive Anomaly Detection: Detecting Intrusions in Unlabelled Audit Data Streams		Wei Wang    , Thomas Guyet, Rene Quiniou, Marie-Odile Cordier, Florent Masseglia	http://editions-rnti.fr/render_pdf.php?p1&p=1000802	http://editions-rnti.fr/render_pdf.php?p=1000802	
777	Revue des Nouvelles Technologies de l'Information	EGC	2009	Partitionnement d'ontologies pour le passage à l'échelle des techniques d'alignement	L'alignement d'ontologies est une tâche importante dans les systèmesd'intégration puisqu'elle autorise la prise en compte conjointe de ressourcesdécrites par des ontologies différentes, en identifiant des appariements entreconcepts. Avec l'apparition de très grandes ontologies dans des domaines commela médecine ou l'agronomie, les techniques d'alignement, qui mettent souventen oeuvre des calculs complexes, se trouvent face à un défi : passer à l'échelle.Pour relever ce défi, nous proposons dans cet article deux méthodes de partition-nement, conçues pour prendre en compte, le plus tôt possible, l'objectif d'ali-gnement. Ces méthodes permettent de décomposer les deux ontologies à aligneren deux ensembles de blocs de taille limitée et tels que les éléments susceptiblesd'être appariés se retrouvent concentrés dans un ensemble minimal de blocs quiseront effectivement comparés. Les résultats des tests effectuées avec nos deuxméthodes sur différents couples d'ontologies montrent leur efficacité.	Fayçal Hamdi, Brigitte Safar, Haïfa Zargayouna, Chantal Reynaud	http://editions-rnti.fr/render_pdf.php?p1&p=1000787	http://editions-rnti.fr/render_pdf.php?p=1000787	lalignement dontologie tâcher important dan systèmesdintégration puisquell autoriser priser compter conjoindre ressourcesdécrite ontologie différenter identifier appariement entreconcept Avec lapparition grand ontologie dan domaine commela médecin lagronomie technique dalignemer mettre souventen oeuvrer calcul complexe trouver face défi   prendre léchellepour relever défi proposer dan article méthode partitionnement conçu prendre compter plaire tôt lobjectif dalignemer méthode permettre décomposer ontologie aligneren ensemble bloc tailler limité élément susceptiblesdêtre apparier retrouver concentrer dan ensemble minimal bloc quiseront effectivement comparé résultat test effectuer deuxméthode couple dontologie montrer efficacité
778	Revue des Nouvelles Technologies de l'Information	EGC	2009	Privacy and Data Mining: New Developments and Challenges	There is little doubt that data mining technologies create new challenges in the area of dataprivacy. In this talk, we will review some of the new developments in Privacy-preserving DataMining. In particular, we will discuss techniques in which data mining results can reveal per-sonal data, and how this can be prevented. We will look at the practically interesting situationswhere data to be mined is distributed among several parties. We will mention new applica-tions in which mining spatio-temporal data can lead to identification of personal information.We will argue that methods that effectively protect personal data, while at the same time pre-serve the quality of the data from the data analysis perspective, are some of the principal newchallenges before the field.	Stan Matwin	http://editions-rnti.fr/render_pdf.php?p1&p=1000730	http://editions-rnti.fr/render_pdf.php?p=1000730	There is little doubt that dater mining technologi create new challenge in the area of dataprivacy In this talk we will review some of the new development in Privacypreserving DataMining In particular we will discuss technique in which dater mining results can reveal personal dater and how thi can be prevented We will look at the practically interesting situationswher dater to be mined is distributed among several party We will mention new application in which mining spatiotemporal dater can lead to identification of personal informationw will arguer that method that effectively protect personal dater while at the same time preserv the quality of the dater from the dater analysis perspectif are some of the principal newchallenge before the field
779	Revue des Nouvelles Technologies de l'Information	EGC	2009	Probabilistic Multi-classifier by SVMs from voting rule to voting features		Anh Phuc Trinh, David Buffoni, Patrick Gallinari	http://editions-rnti.fr/render_pdf.php?p1&p=1000790	http://editions-rnti.fr/render_pdf.php?p=1000790	
780	Revue des Nouvelles Technologies de l'Information	EGC	2009	RDBToOnto : un logiciel dédié à l'apprentissage d'ontologies à partir de bases de données relationnelles	RDBToOnto1 est un logiciel extensible qui permet d'élaborer des on-tologies précises à partir de bases de données relationnelles. Le processus sup-porté est largement automatisé, de l'extraction des données à la génération dumodèle de l'ontologie et son instanciation. Pour affiner le résultat, le processuspeut être orienté par des contraintes locales définies interactivement. C'est aussiun cadre facilitant la mise en oeuvre de nouvelles méthodes d'apprentissage.	Farid Cerbah	http://editions-rnti.fr/render_pdf.php?p1&p=1000820	http://editions-rnti.fr/render_pdf.php?p=1000820	rdbtoonto1 logiciel extensible permettre délaborer ontologie précis partir base donnée relationnel processus supporter largement automatiser lextraction donnée génération dumodèl lontologie instanciation Pour affiner résultat processuspeut orienter contrainte local définir interactivement cest aussiun cadrer faciliter miser oeuvrer méthode dapprentissage
781	Revue des Nouvelles Technologies de l'Information	EGC	2009	Regroupement des Définitions de Sigles Biomédicaux	L'application présentée permet de regrouper les définitions de siglesissues des sciences du vivant par des mesures de proximité lexicale (approcheautomatique) et une intervention de l'expert (approche manuelle).	Ousmane Djanga, Hanine Hamzioui, Mickaël Hatchi, Isabelle Mougenot, Mathieu Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1000816	http://editions-rnti.fr/render_pdf.php?p=1000816	lapplication présenter permettre regrouper définition siglesissue science vivre mesure proximité lexical approcheautomatiqu intervention lexpert approcher manuel
782	Revue des Nouvelles Technologies de l'Information	EGC	2009	Résumé hybride de flux de données par échantillonnage et classification automatique	"Face à la grande volumétrie des données générées par les systèmes informatiques,l'hypothèse de les stocker en totalité avant leur interrogation n'estplus possible. Une solution consiste à conserver un résumé de l'historique duflux pour répondre à des requêtes et pour effectuer de la fouille de données.Plusieurs techniques de résumé de flux de données ont été développées, tellesque l'échantillonnage, le clustering, etc. Selon le champ de requête, ces résuméspeuvent être classés en deux catégories: résumés spécialisés et résumés généralistes.Dans ce papier, nous nous intéressons aux résumés généralistes. Notreobjectif est de créer un résumé de bonne qualité, sur toute la période temporelle,qui nous permet de traiter une large panoplie de requêtes. Nous utilisons deuxalgorithmes : CluStream et StreamSamp. L'idée consiste à les combiner afin detirer profit des avantages de chaque algorithme. Pour tester cette approche, nousutilisons un Benchmark de données réelles ""KDD_99"". Les résultats obtenussont comparés à ceux obtenus séparément par les deux algorithmes."	Nesrine Gabsi, Fabrice Clérot, Georges Hébrail	http://editions-rnti.fr/render_pdf.php?p1&p=1000767	http://editions-rnti.fr/render_pdf.php?p=1000767	face grand volumétrie donnée générer système informatiqueslhypothès stocker totalité interrogation nestplus solution consister conserver résumer lhistoriqu duflux répondre requête effectuer fouiller donnéesplusieurs technique résumer flux donnée développer tellesqu léchantillonnage clustering Selon champ requête résuméspeuvent classer catégorie résumer spécialiser résumer généralistesdans papier intéresser résumé généraliste Notreobjectif créer résumer qualité période temporellequi permettre traiter large panoplie requête utiliser deuxalgorithm   CluStream StreamSamp Lidée consister combiner detirer profit avantage algorithme Pour tester approcher nousutilison Benchmark donnée réel kdd99 résultat obtenussont comparer obtenu séparément algorithme
783	Revue des Nouvelles Technologies de l'Information	EGC	2009	SoftJaccard : une mesure de similarité entre ensembles de chaînes de caractères pour l'unification d'entités nommées	Parmi lesmesures de similarité classiques utilisables sur des ensemblesfigure l'indice de Jaccard. Dans le cadre de cet article, nous en proposons uneextension pour comparer des ensembles de chaînes de caractères. Cette mesurehybride permet de combiner une distance entre chaînes de caractères, telle que ladistance de Levenstein, et l'indice de Jaccard. Elle est particulièrement adaptéepourmettre en correspondance des champs composés de plusieurs chaînes de caractères,comme par exemple, lorsqu'on se propose d'unifier des noms d'entitésnommées.	Christine Largeron, Bernard Kaddour, Maria Fernandez	http://editions-rnti.fr/render_pdf.php?p1&p=1000795	http://editions-rnti.fr/render_pdf.php?p=1000795	Parmi lesmesure similarité classique utilisable ensemblesfigure lindice Jaccard Dans cadrer article proposon uneextension comparer ensemble chaîne caractèr mesurehybride permettre combiner distancer entrer chaîn caractère ladistance Levenstein lindice Jaccard adaptéepourmettr correspondance champ composer chaîne caractèrescomme exemple lorsquon proposer dunifier nom dentitésnommer
784	Revue des Nouvelles Technologies de l'Information	EGC	2009	SPAMS, une nouvelle approche incrémentale pour l'extraction de motifs séquentiels fréquents dans les Data streams	L'extraction de motifs séquentiels fréquents dans les datastreams est un enjeu important traité par la communauté des chercheursen fouille de données. Plus encore que pour les bases de données, denombreuses contraintes supplémentaires sont à considérer de par la na-ture intrinsèque des streams. Dans cet article, nous proposons un nouvelalgorithme en une passe : SPAMS, basé sur la construction incrémentale,avec une granularité très fine par transaction, d'un automate appelé SPA,permettant l'extraction des motifs séquentiels dans les streams. L'infor-mation du stream est apprise à la volée, au fur et à mesure de l'insertionde nouvelles transactions, sans pré-traitement a priori. Les résultats ex-périmentaux obtenus montrent la pertinence de la structure utilisée ainsique l'efficience de notre algorithme appliqué à différents jeux de données.	Lionel Vinceslas, Jean-Emile Symphor, Alban Mancheron, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1000765	http://editions-rnti.fr/render_pdf.php?p=1000765	lextraction motif séquentiel fréquent dan datastream enjeu importer traiter communauté chercheursen fouiller donnée plaire base donnée denombreus contraint supplémentaire considérer nature intrinsèque stream Dans article proposer nouvelalgorithme passer   spam baser construction incrémentaleavec granularité fin transaction dun automate appeler spapermetter lextraction motif séquentiel dan stream linformation stream apprendre voler fur mesurer linsertionde transaction prétraitement priori résultat expérimental obtenir montrer pertinence structurer utiliser ainsiqu lefficience algorithme appliquer jeu donnée
785	Revue des Nouvelles Technologies de l'Information	EGC	2009	SVM incrémental et parallèle sur GPU	Nous présentons un nouvel algorithme incrémental et parallèle deSéparateur à Vaste Marge (SVM ou Support Vector Machine) pour laclassification de très grands ensembles de données en utilisant le processeur dela carte graphique (GPUs, Graphics Processing Units). Les SVMs et lesméthodes de noyaux permettent de construire des modèles avec une bonneprécision mais ils nécessitent habituellement la résolution d'un programmequadratique ce qui requiert une grande quantité de mémoire et un long tempsd'exécution pour les ensembles de données de taille importante. Nousprésentons une extension de l'algorithme de Least Squares SVM (LS-SVM)proposé par Suykens et Vandewalle pour obtenir un algorithme incrémental etparallèle. Le nouvel algorithme est exécuté sur le processeur graphique pourobtenir une bonne performance à faible coût. Les résultats numériques sur lesensembles de données de l'UCI et Delve montrent que notre algorithmeincrémental et parallèle est environ 70 fois plus rapide sur GPU que sur CPUet significativement plus rapide (plus de 1000 fois) que les algorithmesstandards tels que LibSVM, SVM-perf et CB-SVM.	François Poulet, Thanh-Nghi Do, Van-Hoa Nguyen	http://editions-rnti.fr/render_pdf.php?p1&p=1000743	http://editions-rnti.fr/render_pdf.php?p=1000743	présenter nouvel algorithme incrémental parallèle deséparateur Vaste marge SVM Support Vector machiner laclassification grand ensemble donnée utiliser processeur dela carte graphique GPUs Graphics Processing unit svm lesméthod noyau permettre construire modèle bonneprécision nécessiter habituellement résolution dun programmequadratiqu requérir grand quantité mémoire long tempsdexécution ensemble donnée tailler important nousprésenton extension lalgorithme Least squar SVM lssvmproposer suyken Vandewalle obtenir algorithme incrémental etparallèle nouvel algorithme exécuter processeur graphique pourobtenir performance faible coût résultat numérique lesensemble donnée luci Delve montrer algorithmeincrémental parallèle 70 plaire rapide GPU CPUet significativement plaire rapide plaire 1000 algorithmesstandard libsvm SVMperf cbsvm
786	Revue des Nouvelles Technologies de l'Information	EGC	2009	TAAABLE : système de recherche et de création, par adaptation, de recettes de cuisine	TAAABLE is a textual case-based reasoning system that, according to requested/forbiddeningredients, dish types and/or dish origins, retrieves cooking recipes. If no recipe satisifies theconstraints, TAAABLE adapts existing recipes by replacing some ingredients by other ones.	Amélie Cordier, Jean Lieber, Emmanuel Nauer, Yannick Toussaint	http://editions-rnti.fr/render_pdf.php?p1&p=1000812	http://editions-rnti.fr/render_pdf.php?p=1000812	taaable is textual casebased reasoning system that according to requestedforbiddeningredient dish type andor dish origins retriev cooking recipe If no recipe satisifier theconstraint taaable adapts existing recipe by replacing some ingredient by other
787	Revue des Nouvelles Technologies de l'Information	EGC	2009	TraMineR: une librairie R pour l'analyse de données séquentielles		Alexis Gabadinho, Nicolas S. Müller, Gilbert Ritschard, Matthias Studer	http://editions-rnti.fr/render_pdf.php?p1&p=1000814	http://editions-rnti.fr/render_pdf.php?p=1000814	
788	Revue des Nouvelles Technologies de l'Information	EGC	2009	Un algorithme stable de décomposition pour l'analyse des réseaux sociaux dynamiques	Les réseaux dynamiques soulèvent de nouveaux problèmes d'analyses.Un outils efficace d'analyse doit non seulement permettre de décomposerces réseaux en groupes d'éléments similaires mais il doit aussi permettre la détectionde changements dans le réseau. Nous présentons dans cet article une nouvelleapproche pour l'analyse de tels réseaux. Cette technique est basée sur unalgorithme de décomposition de graphe en groupes chevauchants (ou chevauchement).La complexité de notre algorithme est O(|E| · deg2max +|V | · log(|V |))).La faible sensibilité de cet algorithme aux changements structuraux du réseaupermet d'en détecter les modifications majeures au cours du temps.	Romain Bourqui, Paolo Simonetto, Fabien Jourdan	http://editions-rnti.fr/render_pdf.php?p1&p=1000778	http://editions-rnti.fr/render_pdf.php?p=1000778	réseau dynamique soulever problème danalysesUn outil efficace danalyse devoir permettre décomposerce réseau groupe déléments similaire devoir permettre détectiond changement dan réseau présenter dan article nouvelleapproche lanalyse réseau technique baser unalgorithme décomposition graphe groupe chevauchant chevauchementla complexité algorithme oe · deg2max   · logv faible sensibilité algorithme changement structural réseaupermet den détecter modification majeur cours temps
789	Revue des Nouvelles Technologies de l'Information	EGC	2009	Un critère d'évaluation Bayésienne pour la construction d'arbres de décision	Nous présentons dans cet article un nouvel algorithme automatiquepour l'apprentissage d'arbres de décision. Nous abordons le problème selon uneapproche Bayésienne en proposant, sans aucun paramètre, une expression ana-lytique de la probabilité d'un arbre connaissant les données. Nous transformonsle problème de construction de l'arbre en un problème d'optimisation : nousrecherchons dans l'espace des arbres de décision, l'arbre optimum au sens ducritère Bayésien ainsi défini, c'est à dire l'arbre maximum a posteriori (MAP).L'optimisation est effectuée en exploitant une heuristique de pré-élagage. Desexpérimentations comparatives sur trente bases de l'UCI montrent que notreméthode obtient des performances prédictives proches de celles de l'état de l'arttout en étant beaucoup moins complexes.	Nicolas Voisine, Marc Boullé, Carine Hue	http://editions-rnti.fr/render_pdf.php?p1&p=1000740	http://editions-rnti.fr/render_pdf.php?p=1000740	présenter dan article nouvel algorithme automatiquepour lapprentissage darbr décision aborder problème uneapproche Bayésienne proposer paramètre expression analytique probabilité dun arbre connaître donnée transformonsl problème construction larbre problème doptimisation   nousrecherchon dan lespace arbre décision larbr optimum sens ducritère Bayésien définir cest larbr maximum posteriori maploptimisation effectuer exploiter heuristique préélagage desexpérimentation comparatif base luci montrer notreméthod obtenir performance prédictif létat larttout complexe
790	Revue des Nouvelles Technologies de l'Information	EGC	2009	Un nouvel algorithme de forêts aléatoires d'arbres obliques particulièrement adapté à la classification de données en grandes dimensions	L'algorithme des forêts aléatoires proposé par Breiman permet d'ob-tenir de bons résultats en fouille de données comparativement à de nombreusesapproches. Cependant, en n'utilisant qu'un seul attribut parmi un sous-ensembled'attributs tiré aléatoirement pour séparer les individus à chaque niveau de l'arbre,cet algorithme perd de l'information. Ceci est particulièrement pénalisant avecles ensembles de données en grandes dimensions où il peut exister de nom-breuses dépendances entre attributs. Nous présentons un nouvel algorithme deforêts aléatoires d'arbres obliques obtenus par des séparateurs à vaste marge(SVM). La comparaison des performances de notre algorithme avec celles del'algorithme de forêts aléatoires des arbres de décision C4.5 et de l'algorithmeSVM montre un avantage significatif de notre proposition.	Thanh-Nghi Do, Stéphane Lallich, Nguyen-Khang Pham, Philippe Lenca	http://editions-rnti.fr/render_pdf.php?p1&p=1000741	http://editions-rnti.fr/render_pdf.php?p=1000741	lalgorithm forêt aléatoire proposer Breiman permettre dobtenir résultat fouiller donnée comparativement nombreusesapproche nutiliser quun attribut sousensembledattribut tirer aléatoirement séparer individu niveau larbrecet algorithm perdre linformation pénaliser avecl ensemble donnée grand dimension pouvoir exister dépendance entrer attribut présenter nouvel algorithme deforêt aléatoire darbr oblique obtenir séparateur vaste margesvm comparaison performance algorithme delalgorithme forêt aléatoire arbre décision c45 lalgorithmeSVM montrer avantager significatif proposition
791	Revue des Nouvelles Technologies de l'Information	EGC	2009	Un prototype cross-lingue multi-métiers : vers la Gestion Sémantique de Contenu d'Entreprise au service du Collaboratif Opérationnel	Le domaine « Qualité, Hygiène, Sécurité et Environnement »(QHSE) représente à l'heure actuelle un vecteur de progrès majeur pourl'industrie européenne. Le prototype « Semantic Quality Environment » (SQE)introduit dans cet article vise à démontrer la validité d'une architecturesémantique cross-lingue vouée à la collaboration multi-métiers et multilingue,dans le cadre d'un système banalisé de gestion de contenu d'entreprise dédié àl'industrie navale européenne.	Christophe Thovex, Francky Trichet	http://editions-rnti.fr/render_pdf.php?p1&p=1000809	http://editions-rnti.fr/render_pdf.php?p=1000809	domaine « Qualité Hygiène sécurité environnement » qhse représenter lheure actuel vecteur progrès majeur pourlindustrie européen prototype « Semantic Quality Environment » sqeintroduit dan article viser démontrer validité dune architecturesémantiqu crosslingue vouer collaboration multimétier multilinguedans cadrer dun système banaliser gestion contenir dentrepris dédier àlindustrie naval européen
792	Revue des Nouvelles Technologies de l'Information	EGC	2009	Un système pour l'extraction de corrélations linéaires dans des données de génomique médicale		Arriel Benis, Mélanie Courtine	http://editions-rnti.fr/render_pdf.php?p1&p=1000807	http://editions-rnti.fr/render_pdf.php?p=1000807	
793	Revue des Nouvelles Technologies de l'Information	EGC	2009	Une méthode de classification supervisée sans paramètre pour l'apprentissage sur les grandes bases de données	Dans ce papier, nous présentons une méthode de classification super-visée sans paramètre permettant d'attaquer les grandes volumétries. La méthodeest basée sur des estimateurs de densités univariés optimaux au sens de Bayes,sur un classifieur Bayesien naïf amélioré par une sélection de variables et unmoyennage de modèles exploitant un lissage logarithmique de la distribution aposteriori des modèles. Nous analysons en particulier la complexité algorith-mique de la méthode et montrons comment elle permet d'analyser des bases dedonnées nettement plus volumineuses que la mémoire vive disponible. Nous pré-sentons enfin les résultats obtenu lors du récent PASCAL Large Scale LearningChallenge, où notre méthode a obtenu des performances prédictives de premierplan avec des temps de calcul raisonnables.	Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1000770	http://editions-rnti.fr/render_pdf.php?p=1000770	Dans papier présenter méthode classification superviser paramètre permettre dattaquer grand volumétrie méthodeest baser estimateur densité univarié optimal sens Bayessur classifieur Bayesien naïf améliorer sélection variable unmoyennage modèle exploiter lissage logarithmique distribution aposteriori modèle analyser complexité algorithmique méthode montron permettre danalyser base dedonner nettement plaire volumineux mémoire disponible présenter résultat obtenir récent PASCAL Large Scale LearningChallenge méthode obtenir performance prédictif premierplan temps calcul raisonnable
794	Revue des Nouvelles Technologies de l'Information	EGC	2009	Une nouvelle approche pour la classification non supervisée en segmentation d'image	La segmentation des images en régions est un problème crucial pourl'analyse et la compréhension des images. Parmi les approches existantes pourrésoudre ce problème, la classification non supervisée est fréquemment em-ployée lors d'une première étape pour réaliser un partitionnement de l'espacedes intensités des pixels (qu'il s'agisse de niveaux de gris, de couleurs ou de ré-ponses spectrales). Puisqu'elle ignore complètement les notions de voisinagedes pixels, une seconde étape d'analyse spatiale (étiquetage en composantesconnexes par exemple) est ensuite nécessaire pour identifier les régions issuesde la segmentation. La non prise en compte de l'information spatiale est une li-mite majeure de ce type d'approche, ce qui a motivé de nombreux travaux où laclassification est couplée à d'autres techniques pour s'affranchir de ce problème.Dans cet article, nous proposons une nouvelle formulation de la classificationnon supervisée permettant d'effectuer la segmentation des images sans faire ap-pel à des techniques supplémentaires. Plus précisément, nous élaborons une mé-thode itérative de type k-means où les données à partitionner sont les pixels eux-mêmes (et non plus leurs intensités) et où les distances des points aux centresdes classes ne sont plus euclidiennes mais topographiques. La segmentation estalors un processus itératif, et à chaque itération, les classes obtenues peuvent êtreassimilées à des zones d'influence dans le contexte de la morphologie mathéma-tique. Ce parallèle nous permet de bénéficier des algorithmes efficaces proposésdans ce domaine (tels que ceux basés sur les files d'attente), tout en y ajoutantle caractère itératif des méthodes de classification non supervisée considéréesici. Nous illustrons finalement le potentiel de l'approche proposée par quelquesrésultats préliminaires de segmentation sur des images artificielles.	Sébastien Lefèvre	http://editions-rnti.fr/render_pdf.php?p1&p=1000745	http://editions-rnti.fr/render_pdf.php?p=1000745	segmentation image région problème crucial pourlanalys compréhension image Parmi approche existant pourrésoudre problème classification superviser fréquemment employer dune étape réaliser partitionnement lespacede intensité pixel quil sagiss niveau gris couleur réponse spectral puisquelle ignorer complètement notion voisinagede pixel second étape danalyse spatial étiquetage composantesconnexe exemple ensuite nécessaire identifier région issuesd segmentation pris compter linformation spatial limiter majeur typer dapproch motiver travail laclassification coupler dautr technique saffranchir problèmeDans article proposer formulation classificationnon superviser permettre deffectuer segmentation image faire appel technique supplémentaire plaire précisément élaborer méthode itératif typer kmean donnée partitionner pixel euxmême plaire intensité distance point centresde classe plaire euclidien topographique segmentation estalors processus itératif itération classe obtenu pouvoir êtreassimiler zone dinfluence dan contexte morphologie mathématique parallèle permettre bénéficier algorithme efficace proposésdans domaine baser file dattente yu ajoutantle caractère itératif méthode classification superviser considéréesici illustrer finalement potentiel lapproche proposer quelquesrésultats préliminaire segmentation image artificiel
795	Revue des Nouvelles Technologies de l'Information	EGC	2009	Utilisation de l'analyse factorielle des correspondances pour la recherche d'images à grande échelle	Nous nous intéressons à l'utilisation de l'Analyse Factorielle des Cor-respondances (AFC) pour la recherche d'images par le contenu dans une base dedonnées d'images volumineuse. Nous adaptons l'AFC, méthode originellementdéveloppée pour l'Analyse des Données Textuelles (ADT), aux images en utili-sant des descripteurs locaux SIFT. En ADT, l'AFC permet de réduire le nombrede dimensions et de trouver des thèmes. Ici, l'AFC nous permettra de limiter lenombre d'images à examiner au cours de la recherche afin d'accélérer le tempsde réponse pour une requête. Pour traiter de grandes bases d'images, nous pro-posons une version incrémentale de l'algorithme AFC. Ce nouvel algorithmedécoupe une base d'images en blocs et les charge dans la mémoire l'un aprèsl'autre. Nous présentons aussi l'intégration des informations contextuelles (e.g.la Mesure de Dissimilarité Contextuelle (Jegou et al., 2007)) dans notre structurede recherche d'images. Cela améliore considérablement la précision. Nous ex-ploitons cette intégration dans deux axes: (i) hors ligne (la structure de voisinageest corrigée hors ligne) et (ii) à la volée (la structure de voisinage des images estcorrigée au cours de la recherche sur un petit ensemble d'images).	Nguyen-Khang Pham, Annie Morin, Patrick Gros, Quyet-Thang Le	http://editions-rnti.fr/render_pdf.php?p1&p=1000773	http://editions-rnti.fr/render_pdf.php?p=1000773	intéresser lutilisation lanalyse factoriel correspondance AFC rechercher dimager contenir dan baser dedonner dimage volumineux adapter lAFC méthod originellementdévelopper lanalyse donnée textuel ADT image utiliser descripteur local SIFT En adt lafc permettre réduire nombrede dimension trouver thème lAFC permettre limiter lenombre dimag examiner cours rechercher daccélérer tempsde réponse requête Pour traiter grand base dimager proposer version incrémental lalgorithm AFC nouvel algorithmedécoupe baser dimager bloc charger dan mémoire lun aprèslautre présenter lintégration information contextuel egla mesurer dissimilarité Contextuelle Jegou al 2007 dan structurede rechercher dimag celer améliorer considérablemer précision exploiter intégration dan axe ie ligne structurer voisinageest corriger ligne ii voler structurer voisinage image estcorriger cours rechercher petit ensemble dimag
796	Revue des Nouvelles Technologies de l'Information	EGC	2009	Vers la simulation et la détection des changements des données évolutives d'usage du Web	Dans le domaine des flux des données, la prise en compte du tempss'avère nécessaire pour l'analyse de ces données car leur distribution sous-jacentepeut changer au cours du temps. Un exemple typique concerne les modèles desprofils de navigation des internautes. Notre objectif est d'analyser l'évolutionde ces profils, celle-ci peut être liée au changement d'effectifs ou aux déplacementde clusters au cours du temps. Afin d'analyser la validité de notre approche,nous mettons en place uneméthodologie pour la simulation des données d'usageà partir de laquelle il est possible de contrôler l'occurrence des changements	Alzennyr Da Silva, Yves Lechevallier, Francisco de Assis Tenório de Carvalho	http://editions-rnti.fr/render_pdf.php?p1&p=1000800	http://editions-rnti.fr/render_pdf.php?p=1000800	Dans domaine flux donnée priser compter tempssavère nécessaire lanalyse donnée distribution sousjacentepeut changer cours temps exemple typique concerner modèle desprofils navigation internaute objectif danalyser lévolutiond profil celleci pouvoir lier changement deffectif déplacementde cluster cours temps Afin danalyser validité approchenou metton placer uneméthodologie simulation donnée dusageà partir chuter loccurrence changement
797	Revue des Nouvelles Technologies de l'Information	EGC	2009	Vers le traitement à grande échelle de données symboliques		Omar Merroun, Edwin Diday, Philippe Rigaux	http://editions-rnti.fr/render_pdf.php?p1&p=1000791	http://editions-rnti.fr/render_pdf.php?p=1000791	
798	Revue des Nouvelles Technologies de l'Information	EGC	2009	Vers une utilisation améliorée de relations spatiales pour l'apprentissage de données dans les modèles graphiques	Nous nous intéressons dans cet article aux représentations des relationsspatiales pour l'extraction d'information et la modélisation des donnéesvisuelles, en particulier dans le contexte de la catégorisation d'images. Nousmontrons comment la prise en compte d'une relation spatiale entre deux élémentsentraîne l'apparition d'une information supplémentaire entre ces élémentset le reste de l'ensemble à modéliser, ce qui est rarement exploité explicitement.Une représentation floue des relations dans unmodèle graphique est bien adaptéepour les algorithmes d'apprentissage utilisés actuellement et permet d'intégrerce type d'information complémentaire qui concerne l'absence d'une interactionplutôt que sa présence. Nous tentons d'évaluer les bénéfices de cette approchesur un problème de traitement d'images.	Isabelle Bloch, Emanuel Aldea	http://editions-rnti.fr/render_pdf.php?p1&p=1000772	http://editions-rnti.fr/render_pdf.php?p=1000772	intéresser dan article représentation relationsspatiale lextraction dinformation modélisation donnéesvisuelle dan contexte catégorisation dimag nousmontron priser compter dune relation spatial entrer élémentsentraîne lapparition dune information supplémentaire entrer élémentset rester lensembl modéliser exploiter explicitementune représentation flouer relation dan unmodèle graphique adaptéepour algorithme dapprentissage utiliser actuellement permettre dintégrerce typer dinformation complémentaire concerner labsence dune interactionplutôt présence tenter dévaluer bénéfice approchesur problème traitement dimag
799	Revue des Nouvelles Technologies de l'Information	EGC	2008	A spatial rough set for extracting the periurban fringe	To date the availability of spatial data is increasing together withtechniques and methods adopted in geographical analysis. Despite this tendency,classifying in a sharp way every part of the city is more and more complicated.This is due to the growth of city complexity. Rough Set theory maybe a useful method to employ in combining great amounts of data in order tobuild complex knowledge about territory. It represents a different mathematicalapproach to uncertainty by capturing the indiscernibility. Two differentphenomena can be indiscernible in some contexts and classified in the sameway when combining available information about them. Several experiencesexist in the use of Rough Set theory in data mining, knowledge analysis andapproximate pattern classification, but the spatial component lacks in all theseresearch streams.This paper aims to the use of Rough Set methods in geographical analyses.This approach has been applied in a case of study, comparing the resultsachieved by means of both Map Algebra technique and Spatial Rough set. Thestudy case area, Potenza Province, is particularly suitable for the application ofthis theory, because it includes 100 municipalities with a different number ofinhabitants and morphologic features.	Beniamino Murgante, Giuseppe Las Casas, Anna Sansone	http://editions-rnti.fr/render_pdf.php?p1&p=1001234	http://editions-rnti.fr/render_pdf.php?p=1001234	to dater the availability of spatial dater is increasing together withtechniqu and method adopted in geographical analysi Despite this tendencyclassifying in sharp way every partir of the city is more and more complicatedThis is to the growth of city complexity Rough Set theory maybe useful method to employ in combining great amount of dater in order tobuild complex knowledge about territory it represent mathematicalapproach to uncertainty by capturing the indiscernibility Two differentphenomener can be indiscernibl in som context and classified in the sameway when combining availabl information about them several experiencesexist in the us of Rough Set theory in dater mining knowledge analysis andapproximate pattern classification boire the spatial component lacks in all theseresearch streamsthi paper aim to the us of Rough Set method in geographical analysesthis approach has been applied in caser of study comparing the resultsachieved by means of both Map Algebra technique and Spatial Rough set Thestudy caser area Potenza Province is particularly suitabl for the application ofthis theory because it include 100 municipalitie with differer number ofinhabitant and morphologic featur
800	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Algorithmes rapides de boosting de SVM	Les algorithmes de boosting de Newton Support Vector Machine (NSVM), Proximal Support Vector Machine (PSVM) et Least-Squares Support Vector Machine (LS-SVM) que nous présentons visent à la classification de très grands ensembles de données sur des machines standard. Nous présentons une extension des algorithmes de NSVM, PSVM et LS-SVM, pour construire des algorithmes de boosting. A cette fin, nous avons utilisé un terme de régularisation de Tikhonov et le théorème Sherman-Morrison- Woodbury pour adapter ces algorithmes au traitement d'ensembles de données ayant un grand nombre de dimensions. Nous les avons ensuite étendus par construction d'algorithmes de boosting de NSVM, PSVM et LS-SVM afin de traiter des données ayant simultanément un grand nombre d'individus et de dimensions. Les performances des algorithmes sont évaluées sur des grands ensembles de données de l'UCI comme Adult, KDDCup 1999, Forest Covertype, Reuters-21578 et RCV1-binary sur une machine standard (PC-P4, 2,4 GHz, 1024 Mo RAM).	Thanh-Nghi Do, Jean-Daniel Fekete, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1000617	http://editions-rnti.fr/render_pdf.php?p=1000617	algorithme boosting Newton Support Vector machiner nsvm Proximal Support Vector machiner PSVM leastsquar Support Vector Machine LSSVM présenter viser classification grand ensemble donnée machine standard présenter extension algorithme nsvm psvm LSSVM construire algorithme boosting A fin utiliser terme régularisation Tikhonov théorème shermanmorrison Woodbury adapter algorithme traitement densembl donnée grand nombre dimension ensuite étendre construction dalgorithme boosting nsvm psvm LSSVM traiter donnée simultanément grand nombre dindividus dimension performance algorithme évaluer grand ensemble donnée luci Adult KDDCup 1999 Forest Covertype Reuters21578 rcv1binary machiner standard pcp4 24 ghz 1024 Mo ram
801	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Analyse exploratoire d'opinions cinématographiques : co-clustering de corpus textuels communautaires	Les sites communautaires sont un endroit privilégié pour s'exprimer et publier des opinions. Le site www.flixster.com est un exemple de site participatif sur lequel se rassemblent plus de 20 millions de cinéphiles qui partagent des commentaires sur les films qu'ils ont ou non aimés. Explorer les contenus autoproduits est un challenge pour qui veut comprendre les attentes des internautes. Par une méthode d'apprentissage non supervisée, nous montrerons qu'il est possible de mieux comprendre le vocabulaire utilisé pour décrire des opinions. En particulier, grâce à une méthode de co-clustering, nous montrerons qu'un rapprochement peut être fait entre des films particuliers sur la base de l'usage d'un vocabulaire particulier. L'analyse des résultats peut conduire à retrouver une certaine typologie de films ou encore des rapprochements entre films. Cette étude peut être complémentaire avec des analyses linguistiques des corpus, ou encore être exploitée dans un contexte applicatif de recommandation de contenus multimédias.	Damien Poirier, Cécile Bothorel, Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1000654	http://editions-rnti.fr/render_pdf.php?p=1000654	site communautaire endroit privilégier sexprimer publier opinion site wwwflixstercom exemple site participatif rassembler plaire 20 million cinéphile partager commentaire film quil aimé explorer contenu autoproduits challenge vouloir comprendre attente internaute Par méthode dapprentissage superviser montrer quil mieux comprendre vocabulaire utiliser décrire opinion En grâce méthode coclustering montrer quun rapprochement pouvoir faire entrer film baser lusage dun vocabulair lanalyse résultat pouvoir conduire retrouver typologie film rapprochement entrer film étude pouvoir complémentaire analyse linguistique corpus exploiter dan contexte applicatif recommandation contenu multimédia
802	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Apport des traitements morpho-syntaxiques pour l'alignement des définitions par une classification SVM	Cet article propose une méthode d'alignement automatique de définitions destinée à améliorer la fusion entre des terminologies spécialisées et un vocabulaire médical généraliste par un classifieur de type SVM (Support Vecteur Machine) et une représentation compacte et pertinente d'un couple de définitions par concaténation d'un ensemble de mesures de similarité, afin de tenir compte de leur complémentarité, auquelle nous ajoutons les longueurs de chacune des définitions. Trois niveaux syntaxiques ont été investigués. Le modèle fondé sur un apprentissage à partir des groupes nominaux de type Noms-Adjectifs aboutit aux meilleures performances.	Laura Diosan, Alexandrina Rogozan, Jean-Pierre Pécuchet	http://editions-rnti.fr/render_pdf.php?p1&p=1000582	http://editions-rnti.fr/render_pdf.php?p=1000582	article proposer méthode dalignement automatique définition destiner améliorer fusion entrer terminologie spécialiser vocabulair médical généraliste classifieur typer SVM Support Vecteur machiner représentation compact pertinent dun coupler définition concaténation dun ensemble mesure similarité compter complémentarité auquell ajouter longueur définition Trois niveau syntaxique investiguer modeler fonder apprentissage partir groupe nominal typer nomsadjectif aboutir meilleur performance
803	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Approche d'annotation automatique des événements	"Quotidiennement, plusieurs agences de presse publient des milliers d'articles contenant plusieurs événements de toutes sortes (politiques, économiques, culturels, etc.). Les preneurs de décision, se trouvent face à ce grand nombre d'événements dont seulement quelques uns les concernent. Le traitement automatique de tels événements devient de plus en plus nécessaires. Pour cela, nous proposons une approche, qui se base sur l'apprentissage automatique, et qui permet d'annoter les articles de presse pour générer un résumé automatique contenant les principaux événements. Nous avons validé notre approche par le développement du système ""AnnotEv""."	Rim Faiz, Aymen Elkhlifi	http://editions-rnti.fr/render_pdf.php?p1&p=1000554	http://editions-rnti.fr/render_pdf.php?p=1000554	quotidiennement agence presser publier millier darticl contenir événement sorte politique économique culturel preneur décision trouver face grand nombre dévénement concerner traitement automatique événement devenir plaire plaire nécessaire Pour celer proposer approcher baser lapprentissage automatique permettre dannoter article presser générer résumer automatique contenir principal événement valider approcher développement système AnnotEv
804	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Approche hybride de classification à base de treillis de Galois: application à la reconnaissance de visages	La recherche dans le domaine de la reconnaissance de visages profite des solutions obtenues dans le domaine de l'apprentissage automatique. Le problème de classification de visages peut être considéré comme un problème d'apprentissage supervisé où les exemples d'apprentissage sont les visages étiquetés. Notre article introduit dans ce contexte une nouvelle approche hybride de classification qui utilise le paradigme d'apprentissage automatique supervisé. Ainsi, en se basant sur le fondement mathématique des treillis de Galois et leur utilisation pour la classification supervisée, nous proposons un nouvel algorithme de classification baptisé CITREC ainsi que son application pour la reconnaissance de visages. L'originalité de notre approche provient de la combinaison de l'analyse formelle de concepts avec les approches de classification supervisée à inférence bayésienne ou à plus proches voisins. Une validation expérimentale est décrite sur un benchmark du domaine de la reconnaissance de visages.	Yahya Slimani, Cherif Chiraz Latiri, Brahim Douar	http://editions-rnti.fr/render_pdf.php?p1&p=1000618	http://editions-rnti.fr/render_pdf.php?p=1000618	rechercher dan domaine reconnaissance visage profiter solution obtenu dan domaine lapprentissage automatique problème classification visage pouvoir considérer problème dapprentissage superviser exemple dapprentissage visage étiqueté article introduire dan contexte approcher hybride classification utiliser paradigm dapprentissage automatique superviser baser fondement mathématique treillis Galois utilisation classification superviser proposer nouvel algorithme classification baptiser CITREC application reconnaissance visage loriginalité approcher provenir combinaison lanalyse formel concept approche classification superviser inférence bayésienn plaire voisin validation expérimental décrire benchmark domaine reconnaissance visage
805	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Approches de type n-grammes pour l'analyse de parcours de vie familiaux	Cet article porte sur l'analyse de parcours de vie représentés sous forme de séquences d'événements. Plus spécifiquement, on examine les possibilités d'exploiter des codages de type n-grammes de ces séquences pour en extraire des connaissances. En fait, compte tenu de la simultanéité de certains événements, une procédure stricte de n-grammes comme on peut par exemple l'appliquer sur des textes, n'est pas applicable ici. Nous discutons diverses alternatives qui s'avèrent finalement plus proches de la fouille de séquences fréquentes. Les concepts discutés sont illustrés sur des données de l'enquête biographique rétrospective réalisée par le Panel suisse de ménages en 2002. Enfin, on précisera sur quels aspects l'approche proposée peut apporter un éclairage complémentaire utile par rapport à d'autres techniques plus classiques d'analyse exploratoire de parcours de vie.	Matthias Studer, Alexis Gabadinho, Nicolas S. Müller, Gilbert Ritschard	http://editions-rnti.fr/render_pdf.php?p1&p=1000639	http://editions-rnti.fr/render_pdf.php?p=1000639	article porter lanalyse parcours vie représenter sou former séquence dévénement plaire spécifiquement examiner possibilité dexploiter codage typer ngramm séquence extraire connaissance En faire compter simultanéité événement procédure strict ngramme pouvoir exemple lappliquer texte nest applicable discuton alternative saver finalement plaire fouiller séquence fréquent concept discuter illustrer donnée lenquête biographique rétrospectif réaliser Panel suisse ménage 2002 préciser aspect lapproche proposer pouvoir apporter éclairage complémentaire utile rapport dautre technique plaire classique danalyse exploratoire parcours vie
806	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Assignation automatique de solutions à des classes de plaintes liées aux ambiances intérieures polluées	Nous présentons dans cet article un système informatique pour le traitement des plaintes en lien avec des situations de pollution domestique écrites en français. Après la construction automatique d'une base de scenarii de plaintes, un module de recherche apparie la plainte à traiter à la thématique de la plainte la plus similaire. Enfin, il s'agit d'assigner au problème courant la solution correspondante au scénario de pollution auquel est affectée la plainte pertinente. Nous montrons ici l'intérêt de l'introduction dans l'appariement des textes de l'aspect sémantique géré par un dictionnaire généraliste de synonymes et en quoi il n'est pas réalisable pour notre problème particulier de construire une ontologie.	Zoulikha Heddadji, Nicole Vincent, Séverine Kirchner, Georges Stamon	http://editions-rnti.fr/render_pdf.php?p1&p=1000656	http://editions-rnti.fr/render_pdf.php?p=1000656	présenter dan article système informatique traitement plainte lien situation pollution domestique écrit français Après construction automatique dune baser scenarii plainte moduler rechercher appari plaindre traiter thématique plaindre plaire similaire sagit dassigner problème courir solution correspondant scénario pollution affecter plaindre pertinent montrer lintérêt lintroduction dan lappariement texte laspect sémantique gérer dictionnair généraliste synonyme nest réalisable problème construire ontologie
807	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Binary Block GTM : Carte auto-organisatrice probabiliste pour les grands tableaux binaires	Ce papier présente un modèle génératif et son estimation permettant la visualisation de données binaires. Notre approche est basée sur un modèle de mélange de lois de Bernoulli par blocs et les cartes de Kohonen probabilistes. La méthode obtenue se montre à la fois parcimonieuse et pertinente en pratique.	Rodolphe Priam, Mohamed Nadif, Gérard Govaert	http://editions-rnti.fr/render_pdf.php?p1&p=1000614	http://editions-rnti.fr/render_pdf.php?p=1000614	papier présenter modeler génératif estimation permettre visualisation donnée binaire approcher baser modeler mélanger loi Bernoulli bloc carte Kohonen probabilister méthode obtenir montrer parcimonieux pertinent pratiquer
808	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Cas d'utilisation réelle de Nautilus : calculs d'indicateurs chez un opérateur télécom	Nautilus est un logiciel d'analyse de bases de données. Le but de cette application est de généraliser l'utilisation de données clients au sein des entreprises. Elle facilite l'accès aux données en permettant de visualiser et manipuler les données du SGBD sous forme de concepts métiers. Elle inclut un générateur de requêtes SQL et un outil de gestion de tâches désignées pour l'agrégation de grands volumes de données. Le principe de fonctionnement est basé sur l'enchaînement de phases permettant la création des données d'analyse : importation des métadonnées du SGBD ; construction d'un dictionnaire de des concepts métiers ; spécification des champs à calculer. Les différents traitements tels que les jointures et l'alimentation des tables sont optimisés afin de rendre l'application utilisable sur des SGBD d'entreprise	Adrien Schmidt, Serge Fantino	http://editions-rnti.fr/render_pdf.php?p1&p=1000601	http://editions-rnti.fr/render_pdf.php?p=1000601	nautilu logiciel danalyse base donnée boire application généraliser lutilisation donnée client entreprise faciliter laccè donnée permettre visualiser manipuler donnée SGBD sou former concept métier inclure générateur requête SQL outil gestion tâche désigner lagrégation grand volume donnée principe fonctionnement baser lenchaînement phase permettre création donnée danalyse   importation métadonnée sgbd   construction dun dictionnair concept métier   spécification champ calculer traitement jointure lalimentation table optimiser lapplication utilisable sgbd dentrepris
809	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Classification adaptative de séries temporelles : application à l'identification des gènes exprimés au cours du cycle cellulaire	Ce travail s'inscrit dans le cadre de l'étude de la division cellulaire assurant la prolifération des cellules. Une meilleure compréhension de ce phénomène biologique nécessite l'identification des gènes caractérisant chaque phase du cycle cellulaire. Le procédé d'identification est généralement basé sur un ensemble de gènes dits gènes de référence, sélectionnés expérimentalement et considérés comme caractérisant les phases du cycle cellulaire. Les niveaux d'expression des gènes étudiés sont mesurés durant le cycle de la division cellulaire et permettent de construire des profils d'expression. Chaque gène étudié est affecté à la phase du cycle cellulaire correspondant au groupe de gènes de référence le plus similaire. Cette approche classique souffre de deux limites. D'une part les mesures de proximité les plus couramment utilisés entre profils d'expression de gènes sont basées sur les écarts en valeurs sans tenir compte de la forme des profils. D'autre part, dans la littérature, il n'y a pas consensus quant à l'ensemble des gènes de référence à considérer. Dans cet article, notre but est de proposer une classification adaptative, basée sur un indice de dissimilarité incluant les proximités en valeurs et en forme des profils d'expression de gènes, permettant d'identifier les phases d'expression des gènes étudiés, et de présenter un nouvel ensemble de gènes de référence validé par une connaissance biologique.	Alpha Diallo, Ahlame Douzal-Chouakria, Françoise Giroud	http://editions-rnti.fr/render_pdf.php?p1&p=1000637	http://editions-rnti.fr/render_pdf.php?p=1000637	travail sinscrit dan cadrer létude division cellulaire assurer prolifération cellule meilleur compréhension phénomène biologique nécessit lidentification gène caractériser phase cycle cellulaire procéder didentification généralement baser ensemble gène gèn référence sélectionner expérimentalement considérer caractériser phase cycle cellulaire niveau dexpression gène étudier mesurer durer cycle division cellulaire permettre construire profil dexpression gène étudier affecter phase cycle cellulaire correspondre grouper gène référence plaire similaire approcher classique souffrir limite dune partir mesure proximité plaire couramment utiliser entrer profil dexpression gène baser écart compter former profil Dautre partir dan littérature ny consensus lensemble gène référence considérer Dans article boire proposer classification adaptatif baser indice dissimilarité inclure proximiter former profil dexpression gène permettre didentifier phase dexpression gène étudier poster nouvel ensemble gène référence valider connaissance biologique
810	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Classification de documents en réseaux petits mondes en vue d'apprentissage		Mohamed Khazri, Mohamed Tmar, Mohand Boughanem, Mohamed Abid	http://editions-rnti.fr/render_pdf.php?p1&p=1000581	http://editions-rnti.fr/render_pdf.php?p=1000581	
811	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Clustering en haute dimension par accumulation de clusterings locaux	"Le clustering est une tâche fondamentale de la fouille de données. Ces dernières années, les méthodes de type cluster ensembles ont été l'objet d'une attention soutenue. Il s'agit d'agréger plusieurs clusterings d'un jeu de données afin d'obtenir un clustering ""moyen"". Les clusterings individuels peuvent être le résultat de différents algorithmes. Ces méthodes sont particulièrement utiles lorsque la dimensionalité des données ne permet pas aux méthodes classiques basées sur la distance et/ou la densité de fonctionner correctement. Dans cet article, nous proposons une méthode pour obtenir des clusterings individuels à faible coût, à partir de projections partielles du jeu de données. Nous évaluons empiriquement notre méthode et la comparons à trois méthodes de différents types. Nous constatons qu'elle donne des résultats sensiblement supérieurs aux autres."	Marc-Ismaël Akodjènou-Jeannin, Kavé Salamatian, Patrick Gallinari	http://editions-rnti.fr/render_pdf.php?p1&p=1000612	http://editions-rnti.fr/render_pdf.php?p=1000612	clustering tâcher fondamental fouiller donnée dernière année méthode typer cluster ensembl lobjet dune attention soutenir sagit dagréger clustering dun jeu donnée dobtenir clustering moyen clustering individuel pouvoir résultat algorithme méthode utile dimensionalité donnée permettre méthode classique baser distancer etou densité fonctionner correctement Dans article proposer méthode obtenir clustering individuel faible coût partir projection partiel jeu donnée évaluer empiriquement méthode comparer méthode type constater donner résultat sensiblement supérieur
812	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Clustering Visuel Semi-Supervisé pour des systèmes en coordonnées en étoiles 3D	Dans cet article, nous proposons une approche qui combine les méthodes statistiques avancées et la flexibilité des approches interactives manuelles en clustering visuel. Nous présentons l'interface Semi-Supervised Visual Clustering (SSVC). Sa contribution principale est l'apprentissage d'une métrique de projection optimale pour la visualisation en coordonnées en étoiles ainsi que pour l'extension 3D que nous avons développée. La métrique de distance de projection est apprise à partir des retours de l'utilisateur soit en termes de similarité/ dissimilarité entre les items, soit par l'annotation directe. L'interface SSVC permet, de plus, une utilisation hybride dans laquelle un ensemble de paramètres sont manuellement fixés par l'utilisateur tandis que les autres paramètres sont déterminés par un algorithme de distance optimale.	Loïc Lecerf, Boris Chidlovskii	http://editions-rnti.fr/render_pdf.php?p1&p=1000559	http://editions-rnti.fr/render_pdf.php?p=1000559	Dans article proposer approcher combiner méthode statistique avancer flexibilité approche interactif manuel clustering visuel présenter linterface SemiSupervised Visual Clustering SSVC contribution principal lapprentissage dune métrique projection optimal visualisation coordonner étoile lextension 3D développer métrique distancer projection apprendre partir lutilisateur terme similarité dissimilarité entrer item lannotation direct Linterface SSVC permettre plaire utilisation hybride dan ensemble paramètre manuellement fixer lutilisateur paramètre déterminer algorithme distancer optimal
813	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Co-classification sous contraintes par la somme des résidus quadratiques	"Dans de nombreuses applications, une co-classification est plus facile à interpréter qu'une classification mono-dimensionnelle. Il s'agit de calculer une bi-partition ou collection de co-clusters : chaque co-cluster est un groupe d'objets associé à un groupe d'attributs et les interprétations peuvent s'appuyer naturellement sur ces associations. Pour exploiter la connaissance du domaine et ainsi améliorer la pertinence des partitions, plusieurs méthodes de classification sous contraintes ont été proposées pour le cas mono-dimensionnel, e.g., l'exploitation de contraintes ""must-link"" et ""cannot-link"". Nous considérons ici la co-classification sous contraintes avec la gestion de telles contraintes étendues aux dimensions des objets et des attributs, mais aussi l'expression de contraintes de contiguité dans le cas de domaines ordonnés. Nous proposons un algorithme itératif qui minimise la somme des résidus quadratiques et permet l'exploitation active des contraintes spécifiées par les analystes. Nous montrons la valeur ajoutée de ce type d'extraction sur deux applications en analyse du transcriptome."	Ruggero G. Pensa, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1000665	http://editions-rnti.fr/render_pdf.php?p=1000665	Dans application coclassification plaire facile interpréter quune classification monodimensionnell sagit calculer bipartition collection cocluster   cocluster grouper dobjet associer grouper dattribut interprétation pouvoir sappuyer naturellement association Pour exploiter connaissance domaine améliorer pertinence partition méthode classification sou contraint proposer cas monodimensionnel eg lexploitation contrainte mustlink cannotlink considérer coclassification sou contrainte gestion contraint étendu dimension objet attribut lexpression contrainte contiguité dan cas domaine ordonner proposer algorithme itératif minimiser sommer résidu quadratique permettre lexploitation activer contrainte spécifier analyste montrer ajouter typer dextraction application analyser transcriptome
814	Revue des Nouvelles Technologies de l'Information	EGC	2008	Conception de systèmes d'information spatio-temporelle adaptatifs avec ASTIS	Les avancées technologiques récentes du Web et du sans fil,conjuguées au succès des applications spatialisées grand public, sont àl'origine d'un accès accru aux systèmes d'information spatio-temporelle(SIST) par une grande diversité d'utilisateurs, munis des dispositifs d'accèset dans des contextes d'utilisation variés. Adapter ces systèmes à l'utilisateurdevient donc une nécessité, un gage d'utilisabilité et de pérennité. Cet articleprésente une approche générique pour la conception et la génération desystèmes d'information spatio-temporelle adaptés à l'utilisateur, appeléASTIS. ASTIS offre des modalités générales de mise en oeuvre del'adaptation à l'utilisateur, visant tant le contenu que la présentation desapplications. Elle permet aux concepteurs d'intégrer ces modalitésd'adaptation dans des applications traitant des données spatio-temporelles.Afin de définir les besoins et types d'adaptation propres à leur application, ilsuffit aux concepteurs de créer des modèles conceptuels, par spécialisation etinstanciation des modèles offerts par notre architecture	Bogdan Moisuc, Jérôme Gensel, Hervé Martin	http://editions-rnti.fr/render_pdf.php?p1&p=1001232	http://editions-rnti.fr/render_pdf.php?p=1001232	avancée technologique récent web filconjuguée succès application spatialiser grand public àlorigine dun accès accroître système dinformation spatiotemporellesist grand diversité dutilisateur muni dispositif daccèset dan contexte dutilisation varier adapter système lutilisateurdevient nécessiter gager dutilisabilité pérennité articleprésente approcher générique conception génération desystèm dinformation spatiotemporell adapter lutilisateur appeléastis ASTIS offrir modalité général miser oeuvrer deladaptation lutilisateur viser contenir présentation desapplications permettre concepteur dintégrer modalitésdadaptation dan application traiter donnée spatiotemporellesafin définir besoin type dadaptation propre application ilsuffit concepteur créer modèle conceptuel spécialisation etinstanciation modèle offert architecturer
815	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Data mining for activity extraction in video data	The exploration of large video data is a task which is now possible because of the advances made on object detection and tracking. Data mining techniques such as clustering are typically employed. Such techniques have mainly been applied for segmentation/indexation of video but knowledge extraction of the activity contained in the video has been only partially addressed. In this paper we present how video information is processed with the ultimate aim to achieve knowledge discovery of people activity in the video. First, objects of interest are detected in real time. Then, in an off-line process, we aim to perform knowledge discovery at two stages: 1) finding the main trajectory patterns of people in the video. 2) finding patterns of interaction between people and contextual objects in the scene. An agglomerative hierarchical clustering is employed at each stage. We present results obtained on real videos of the Torino metro (Italy).	Monique Thonnat, Jose Luis Patino, Etienne Corvée, François Brémond	http://editions-rnti.fr/render_pdf.php?p1&p=1000632	http://editions-rnti.fr/render_pdf.php?p=1000632	The exploration of large video dater is task which is now because of the advance mad object detection and tracking Data mining technique such clustering are typically employed Such technique hav mainly been applied for segmentationindexation of video boire knowledg extraction of the activity contained in the video has been only partially addressed In this paper we preser how video information is processed with the ultimate aim to achiev knowledge discovery of people activity in the video First object of interest are detected in real time Then in an offline process we aim to perform knowledg discovery at two stage 1 finding the main trajectory pattern of people in the video 2 finding pattern of interaction between people and contextual objects in the scene an agglomerative hierarchical clustering is employed at each stage We preser results obtained real videos of the Torino metro Italy
816	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Découverte de motifs séquentiels et de règles inattendus	Les travaux autour de l'extraction de motifs séquentiels se sont particulièrement focalisés sur la définition d'approches efficaces pour extraire, en fonction d'une fréquence d'apparition, des corrélations entre des éléments dans des séquences. Même si ce critère de fréquence est déterminant, le décideur est également de plus en plus intéressé par des connaissances qui sont représentatives d'un comportement inattendu dans ces données (erreurs dans les données, fraudes, nouvelles niches, ... ). Dans cet article, nous introduisons le problème de la détection de motifs séquentiels inattendus par rapport aux croyances du domaine. Nous proposons l'approche USER dont l'objectif est d'extraire les motifs séquentiels et les règles inattendues dans une base de séquences.	Dong (Haoyuan) Li, Anne Laurent, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1000641	http://editions-rnti.fr/render_pdf.php?p=1000641	travail autour lextraction motif séquentiel focaliser définition dapprocher efficace extraire fonction dune fréquence dapparition corrélation entrer élément dan séquence critère fréquence déterminer décideur également plaire plaire intéresser connaissance représentativer dun comportement inattendu dan donnée erreur dan donnée fraude nich    Dans article introduire problème détection motif séquentiel inattendu rapport croyance domaine proposer lapproch USER lobjectif dextraire motif séquentiel règle inattendu dan baser séquence
817	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Délestage pour l'analyse multidimensionnelle de flux de données	Dans le contexte de la gestion de flux de données, les données entrent dans le système à leur rythme. Des mécanismes de délestage sont à mettre en place pour qu'un tel système puisse faire face aux situations où le débit des données dépasse ses capacités de traitement. Le lien entre réduction de la charge et dégradation de la qualité des résultats doit alors être quantifié. Dans cet article, nous nous plaçons dans le cas où le système est un cube de données, dont la structure est connue a priori, alimenté par un flux de données. Nous proposons un mécanisme de délestage pour les situations de surcharge et quantifions la dégradation de la qualité des résultats dans les cellules du cube. Nous exploitons l'inégalité de Hoeffding pour obtenir une borne probabiliste sur l'écart entre la valeur attendue et la valeur estimée.	Sylvain Ferrandiz, Georges Hébrail	http://editions-rnti.fr/render_pdf.php?p1&p=1000580	http://editions-rnti.fr/render_pdf.php?p=1000580	Dans contexte gestion flux donnée donnée entrer dan système rythmer mécanisme délestage mettre placer quun système pouvoir faire face situation débit donnée dépasser capacité traitement lien entrer réduction charger dégradation qualité résultat devoir quantifier Dans article placer dan cas système cuber donnée structurer connaître priori alimenter flux donnée proposer mécanisme délestage situation surcharger quantifion dégradation qualité résultat dan cellule cuber exploiter linégalité Hoeffding obtenir born probabiliste lécart entrer attendre estimer
818	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Détection de groupes atypiques pour une variable cible quantitative	"Une tâche importante en analyse des données est la compréhension de comportements inattendus ou atypiques de groupes d'individus. Quelles sont les catégories d'individus qui gagnent de particulièrement forts salaires ou au contraire, quelles sont celles qui ont de très faibles salaires ? Nous présentons le problème d'extraction de tels groupes atypiques vis-à-vis d'une variable cible quantitative, comme par exemple la variable ""salaire"", et plus particulièrement pour les faibles et fortes valeurs d'un intervalle déterminé par l'utilisateur. Il s'agit donc de rechercher des conjonctions de variables dont la distribution diffère significativement de celle de l'ensemble d'apprentissage pour les faibles et fortes valeurs de l'intervalle de cette variable cible. Une adaptation d'une mesure statistique existante, l'intensité d'inclination, nous permet de découvrir de tels groupes atypiques. Cette mesure nous libère de l'étape de transformation des variables quantitatives, à savoir l'étape de discrétisation suivie d'un codage disjonctif complet. Nous proposons donc un algorithme d'extraction de tels groupes avec des règles d'élagage pour réduire la complexité du problème. Cet algorithme a été développé et intégré au logiciel d'extraction de connaissances WEKA. Nous terminons par un exemple d'extraction sur la base de données IPUMS du bureau de recensement américain."	Sylvie Guillaume, Florian Guillochon, Michel Schneider	http://editions-rnti.fr/render_pdf.php?p1&p=1000627	http://editions-rnti.fr/render_pdf.php?p=1000627	tâcher important analyser donnée compréhension comportement inattendu atypiquer groupe dindividus quell catégorie dindividus gagner fort salaire contraire quell faible salaire   présenter problème dextraction groupe atypique visàvi dune variable cibl quantitatif exemple variable salair plaire faible fort dun intervall déterminer lutilisateur sagit rechercher conjonction variable distribution différer significativement lensembl dapprentissage faible fort lintervalle variable cibler adaptation dune mesurer statistique existant lintensité dinclination permettre découvrir groupe atypique mesurer libérer létape transformation variable quantitatif savoir létape discrétisation dun codage disjonctif complet proposer algorithme dextraction groupe règle délagage réduire complexité problème algorithme développer intégré logiciel dextraction connaissance WEKA terminer exemple dextraction baser donnée ipum bureau recensement américain
819	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Discretization of Continuous Features by Resampling	Les arbres de décision sont largement utilisés pour générer des classificateurs à partir d'un ensemble de données. Le processus de construction est une partitionnement récursif de l'ensemble d'apprentissage. Dans ce contexte, les attributs continus sont discrétisés. Il s'agit alors, pour chaque variable à discrétiser de trouver l'ensemble des points de coupure. Dans ce papier nous montrons que la recherche des ces points de coupure par une méthode de ré-échantillonnage, comme le BOOTSTRAP conduit à des meilleurs résultats. Nous avons testé cette approche avec les méthodes principales de discrétisation comme MDLPC, FUSBIN, FUSINTER, CONTRAST, Chi-Merge et les résultats sont systématiquement meilleurs en utilisant le bootstrap. Nous exposons ces principaux résultats et ouvrons de nouvelles pistes pour la construction d'arbres de décision.	Taimur Qureshi, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1000622	http://editions-rnti.fr/render_pdf.php?p=1000622	arbre décision largement utiliser générer classificateur partir dun ensemble donnée processus construction partitionnement récursif lensembl dapprentissage Dans contexte attribut continu discrétiser sagit variable discrétiser trouver lensembl point coupure Dans papier montrer rechercher point coupure méthode rééchantillonnage BOOTSTRAP conduire meilleur résultat tester approcher méthode principal discrétisation mdlpc fusbin fusinter contrast chimerge résultat systématiquement meilleur utiliser bootstrap exposer principal résultat ouvron piste construction darbr décision
820	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Echantillonnage adaptatif de jeux de données déséquilibrés pour les forêts aléatoires	Dans nombre d'applications, les données présentent un déséquilibre entre les classes. La prédiction est alors souvent détériorée pour la classe minoritaire. Pour contourner cela, nous proposons un échantillonnage guidé, lors des itérations successives d'une forêt aléatoire, par les besoins de l'utilisateur.	Elie Prudhomme, Julien Thomas, Pierre-Emmanuel Jouve	http://editions-rnti.fr/render_pdf.php?p1&p=1000588	http://editions-rnti.fr/render_pdf.php?p=1000588	Dans nombre dapplication donnée présenter déséquilibrer entrer classe prédiction détériorer classer minoritaire Pour contourner celer proposer échantillonnage guider itération successif dune forêt aléatoire besoin lutilisateur
821	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Echantillonnage pour l'extraction de motifs séquentiels : des bases de données statiques aux flots de données	"Depuis quelques années, la communauté fouille de données s'est intéressée à la problématique de l'extraction de motifs séquentiels à partir de grandes bases de données en considérant comme hypothèse que les données pouvaient être chargées en mémoire centrale. Cependant, cette hypothèse est mise en défaut lorsque les bases manipulées sont trop volumineuses. Dans cet article, nous étudions une technique d'échantillonnage basée sur des réservoirs et montrons comment cette dernière est particulièrement bien adaptée pour résumer de gros volumes de données. Nous nous intéressons ensuite à la problématique plus récente de la fouille sur des données disponibles sous la forme d'un flot continu et éventuellement infini (""data stream""). Nous étendons l'approche d'échantillonnage à ce nouveau contexte et montrons que nous sommes à même d'extraire des motifs séquentiels de flots tout en garantissant les taux d'erreurs sur les résultats. Les différentes expérimentations menées confirment nos résultats théoriques."	Chedy Raïssi, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1000567	http://editions-rnti.fr/render_pdf.php?p=1000567	Depuis année communauté fouill donnée sest intéressé problématique lextraction motif séquentiel partir grand base donnée considérer hypothèse donnée pouvoir charger mémoire central hypothèse mettre défaut base manipuler volumineuser Dans article étudier technique déchantillonnage baser réservoir montron adapter résumer gros volume donnée intéresser ensuite problématique plaire récent fouiller donnée disponible sou former dun flot continu éventuellement infini dater stream étendre lapproche déchantillonnage contexte montron dextraire motif séquentiel flot garantir taux derreur résultat expérimentation mener confirmer résultat théorique
822	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Echantillonnage spatio-temporel de flux de données distribués	Ces dernières années, sont apparues de nombreuses applications, utilisant des données potentiellement infinies, provenant de façon continue de capteurs distribués. On retrouve ces capteurs dans des domaines aussi divers que la météorologie (établir des prévisions), le domaine militaire (surveiller des zones sensibles), l'analyse des consommations électriques (transmettre des alertes en cas de consommation anormale),... Pour faire face à la volumétrie et au taux d'arrivée des flux de données, des traitements sont effectués 'à la volée' sur les flux. En particulier, si le système n'est pas assez rapide pour traiter toutes les données d'un flux, il est possible de construire des résumés de l'information. Cette communication a pour objectif de faire un premier point sur nos travaux d'échantillonnage dans un environnement de flux de données fortement distribués. Notre approche est basée sur la théorie des sondages, l'analyse des données fonctionnelles et la gestion de flux de données. Cette approche sera illustrée par un cas réel : celui des mesures de consommations électriques	Raja Chiky, Jérôme Cubillé, Alain Dessertaine, Georges Hébrail, Marie-Luce Picard	http://editions-rnti.fr/render_pdf.php?p1&p=1000569	http://editions-rnti.fr/render_pdf.php?p=1000569	année apparu application utiliser donnée potentiellement infinie provenir continuer capteur distribué retrouver capteur dan domaine météorologie établir prévision domaine militaire surveiller zone sensible lanalyse consommation électrique transmettre alerte cas consommation anormal Pour faire face volumétrie taux darriver flux donnée traitement effectuer voler flux En système nest rapide traiter donnée dun flux construire résumé linformation communication objectif faire poindre travail déchantillonnage dan environnement flux donnée fortement distribuer approcher baser théorie sondage lanalyse donnée fonctionnel gestion flux donnée approcher illustrer cas réel   mesure consommation électrique
823	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Enhancing Personal File Retrieval in Semantic File Systems with Tag-Based Context	Recently, tagging systems are widely used on the Internet. On desktops, tags are also supported by some semantic file systems and desktop search tools. In this paper, we focus on personal tag organization to enhance personal file retrieval. Our approach is based on the notion of context. A context is a set of tags assigned to a file by a user. Based on tag popularity and relationships between tags, our proposed algorithm creates a hierarchy of contexts on which a user can navigate to retrieve files in an effective manner.	Ba-Hung Ngo, Frédérique Silber-Chaussumier, Christian Bac	http://editions-rnti.fr/render_pdf.php?p1&p=1000558	http://editions-rnti.fr/render_pdf.php?p=1000558	recently tagging systems are widely used the Internet desktops tags are also supported by some semantic filer system and desktop search tool in this paper we focus personal tag organization to enhance personal fil retrieval Our approach is based the notion of context context is set of tag assigned to filer by user Based tag popularity and relationships between tag our proposed algorithm creat hierarchy of context which user can navigate to retriev fil in an effectif manner
824	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Étude comparative de deux approches de classification recouvrante : MOC vs. OKM	La classification recouvrante désigne les techniques de regroupements de données en classes pouvant s'intersecter. Particulièrement adaptés à des domaines d'application actuels (e.g. Recherche d'Information, Bioinformatique) quelques modèles théoriques de classification recouvrante ont été proposés très récemment parmi lesquels le modèle MOC (Banerjee et al. (2005a)) utilisant les modèles de mélanges et l'approche OKM (Cleuziou (2007)) consistant à généraliser l'algorithme des k-moyennes. La présente étude vise d'une part à étudier les limites théoriques et pratiques de ces deux modèles, et d'autre part à proposer une formulation de l'approche OKM en terme de modèles de mélanges gaussiens, laissant ainsi entrevoir des perspectives intéressantes quant à la variabilité des schémas de recouvrements envisageables.	Guillaume Cleuziou, Jacques-Henri Sublemontier	http://editions-rnti.fr/render_pdf.php?p1&p=1000666	http://editions-rnti.fr/render_pdf.php?p=1000666	classification recouvrant désigner technique regroupement donnée classe pouvoir sintersecter adapter domaine dapplication actuel eg rechercher dinformation Bioinformatique modèle théorique classification recouvrant proposer récemment modeler MOC Banerjee al 2005a utiliser modèle mélange lapproche OKM cleuziou 2007 consister généraliser lalgorithme kmoyenne présent étude viser dune partir étudier limite théorique pratique modèle dautre partir proposer formulation lapproche OKM terme modèle mélange gaussien entrevoir perspective intéressant variabilité schéma recouvrement envisageable
825	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Étude de l'interaction entre variables pour l'extraction des règles d'influence	Cet article présente une méthode efficace pour l'extraction de règles d'influence quantitatives positives et négatives. Ces règles d'influence introduisent une nouvelle sémantique qui vise à faciliter l'analyse d'un volume important de données. Cette sémantique fixe la direction de la règle entre deux variables en positionnant, au préalable, l'une comme étant l'influent et l'autre comme étant l'influé. Elle permet, de ce fait, d'exprimer la nature de l'influence : positive, en maximisant le nombre d'éléments en commun ou négative, en maximisant le nombre d'éléments qui violent l'influé. Notre approche s'appuie sur une stratégie qui comporte cinq étapes dont deux exécutées en parallèle. Ces deux étapes constituent les étapes clé de notre approche. La première combine une méthode d'élagage et de regroupement tabulaire basée sur les tableaux de contingence. Cette dernière construit et classe les zones potentiellement intéressantes. La seconde, injecte la sémantique et évalue le degré d'influence que produirait l'introduction d'une nouvelle variable sur un ensemble de variables en utilisant une nouvelle mesure d'intérêt, l'Influence. Cette étape vient affiner les résultats de la première étape, et permet de se focaliser sur des zones valides par rapport aux contraintes spécifiées. Enfin, un système de règles d'influence jugées intéressantes est construit basé sur la juxtaposition des résultats des deux étapes clé de notre approche.	Leila Nemmiche Alachaher, Sylvie Guillaume	http://editions-rnti.fr/render_pdf.php?p1&p=1000629	http://editions-rnti.fr/render_pdf.php?p=1000629	article présenter méthode efficace lextraction règle dinfluence quantitatif positif négatif règle dinfluence introduire sémantique viser faciliter lanalyse dun volume importer donnée sémantique fixer direction régler entrer variable positionner préalable lun linfluent lautre linfluer permettre faire dexprimer nature linfluence   positif maximiser nombre déléments commun négatif maximiser nombre déléments violer linfluer approcher sappui stratégie comporter étape exécuter parallèle étape constituer étape clé approcher combiner méthode délagage regroupement tabulaire baser tableau contingence construire classer zone potentiellement intéressant second injecter sémantique évaluer degré dinfluence produire lintroduction dune variable ensemble variable utiliser mesurer dintérêt linfluence étape venir affiner résultat étape permettre focaliser zone valider rapport contraint spécifier Enfin système règle dinfluence juger intéressant construire baser juxtaposition résultat étape clé approcher
826	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Évaluation des critères asymétriques pour les arbres de décision	Pour construire des arbres de décision sur des données déséquilibrées, des auteurs ont proposés des mesures d'entropie asymétriques. Le problème de l'évaluation de ces arbres se pose ensuite. Cet article propose d'évaluer la qualité d'arbres de décision basés sur une mesure d'entropie asymétrique.	Gilbert Ritschard, Simon Marcellin, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1000587	http://editions-rnti.fr/render_pdf.php?p=1000587	Pour construire arbre décision donnée déséquilibrer auteur proposer mesure dentropie asymétrique problème lévaluation arbre poser ensuite article proposer dévaluer qualité darbre décision baser mesurer dentropie asymétrique
827	Revue des Nouvelles Technologies de l'Information	EGC 	2008	ExpLSA : utilisation d'informations syntaxico-sémantiques associées à LSA pour améliorer les méthodes de classification conceptuelle	L'analyse sémantique latente (LSA - Latent Semantic Analysis) est aujourd'hui utilisée dans de nombreux domaines comme la modélisation cognitive, les applications éducatives mais aussi pour la classification. L'approche présentée dans cet article consiste à ajouter des informations grammaticales à LSA. Différentes méthodes pour exploiter ces informations grammaticales sont étudiées dans le cadre d'une tâche de classification conceptuelle.	Nicolas Béchet, Mathieu Roche, Jacques Chauché	http://editions-rnti.fr/render_pdf.php?p1&p=1000658	http://editions-rnti.fr/render_pdf.php?p=1000658	lanalyse sémantique latent LSA   Latent Semantic Analysis aujourdhui utiliser dan domaine modélisation cognitif application éducatif classification Lapproche présenter dan article consister ajouter information grammatical LSA méthode exploiter information grammatical étudier dan cadrer dune tâcher classification conceptuel
828	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Extraction d'itemsets compacts	L'extraction d'itemsets fréquents est un sujet majeur de l'ECD et son but est de découvrir des corrélations entre les enregistrements d'un ensemble de données. Cependant, le support est calculé en fonction de la taille de la base dans son intégralité. Dans cet article, nous montrons qu'il est possible de prendre en compte des périodes difficiles à déceler dans l'organisation des données et qui contiennent des itemsets fréquents sur ces périodes. Nous proposons ainsi la définition des itemsets compacts, qui représentent un comportement cohérent sur une période spécifique et nous présentons l'algorithme DEICO qui permet leur découverte.	Bashar Saleh, Florent Masseglia	http://editions-rnti.fr/render_pdf.php?p1&p=1000628	http://editions-rnti.fr/render_pdf.php?p=1000628	lextraction ditemset fréquent majeur lecd boire découvrir corrélation entrer enregistrement dun ensemble donnée support calculer fonction tailler baser dan intégralité Dans article montrer quil prendre compter période difficile déceler dan lorganisation donnée contenir itemset fréquent période proposer définition itemset compact représenter comportement cohérent période spécifique présenter lalgorithm DEICO permettre découvrir
829	Revue des Nouvelles Technologies de l'Information	EGC	2008	Extraction d'un modèle numérique de terrain à partir de photographies par drone	Dans le suivi et la modélisation de l'érosion en montagne, lareprésentation fine du relief est une composante importante. En effet, laconnaissance des zones de concentration des eaux, notamment à traversl'apparition de rigoles élémentaires, est fondamentale pour bien décrire lesconnectivités entre les zones de mobilisation des sédiments sur le versant et leréseau hydrographique stabilisé. La résolution au sol permise par lesphotographies aériennes classiques ne permet pas d'accéder à unereprésentation 3D suffisamment fine des ravines élémentaires. Nous testonsl'utilisation de photographies stéréoscopiques à résolution centimétrique prisesà basse altitude par un drone pour obtenir un MNT précis. La question majeureconcerne les règles à suivre pour un meilleur compromis entre précision etfacilité d'élaboration, et l'évaluation de l'importance relative de chaque étapesur la qualité finale de la restitution. La zone d'étude est située dans lesBadlands de Draix (Alpes de Haute Provence).	Andres Jacome, Christian Puech, Damien Raclot, Jean-Stéphane Bailly, Bruno Roux	http://editions-rnti.fr/render_pdf.php?p1&p=1001233	http://editions-rnti.fr/render_pdf.php?p=1001233	Dans modélisation lérosion montagne lareprésentation fin relief composant important En laconnaissance zone concentration eau traverslapparition rigole élémentaire fondamental décrir lesconnectivité entrer zone mobilisation sédiment verser leréseau hydrographique stabiliser résolution sol permettre lesphotographie aérien classique permettre daccéder unereprésentation 3D suffisamment fin ravine élémentaire testonslutilisation photographie stéréoscopique résolution centimétrique prisesà altitude drone obtenir mnt précis question majeureconcerne règle meilleur compromis entrer précision etfacilité délaboration lévaluation limportance relatif étapesur qualité final restitution zone détude situer dan lesbadlands Draix Alpes Haute Provence
830	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Extraction de Motifs Séquentiels Multidimensionnels Clos sans Gestion d'Ensemble de Candidats	L'extraction de motifs séquentiels permet de découvrir des corrélations entre événements au cours du temps. Introduisant plusieurs dimensions d'analyse, les motifs séquentiels multidimensionnels permettent de découvrir des motifs plus pertinents. Mais le nombre de motifs obtenus peut devenir très important. C'est pourquoi nous proposons, dans cet article, de définir une représentation condensée garantie sans perte d'information : les motifs séquentiels multidimensionnels clos extraits ici sans gestion d'ensemble de candidats.	Marc Plantevit, Anne Laurent, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000642	http://editions-rnti.fr/render_pdf.php?p=1000642	lextraction motif séquentiel permettre découvrir corrélation entrer événement cours temps introduire dimension danalyse motif séquentiel multidimensionnel permettre découvrir motif plaire pertinent Mais nombre motif obtenir pouvoir devenir importer cest proposer dan article définir représentation condensé garantir perte dinformation   motif séquentiel multidimensionnel clore extrait gestion densembl candidat
831	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Extraction et exploitation des annotations contextuelles	Dans la perspective d'offrir un web sémantique, des travaux ont cherché à automatiser l'extraction des annotations sémantiques à partir de textes pour représenter au mieux la sémantique que vise à transmettre une page web. Dans cet article nous proposons une approche d'extraction des annotations qui représentent le plus précisément possible le contenu d'un document. Nous proposons de prendre en compte la notion de contexte modélisé par des relations contextuelles émanant, à la fois, de la structure et de la sémantique du texte.	Noureddine Mokhtari, Rose Dieng-Kuntz	http://editions-rnti.fr/render_pdf.php?p1&p=1000551	http://editions-rnti.fr/render_pdf.php?p=1000551	Dans perspectif doffrir web sémantique travail chercher automatiser lextraction annotation sémantique partir texte représenter mieux sémantique viser transmettre page web Dans article proposer approcher dextraction annotation représenter plaire précisément contenir dun document proposer prendre compter notion contexte modéliser relation contextuel émaner structurer sémantique texte
832	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Extraction et validation par croisement des relations d'une ontologie de domaine		Lobna Karoui	http://editions-rnti.fr/render_pdf.php?p1&p=1000594	http://editions-rnti.fr/render_pdf.php?p=1000594	
833	Revue des Nouvelles Technologies de l'Information	EGC 	2008	FIASCO : un nouvel algorithme d'extraction d'itemsets fréquents dans les flots de données	Nous présentons dans cet article un nouvel algorithme permettant la construction et la mise à jour incrémentale du FIA : FIASCO. Notre algorithme effectue un seul passage sur les données et permet de prendre en compte les nouveaux batches, itemset par itemset et pour chaque itemset, item par item.	Lionel Vinceslas, Jean-Emile Symphor, Alban Mancheron, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1000603	http://editions-rnti.fr/render_pdf.php?p=1000603	présenter dan article nouvel algorithme permettre construction miser jour incrémentale fier   fiasco algorithme effectuer passage donnée permettre prendre compter batche itemset itemset itemset item item
834	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Fouille de données audio pour la classification automatique de mots homophones	Cet article présente une contribution à la modélisation acoustique des mots à partir de grands corpus oraux, faisant appel aux techniques de fouilles de données. En transcription automatique, de nombreuses erreurs concernent des mots fréquents homophones. Deux paires de mots (quasi-)homophones à/a et et/est sont sélectionnées dans les corpus, pour lesquels sont définis et examinés 41 descripteurs acoustiques permettant potentiellement de les distinguer. 17 algorithmes de classification, mis à l'épreuve pour la discrimination automatique de ces deux paires de mots, donnent en moyenne 77% de classification correcte sur les 5 meilleurs algorithmes. En réduisant le nombre de descripteurs à 10 (sélectionnés par l'algorithme le plus performant), les résultats de classification restent proches du résultat obtenu avec 41 attributs. Cette comparaison met en évidence le caractère discriminant de certains attributs, qui pourront venir enrichir à la fois la modélisation acoustique et nos connaissances des prononciations de l'oral.	Rena Nemoto, Martine Adda-Decker, Iona Vasilescu	http://editions-rnti.fr/render_pdf.php?p1&p=1000633	http://editions-rnti.fr/render_pdf.php?p=1000633	article présenter contribution modélisation acoustique partir grand corpus oral faire appel technique fouille donnée En transcription automatique erreur concerner fréquent homophone Deux paire quasihomophones àa etest sélectionner dan corpus définir examiner 41 descripteur acoustique permettre potentiellement distinguer 17 algorithme classification mettre lépreuve discrimination automatique paire donner moyenner 77 classification correct 5 meilleur algorithm En réduire nombre descripteur 10 sélectionné lalgorithme plaire performer résultat classification ruer résultat obtenir 41 attribut comparaison mettre évidence caractère discriminer attribut pouvoir venir enrichir modélisation acoustique connaissance prononciation loral
835	Revue des Nouvelles Technologies de l'Information	EGC 	2008	From Mining the Web to Inventing the New Sciences Underlying the Internet	As the Internet continues to change the way we live, find information, communicate, and do business, it has also been taking on a dramatically increasing role in marketing and advertising. Unlike any prior mass medium, the Internet is a unique medium when it comes to interactivity and offers ability to target and program messaging at the individual level. Coupled with its uniqueness in the richness of the data that is available for measurability, in the variety of ways to utilize the data, and in the great dependence of effective marketing on applications that are heavily data-driven, makes data mining and statistical data analysis, modeling, and reporting an essential mission-critical part of running the on-line business. However, because of its novelty and the scale of data sets involved, few companies have figured out how to properly make use of this data. In this talk, I will review some of the challenges and opportunities in the utilization of data to drive this new generation of marketing systems. I will provide several examples of how data is utilized in critical ways to drive some of these capabilities. The discussion will be framed with theMore general framework of Grand Challenges for data mining : pragmatic and technical. I will conclude this presentation with a consideration of the larger issues surrounding the Internet as a technology that is ubiquitous in our lives, yet one where very little is understood, at the scientific level, in defining and understanding many of the basics the Internet enables : Community, Personalization, and the new Microeconomics of the web. This leads to an overview of the new Yahoo ! Research organization and its aims : inventing the new sciences underlying what we do on the Internet, focusing on areas that have received little attention in the traditional academic circles. Some illustrative examples will be reviewed to make the ultimate goals more concrete.	Usama M. Fayyad	http://editions-rnti.fr/render_pdf.php?p1&p=1000550	http://editions-rnti.fr/render_pdf.php?p=1000550	the Internet continu to changer the way we live find information communicate and do business it has also been taking dramatically increasing role in marketing and advertising Unlike any prior mas medium the Internet is medium when it come to interactivity and offer ability to target and program messaging at the individual level Coupled with it uniquenes in the richness of the dater that is availabl for measurability in the variety of ways to utiliz the dater and in the great dependence of effectif marketing application that are heavily datadriven makes dater mining and statistical dater analysis modeling and reporting an essential missioncritical partir of running the onlin business However becaus of its novelty and the scale of dater set involved few companie hav figured out how to properly make us of this dater In this talk ie will review some of the challenge and opportunitier in the utilization of dater to driver this new generation of marketing system ie will provid several exampl of how dater is utilized in critical ways to driver some of these capabilitier The discussion will be framed with themore general framework of grand Challenges for dater mining   pragmatic and technical ie will conclude thi presentation with consideration of the larger issu surrounding the Internet technology that is ubiquitous in our liv yet one where very little is understood at the scientific level in defining and understanding many of the basics the Internet enabl   Community Personalization and the new Microeconomics of the web This leads to an overview of the new Yahoo   Research organization and it aim   inventing the new science underlying what we do the Internet focusing areer that hav received littl attention in the traditional academic circler Some illustrative exampl will be reviewed to make the ultimate goal more concret
836	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Génération de séquence résumé par une nouvelle approche basée sur le Soft Computing	Cet article propose une approche d'abstraction des séquences vidéo basée sur le soft computing. Etant donné une longueur cible du condensé vidéo, on cherche les segments vidéo qui couvrent le maximum du visuel de la vidéo originale en respectant la longueur du condensé.	Youssef Hadi, Rachid El Meziane, Rachid Oulad Haj Thami	http://editions-rnti.fr/render_pdf.php?p1&p=1000586	http://editions-rnti.fr/render_pdf.php?p=1000586	article proposer approcher dabstraction séquence vidéo baser soft computing eter donner longueur cibl condenser vidéo chercher segment vidéo couvrir maximum visuel vidéo original respecter longueur condenser
837	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Gradients de prototypicalité conceptuelle et lexicale	Longtemps les ontologies ont été limitées à des domaines scientifiques et techniques, favorisant au passage l'essor du concept de « connaissances universelles et objectives ». Avec l'émergence et l'engouement actuel pour les sciences cognitives, couplés à l'application des ontologies à des domaines relatifs aux Sciences Humaines et Sociales (SHS), la subjectivité des connaissances devient une dimension incontournable qui se doit d'être intégrée et prise en compte dans le processus d'ingénierie ontologique (IO). L'objectif de nos travaux est de développer la notion d'Ontologie Pragmatisée Vernaculaire de Domaine (OPVD). Le principe sous-jacent à de telles ressources consiste à considérer que chaque ontologie est non seulement propre à un domaine, mais également à un endogroupe donné, doté d'une pragmatique qui est fonction tant de la culture que de l'apprentissage et de l'état émotionnel du dit endogroupe. Cette pragmatique, qui traduit un processus d'appropriation et de personnalisation de l'ontologie considérée, est qualifiée à l'aide de deux mesures : un gradient de prototypicalité conceptuelle et un gradient de prototypicalité lexicale	Xavier Aimé, Frédéric Fürst, Pascale Kuntz, Francky Trichet	http://editions-rnti.fr/render_pdf.php?p1&p=1000564	http://editions-rnti.fr/render_pdf.php?p=1000564	Longtemps ontologie limiter domaine scientifique technique favoriser passage lessor concept « connaissance universel objective » Avec lémergence lengouement actuel science cognitif coupler lapplication ontologie domaine relatif science humain Sociales SHS subjectivité connaissance devenir dimension incontournable devoir dêtre intégré priser compter dan processus dingénierie ontologique io Lobjectif travail développer notion dontologie Pragmatisée Vernaculaire Domaine OPVD principe sousjacer ressource consister considérer ontologie propre domaine également endogroupe donner doter dune pragmatique fonction culture lapprentissage létat émotionnel endogroupe pragmatique traduire processus dappropriation personnalisation lontologie considéré qualifier laid mesure   gradient prototypicalité conceptuel gradient prototypicalité lexical
838	Revue des Nouvelles Technologies de l'Information	EGC	2008	HyperSmooth : calcul et visualisation de cartes de potentiel interactives	Le groupe de recherche Hypercarte propose HyperSmooth,un nouvel outil cartographique pour l'analyse spatiale de phénomènessociaux économiques mettant en oeuvre une méthode de calcul depotentiel. L'objectif est de pouvoir représenter de façon continue et enchangeant d'échelle d'analyse une information statistiqueéchantillonnée sur toutes sortes de maillages, réguliers ou non. Le défitechnologique est de fournir un outil accessible sur le Web, interactifet rapide, ceci malgré le coût élevé du calcul, et qui assure laconfidentialité des données. Nous présentons notre solution basée surune architecture client serveur : le serveur calcule les cartes depotentiel en utilisant des techniques d'optimisation particulières, alorsque le client est en charge de la visualisation et du paramétrage del'analyse, et les deux parties communiquent via un protocole Web.	Christine Plumejeaud, Jean-Marc Vincent, Claude Grasland, Jérôme Gensel, Hélène Mathian, Serge Guelton, Joël Boulier	http://editions-rnti.fr/render_pdf.php?p1&p=1001230	http://editions-rnti.fr/render_pdf.php?p=1001230	grouper rechercher Hypercarte proposer hypersmoothun nouvel outil cartographique lanalyse spatial phénomènessociaux économique mettre oeuvrer méthode calcul depotentiel Lobjectif pouvoir représenter continuer enchangeant déchelle danalyse information statistiqueéchantillonné sorte maillage régulier défitechnologiqu fournir outil accessible web interactifet rapide coût élever calcul assurer laconfidentialiter donnée présenter solution basé surune architecturer client serveur   serveur calculer carte depotentiel utiliser technique doptimisation alorsqu client charger visualisation paramétrage delanalys party communiquer protocole Web
839	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Industrialiser le data mining : enjeux et perspectives	L'informatique décisionnelle est un secteur en forte croissance dans toutes les entreprises. Les techniques classiques (reporting simple & Olap), qui s'intéressent essentiellement à présenter les données, sont aujourd'hui très largement déployées. Le data mining commence à se répandre, apportant des capacités de prévision à forte valeur ajoutée pour les entreprises les plus compétitives. Ce développement est rendu possible par la disponibilité croissante de masses de données importantes et la puissance de calcul dorénavant disponible. Cependant, la mise en IJuvre industrielle des projets de data mining pose des contraintes tant théoriques (quels algorithmes utiliser pour produire des modèles d'analyses exploitant des milliers de variables pour des millions d'exemples) qu'opérationnelles (comment mettre en production et contrôler le bon fonctionnement de centaines de modèles). Je présenterai ces contraintes issues des besoins des entreprises ; je montrerai comment exploiter des résultats théoriques (provenant des travaux de Vladimir Vapnik) pour produire des modèles robustes ; je donnerai des exemples d'applications réelles en gestion de la relation client et en analyse de qualité. Je conclurai en présentant quelques perspectives (utilisation du texte et des réseaux sociaux).	Françoise Fogelman-Soulié	http://editions-rnti.fr/render_pdf.php?p1&p=1000548	http://editions-rnti.fr/render_pdf.php?p=1000548	Linformatique décisionnel secteur fort croissance dan entreprise technique classique reporting simple   Olap sintéresser essentiellement poster donnée aujourdhui largement déployer dater mining commencer répandre apporter capacité prévision fort ajouter entreprise plaire compétitif développement disponibilité croissant mass donnée important puissance calcul dorénavant disponible miser ijuvre industriel projet dater mining poser contrainte théorique algorithme utiliser produire modèle danalyse exploiter millier variable million dexempl quopérationnell mettre production chuter fonctionnement centaine modèle présenter contraint issu besoin entreprise   montrer exploiter résultat théorique provenir travail Vladimir Vapnik produire modèle robuste   donner exemple dapplication réel gestion relation client analyser qualité conclure présenter perspective utilisation texte réseau social
840	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Intégration de contraintes dans les cartes auto-organisatrices	Le travail présenté dans cet article décrit une nouvelle version des cartes topologiques que nous appelons CrTM. Cette version consiste à modifier l'algorithme de Kohonen de telle façon à ce qu'il contrôle les violations des contraintes lors de la construction de la topologie de la carte. Nous validons notre approche sur des données connues de la littérature en utilisant des contraintes artificielles. Une validation supplémentaire sera faite sur des données réelles issues d'images médicales pour la classification des mélanomes chez l'humain sous contraintes médicales.	Anouar Benhassena, Khalid Benabdeslem, Fazia Bellal, Alexandre Aussem, Bruno Canitia	http://editions-rnti.fr/render_pdf.php?p1&p=1000663	http://editions-rnti.fr/render_pdf.php?p=1000663	travail présenter dan article décrire version carte topologique appeler CrTM version consister modifier lalgorithme Kohonen quil contrôler violation contrainte construction topologie carte valider approcher donnée connu littérature utiliser contrainte artificiel validation supplémentaire faire donnée réel issu dimag médical classification mélanome lhumain sou contraint médical
841	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Intégration de la structure dans un modèle probabiliste de document	En fouille de textes comme en recherche d'information, différents modèles, de type probabiliste, vectoriel ou booléen, se sont révélés bien adaptés pour représenter des documents textuels mais, ces modèles présentent l'inconvénient de ne pas tenir compte de la structure du document. Or la plupart des informations disponibles aujourd'hui sur Internet ou dans des bases documentaires sont fortement structurées. Dans cet article, nous proposons d'étendre le modèle probabiliste de représentation des documents de façon à tenir compte du poids d'une certaine catégorie d'éléments structurels : les balises représentant la structure logique et la structure de mise en forme. Ce modèle a été évalué à l'aide de la collection de la campagne d'évaluation INEX 2006.	Mathias Géry, Christine Largeron, Franck Thollard	http://editions-rnti.fr/render_pdf.php?p1&p=1000660	http://editions-rnti.fr/render_pdf.php?p=1000660	En fouiller texte rechercher dinformation modèle typer probabiliste vectoriel booléen révéler adapter représenter document textuel modèle présenter linconvénient compter structurer document Or information disponible aujourdhui Internet dan base documentaire fortement structurer Dans article proposer détendre modeler probabiliste représentation document compter poids dune catégorie déléments structurel   balise représenter structurer logique structurer miser former modeler évaluer laid collection campagne dévaluation INEX 2006
842	Revue des Nouvelles Technologies de l'Information	EGC	2008	Interprétation automatique d'itinéraires à partir d'un corpus de récits de voyages pilotée par un usage pédagogique.	De larges corpus à fort ancrage territorial deviennent disponibles sousforme numérique dans les médiathèques et plus particulièrement dans les médiathèquesde dimension régionale. Les défis qu'offrent ces gigas octets de documentsbruts sont énormes en terme de traitement automatique des contenus.Nous proposons dans cet article deux modèles computationnels et une méthodecomplète permettant de réaliser un traitement automatique afin d'extraire des itinérairesdans des textes relatant des récits de voyage. Le premier modèle est unmodèle des attendus. Il s'intéresse au concept d'itinéraire et adopte le point devue du pédagogue et fait intervenir très tôt les usages envisagés. Le deuxièmemodèle est un modèle d'extraction, il permet de modéliser l'expression du déplacementdans des textes du genre récit de voyage. Nous proposons alors uneméthode automatique pour : d'une part extraire et interpréter automatiquementles déplacements d'un récit et d'autre part passer des déplacements à l'itinéraire,c'est-à-dire alimenter de manière automatique le modèle des attendus à partir dumodèle d'extraction. Nous montrons également comment les itinéraires extraitsinterviennent soit dans la phase de construction d'activités pédagogiques soitdirectement comme matériau dans une activité d'apprentissage. Nous présentonsenfin ¼R, un Prototype pour l'Interprétation d'Itinéraires dans des Récitsde voyages, qui implémente notre approche. Il prend en entrée un texte brut etfournit l'interprétation de l'itinéraire décrit dans le texte. Il permet également devisualiser sur un fond cartographique l'itinéraire extrait.	Pierre Loustau, Mauro Gaio, Thierry Nodenot	http://editions-rnti.fr/render_pdf.php?p1&p=1001237	http://editions-rnti.fr/render_pdf.php?p=1001237	De large corpu fort ancrage territorial devenir disponible sousform numérique dan médiathèque plaire dan médiathèquesde dimension régional défi quoffrent giga octet documentsbrut énorme terme traitement automatique contenusnou proposon dan article modèle computationnel méthodecomplète permettre réaliser traitement automatique dextraire itinérairesdan texte relater récit voyager modeler unmodèl sintéresse concept ditinérair adopter poindre devue pédagogue faire intervenir tôt usage envisagé deuxièmemodèle modeler dextraction permettre modéliser lexpression déplacementdan texte genre récit voyager proposer uneméthode automatique   dune partir extrair interpréter automatiquementl déplacement dun récit dautre partir prendre déplacement litinérairecestàdire alimenter manière automatique modeler partir dumodèle dextraction montrer également itinéraire extraitsintervienner dan phase construction dactivité pédagogique soitdirectement matériau dan activité dapprentissage présentonsenfin ¼r prototype linterprétation ditinérair dan récitsde voyage implémente approcher prendre entrer texte brut etfournit linterprétation litinérair décrire dan texte permettre également devisualiser fondre cartographique litinérair extraire
843	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Interprétation d'images basée sur une approche évolutive guidée par une ontologie	Les approches de fouille et d'interprétation d'images consistant à considérer les pixels de façon indépendante ont montré leurs limites pour l'analyse d'images complexes. Pour résoudre ce problème, de nouvelles méthodes s'appuient sur une segmentation préalable de l'image qui consiste en une agrégation des pixels connexes afin de former des régions homogènes au sens d'un certain critère. Cependant le lien est souvent complexe entre la connaissance de l'expert sur les objets qu'il souhaite identifier dans l'image et les paramètres nécessaires à l'étape segmentation permettant de les identifier. Dans cet article la connaissance de l'expert est modélisée dans une ontologie qui est ensuite utilisée pour guider un processus de segmentation par une approche évolutive. Cette méthode trouve automatiquement des paramètres de segmentation permettant d'identifier les objets décrits par l'expert dans l'ontologie.	Germain Forestier, Sébastien Derivaux, Cédric Wemmert, Pierre Gançarski	http://editions-rnti.fr/render_pdf.php?p1&p=1000635	http://editions-rnti.fr/render_pdf.php?p=1000635	approche fouiller dinterprétation dimager consister considérer pixel indépendant montrer limite lanalyse dimage complexe Pour résoudre problème méthode sappuient segmentation préalable limage consister agrégation pixel connexe former région homogène sentir dun critère lien complexe entrer connaissance lexpert objet quil souhaiter identifier dan limage paramètre nécessaire létape segmentation permettre identifier Dans article connaissance lexpert modéliser dan ontologie ensuite utiliser guider processus segmentation approcher évolutif méthode trouver automatiquement paramètre segmentation permettre didentifier objet décrire lexpert dan lontologie
844	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Khiops: outil de préparation et modélisation des données pour la fouille des grandes bases de données	Khiops est un outil de préparation des données et de modélisation pour l'apprentissage supervisé et non supervisé. L'outil permet d'évaluer de façon non paramétrique la corrélation entre tous types de variables dans le cas non supervisé et l'importance prédictive des variables et paires de variables dans le cas de la classification supervisée. Ces évaluations sont effectuées au moyen de modèles de discrétisation dans le cas numérique et de groupement de valeurs dans le cas catégoriel, ce qui permet de rechercher une représentation des données efficace au moyen d'un recodage des variables. L'outil produit également un modèle de scoring pour les tâches d'apprentissage supervisé, selon un classifieur Bayesien naif avec sélection de variables et moyennage de modèles. L'outil est adapté à l'analyse des grandes bases de données, avec des centaines de milliers d'individus et des dizaines de milliers de variables, et a permis de participer avec succès à plusieurs challenges internationaux récents.	Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1000598	http://editions-rnti.fr/render_pdf.php?p=1000598	Khiops outil préparation donnée modélisation lapprentissage superviser superviser Loutil permettre dévaluer paramétrique corrélation entrer type variable dan cas superviser limportance prédictif variable paire variable dan cas classification superviser évaluation effectuer moyen modèle discrétisation dan cas numérique groupement dan cas catégoriel permettre rechercher représentation donnée efficace moyen dun recodage variable Loutil produire également modeler scoring tâche dapprentissage superviser classifieur Bayesien naif sélection variable moyennage modèle Loutil adapter lanalyse grand base donnée centaine millier dindividus dizaine millier variable permettre participer succès challenge international récent
845	Revue des Nouvelles Technologies de l'Information	EGC	2008	L'intelligence collective géospatiale au service du diagnostic de territoire : GEOdoc	Le diagnostic de territoire constitue une étape obligatoire dans toutprojet d'aménagement ou dans toute volonté politique de modifier durablementl'espace. Les décideurs politiques doivent avoir une vision objective des actionsà mener en fondant leurs réflexions sur des études et des documents ;qu'ils soient à caractère géographique ou non. Il est donc fondamentald'améliorer l'accès et la consultation, par les décideurs stratégiques, de ce quel'on peut appeler des documents géographiques. Le but de cet article est deprésenter certains concepts et solutions technologiques qui peuvent être utilisésafin de mieux organiser, de naviguer (dans) et de visualiser ces documents. Ilpropose une mise en perspective commune de certaines de ces approches, surlaquelle est fondée la conception d'une première maquette d'un outil de visualisation(et de navigation) de documents géographiques nommé GEOdoc.	Stéphane Roche, Benoit Kiene, Claude Caron	http://editions-rnti.fr/render_pdf.php?p1&p=1001231	http://editions-rnti.fr/render_pdf.php?p=1001231	diagnostic territoire constituer étape obligatoire dan toutprojet daménagement dan volonté politique modifier durablementlespace décideur politique devoir vision objectiver actionsà mener fondre réflexion étude document quils caractère géographique fondamentaldaméliorer laccè consultation décideur stratégique quelon pouvoir appeler document géographique boire article deprésenter concept solution technologique pouvoir utilisésafin mieux organiser naviguer dan visualiser document ilpropose miser perspectif commun approche surlaquell fonder conception dune maquette dun outil visualisationet navigation document géographique nommer geodoc
846	Revue des Nouvelles Technologies de l'Information	EGC 	2008	La prise en compte de la dimension temporelle dans la classification de données	Dans un contexte d'ingénierie de la connaissance, l'analyse des données relationnelles évolutives est une question centrale. La représentation de ce type de données sous forme de graphe optimisé en facilite l'analyse et l'interprétation par l'utilisateur non expert. Cependant, ces graphes peuvent rapidement devenir trop complexes pour être étudiés dans leur globalité, il faut alors les décomposer de manière à en faciliter la lecture et l'analyse. Pour cela, une solution est de les simplifier, dans un premier temps, en un graphe réduit dont les sommets représentent chacun un groupe distinct de sommets : acteurs ou termes du domaine étudié. Dans un second temps, il faut les décomposer en instances (un graphe par période) afin de prendre en compte la dimension temporelle.La plateforme de veille stratégique Tétralogie, développée dans notre laboratoire, permet de synthétiser les données relationnelles évolutives sous forme de matrices de cooccurrence 3D et VisuGraph, son module de visualisation, permet de les représenter sous forme de graphes évolutifs.VisuGraph assimile les différentes périodes à des repères temporels et chaque sommet est placé en fonction de son degré d'appartenance aux différentes périodes. Ce prototype est aussi doté d'un module de la classification interactive de données relationnelles basé sur une technique de Markov Clustering, qui conduit à une visualisation sous forme de graphe réduit. Nous proposons ici de prendre en compte la dimension temporelle dans notre processus de classification des données. Ainsi, par la visualisation successive des différentes instances, il devient plus facile d'analyser l'évolution des classes au niveau intra mais aussi au niveau inter classes.	Eloïse Loubier, Bernard Dousset	http://editions-rnti.fr/render_pdf.php?p1&p=1000653	http://editions-rnti.fr/render_pdf.php?p=1000653	Dans contexte dingénierie connaissance lanalyse donnée relationnel évolutif question central représentation typer donnée sou former graphe optimiser faciliter lanalys linterprétation lutilisateur expert graphe pouvoir rapidement devenir complexe étudier dan globalité falloir décomposer manière faciliter lecture lanalyse Pour celer solution simplifier dan temps graph réduire sommet représenter grouper distinct sommet   acteur terme domaine étudier Dans second temps falloir décomposer instance graphe période prendre compter dimension temporellela plateform veiller stratégique tétralogie développer dan laboratoire permettre synthétiser donnée relationnel évolutif sou former matrice cooccurrence 3D visugraph moduler visualisation permettre représenter sou former graphe évolutifsvisugraph assimiler période repère temporel sommet placer fonction degré dappartenance période prototype doter dun moduler classification interactif donnée relationnel baser technique Markov Clustering conduire visualisation sou former graphe réduire proposer prendre compter dimension temporel dan processus classification donnée visualisation successif instance devenir plaire facile danalyser lévolution classe niveau intra niveau inter classe
847	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Le FIA: un nouvel automate permettant l'extraction efficace d'itemsets fréquents dans les flots de données	Le FIA (Frequent Itemset Automaton) est un nouvel automate qui permet de traiter de façon efficace la problématique de l'extraction des itemsets fréquents dans les flots de données. Cette structure de données est très compacte et informative, et elle présente également des propriétés incrémentales intéressantes pour les mises à jour avec une granularité très fine. L'algorithme développé pour la mise à jour du FIA effectue un unique passage sur les données qui sont prises en compte tout d'abord par batch (i.e., itemset par itemset), puis pour chaque itemset, item par item. Nous montrons que dans le cadre d'une approche prédictive et par l'intermédiaire de la bordure statistique, le FIA permet d'indexer les itemsets véritablement fréquents du flot en maximisant le rappel et en fournissant à tout moment une information sur la pertinence statistique des itemsets indexés avec la P-valeur.	Jean-Emile Symphor, Alban Mancheron, Lionel Vinceslas, Pascal Poncelet	http://editions-rnti.fr/render_pdf.php?p1&p=1000568	http://editions-rnti.fr/render_pdf.php?p=1000568	fier Frequent Itemset Automaton nouvel automate permettre traiter efficace problématique lextraction itemset fréquent dan flot donnée structurer donnée compact informatif présenter également propriété incrémental intéressant mise jour granularité fin Lalgorithme développer miser jour fier effectuer passage donnée prendre compter dabord batch ie itemset itemset pouvoir itemset item item montrer dan cadrer dune approcher prédictif lintermédiaire bordure statistique fier permettre dindexer itemset véritablement fréquent flot maximiser rappel fournir moment information pertinence statistique itemset indexé pvaleur
848	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Le forage de réseaux sociaux	L'exploitation des réseaux sociaux pour l'extraction de connaissances n'est pas nouvelle. Les anthropologues, sociologues et épidémiologies se sont déjà penchés sur la question. C'est probablement le succès du moteur de recherche Google qui a vulgarisé l'utilisation des parcours aléatoires des réseaux sociaux pour l'ordonnancement par pertinence. Plusieurs applications ont depuis vu naissance. La découverte des communautés dans les réseaux sociaux est aussi une nouvelle tendance de recherche très prisée. Durant cet exposé nous parlerons de l'analyse des réseaux sociaux, la découverte de communautés, et présenterons quelques applications dont l'ordonnancement dans les bases de données	Osmar R. Zaïane	http://editions-rnti.fr/render_pdf.php?p1&p=1000549	http://editions-rnti.fr/render_pdf.php?p=1000549	lexploitation réseau social lextraction connaissance nest anthropologue sociologu épidémiologie déjà pencher question cest probablement succès moteur rechercher Google vulgariser lutilisation parcours aléatoire réseau social lordonnancement pertinence application voir naissance découvrir communauté dan réseau social tendance rechercher priser Durant exposer lanalyse réseau social découvrir communauté présenter application lordonnancemer dan base donnée
849	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Le logiciel SODAS : avancées récentes Un outil pour analyser et visualiser des données symboliques		Myriam Touati, Mohamed Rahal, Filipe Afonso, Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1000607	http://editions-rnti.fr/render_pdf.php?p=1000607	
850	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Les cartes cognitives hiérarchiques	Une carte cognitive fournit une représentation graphique d'un réseau d'influence entre des concepts. Les cartes cognitives de dimensions importantes ont l'inconvénient d'être difficiles à appréhender, interpréter et exploiter. Cet article présente un modèle de cartes cognitives hiérarchiques permettant au concepteur d'effectuer des regroupements de concepts qui sont ensuite utilisés dans un mécanisme permettant à l'utilisateur d'obtenir des vues partielles et synthétiques d'une carte.	Lionel Chauvin, David Genest, Stéphane Loiseau	http://editions-rnti.fr/render_pdf.php?p1&p=1000560	http://editions-rnti.fr/render_pdf.php?p=1000560	carte cognitif fournir représentation graphique dun réseau dinfluence entrer concept carte cognitif dimension important linconvénient dêtre difficile appréhender interpréter exploiter article présenter modeler carte cognitif hiérarchique permettre concepteur deffectuer regroupement concept ensuite utiliser dan mécanisme permettre lutilisateur dobtenir vue partiel synthétique dune carte
851	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Mesures hiérarchiques pondérées pour l'évaluation d'un système semi-automatique d'annotation de génomes utilisant des arbres de décision	L'annotation d'une protéine consiste, entre autres, à lui attribuer une classe dans une hiérarchie fonctionnelle. Celle-ci permet d'organiser les connaissances biologiques et d'utiliser un vocabulaire contrôlé. Pour estimer la pertinence des annotations, des mesures telles que la précision, le rappel, la spécificité et le Fscore sont utilisées. Cependant ces mesures ne sont pas toujours bien adaptées à l'évaluation de données hiérarchiques, car elles ne permettent pas de distinguer les erreurs faites aux différents niveaux de la hiérarchie. Nous proposons ici une représentation formelle pour les différents types d'erreurs adaptés à notre problème.	Lucie Gentils, Jérôme Azé, Claire Toffano-Nioche, Valentin Loux, Anne Poupon, Jean-François Gibrat, Christine Froidevaux	http://editions-rnti.fr/render_pdf.php?p1&p=1000565	http://editions-rnti.fr/render_pdf.php?p=1000565	lannotation dune protéine consister entrer luire attribuer classer dan hiérarchie fonctionnel Celleci permettre dorganiser connaissance biologique dutiliser vocabulair contrôler Pour estimer pertinence annotation mesure précision rappel spécificité Fscore utiliser mesure adapter lévaluation donnée hiérarchique permettre distinguer erreur niveau hiérarchie proposer représentation formel type derreurs adapter problème
852	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Méthodologie d'Evaluation Intelligente des Concepts Ontologiques	"Un des problèmes majeurs dans la gestion des ontologies est son évaluation. Cet article traite l'évaluation des concepts ontologiques qui sont extraits de pages Web. Pour cela, nous avons proposé une méthodologie d'évaluation des concepts basée trois critères révélateurs : ""le degré de crédibilité""; ""le degré de cohésion"" et ""le degré d'éligibilité"". Chaque critère correspond à un apport de connaissance pour la tâche d'évaluation. Notre méthode d'évaluation assure une évaluation qualitative grâce aux associations de mots ainsi qu'une évaluation quantitative par le biais des trois degrés. Nos résultats et discussions avec les experts et les utilisateurs ont montré que notre méthode facilite la tâche d'évaluation."	Lobna Karoui, Marie-Aude Aufaure	http://editions-rnti.fr/render_pdf.php?p1&p=1000566	http://editions-rnti.fr/render_pdf.php?p=1000566	problème majeur dan gestion ontologie évaluation article traire lévaluation concept ontologique extraire page Web Pour celer proposer méthodologie dévaluation concept baser critère révélateur   degré crédibilité degré cohésion degré déligibilité critère correspondre apport connaissance tâcher dévaluation méthode dévaluation assurer évaluation qualitatif grâce association quune évaluation quantitatif biais degré résultat discussion expert utilisateur montrer méthode faciliter tâcher dévaluation
853	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Méthodologie de définition de e-services pour la gestion des connaissances à partir d'un plateau de créativité : application au e-learning instrumental	En s'appuyant sur la théorie de l'activité, nous avons mis au point une méthodologie de gestion des connaissances à base de e-services sur un plateau de créativité visant à faire piloter le processus de fabrication métier par celui des usages. Nous l'avons testé avec la réalisation d'un e-service d'apprentissage instrumental de pièces de musique à la guitare (E-guitare).	David Grosser, Noël Conruyt, Olivier Sebastien	http://editions-rnti.fr/render_pdf.php?p1&p=1000591	http://editions-rnti.fr/render_pdf.php?p=1000591	En sappuyant théorie lactivité mettre poindre méthodologie gestion connaissance baser eservice plateau créativité viser faire piloter processus fabrication métier usage laver tester réalisation dun eservice dapprentissage instrumental musiquer guitare Eguitare
854	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Mining Implications from Lattices of Closed Trees	We propose a way of extracting high-confidence association rules from datasets consisting of unlabeled trees. The antecedents are obtained through a computation akin to a hypergraph transversal, whereas the consequents follow from an application of the closure operators on unlabeled trees developed in previous recent works of the authors. We discuss in more detail the case of rules that always hold, independently of the dataset, since these are more complex than in itemsets due to the fact that we are no longer working on a lattice.	José L. Balcázar, Albert Bifet, Antoni Lozano	http://editions-rnti.fr/render_pdf.php?p1&p=1000625	http://editions-rnti.fr/render_pdf.php?p=1000625	We proposer way of extracting highconfidence association ruler from dataset consisting of unlabeled trees The antecedent are obtained through computation akin to hypergraph transversal wherea the consequent follow from an application of the closure operators unlabeled trees developed in previous recer works of the authors We discus in more detail the caser of ruler that always hold independently of the dataset since these are more complex than in itemset to the fact that we are no longer working lattice
855	Revue des Nouvelles Technologies de l'Information	EGC	2008	Modélisation conceptuelle des trajectoires	Une perception intelligente du mouvement d'objets mobiles(personnes, voitures, colis, etc.) est à la base de nombreuses applications (parexemple le suivi d'une distribution postale à travers le monde, l'optimisation dutrafic routier ou l'étude de la migration d'animaux). Les systèmes de gestion debases de données actuels n'offrent ni les concepts ni les fonctions nécessaires àune analyse sémantique du mouvement, se limitant au stockage et àl'interrogation de positions spatiales individuelles, hors contexte temporel. Destravaux de recherche précédents ont introduit et développé le concept d'objetmobile ou spatio-temporel. Dans cet article nous allons plus loin en proposantle concept de trajectoire comme unité sémantique de mouvement sur laquellese construit la vision applicative. Nous proposons de décrire les trajectoires, auniveau conceptuel, avec leurs aspects géométriques, temporels et sémantiqueset leurs composants structurels : point de départ, point d'arrivée, arrêts etdéplacements intermédiaires. Chaque élément, trajectoire, arrêt, déplacement,voire partie de déplacement, peut recevoir des annotations sémantiques sousforme de valeurs d'attributs ou de liens vers des objets de la base. L'approchede modélisation décrite dans cet article est basée sur les patrons demodélisation, qui permettent une solution générique pour modéliser lescaractéristiques standard des trajectoires tout en étant ouverte auxcaractéristiques spécifiques à l'application envisagée. Enfin, l'implémentationdans une base de données relationnelle étendue est présentée.	Christine Parent, Stefano Spaccapietra, Christelle Vangenot, Maria-Luisa Damiani, José de Macedo, Fabio Porto	http://editions-rnti.fr/render_pdf.php?p1&p=1001236	http://editions-rnti.fr/render_pdf.php?p=1001236	perception intelligent mouvement dobjet mobilespersonn voitur colis baser application parexempl dune distribution postal travers monder loptimisation dutrafic routier létude migration danimaux système gestion debas donnée actuel noffrer concept fonction nécessaire àune analyser sémantique mouvement limiter stockage àlinterrogation position spatial individuel contexte temporel destravaux rechercher précédent introduire développer concept dobjetmobil spatiotemporel Dans article aller plaire loin proposantle concept trajectoire unité sémantique mouvement laquellese construire vision applicativ proposer décrire trajectoire auniveau conceptuel aspect géométrique temporel sémantiqueset composant structurel   poindre départir poindre darrivé arrêt etdéplacement intermédiaire élément trajectoir arrêt déplacementvoir partir déplacement pouvoir recevoir annotation sémantique sousform dattribut lien ver objet baser Lapprochede modélisation décrire dan article baser patron demodélisation permettre solution générique modéliser lescaractéristiqu standard trajectoire ouvrir auxcaractéristiqu spécifique lapplication envisagé limplémentationdan baser donnée relationnel étendu présenter
856	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Nouvelle approche pour la recherche d'images par le contenu	On utilise l'analyse factorielle des correspondances (AFC) pour la recherche d'images par le contenu en s'inspirant directement de son utilisation en analyse des données textuelles (ADT). L'AFC permet ici de réduire les dimensions du problème et de sélectionner des indicateurs pertinents pour la recherche par le contenu. En ADT, l'AFC est appliquée à un tableau de contingence croisant mots et documents. La première étape consiste donc à définir des « mots visuels » dans les images (analogue des mots dans les textes). Ces mots sont construits à partir des descripteurs locaux (SIFT) des images. La méthode a été testée sur la base Caltech4 (Sivic et al., 2005) sur laquelle elle fournit de meilleurs résultats (qualité des résultats de recherche et temps d'exécution) que des méthodes plus classiques comme TF*IDF/Rocchio (Rocchio, 1971) ou pLSA (Hofmann, 1999a, 1999b). Enfin, pour passer à l'échelle et améliorer la qualité de recherche, nous proposons un nouveau prototype de recherche qui utilise des fichiers inversés basés sur la qualité de représentation des images sur les axes après avoir fait une AFC. Chaque fichier inversé est associé à une partie d'un axe (positive ou négative) et contient des images ayant une bonne qualité de représentation sur cet axe. Les tests réalisés montrent que ce nouveau prototype réduit le temps de recherche sans perte de qualité de résultat et dans certains cas, améliore le taux de précision par rapport à la méthode exhaustive.	Nguyen-Khang Pham, Annie Morin	http://editions-rnti.fr/render_pdf.php?p1&p=1000636	http://editions-rnti.fr/render_pdf.php?p=1000636	utiliser lanalys factoriel correspondance AFC rechercher dimager contenir sinspirer utilisation analyser donnée textuel ADT LAFC permettre réduire dimension problème sélectionner indicateur pertinent rechercher contenir En ADT lafc appliquer tableau contingence croiser document étape consister définir « visuel » dan image analogue dan texte construit partir descripteur local sift image méthode tester baser Caltech4 Sivic al 2005 fournir meilleur résultat qualiter résultat rechercher temps dexécution méthode plaire classique tfidfrocchio Rocchio 1971 plsa Hofmann 1999a 1999b prendre léchelle améliorer qualité rechercher proposer prototype rechercher utiliser fichier inverser baser qualité représentation image axe faire afc fichier inverser associer partir dun axer positif négatif contenir image qualité représentation axer test réaliser montrer prototyp réduire temps rechercher perte qualité résultat dan cas améliorer taux précision rapport méthode exhaustif
857	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Ontologies et raisonnement à partir de cas : Application à l'analyse des risques industriels	L'analyse de risques est un processus visant à décrire les scénarios conduisant à des phénomènes dangereux et à des accidents potentiels sur une installation industrielle. Pour réaliser une analyse de risques, un expert dispose de nombreuses ressources : rapports, études de dangers, bases d'accidents, etc. Ces ressources sont cependant souvent difficiles à exploiter parce qu'elles ne sont pas suffisamment structurées ni formalisées. Dans le cadre du projet KMGR (Knowledge Management pour la Gestion des Risques), mené en partenariat avec l'Institut National de l'Environnement industriel et des RISques (INERIS), nous proposons de traiter ce problème en développant un système de recherche d'information basé sur des ontologies, et de le compléter par un système de raisonnement à partir de cas (RàPC) pour tenir compte des expériences passées.	Amjad Abou Assali, Dominique Lenne, Bruno Debray	http://editions-rnti.fr/render_pdf.php?p1&p=1000595	http://editions-rnti.fr/render_pdf.php?p=1000595	lanalyse risque processus viser décrire scénario conduire phénomène dangereux accident potentiel installation industriel Pour réaliser analyser risque expert disposer ressource   rapport étude danger daccident ressource difficile exploiter suffisamment structurer formaliser Dans cadrer projet KMGR Knowledge management gestion risque mener partenariat linstitut national lenvironnement industriel risque ineri proposer traiter problème développer système rechercher dinformation baser ontologie compléter système raisonnement partir cas RàPC compter expérience passer
858	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Optimisation du Primal pour les SVM	L'apprentissage de SVM par optimisation directe du primal est très étudié depuis quelques temps car il ouvre de nouvelles perspectives notamment pour le traitement de données structurées. Nous proposons un nouvel algorithme de ce type qui combine de façon originale un certain nombre de techniques et idées comme la méthode du sous-gradient, l'optimisation de fonctions continues non partout différentiables, et une heuristique de shrinking.	Trinh Minh Tri Do, Thierry Artières	http://editions-rnti.fr/render_pdf.php?p1&p=1000615	http://editions-rnti.fr/render_pdf.php?p=1000615	lapprentissage svm optimisation direct primal étudier temps ouvrir perspective traitement donnée structurer proposer nouvel algorithme typer combiner original nombre technique idéer méthode sousgradient loptimisation fonction continu partout différentiable heuristique shrinking
859	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Optimisation incrémentale de réseaux de neurones RBF pour la régression via un algorithme évolutionnaire : RBF-Gene	Les réseaux de neurones RBF sont d'excellents régresseurs. Ils sont cependant difficiles à utiliser en raison du nombre de paramètres libres : nombre de neurones, poids des connexions, ... Des algorithmes évolutionnaires permettent de les optimiser mais ils sont peu nombreux et complexes.Nous proposons ici un nouvel algorithme, RBF-Gene, qui permet d'optimiser la structure et les poids du réseau, grâce à une inspiration biologique. Il est compétitif avec les autres techniques de régression mais surtout l'évolution peut choisir dynamiquement le nombre de neurones et la précision des différents paramètres.	Virginie Lefort, Guillaume Beslon	http://editions-rnti.fr/render_pdf.php?p1&p=1000620	http://editions-rnti.fr/render_pdf.php?p=1000620	réseau neuron RBF dexcellent régresseur difficile utiliser raison nombre paramètre libre   nombre neuron poids connexion   algorithme évolutionnair permettre optimiser complexesnous proposer nouvel algorithm RBFGene permettre doptimiser structurer poids réseau grâce inspiration biologique compétitif technique régression lévolution pouvoir choisir dynamiquement nombre neurone précision paramètre
860	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Pondération locale des variables en apprentissage numérique non-supervisé	Dans cet article, nous proposons une nouvelle approche de pondérations des variables durant un processus d'apprentissage non supervisé. Cette méthode se base sur l'algorithme « batch » des cartes auto-organisatrices. L'estimation des coefficients de pondération se fait en parallèle avec la classification automatique. Ces pondérations sont locales et associées à chaque référent de la carte auto-organisatrice. Elles reflètent l'importance locale de chaque variable pour la classification. Les pondérations locales sont utilisées pour la segmentation de la carte topologique permettant ainsi un découpage plus riche tenant compte des pertinences des variables. Les résultats de l'évaluation montrent que l'approche proposée, comparée à d'autres méthodes de classification, offre une segmentation plus fine de la carte et de meilleure qualité.	Nistor Grozavu, Younès Bennani, Mustapha Lebbah	http://editions-rnti.fr/render_pdf.php?p1&p=1000619	http://editions-rnti.fr/render_pdf.php?p=1000619	Dans article proposer approcher pondération variable durer processus dapprentissage superviser méthode baser lalgorithme « batch » carte autoorganisatrice lestimation coefficient pondération faire parallèle classification automatique pondération local associer référent carte autoorganisatrice refléter limportance local variable classification pondération local utiliser segmentation carte topologique permettre découpage plaire riche compter pertinence variable résultat lévaluation montrer lapproche proposé comparer dautr méthode classification offrir segmentation plaire fin carte meilleur qualité
861	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Prétraitement des bases de données de réactions chimiques pour la fouille de schémas de réactions	Un grand nombre de réactions chimiques sont aujourd'hui répertoriées dans des bases de données. Les chimistes aimeraient pouvoir fouiller les graphes moléculaires contenus dans ces données pour en extraire des schémas de réactions fréquents. Deux obstacles s'opposent à cela : d'une part la manière dont les chimistes représentent les réactions par des graphes ne permet pas aux techniques de fouille de graphes d'extraire les schémas de réactions fréquents. D'autre part les bases de données contiennent des descriptions de réactions souvent incomplètes, ambiguës ou erronées. Le présent article décrit un processus de prétraitement opérationnel qui permet de filtrer, compléter puis transformer le contenu d'une base de réactions en des données fiables constituées de graphes abstraits répondant au problème de la fouille de schémas de réactions. Le processus place ainsi les bases de réactions à portée des techniques de fouille de graphes comme en attestent les résultats expérimentaux.	Frédéric Pennerath, Géraldine Polaillon, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1000652	http://editions-rnti.fr/render_pdf.php?p=1000652	grand nombre réaction chimique aujourdhui répertorier dan base donnée chimiste aimer pouvoir fouiller graphe moléculaire contenir dan donnée extraire schéma réaction fréquent Deux obstacle sopposer celer   dune partir manière chimiste représenter réaction graphe permettre technique fouiller graphe dextraire schéma réaction fréquent dautre partir base donnée contenir description réaction incomplet ambiguë erroner présent article décrire processus prétraitement opérationnel permettre filtrer compléter pouvoir transformer contenir dune baser réaction donnée fiable constituer graphe abstrait répondre problème fouiller schéma réaction processus placer base réaction porter technique fouiller graphe attester résultat expérimental
862	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Principes d'Analyse des données symboliques et application à la détection d'anomalies sur des ouvrages publics	L'analyse des données Symboliques a pour objectif de fournir des résultatscomplémentaires à ceux fournis par la fouille de données classique encréant des concepts issus de données simples ou complexes puis en analysantces concepts par des descriptions symboliques où les variables expriment lavariation des instances de ces concepts en prenant des valeurs intervalle, histogramme,suites, munies de règles et de taxonomies, etc.	Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1001800	http://editions-rnti.fr/render_pdf.php?p=1001800	lanalyse donnée symbolique objectif fournir résultatscomplémentaire fournir fouiller donnée classique encréer concept issu donnée simple complexe pouvoir analysantce concept description symbolique variable exprimer lavariation instance concept prendre intervall histogrammesuite munir règle taxonomie
863	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Processus d'acquisition d'un dictionnaire de sigles et de leurs définitions à partir d'un corpus	Le logiciel présenté dans cet article s'appuie sur une approche d'acquisition de sigles à partir de données textuelles	Vladislav Matviico, Nicolas Muret, Mathieu Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1000599	http://editions-rnti.fr/render_pdf.php?p=1000599	logiciel présenter dan article sappuie approcher dacquisition sigle partir donnée textuel
864	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Proposition d'une nouvelle approche de détection d'intrusions basée sur les règles associatives génériques de classification	Les systèmes de détection d'intrusions (SDIs) ont pour objectif la sécurité des réseaux informatiques. Dans ce papier, nous proposons une nouvelle approche de détection d'intrusions basée sur des règles associatives génériques de classification pour améliorer la qualité de la détection d'intrusions.	Imen Brahmi, Sadok Ben Yahia, Yahya Slimani	http://editions-rnti.fr/render_pdf.php?p1&p=1000589	http://editions-rnti.fr/render_pdf.php?p=1000589	système détection dintrusion sdis objectif sécurité réseau informatique Dans papier proposer approcher détection dintrusion baser règle associatif générique classification améliorer qualité détection dintrusion
865	Revue des Nouvelles Technologies de l'Information	EGC	2008	Proposition pour l'intégration de l'analyse spatiale et de l'analyse multidimensionnelle	L'introduction de l'information spatiale dans les modèlesmultidimensionnels a donné naissance au concept de Spatial OLAP (SOLAP).Dans cet article, nous montrons en quoi les spécificités de l'informationgéographique et de l'analyse spatiale ne sont pas entièrement prises en comptedans l'analyse et les modèles multidimensionnels SOLAP. Pour pallier ceslimites, nous proposons le concept de dimension géographique et décrivons lesdifférents types de hiérarchies associées. Nous proposons l'introduction denouveaux opérateurs qui permettent d'adapter les opérateurs d'analyse spatialeau paradigme multidimensionnel. Enfin, nous présentons notre prototype quioffre une interface web de navigation spatiale et multidimensionnelle, etpermet l'intégration de ces nouveaux concepts.	Sandro Bimonte,  Anne Tchounikine, Maryvonne Miquel, Robert Laurini	http://editions-rnti.fr/render_pdf.php?p1&p=1001235	http://editions-rnti.fr/render_pdf.php?p=1001235	lintroduction linformation spatial dan modèlesmultidimensionnel donner naissance concept Spatial OLAP SOLAPDans article montrer spécificité linformationgéographique lanalyse spatial entièrement prendre comptedans lanalyse modèle multidimensionnel solap Pour pallier ceslimit proposer concept dimension géographique décrivon lesdifférent type hiérarchie associer proposer lintroduction denouveaux opérateur permettre dadapter opérateur danalyse spatialeau paradigm multidimensionnel présenter prototype quioffre interface web navigation spatial multidimensionnel etpermet lintégration concept
866	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Recherche adaptative de structures de régulation génétique	Nous avons proposé un algorithme original de Fouille de Données, LICORN, afin d'inférer des relations de régulation coopérative à partir de données d'expression. LICORN donne de bons résultats s'il est appliqué à des données de levure, mais le passage à l'échelle sur des données plus complexes (e.g., humaines) est difficile. Dans cet article, nous proposons une extension de LICORN afin qu'il puisse gérer une contrainte de co-régulation adaptative. Une évaluation préliminaire sur des données de transcriptome de tumeurs de vessie montre que les réseaux significatifs sont obtenus à l'aide d'une contrainte de corégulation adaptative de manière beaucoup plus efficace, et qu'ils ont des performances de prédiction équivalentes voire meilleures que celles obtenues par LICORN.	Mohamed Elati, Céline Rouveirol	http://editions-rnti.fr/render_pdf.php?p1&p=1000631	http://editions-rnti.fr/render_pdf.php?p=1000631	proposer algorithme original Fouille donnée LICORN dinférer relation régulation coopératif partir donnée dexpression licorn donner résultat sil appliquer donnée levure passage léchelle donnée plaire complexe eg humain difficile Dans article proposer extension LICORN quil pouvoir gérer contraint corégulation adaptative évaluation préliminaire donnée transcriptome tumeur vessie montrer réseau significatif obtenir laid dune contraint corégulation adaptatif manière plaire efficace quils performance prédiction équivalent voire meilleur obtenu LICORN
867	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Recherche d'images par noyaux sur graphes de régions	Dans le cadre de la recherche interactive d'images dans une base de données, nous nous intéressons à des mesures de similarité d'image qui permettent d'améliorer l'apprentissage et utilisables en temps réel lors de la recherche. Les images sont représentées sous la forme de graphes d'adjacence de régions floues. Pour comparer des graphes valués nous employons des noyaux de graphes s'appuyant sur des ensembles de chaînes, extraites des graphes comparés. Nous proposons un cadre général permettant l'emploi de différents noyaux et différents types de chaînes(sans cycle, avec boucles) autorisant des appariements inexacts. Nous avons effectué des comparaisons sur deux bases issues de Columbia et Caltech et montré que des chaînes de très faible dimension (longueur inférieur à 3) sont les plus efficaces pour retrouver des classes d'objets.	Philippe-Henri Gosselin, Justine Lebrun, Sylvie Philipp-Foliguet	http://editions-rnti.fr/render_pdf.php?p1&p=1000634	http://editions-rnti.fr/render_pdf.php?p=1000634	Dans cadrer rechercher interactif dimager dan baser donnée intéresser mesure similarité dimage permettre daméliorer lapprentissage utilisable temps réel rechercher image représenter sou former graphe dadjacence région flou Pour comparer graphe valuer employer noyau graphe sappuyer ensemble chaîne extrait graphe comparer proposer cadrer général permettre lemploi noyau type chaînessan cycle boucler autoriser appariement inexact effectuer comparaison base issu columbia Caltech montrer chaîne faible dimension longueur inférieur 3 plaire efficace retrouver classe dobjet
868	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Recherche d'information personnalisée dans les bibliothèques numériques scientifiques	Dans cet article nous présentons nos travaux sur la recherche d'information personnalisée dans les bibliothèques numériques. Nous utilisons des profils utilisateurs qui représentent des intérêts et des préférences des utilisateurs. Les résultats de recherche peuvent être retriés en tenant compte des besoins d'informations spécifiques de différentes personnes, ce qui donne une meilleure précision. Nous étudions différentes méthodes basées sur les citations, sur le contenu textuel des documents et des approches hybrides. Les résultats des expérimentations montrent que nos approches sont efficaces et applicables dans le cadre des bibliothèques numériques.	Thanh-Trung Van, Michel Beigbeder	http://editions-rnti.fr/render_pdf.php?p1&p=1000556	http://editions-rnti.fr/render_pdf.php?p=1000556	Dans article présenter travail rechercher dinformation personnaliser dan bibliothèqu numérique utiliser profil utilisateur représenter intérêt préférence utilisateur résultat rechercher pouvoir retrier compter besoin dinformation spécifique donner meilleur précision étudier méthode baser citation contenir textuel document approche hybride résultat expérimentation montrer approche efficace applicable dan cadrer bibliothèque numérique
869	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Recherche de motifs spatio-temporels de cas atypiques pour le trafic routier urbain	Un large panel de domaines d'application utilise des réseaux de capteurs géoréférencés pour mesurer divers évènements. Les séries temporelles fournies par ces réseaux peuvent être utilisées dans le but de dégager des connaissances sur les relations spatio-temporelles de l'activité mesurée. Dans cet article, nous proposons une méthode permettant d'abord de détecter des situations atypiques (au sens de l'occurrence) puis de construire des motifs spatio-temporels relatant leur propagation sur un réseau. Le cas étudié est celui du trafic routier urbain. Notre raisonnement se fonde sur l'application de la méthode Space-Time Principal Component Analysis (STPCA) et de la combinaison entre l'information mutuelle et l'algorithme Isomap. Les résultats expérimentaux exécutés sur des données réelles de trafic routier démontrent l'efficacité de la méthode introduite à identifier la propagation de cas atypiques fournissant ainsi un outil performant de prédiction de la circulation intraday à court et moyen terme.	Marc Joliveau, Florian De Vuyst	http://editions-rnti.fr/render_pdf.php?p1&p=1000640	http://editions-rnti.fr/render_pdf.php?p=1000640	large panel domaine dapplication utiliser réseau capteur géoréférencé mesurer évènement série temporel fournie réseau pouvoir utiliser dan boire dégager connaissance relation spatiotemporell lactivité mesurer Dans article proposer méthode permettre dabord détecter situation atypique sens loccurrence pouvoir construire motif spatiotemporel relater propagation réseau cas étudier trafic routier urbain raisonnement fondre lapplication méthode spacetime principal Component Analysis stpca combinaison entrer linformation mutuel lalgorithm Isomap résultat expérimental exécuter donnée réel trafic routier démontrer lefficacité méthode introduire identifier propagation cas atypique fournir outil performer prédiction circulation intraday courir moyen terme
870	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Requêtes alternatives dans le contexte d'un entrepôt de données génomiques	Afin d'aider les biologistes à annoter des génomes, ce qui nécessite l'analyse, le croisement, et la comparaison de données provenant de sources diverses, nous avons conçu un entrepôt de données de génomique microbienne. Nous présentons la structure globale flexible de l'entrepôt et son architecture multi-niveaux et définissons des correspondances entre ces niveaux. Nous introduisons ensuite la notion de requête alternative et montrons comment le système peut construire l'ensemble des requêtes alternatives à une requête initiale. Pour cela, nous introduisons un mécanisme d'interrogation qui repose sur l'architecture multi-niveaux, et donnons un algorithme de calcul des requêtes alternatives.	Christine Froidevaux, Frédéric Lemoine	http://editions-rnti.fr/render_pdf.php?p1&p=1000557	http://editions-rnti.fr/render_pdf.php?p=1000557	Afin daider biologiste annoter génome nécessiter lanalyse croisement comparaison donnée provenir source concevoir entrepôt donnée génomique microbien présenter structurer global flexible lentrepôt architecturer multiniveaux définisson correspondance entrer niveau introduire ensuite notion requête alternatif montron système pouvoir construire lensembl requête alternative requête initial Pour celer introduire mécanisme dinterrogation reposer larchitecture multiniveaux donnon algorithme calcul requête alternatif
871	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Segmentation hiérarchique des cartes topologiques	Dans ce papier, nous présentons une nouvelle mesure de similarité pour la classification des référents de la carte auto-organisatrice qui sera réalisée à l'aide d'une nouvelle approche de classification hiérarchique. (1) La mesure de similarité est composée de deux termes : la distance de Ward pondérée et la distance euclidienne pondérée par la fonction de voisinage sur la carte topologique. (2) Un algorithme à base de fourmis artificielles nommé AntTree sera utilisé pour segmenter la carte auto-organisatrice.Cet algorithme a l'avantage de prendre en compte le voisinage entre les référents et de fournir une hiérarchie des référents avec une complexité proche du nlog(n). La segmentation incluant la nouvelle mesure est validée sur plusieurs bases de données publiques.	Mustapha Lebbah, Hanane Azzag	http://editions-rnti.fr/render_pdf.php?p1&p=1000662	http://editions-rnti.fr/render_pdf.php?p=1000662	Dans papier présenter mesurer similarité classification référent carte autoorganisatrice réaliser laid dune approcher classification hiérarchique 1 mesurer similarité composer terme   distancer Ward pondérer distancer euclidien pondérer fonction voisinage carte topologique 2 algorithme baser fourmi artificiel nommer anttree utiliser segmenter carte autoorganisatricecet algorithm lavantage prendre compter voisinage entrer référent fournir hiérarchie référent complexité nlogn segmentation inclure mesurer valider base donnée public
872	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Semantics of Spatial Window over Spatio-Temporal Data Stream	Dans les systèmes DSMS (Data Stream Management Systems), les données en entrée sont infinies et les requêtes sur celles-ci sont actives tout le temps. Dans le but de satisfaire ces caractéristiques, le fenêtrage temporel est largement utilisée pour convertir le flux infini de données sous forme de relations finies. Mais cette technique est inadaptée pour de nombreuses applications émergentes, en particulier les services de localisation. De nombreuses requêtes ne peuvent pas être traitées en utilisant le fenêtrage temporel, ou seraient traitées plus ecacement à l'aide d'un fenêtrage basé sur l'espace (fenêtrage spatial). Dans cet article, nous analysons la nécessité d'un fenêtrage spatial sur des flux de données spatio-temporels, et proposons, sur la base du langage de requêtes CQL (Continuous Query Language), une syntaxe et une sémantique associées au fenêtrage spatial.	Yi Yu, Talel Abdessalem	http://editions-rnti.fr/render_pdf.php?p1&p=1000570	http://editions-rnti.fr/render_pdf.php?p=1000570	Dans système DSMS Data Stream management Systems donnée entrer infinier requête cellesci activer temps Dans boire satisfaire caractéristique fenêtrage temporel largement utiliser convertir flux infini donnée sou former relation fini Mais technique inadapter application émergent service localisation requête pouvoir traiter utiliser fenêtrage temporel traiter plaire ecacement laid dun fenêtrage baser lespace fenêtrage spatial Dans article analyser nécessiter dun fenêtrage spatial flux donnée spatiotemporel proposon baser langage requête CQL Continuous Query language syntaxe sémantique associé fenêtrage spatial
873	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Sémantique et Réutilisation d'ontologie générique	Dans ce papier, nous enrichissons la méthode Terminae de construction d'ontologie à partir de textes en proposant une semi-automatisation de la construction du modèle conceptuel. Nous présentons un algorithme permettant la conceptualisation d'un terme en s'appuyant sur les informations linguistiques contenues dans l'ontologie générique de référence.	Sylvie Desprès, Sylvie Szulman	http://editions-rnti.fr/render_pdf.php?p1&p=1000563	http://editions-rnti.fr/render_pdf.php?p=1000563	Dans papier enrichir méthode terminae construction dontologie partir texte proposer semiautomatisation construction modeler conceptuel présenter algorithme permettre conceptualisation dun terme sappuyer information linguistique contenu dan lontologie générique référence
874	Revue des Nouvelles Technologies de l'Information	EGC 	2008	SOM pour la Classification Automatique Non supervisée de Documents Textuels basés sur Wordnet	Dans cet article, nous proposons la méthode des SOM (cartes auto-organisatrices de Kohonen) pour la classification non supervisée de documents textuels basés sur les n-grammes. La même méthode basée sur les synsets de WordNet comme termes pour la représentation des documents est étudiée par la suite. Ces combinaisons sont évaluées et comparées.	Mimoun Malki, Abdelmalek Amine, Zakaria Elberrichi, Michel Simonet	http://editions-rnti.fr/render_pdf.php?p1&p=1000596	http://editions-rnti.fr/render_pdf.php?p=1000596	Dans article proposer méthode som cart autoorganisatrice Kohonen classification superviser document textuel baser ngramme méthod baser synset WordNet terme représentation document étudier suite combinaison évaluer comparer
875	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Stratégies de classification non supervisée basées sur fenêtres superposées : application aux données d'usage du Web	Un problème majeur se pose dans le domaine des flux de données : la distribution sous-jacente des données peut changer sur le temps. Dans cet article, nous proposons trois stratégies de classification non supervisée basée sur des fenêtres superposées. Notre objectif est de pouvoir repérer ces changements dans le temps. Notre approche est appliquée sur un benchmark de données réelles et les conclusions obtenues sont basées sur deux indices de comparaison de partitions.	Alzennyr Da Silva, Yves Lechevallier	http://editions-rnti.fr/render_pdf.php?p1&p=1000592	http://editions-rnti.fr/render_pdf.php?p=1000592	problème majeur poser dan domaine flux donnée   distribution sousjacent donnée pouvoir changer temps Dans article proposer stratégie classification superviser baser fenêtre superposer objectif pouvoir repérer changement dan temps approcher appliquer benchmark donnée réel conclusion obtenu baser indice comparaison partition
876	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Structure Inference of Bayesian Networks from Data: A New Approach Based on Generalized Conditional Entropy	We propose a novel algorithm for extracting the structure of a Bayesian network from a dataset. Our approach is based on generalized conditional entropies, a parametric family of entropies that extends the usual Shannon conditional entropy. Our results indicate that with an appropriate choice of a generalized conditional entropy we obtain Bayesian networks that have superior scores compared to similar structures obtained by classical inference methods.	Dan A. Simovici, Saaid Baraty	http://editions-rnti.fr/render_pdf.php?p1&p=1000621	http://editions-rnti.fr/render_pdf.php?p=1000621	We proposer novel algorithm for extracting the structurer of Bayesian network from dataset Our approach is based generalized conditional entropier parametric family of entropie that extend the usual Shannon conditional entropy Our results indicate that with an appropriat choice of generalized conditional entropy we obtain Bayesian network that hav superior score compared to similar structure obtained by classical inferenc method
877	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Suppression des Itemsets Clés Non Essentiels en Classification basée sur les Règles d'Association	En classification basée sur les règles d'association, les itemsets clés sont essentiels : la suppression des itemsets non clés n'affecte pas la précision du classifieur en construction. Ce travail montre que parmi ces itemsets clés, on peut s'intéresser seulement à ceux de petites tailles. Plus loin encore, il étudie une généralisation d'une propriété importante des itemsets non clés et montre que parmi les itemsets clés de petites tailles, il y a ceux qui ne sont pas significatifs pour la classification. Ces itemsets clés sont dits non essentiels. Ils sont définis via un test de 2. Les expériences menées sur les grands jeux de données montrent que l'optimisation par la suppression de ces itemsets est correcte et efficace.	Viet Phan Luong	http://editions-rnti.fr/render_pdf.php?p1&p=1000626	http://editions-rnti.fr/render_pdf.php?p=1000626	En classification basé règle dassociation itemset clé essentiel   suppression itemset clé naffect précision classifieur construction travail montr itemset cler pouvoir sintéresser petit taille plaire loin étudier généralisation dune propriété important itemset clé montrer itemset cler petit taille yu significatif classification itemset cler essentiel définir test  2 expérience mener grand jeu donnée montrer loptimisation suppression itemset correct efficace
878	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Système multi-agent argumentatif pour la classification des connaissances cruciales	Dans cet article, nous proposons une approche multi-agent argumentative permettant d'automatiser la résolution des conflits entre décideurs dans un système d'aide à l'identification des connaissances cruciales nommé K-DSS. En effet, des divergences concernant la crucialité des connaissances peuvent apparaître entre les décideurs et aboutir ainsi à des incohérences dans la base commune de connaissances la rendant inexploitable. Notre objectif à travers ce travail est de proposer une approche argumentative permettant de résoudre les conflits entre décideurs. Afin de concevoir cette approche, nous nous appuyons sur la théorie multi-agents pour représenter les acteurs humains par des agents logiciels connaissant leurs préférences et leurs règles de décision et pouvant ainsi argumenter leurs choix ou mettre à jour leurs croyances en fonction des arguments qu'ils reçoivent des autres agents décideurs.	Inès Saad, Imène Brigui-Chtioui	http://editions-rnti.fr/render_pdf.php?p1&p=1000667	http://editions-rnti.fr/render_pdf.php?p=1000667	Dans article proposer approcher multiagent argumentatif permettre dautomatiser résolution conflit entrer décideur dan système daid lidentification connaissance crucial nommer kds En divergence concerner crucialité connaissance pouvoir apparaître entrer décideur aboutir incohérence dan baser commun connaissance inexploitable objectif travers travail proposer approcher argumentatif permettre résoudre conflit entrer décideur Afin concevoir approcher appuyer théorie multiagent représenter acteur humain agent logiciel connaître préférence règle décision pouvoir argumenter choix mettre jour croyance fonction argument quils recevoir agent décideur
879	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Un algorithme de classification topographique non supervisée à deux niveaux simultanés	Une des questions les plus importantes pour la plupart des applications réelles de la classification est de déterminer un nombre approprié de groupes (clusters). Déterminer le nombre optimal de groupes est un problème difficile, puisqu'il n'y a pas de moyen simple pour connaître ce nombre sans connaissance a priori. Dans cet article, nous proposons un nouvel algorithme de classification non supervisée à deux niveaux, appelé S2L-SOM (Simultaneous Twolevel Clustering - Self Organizing Map), qui permet de déterminer automatiquement le nombre optimal de groupes, pendant l'apprentissage d'une carte auto-organisatrice. L'estimation du nombre correct de groupes est en relation avec la stabilité de la segmentation et la validité des groupes générés. Pour mesurer cette stabilité nous utilisons une méthode de sous-échantillonnage. Le principal avantage de l'algorithme proposé, comparé aux méthodes classiques de classification, est qu'il n'est pas limité à la détection de groupes convexes, mais est capable de détecter des groupes de formes arbitraires. La validation expérimentale de cet algorithme sur un ensemble de problèmes fondamentaux pour la classification montre sa supériorité sur les méthodes standards de classification à deux niveaux comme SOM+K-Moyennes et SOM+Hierarchical- Agglomerative-Clustering.	Guénaël Cabanes, Younès Bennani	http://editions-rnti.fr/render_pdf.php?p1&p=1000661	http://editions-rnti.fr/render_pdf.php?p=1000661	question plaire important application réel classification déterminer nombre approprier groupe cluster déterminer nombre optimal groupe problème difficile puisquil ny moyen simple connaître nombre connaissance priori Dans article proposer nouvel algorithme classification superviser niveau appeler S2LSOM Simultaneous Twolevel Clustering   Self Organizing Map permettre déterminer automatiquement nombre optimal groupe pendre lapprentissage dune carte autoorganisatric lestimation nombre correct groupe relation stabilité segmentation validité groupe généré Pour mesurer stabilité utiliser méthode souséchantillonnage principal avantager lalgorithm proposer comparer méthode classique classification quil nest limité détection groupe convexe capable détecter groupe forme arbitraire validation expérimental algorithme ensemble problème fondamental classification montrer supériorité méthode standard classification niveau somkmoyenne somhierarchical agglomerativeclustering
880	Revue des Nouvelles Technologies de l'Information	EGC	2008	Un cyber cartogramme gravitationnel pour l'analyse visuelle de données spatiotemporelles complexes	Le cartogramme présenté dans cet article est destiné à faciliterl'analyse visuelle de données spatiotemporelles complexes. Pour cela, il offrela possibilité de représenter simultanément les trois dimensions nécessaires àtoute forme d'analyse géographique que sont les dimensions spatiale (où),thématique (quoi) et temporelle (quand), à partir de trois composantes principales: (1) une représentation unidimensionnelle (1D) de l'espace géographiquede forme semi-circulaire centrée sur une origine (ex. le Canada) ; (2) desentités géographiques (ex. pays) qui viennent graviter autour de cette origineen fonction de valeurs attributaires ; et (3) une ligne de temps interactive permettantd'explorer la dimension temporelle de l'information représentée. Lacombinaison de ces trois composantes offre de multiples potentialités pourl'analyse spatio-temporelle de différentes formes de proximités qu'elles soientéconomiques, culturelles, sociales ou démographiques. Les fonctionnalités etpotentialités de ce cartogramme développé en source ouverte sont illustrées àpartir d'exemples issus de l'atlas cybercartographique du commerce Canadien.Cet article reprend les grandes lignes d'une communication présentée lors de laconférence SAGEO 2007.	Sébastien Caquard, Jean-Pierre Fiset	http://editions-rnti.fr/render_pdf.php?p1&p=1001229	http://editions-rnti.fr/render_pdf.php?p=1001229	cartogramme présenter dan article destiner faciliterlanalyse visuel donnée spatiotemporell complexe Pour celer offrela possibiliter représenter simultanément dimension nécessaire àtoute former danalyse géographique dimension spatial oùthématiqu temporel partir composante principal 1 représentation unidimensionnel 1d lespace géographiqued former semicirculair centrer origine ex Canada   2 desentité géographique ex pays venir graviter autour origineen fonction attributair   3 ligne temps interactif permettantdexplorer dimension temporel linformation représenter Lacombinaison composante offrir potentialité pourlanalys spatiotemporell forme proximité quell soientéconomiqu culturel sociale démographique fonctionnalité etpotentialiter cartogramme développer source illustrer àpartir dexempl issu latla cybercartographique commercer canadiencet article reprendre grand ligne dune communication présenter laconférence sageo 2007
881	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Un modèle d'espace vectoriel de concepts pour noyaux sémantiques	Les noyaux ont été largement utilisés pour le traitement de données textuelles comme mesure de similarité pour des algorithmes tels que les Séparateurs à VasteMarge (SVM). Le modèle de l'espace vectoriel (VSM) a été amplement utilisé pour la représentation spatiale des documents. Cependant, le VSM est une représentation purement statistique. Dans ce papier, nous présentons un modèle d'espace vectoriel de concepts (CVSM) qui se base sur des connaissances linguistiques a priori pour capturer le sens des documents. Nous proposons aussi un noyau linéaire et un noyau latent pour cet espace. Le noyau linéaire exploite les concepts linguistiques pour l'extraction du sens alors que le noyau latent combine les concepts statistiques et linguistiques. En effet, le noyau latent utilise des concepts latents extraits par l'Analyse Sémantique Latente (LSA) dans le CVSM. Les noyaux sont évalués sur une tâche de catégorisation de texte dans le domaine biomédical. Le corpus Ohsumed, bien connu pour sa difficulté de catégorisation, a été utilisé. Les résultats ont montré que les performances de catégorisation sont améliorées dans le CSVM.	Sujeevan Aseervatham	http://editions-rnti.fr/render_pdf.php?p1&p=1000659	http://editions-rnti.fr/render_pdf.php?p=1000659	noyau largement utiliser traitement donnée textuel mesurer similarité algorithme séparateur vastemarge svm modeler lespace vectoriel VSM amplement utiliser représentation spatial document VSM représentation purement statistique Dans papier présenter modeler despac vectoriel concept cvsm baser connaissance linguistique priori capturer sens document proposer noyau linéaire noyau latent espacer noyau linéaire exploiter concept linguistique lextraction sens noyau latent combin concept statistique linguistique En noyau latent utiliser concept latent extrait lanalyse Sémantique Latente LSA dan cvsm noyau évaluer tâcher catégorisation texte dan domaine biomédical corpus Ohsumed connaître difficulté catégorisation utiliser résultat montrer performance catégorisation améliorer dan csvm
882	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Un modèle et une algèbre pour les systèmes de gestion d'ontologies	Nous présentons ici une approche pour la gestion de bases d'ontologies basée sur un modèle comprenant, outre la définition formelle des concepts (sous forme d'axiomes de logique de description), d'autres éléments descriptifs (termes, commentaires et arguments), ainsi que leurs liens d'alignement avec des concepts d'autres ontologies. L'adaptation ou la combinaison d'ontologies se font grâce à une algèbre comprenant des opérations telles que la sélection, la projection, l'union ou la jointure d'ontologies. Ces opérations agissent au niveau des axiomes, des éléments descriptifs et des liens d'alignement.	Gilles Falquet, Claire-Lise Mottaz Jiang, Jacques Guyot	http://editions-rnti.fr/render_pdf.php?p1&p=1000670	http://editions-rnti.fr/render_pdf.php?p=1000670	présenter approcher gestion base dontologie baser modeler comprendre outrer définition formel concept sou former daxiom logique description dautr élément descriptif terme commentaire argument lien dalignemer concept dautr ontologie Ladaptation combinaison dontologier faire grâce algèbre comprendre opération sélection projection lunion jointure dontologier opération agir niveau axiome élément descriptif lien dalignemer
883	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Un nouveau système immunitaire artificiel pour l'apprentissage non supervisé	Nous proposons dans ce papier un nouveau système immunitaire artificiel (SIA) appelé système NK, pour la détection de comportement du soi non soi avec une approche non supervisée basée sur le mécanisme de cellule NK (Naturel Killer). Dans ce papier, le système NK est appliqué à la détection de fraude en téléphonie mobile.	Rachid Elmeziane, Ilham Berrada, Ismail Kassou	http://editions-rnti.fr/render_pdf.php?p1&p=1000585	http://editions-rnti.fr/render_pdf.php?p=1000585	proposer dan papier système immunitaire artificiel SIA appeler système nk détection comportement approcher superviser baser mécanisme cellule NK Naturel killer Dans papier système NK appliquer détection frauder téléphonie mobile
884	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Un processus d'acquisition d'information pour les besoins de l'enrichissement des BDG	Les données constituent l'élément central d'un Système d'Information Géographiques (SIG) et leur coût est souvent élevé en raison de l'investissement substantiel qui permet leur production. Cependant, ces données sont souvent restreintes à un service ou pour une catégorie d'utilisateurs. Ce qui a fait ressortir la nécessité de proposer des moyens d'enrichissement en informations pertinentes pour un nombre plus important d'utilisateurs. Nous présentons dans ce papier notre approche d'enrichissement de données qui se déroule selon trois étapes : une identification de segments et de thèmes associés, une délégation et enfin, un filtrage textuel. Un processus de raffinement est également offert. Notre approche globale a été intégrée à un SIG. Son évaluation a été accomplie montrant ainsi sa performance.	Khaoula Mahmoudi, Sami Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1000668	http://editions-rnti.fr/render_pdf.php?p=1000668	donnée constituer lélément central dun système dinformation Géographiques SIG coût élever raison linvestissement substantiel permettre production donnée restreindre service catégorie dutilisateur faire ressortir nécessiter proposer moyen denrichissemer information pertinent nombre plaire importer dutilisateur présenter dan papier approcher denrichissemer donnée dérouler étape   identification segment thème associé délégation filtrage textuel processus raffinement également offrir approcher global intégrer sig évaluation accomplir montrer performance
885	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Un système de vote pour la classification de textes d'opinion	Les tâches de classification textuelle ont souvent pour objectif de regrouper thématiquement différents textes. Dans cet article, nous nous sommes intéressés à la classification de documents en fonction des opinions et jugements de valeurs qu'ils contiennent. L'approche proposée est fondée sur un système de vote utilisant plusieurs méthodes de classification.	Michel Plantié, Mathieu Roche, Gérard Dray	http://editions-rnti.fr/render_pdf.php?p1&p=1000657	http://editions-rnti.fr/render_pdf.php?p=1000657	tâche classification textuel objectif regrouper thématiquement texte Dans article intéresser classification document fonction opinion jugement quil contenir Lapproche proposer fonder système voter utiliser méthode classification
886	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Une aide à la découverte de mappings dans SomeRDFS	Dans cet article, nous nous intéressons à la découverte de mises en correspondance entre ontologies distribuées modélisant les connaissances de pairs du système de gestion de données P2P SomeRDFS. Plus précisément, nous montrons comment exploiter les mécanismes de raisonnement mis en oeuvre dans SomeRDFS pour aider à découvrir des mappings entre ontologies. Ce travail est réalisé dans le cadre du projet MediaD en partenariat avec France Telecom R&D.	François-Élie Calvier, Chantal Reynaud	http://editions-rnti.fr/render_pdf.php?p1&p=1000671	http://editions-rnti.fr/render_pdf.php?p=1000671	Dans article intéresser découvrir mise correspondance entrer ontologie distribuer modélisant connaissance pair système gestion donnée p2p somerdf plaire précisément montrer exploiter mécanisme raisonnement mettre oeuvrer dan somerdf aider découvrir mapping entrer ontologie travail réaliser dan cadrer projet mediad partenariat France telecom RD
887	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Une approche ensembliste inspirée du boosting en classification non supervisée	En classification supervisée, de nombreuses méthodes ensemblistes peuvent combiner plusieurs hypothèses de base afin de créer une règle de décision finale plus performante. Ainsi, il a été montré que des méthodes comme le bagging ou le boosting pouvaient se révéler intéressantes, tant dans la phase d'apprentissage qu'en généralisation. Dès lors, il est tentant de vouloir s'inspirer des grands principes d'une méthode comme le boosting en classification non supervisée. Or, il convient préalablement de se confronter aux difficultés connues de la thématique des ensembles de regroupeurs (correspondance des classes, agrégation des résultats, qualité) puis d'introduire l'idée du boosting dans un processus itératif. Cet article propose une méthode ensembliste inspirée du boosting, qui, à partir d'un partitionnement flou obtenu par les c-moyennes floues (fuzzy-c-means), va insister itérativement sur les exemples difficiles pour former une partition dure finale plus pertinente.	Romain Billot, Henri-Maxime Suchier, Stéphane Lallich	http://editions-rnti.fr/render_pdf.php?p1&p=1000624	http://editions-rnti.fr/render_pdf.php?p=1000624	En classification superviser méthode ensembliste pouvoir combiner hypothèse baser créer régler décision final plaire performant montrer méthode bagging boosting pouvoir révéler intéressant dan phase dapprentissage quen généralisation Dès tenter vouloir sinspirer grand principe dune méthode boosting classification superviser Or convier préalablement confronter difficulté connu thématique ensemble regroupeur correspondance classe agrégation résultat qualité pouvoir dintroduir lider boosting dan processus itératif article proposer méthode ensembliste inspirer boosting partir dun partitionnement flou obtenir cmoyenne flou fuzzycmean aller insister itérativemer exemple difficile former partition dur final plaire pertinent
888	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Une approche ontologique pour automatiser le contrôle de conformité dans le domaine du bâtiment	Cet article présente la méthode et le système C3R pour vérifier de façon semi-automatique la conformité d'un projet de construction par rapport à des normes du bâtiment. Les projets de construction sont représentés par des graphes RDF et les normes par des requêtes SPARQL ; le processus de contrôle consiste en l'appariement des requêtes et des graphes. Son efficacité repose sur l'acquisition de connaissances ontologiques et sur un processus d'extraction de connaissances guidé par ce but spécifique de contrôle de conformité qui prend en compte les connaissances ontologiques acquises. Elle repose ensuite sur des méta-connaissances acquises auprès des experts du CSTB qui permettent de guider le contrôle lui-même : les requêtes représentant les normes sont annotées et organisées selon ces annotations. Ces annotations sont également utilisées dans les interactions avec l'utilisateur de C3R pour expliquer les résultats du processus de validation, en particulier en cas d'échec.	Anastasiya Yurchyshyna, Catherine Faron-Zucker, Nhan Le Thanh, Celson Lima	http://editions-rnti.fr/render_pdf.php?p1&p=1000562	http://editions-rnti.fr/render_pdf.php?p=1000562	article présenter méthode système C3R vérifier semiautomatiqu conformité dun projet construction rapport norme bâtiment projet construction représenter graphe RDF norme requête sparql   processus contrôler consister lappariement requête graphe efficacité reposer lacquisition connaissance ontologique processus dextraction connaissance guider boire spécifique contrôler conformité prendre compter connaissance ontologique acquérir reposer ensuite métaconnaissance acquérir auprès expert CSTB permettre guider contrôler luimêm   requête représenter norme annoter organiser annotation annotation également utiliser dan interaction lutilisateur C3R expliquer résultat processus validation cas déchec
889	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Une J-mesure orientée pour élaguer des modèles de chroniques		Marc Le Goc, Nabil Benayadi	http://editions-rnti.fr/render_pdf.php?p1&p=1000593	http://editions-rnti.fr/render_pdf.php?p=1000593	
890	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Une mesure de similarité contextuelle pour l'aide à la navigation dans un treillis	La recherche d'information et la navigation dans les pages web s'avèrent complexes du fait du volume croissant des données et de leur manque de structure. La formalisation conceptuelle d'un contexte associé à une ontologie rend possible l'amélioration de ce processus. Nous définissons un contexte conceptuel comme étant l'association d'un treillis de concepts construit à partir de pages web avec des ontologies. La recherche et la navigation peuvent alors s'effectuer à plusieurs niveaux d'abstraction : le niveau des données, le niveau conceptuel et le niveau sémantique. Cet article s'intéresse essentiellement au niveau conceptuel grâce à une représentation par les treillis de concepts des documents selon les termes qu'ils ont en commun. Notre objectif est de proposer une mesure de similarité permettant à l'utilisateur de mieux naviguer dans le treillis. En effet, une bonne interprétation du treillis devrait passer par un choix rigoureux des concepts, objets, relations et propriétés les plus intéressants. Pour faciliter la navigation, il faut pouvoir indiquer à l'utilisateur les concepts les plus pertinents par rapport au concept correspondant à sa requête ou pouvoir lui proposer un point de départ. L'originalité de notre proposition réside dans le fait de considérer un lien sémantique entre les concepts du treillis, basé sur une extension des mesures de similarité utilisées dans le cadre des ontologies, afin de permettre une meilleure exploitation de ce treillis. Nous présentons les résultats expérimentaux de l'application de cette mesure sur des treillis construits à partir de pages web dans le domaine du tourisme.	Saoussen Sakji, Marie-Aude Aufaure, Géraldine Polaillon, Bénédicte Le Grand, Michel Soto	http://editions-rnti.fr/render_pdf.php?p1&p=1000561	http://editions-rnti.fr/render_pdf.php?p=1000561	rechercher dinformation navigation dan page web savèrent complexe faire volume croître donnée manquer structurer formalisation conceptuel dun contexte associer ontologie lamélioration processus définir contexte conceptuel lassociation dun treillis concept construire partir page web ontologie rechercher navigation pouvoir seffectuer niveau dabstraction   niveau donnée niveau conceptuel niveau sémantique article sintéresse essentiellement niveau conceptuel grâce représentation treillis concept document terme quils commun objectif proposer mesurer similarité permettre lutilisateur mieux naviguer dan treillis En interprétation treillis devoir prendre choix rigoureux concept objet relation propriété plaire intéressant Pour faciliter navigation falloir pouvoir indiquer lutilisateur concept plaire pertinent rapport concept correspondre requête pouvoir luire proposer poindre départir Loriginalité proposition résider dan faire considérer lien sémantique entrer concept treillis baser extension mesure similarité utiliser dan cadrer ontologie permettre meilleur exploitation treillis présenter résultat expérimental lapplication mesurer treillis construit partir page web dan domaine tourisme
891	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Une nouvelle approche du boosting face aux données bruitées	La réduction de l'erreur en généralisation est l'une des principales motivations de la recherche en apprentissage automatique. De ce fait, un grand nombre de travaux ont été menés sur les méthodes d'agrégation de classifieurs afin d'améliorer, par des techniques de vote, les performances d'un classifieur unique. Parmi ces méthodes d'agrégation, le boosting est sans doute le plus performant grâce à la mise à jour adaptative de la distribution des exemples visant à augmenter de façon exponentielle le poids des exemples mal classés. Cependant, en cas de données fortement bruitées, cette méthode est sensible au sur-apprentissage et sa vitesse de convergence est affectée. Dans cet article, nous proposons une nouvelle approche basée sur des modifications de la mise à jour des exemples et du calcul de l'erreur apparente effectuées au sein de l'algorithme classique d'AdaBoost. Une étude expérimentale montre l'intérêt de cette nouvelle approche, appelée Approche Hybride, face à AdaBoost et à BrownBoost, une version d'AdaBoost adaptée aux données bruitées.	Emna Bahri, Mondher Maddouri	http://editions-rnti.fr/render_pdf.php?p1&p=1000623	http://editions-rnti.fr/render_pdf.php?p=1000623	réduction lerreur généralisation lune principal motivation rechercher apprentissage automatique De faire grand nombre travail mener méthode dagrégation classifieur daméliorer technique voter performance dun classifieur Parmi méthode dagrégation boosting douter plaire performer grâce miser jour adaptatif distribution exemple viser augmenter exponentiel poids exemple mal classer cas donnée fortement bruiter méthode sensible surapprentissage vitesse convergence affecter Dans article proposer approcher basé modification miser jour exemple calcul lerreur apparent effectuer lalgorithme classique dadaboost étude expérimental montr lintérêt approcher appeler approcher hybride face AdaBoost BrownBoost version dadaboost adapter donnée bruiter
892	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Une nouvelle méthode divisive en classification non supervisée pour des données symboliques intervalles	Dans cet article nous présentons une nouvelle méthode de classification non supervisée pour des données symboliques intervalles. Il s'agit de l'extension d'une méthode de classification non supervisée classique à des données intervalles. La méthode classique suppose que les points observés sont la réalisation d'un processus de Poisson homogène dans k domaines convexes disjoints de Rp. La première partie de la nouvelle méthode est une procédure monothétique divisive. La règle de coupure est basée sur une extension à des données intervalles du critère de classification des Hypervolumes. L'étape d'élagage utilise un test statistique basé sur le processus de Poisson homogène. Le résultat est un arbre de décision. La seconde partie de la méthode consiste en une étape de recollement, qui permet, dans certains cas, d'améliorer la classification obtenue à la fin de la première partie de l'algorithme. La méthode est évaluée sur un ensemble de données réelles.	Nathanaël Kasoro, André Hardy	http://editions-rnti.fr/render_pdf.php?p1&p=1000664	http://editions-rnti.fr/render_pdf.php?p=1000664	Dans article présenter méthode classification superviser donnée symbolique intervalle sagit lextension dune méthode classification superviser classique donnée intervalle méthode classique supposer point observer réalisation dun processus Poisson homogène dan domaine convexe disjoint rp partir méthode procédure monothétiqu divisiv régler coupure baser extension donnée intervall critère classification hypervolume Létape délagage utiliser test statistique baser processus Poisson homogène résultat arbre décision second partir méthode consister étape recollement permettre dan cas daméliorer classification obtenir fin partir lalgorithme méthode évaluer ensemble donnée réel
893	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Une proposition pour l'extraction de relations non prédicatives	Les relations sémantiques généralement reconnues par les méthodes d'extraction sont portées par des structures de type prédicats-arguments. Or, l'information recherchée est souvent répartie sur plusieurs phrases. Pour détecter ces relations dites complexes, nous proposons un modèle de représentation des connaissances basé sur les graphes conceptuels.	Mouna Kamel	http://editions-rnti.fr/render_pdf.php?p1&p=1000590	http://editions-rnti.fr/render_pdf.php?p=1000590	relation sémantique généralement reconnu méthode dextraction porter structure typer prédicatsargument Or linformation rechercher répartir phrase Pour détecter relation complexe proposer modeler représentation connaissance baser graphe conceptuel
894	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Utilisation du Web Sémantique pour la gestion d'une liste de diffusion d'une CoP	Cet article décrit une approche de création semi-automatique d'ontologies et d'annotations sémantiques à partir de messages électroniques échangés dans une liste de diffusion dédiée au support informatique. Les ressources sémantiques générées permettront d'identifier les questions fréquemment posées (FAQ) à travers une recherche guidée par cette ontologie.	Bassem Makni, Khaled Khelif, Rose Dieng-Kuntz, Hacène Cherfi	http://editions-rnti.fr/render_pdf.php?p1&p=1000553	http://editions-rnti.fr/render_pdf.php?p=1000553	article décrire approcher création semiautomatiqu dontologie dannotation sémantique partir message électronique échanger dan liste diffusion dédier support informatique ressource sémantique généré permettre didentifier question fréquemment poser faq travers rechercher guider ontologie
895	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Vers des Machines à Vecteurs de Support “Actionnables” : Une Approche Fondée sur le Classement	"Une des principales critiques que l'on puisse faire aux Séparateurs à Vaste Marge (SVM) est le manque d'intelligibilité des résultats. En effet, il s'agit d'une technique ""boite noire"" qui ne fournit pas d'explications ni d'indices quant aux raisons d'une classification. Les résultats doivent être pris tels quels en faisant confiance au système qui les a produits. Pourtant selon notre expérience pratique, les experts du domaine préfèrent largement une méthode d'apprentissage avec explications et recommandation d'actions plutôt qu'une boite noire, aussi performante et prédictive soit-elle. Dans cette thématique, nous proposons une nouvelle approche qui consiste a rendre les SVM plus ""actionnables"". Ce but est atteint en couplant des modèles de classement des résultats des SVM à des méthodes d'apprentissage de concepts. Nous présentons une application de notre méthode sur diverses données dont des données médicales concernant des patients de l'athérosclérose. Nos résultats empiriques semblent très prometteurs et montrent l'utilité de notre approche quant à l'intelligibilité et l'actionnabilité des résultats produits par SVM."	Ansaf Salleb-Aouissi, Bert C. Huang, David L. Waltz	http://editions-rnti.fr/render_pdf.php?p1&p=1000616	http://editions-rnti.fr/render_pdf.php?p=1000616	principal critique lon pouvoir faire séparateur Vaste marge SVM manquer dintelligibilité résultat En sagit dune technique boiter noir fournir dexplication dindice raison dune classification résultat devoir prendre faire confiance système produire pourtant expérience pratiquer expert domaine préférer largement méthode dapprentissage explication recommandation dacter quune boiter noir performant prédictif soitell Dans thématique proposer approcher consister svm plaire actionnable boire atteindre coupler modèle classement résultat svm méthode dapprentissage concept présenter application méthode donnée donnée médical concerner patient lathérosclérose résultat empirique sembler prometteur montrer lutilité approcher lintelligibilité lactionnabilité résultat produire svm
896	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Vers l'exploitation de grandes masses de données	Une tendance lourde depuis la fin du siècle dernier est l'augmentation exponentielle du volume des données stockées. Cette augmentation ne se traduit pas nécessairement par une information plus riche puisque la capacité à traiter ces données ne progresse pas aussi rapidement. Avec les technologies actuelles, un difficile compromis doit être trouvé entre le coût de mise en oeuvre et la qualité de l'information produite. Nous proposons une approche industrielle permettant d'augmenter considérablement notre capacité à transformer des données en information grâce à l'automatisation des traitements et à la focalisation sur les seules données pertinentes.	Raphaël Feraud, Marc Boullé, Fabrice Clérot, Françoise Fessant	http://editions-rnti.fr/render_pdf.php?p1&p=1000609	http://editions-rnti.fr/render_pdf.php?p=1000609	tendance lourd fin siècle laugmentation exponentiel volume donnée stocker augmentation traduire nécessairement information plaire riche capacité traiter donnée progresser rapidement Avec technologie actuel difficile compromis devoir trouver entrer coût miser oeuvrer qualité linformation produire proposer approcher industriel permettre daugmenter considérablemer capacité transformer donnée information grâce lautomatisation traitement focalisation donnée pertinent
897	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Vers l'intégration de la prédiction dans les cubes OLAP		Anouck Bodin-Niemczuk, Riadh Ben Messaoud, Sabine Loudcher, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1000584	http://editions-rnti.fr/render_pdf.php?p=1000584	
898	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Vers une fouille sémantique des brevets : Application au domaine biomédical	"Les brevets sont une source d'information très riche puisque ce sont des documents qui servent à décrire les inventions. L'accès aux documents de brevets en ligne est possible grâce aux efforts des offices nationaux de la propriété intellectuelle. Par ailleurs, ayant des objectifs différents, la présentation de ces documents a pris des formes variées loin d'être unifiées. Ce papier présente une méthode et un système permettant l'analyse de brevets ""Patent Mining"" pour générer des annotations sémantiques. L'idée principale est de pouvoir prendre en considération la structure des brevets pour pouvoir trouver un lien entre le contenu du brevet et les concepts des différentes ontologies."	Nizar Ghoula, Khaled Khelif, Rose Dieng-Kuntz	http://editions-rnti.fr/render_pdf.php?p1&p=1000552	http://editions-rnti.fr/render_pdf.php?p=1000552	brevet source dinformation riche document servir décrire invention Laccès document brevet ligne grâce effort office national propriété intellectuel Par objectif présentation document prendre forme varier loin dêtre unifier papier présenter méthode système permettre lanalyse brevet Patent Mining générer annotation sémantique Lidée principal pouvoir prendre considération structurer brevet pouvoir trouver lien entrer contenir brevet concept ontologie
899	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Visualisation des motifs séquentiels extraits à partir d'un corpus en Ancien Français	Cet article présente une interface permettant de visualiser des motifs séquentiels extraits à partir de données textuelles en Ancien Français.	Julien Rabatel, Yuan Lin, Yoann Pitarch, Hassan Saneifar, Claire Serp, Mathieu Roche, Anne Laurent	http://editions-rnti.fr/render_pdf.php?p1&p=1000605	http://editions-rnti.fr/render_pdf.php?p=1000605	article présenter interface permettre visualiser motif séquentiel extrait partir donnée textuel ancien français
900	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Visualisation et classification des parcours de vie	Cet article propose une méthodologie pour la visualisation et la classification des parcours de vie. Plus spécifiquement, nous considérons les parcours de vie d'individus suisses nés durant la première moitié du XXème siècle en utilisant les données provenant de l'enquête biographique rétrospective menée en 2002 par le Panel suisse de ménages. Nous nous sommes concentrés sur ces événements du parcours de vie : le départ du foyer parental, la naissance du premier enfant, le premier mariage et le premier divorce. A partir des données de base sur ces événements, nous discutons de leur transformation en séquences d'états. Nous présentons ensuite notre méthodologie pour extraire de la connaissance des parcours de vie. Cette méthodologie repose sur des distances calculées par un algorithme d'optimal matching. Ces distances sont ensuite utilisées pour la classification des parcours de vie et leur visualisation à l'aide de techniques de « Multi Dimensional Scaling ». Cet article s'intéresse en particulier aux problématiques entourant l'application de ces méthodes aux données de parcours de vie.	Nicolas S. Müller, Sylvain Lespinats, Gilbert Ritschard, Matthias Studer, Alexis Gabadinho	http://editions-rnti.fr/render_pdf.php?p1&p=1000638	http://editions-rnti.fr/render_pdf.php?p=1000638	article proposer méthodologie visualisation classification parcours vie plaire spécifiquement considérer parcours vie dindividus suisse naître durer moitié xxèm siècle utiliser donnée provenir lenquête biographique rétrospectif mener 2002 Panel suisse ménage concentrer événement parcours vie   départir foyer parental naissance enfant mariage divorcer partir donnée baser événement discuter transformation séquence détat présenter ensuite méthodologie extraire connaissance parcours vie méthodologie reposer distance calculer algorithme doptimal matching distance ensuite utiliser classification parcours vie visualisation laid technique « multi Dimensional Scaling » article sintéresse problématique entourer lapplication méthode donnée parcours vie
901	Revue des Nouvelles Technologies de l'Information	EGC 	2008	Web Content Data Mining : la classification croisée pour l'analyse textuelle d'un site Web	Notre objectif dans cet article est l'analyse textuelle d'un site Web indépendamment de son usage. Notre approche se déroule en trois étapes. La première étape consiste au typage des pages afin de distinguer les pages de navigation ou pages « auxiliaires » des pages de contenu. La deuxième étape consiste au prétraitement du contenu des pages de contenu afin de représenter chaque page par un vecteur de descripteurs. La dernière étape consiste au block clustering ou la classification simultanée des lignes et des colonnes de la matrice croisant les pages aux descripteurs de pages afin de découvrir des biclasses de pages et de descripteurs. L'application de cette approche au site de tourisme de Metz prouve son efficacité et son applicabilité. L'ensemble de classes de pages groupés en thèmes facilitera l'analyse ultérieure de l'usage du site.	Malika Charrad, Yves Lechevallier, Gilbert Saporta, Mohamed Ben Ahmed	http://editions-rnti.fr/render_pdf.php?p1&p=1000555	http://editions-rnti.fr/render_pdf.php?p=1000555	objectif dan article lanalys textuel dun site Web indépendammer usage approcher dérouler étape étape consister typage page distinguer page navigation page « auxiliaire » page contenir étape consister prétraitement contenir page contenir représenter page vecteur descripteur étape consister block clustering classification simultaner ligne colonne matrice croiser page descripteur page découvrir biclasse page descripteurs lapplication approcher site tourisme Metz prouver efficacité applicabilité lensembl classe page groupé thème faciliter lanalys ultérieur lusage site
902	Revue des Nouvelles Technologies de l'Information	EGC	2007	Alignement de ressources sémantiques à partir de règles	Ce papier présente une approche automatique pour aligner des ressources sémantiques. L'alignement se traduit par la mise en correspondance des entités (termes, concepts, rôles) appartenant à des ressources d'un même domaine qui peuvent avoir des niveaux de formalisation différents. Les entités correspondantes sont de même nature et un coefficient caractérise leur degré de ressemblance.L'approche proposée est fondée sur des règles d'appariement entre les entités des deux ressources. Dans une première phase, ces règles d'appariement sont identifiées empiriquement. Des algorithmes combinant les différentes règles identifiées sont ensuite définis afin d'établir des correspondances entre les entités des ressources considérées.Ce papier présente un ensemble de règles d'appariement exploitant des éléments situés à différents niveaux conceptuels. Cet ensemble constitue un cadre pour l'alignement automatique des ressources sémantiques. Les résultats d'une première expérimentation qui a porté sur l'alignement de deux ressources du domaine de l'accidentologie sont également présentés.	Valentina Ceausu, Sylvie Desprès	http://editions-rnti.fr/render_pdf.php?p1&p=1001444	http://editions-rnti.fr/render_pdf.php?p=1001444	papier présenter approcher automatique aligner ressource sémantique Lalignement traduire miser correspondance entité terme concept rôl appartenir ressource dun domaine pouvoir niveau formalisation entité correspondant nature coefficient caractériser degré ressemblancelapproche proposer fonder règle dappariemer entrer entité ressource Dans phase règle dappariement identifier empiriquement algorithme combiner règle identifier ensuite définir détablir correspondance entrer entité ressource considéréesCe papier présenter ensemble règle dappariement exploiter élément situer niveau conceptuel ensemble constituer cadrer lalignement automatique ressource sémantique résultat dune expérimentation porter lalignement ressource domaine laccidentologie également présenter
903	Revue des Nouvelles Technologies de l'Information	EGC	2007	Annotation et navigation de données archéologiques	Dans cet article, nous proposons un cadre et un outil pour l'annotation et la navigation de données archéologiques. L'objectif principal est de structurer les annotations de façon à permettre une navigation incrémentale où l'utilisateur peut, à partir d'un ensemble d'objets initialement retournés par une requête, découvrir des liens approximatifs avec d'autres objets de la base. L'approche a été implémentée et est en cours de validation.	Bernardo Lopez, Samira Hammiche, Samir Sebahi, Mohand-Said Hacid	http://editions-rnti.fr/render_pdf.php?p1&p=1001350	http://editions-rnti.fr/render_pdf.php?p=1001350	Dans article proposer cadrer outil lannotation navigation donnée archéologique Lobjectif principal structurer annotation permettre navigation incrémental lutilisateur pouvoir partir dun ensemble dobjet initialement retourner requête découvrir lien approximatif dautr objet baser Lapproche implémenter cours validation
904	Revue des Nouvelles Technologies de l'Information	EGC	2007	Annotation sémantique floue de tableaux guidée par une ontologie	Nous présentons dans cet article différentes étapes de l'annotation de tableaux de données à l'aide d'une ontologie. Tout d'abord, nous distinguons les colonnes de données numériques et symboliques. Les données symboliques sont ensuite annotées de manière floue à l'aide des termes de l'ontologie. Cette annotation nous permet de déduire le type des colonnes de données symboliques. Pour trouver le type des colonnes de données numériques, nous utilisons à la fois le titre de la colonne et les valeurs numériques et unités présentes dans la colonne. Chaque étape de notre annotation est validée expérimentalement.	Gaëlle Hignette, Patrice Buche, Juliette Dibie-Barthélemy, Ollivier Haemmerlé	http://editions-rnti.fr/render_pdf.php?p1&p=1001441	http://editions-rnti.fr/render_pdf.php?p=1001441	présenter dan article étape lannotation tableau donnée laid dune ontologie dabord distinguer colonne donnée numérique symbolique donnée symbolique ensuite annoter manière flou laid terme lontologie annotation permettre déduire typer colonne donnée symbolique Pour trouver typer colonne donnée numérique utiliser titrer colonne numérique unité présenter dan colonne étape annotation valider expérimentalement
905	Revue des Nouvelles Technologies de l'Information	EGC	2007	Application des réseaux bayésiens à l'analyse des facteurs impliqués dans le cancer du Nasopharynx	L'apprentissage de la structure des réseaux bayésien à partir de données est un problème NP-difficile. Une nouvelle heuristique de complexité polynômiale, intitulée Polynomial Max-Min Skeleton (PMMS), a été proposée en 2005 par Tsamardinos et al. et validée avec succès sur de nombreux bancs d'essai. PMMS présente, en outre, l'avantage d'être performant avec des jeux de données réduits. Néanmoins, comme tous les algorithmes sous contraintes, celui-ci échoue lorsque des dépendances fonctionnelles (déterministes) existent entre des groupes de variables. Il ne s'applique, par ailleurs, qu'aux données complètes. Aussi, dans cet article, nous apportons quelques modifications pour remédier à ces deux problèmes. Après validation sur le banc d'essai Asia, nous l'appliquons aux données d'une étude épidémiologique cas-témoins du cancer du nasopharynx (NPC) de 1289 observations, 61 variables et 5% de données manquantes issues d'un questionnaire. L'objectif est de dresser un profil statistique type de la population étudiée et d'apporter un éclairage utile sur les différents facteurs impliqués dans le NPC	Alexandre Aussem, Sergio Rodrigues de Morais, Marilys Corbex	http://editions-rnti.fr/render_pdf.php?p1&p=1001318	http://editions-rnti.fr/render_pdf.php?p=1001318	lapprentissage structurer réseau bayésien partir donnée problème npdifficil heuristique complexité polynômial intituler polynomial maxmin Skeleton PMMS proposer 2005 Tsamardinos al valider succès banc dessai pmms présent outrer lavantage dêtre performer jeu donnée réduire tou algorithme sou contraindre celuici échouer dépendance fonctionnel déterministe exister entrer groupe variable sappliqu quaux donnée compléter dan article apporter modification remédier problème Après validation banc dessai Asia lappliquer donnée dune étude épidémiologique castémoins cancer nasopharynx npc 1289 observation 61 variable 5 donnée manquant issu dun questionnaire Lobjectif dresser profil statistique typer population étudié dapporter éclairage utile facteur impliquer dan npc
906	Revue des Nouvelles Technologies de l'Information	EGC	2007	Apport du Web sémantique dans la réalisation d'un moteur de recherche géo-localisé à usage des entreprises	La recherche d'une entreprise sur le Web, relative à un savoir-faire particulier, n'est pas une tâche toujours facile à mener. Les outils mis à la disposition de l'internaute ne donnent pas entièrement satisfaction. D'un côté les moteurs de recherche éprouvent des difficultés à faire ressortir clairement le résultat escompté. De l'autre côté, les annuaires spécialisés (type Pages Jaunes) sont tributaires d'une organisation figée, nuisant à leur efficacité. Face à ce constat, nous nous proposons de créer un nouveau moteur spécialisé dans la recherche d'entreprise, associant Web sémantique et géo-localisation. Cette approche novatrice nécessite l'implémentation d'une ontologie ayant pour objectif la formalisation des connaissances du domaine. Cette tâche a mis en évidence l'intérêt des structures économiques, maintenues par l'INSEE, et leur utilisation au sein de l'ontologie. Les nomenclatures économiques ont été retenues pour gérer la classification des activités et produits pouvant être dispensés par les entreprises. La structure des unités administratives, telle que gérée au sein du fichier SIRENE, s'est avérée judicieuse pour répondre à la problématique de géo-localisation des entreprises. Une opération de désambiguïsation est réalisée en associant à chaque noeud d'activité les mots clés et synonymes lui correspondant. Enfin, nous comparons les résultats obtenus par notre moteur à ceux obtenu par le principal moteur de recherche d'activités géo-localisées en France : les Pages jaunes. Que ce soit au niveau de la précision et du rappel, notre moteur obtient des résultats significativement meilleurs.	Frédéric Triou, Fabien Picarougne, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1001307	http://editions-rnti.fr/render_pdf.php?p=1001307	rechercher dune entreprendre web relatif savoirfaire nest tâcher facile mener outil mettre disposition linternaute donner entièrement satisfaction Dun côter moteur rechercher éprouver difficulté faire ressortir clairement résultat escompter De lautre côté annuaire spécialiser typer Pages Jaunes tributairer dune organisation figer nuire efficacité face constat proposer créer moteur spécialiser dan rechercher dentrepris associer Web sémantique géolocalisation approcher novateur nécessit limplémentation dune ontologie objectif formalisation connaissance domaine tâcher mettre évidence lintérêt structure économique maintenu linsee utilisation lontologie nomenclature économique retenir gérer classification activité produit pouvoir dispenser entreprise structurer unité administratif gérer fichier SIRENE sest avéré judicieux répondre problématique géolocalisation entreprise opération désambiguïsation réaliser associer noeud dactiviter clé synonyme luire correspondre comparer résultat obtenir moteur obtenir principal moteur rechercher dactivités géolocalisé France   page jaune Que niveau précision rappel moteur obtenir résultat significativement meilleur
907	Revue des Nouvelles Technologies de l'Information	EGC	2007	Apprentissage actif d'émotions dans les dialogues Homme-Machine	La prise en compte des émotions dans les interactions Homme-machine permet de concevoir des systèmes intelligents, capables de s'adapter aux utilisateurs. Les techniques de redirection d'appels dans les centres téléphoniques automatisés se basent sur la détection des émotions dans la parole. Les principales difficultés pour mettre en oeuvre de tels systèmes sont l'acquisition et l'étiquetage des données d'apprentissage. Cet article propose l'application de deux stratégies d'apprentissage actif à la détection d'émotions dans des dialogues en interaction homme-machine. L'étude porte sur des données réelles issues de l'utilisation d'un serveur vocal et propose des outils adaptés à la conception de systèmes automatisés de redirection d'appels.	Alexis Bondu, Vincent Lemaire, Barbara Poulain	http://editions-rnti.fr/render_pdf.php?p1&p=1001428	http://editions-rnti.fr/render_pdf.php?p=1001428	priser compter émotion dan interaction Hommemachine permettre concevoir système intelligent capable sadapter utilisateur technique redirection dappel dan centre téléphonique automatiser baser détection émotion dan principal difficulté mettre oeuvrer système lacquisition létiquetage donnée dapprentissage article proposer lapplication stratégie dapprentissage actif détection démotion dan dialogue interaction hommemachin Létude porter donnée réel issu lutilisation dun serveur vocal proposer outil adapter conception système automatiser redirection dappel
908	Revue des Nouvelles Technologies de l'Information	EGC	2007	Apprentissage semi-supervisé de fonctions d'ordonnancement	Nous présentons dans cet article un algorithme inductif semi-supervisé pour la tâche d'ordonnancement bipartite. Les algorithmes semi–supervisés proposés jusqu'à maintenant ont été étudiés dans le cadre strict de la classification. Récemment des travaux ont été réalisés dans le cadre transductif pour étendre les modèles existants en classification au cadre d'ordonnancement. L'originalité de notre approche est qu'elle est capable d'inférer un ordre sur une base test non– utilisée pendant la phase d'apprentissage, ce qui la rend plus générique qu'une méthode transductive pure. Les résultats empiriques sur la base CACM contenant les titres et les résumés du journal Communications of the Association for Computer Machinery montrent que les données non–étiquetées sont bénéfiques pour l'apprentissage de fonctions d'ordonnancement.	Tuong-Vinh Truong, Massih-Reza Amini	http://editions-rnti.fr/render_pdf.php?p1&p=1001425	http://editions-rnti.fr/render_pdf.php?p=1001425	présenter dan article algorithme inductif semisuperviser tâcher dordonnancement bipartite algorithme semi–supervisé proposer jusquà maintenir étudier dan cadrer strict classification récemment travail réaliser dan cadrer transductif étendre modèle existant classification cadrer dordonnancement Loriginalité approcher capable dinférer ordre baser test non– utiliser pendre phase dapprentissage plaire générique quun méthode transductif résultat empirique baser cacm contenir titre résumé journal Communications of the Association for Computer Machinery montrer donnée non–étiqueté bénéfique lapprentissage fonction dordonnancement
909	Revue des Nouvelles Technologies de l'Information	EGC	2007	Apprentissage statistique de la topologie d'un ensemble de données étiquetées	Découvrir la topologie d'un ensemble de données étiquetées dans un espace Euclidien peut aider à construire un meilleur système de décision. Dans ce papier, nous proposons un modèle génératif basé sur le graphe de Delaunay de plusieurs prototypes représentant les données étiquetées dans le but d'extraire de ce graphe la topologie des classes.	Pierre Gaillard, Michaël Aupetit, Gérard Govaert	http://editions-rnti.fr/render_pdf.php?p1&p=1001419	http://editions-rnti.fr/render_pdf.php?p=1001419	découvrir topologie dun ensemble donnée étiqueter dan espacer Euclidien pouvoir aider construire meilleur système décision Dans papier proposer modeler génératif baser graphe Delaunay prototype représenter donnée étiqueter dan boire dextrair graphe topologie classe
910	Revue des Nouvelles Technologies de l'Information	EGC	2007	Approche connexionniste pour l'extraction de profils cas-témoins du cancer du Nasopharynx à partir des données issues d'une étude épidémiologique	Dans cet article, nous présentons un système de découverte de connaissances à partir de données issues d'une étude épidémiologique cas-témoins du cancer du Nasopharynx (NPC). Ces données étant obtenues par une collecte de questionnaires, elles ont d'une part, la particularité d'être qualitatives et, d'autre part, de présenter des valeurs manquantes. Prenant en compte ces deux dernières contraintes, le système que nous proposons suit une démarche d'exploration de données qui consiste à (1) définir une procédure de codage des données qualitatives en présence de valeurs manquantes ; (2) étudier les propriétés de l'algorithme des cartes auto-organisatrices de Kohonen et son adaptation à ce type de données dans un cadre de découverte et de visualisation de groupes homogènes des cas cancer / non-cancer ; (3) post-traiter le resultat de cet algorithme par une classification automatique pour optimiser le nombre de groupes ainsi trouvés, et (4) donner une interprétation sémantique des profils extraits de chaque groupe. L'objectif général de cette étude est d'éclater le profil statistique global de la population étudiée en un ensemble de profils types (cancer ou non-cancer) et d'extraire pour chaque profil l'ensemble de variables explicatives du NPC à partir d'une cartographie bidimensionnelle.	Khalid Benabdeslem, Mustapha Lebbah, Alexandre Aussem, Marilys Corbex	http://editions-rnti.fr/render_pdf.php?p1&p=1001418	http://editions-rnti.fr/render_pdf.php?p=1001418	Dans article présenter système découvrir connaissance partir donnée issu dune étude épidémiologique castémoins cancer Nasopharynx NPC donnée obtenu collecter questionnaire dune partir particularité dêtre qualitatif dautr partir poster manquant prendre compter dernière contraint système proposer démarcher dexploration donnée consister 1 définir procédure codage donnée qualitatif présence manquant   2 étudier propriété lalgorithme carte autoorganisatrice Kohonen adaptation typer donnée dan cadrer découvrir visualisation groupe homogène cas cancer   noncancer   3 posttraiter resultat algorithme classification automatique optimiser nombre groupe trouver 4 donner interprétation sémantique profil extrait grouper Lobjectif général étude déclater profil statistique global population étudier ensemble profil typer cancer noncancer dextrair profil lensembl variable explicatif npc partir dune cartographie bidimensionnel
911	Revue des Nouvelles Technologies de l'Information	EGC	2007	Approche logique pour la réconciliation de références	Le problème de réconciliation de références consiste à décider si deux descriptions provenant de sources distinctes réfèrent ou non à la même entité du monde réel. Dans cet article, nous étudions ce problème quand le schéma des données est décrit en RDFS étendu par certaines primitives de OWL-DL. Nous décrivons et montrons l'intérêt d'une approche logique basée sur des règles de réconciliation qui peuvent être générées automatiquement à partir des axiomes du schéma. Ces règles traduisent de façon déclarative les dépendances entre réconciliations qui découlent de la sémantique du schéma. Les premiers résultats ont été obtenus sur des données réelles dans le cadre du projet PICSEL 3 en collaboration avec France Telecom R&D.	Fatiha Saïs, Nathalie Pernelle, Marie-Christine Rousset	http://editions-rnti.fr/render_pdf.php?p1&p=1001449	http://editions-rnti.fr/render_pdf.php?p=1001449	problème réconciliation référence consister décider description provenir source distinct référer entité monder réel Dans article étudier problème schéma donnée décrire RDFS étendre primitive owldl décrivon montron lintérêt dune approcher logique baser règle réconciliation pouvoir générer automatiquement partir axiome schéma règle traduire déclaratif dépendance entrer réconciliation découler sémantique schéma résultat obtenir donnée réel dan cadrer projet PICSEL 3 collaboration France telecom RD
912	Revue des Nouvelles Technologies de l'Information	EGC	2007	Calcul et représentation efficace de cubes de données pour une visualisation orientée pixel	Les cubes de données fournissent une aide non négligeable lorsqu'il s'agit d'interroger des entrepôts de données. Un cube de données représente un pré-calcul de toutes les requêtes OLAP et ainsi améliore leur temps de réponses. Les approches proposées jusqu'à présent réduisent les temps de calcul et d'entrée sortie mais leur utilisation reste très coûteuse. D'autres travaux de recherche se sont intéressés à la visualisation de données pour les exploiter de façon interactive.Nous proposons une adaptation de la représentation condensée des cubes de données basée sur le modèle partitionnel. Cette technique nous permet de calculer efficacement un cube de données et de représenter les liens entre les données pour la visualisation. La visualisation proposée dans cet article est basée sur des techniques de visualisation orientée pixel et sur des techniques de diagramme de liens entre noeuds pour offrir à la fois une vision globale et locale pour l'exploitation. Cette nouvelle approche utilise d'une part les calculs efficaces de cubes de données et d'autre part les techniques avancées de visualisation.	Noel Novelli, David Auber	http://editions-rnti.fr/render_pdf.php?p1&p=1001362	http://editions-rnti.fr/render_pdf.php?p=1001362	cube donnée fournir aid négligeable lorsquil sagit dinterroger entrepôt donnée cuber donnée représenter précalcul requête OLAP améliorer temps réponse approche proposer jusquà présent réduire temps calcul dentré sortir utilisation rester coûteux dautre travail rechercher intéresser visualisation donnée exploiter interactivenous proposer adaptation représentation condenser cube donnée baser modeler partitionnel technique permettre calculer efficacement cuber donnée représenter lien entrer donnée visualisation visualisation proposer dan article baser technique visualisation orienté pixel technique diagramme lien entrer noeud offrir vision global local lexploitation approcher utiliser dune partir calcul efficace cuber donnée dautre partir technique avancer visualisation
913	Revue des Nouvelles Technologies de l'Information	EGC	2007	Caractérisation des transitions temporisées dans les logs de conversation de services Web	La connaissance du protocole de conversation d'un service Web est importante pour les utilisateurs et les fournisseurs, car il en modélise le comportement externe ; mais, il n'est souvent pas spécifié lors de la conception. Notre travail s'inscrit dans une thématique d'extraction du protocole de conversation d'un service existant à partir de ses données d'exécution. Nous en étudions un sous-problème important qui est la découverte des transitions temporisées (i.e. les changements d'état liés à des contraintes temporelles). Nous proposons un cadre formel aboutissant à la définition des expirations propres, qui représentent un équivalent dans les logs des transitions temporisées. A notre connaissance, ceci représente la première contribution à la résolution de ce problème.	Didier Devaurs, Fabien De Marchi, Mohand-Said Hacid	http://editions-rnti.fr/render_pdf.php?p1&p=1001302	http://editions-rnti.fr/render_pdf.php?p=1001302	connaissance protocole conversation dun service Web important utilisateur fournisseur modélise comportement externe   nest spécifier conception travail sinscrit dan thématique dextraction protocole conversation dun service exister partir donnée dexécution étudier sousproblème importer découvrir transition temporiser ie changement détat lier contrainte temporel proposer cadrer formel aboutir définition expiration propre représenter équivaloir dan log transition temporiser A connaissance représenter contribution résolution problème
914	Revue des Nouvelles Technologies de l'Information	EGC	2007	Cartographie de l'organisation : une approche topologique des connaissances	La gestion des connaissances est devenue aujourd'hui un enjeu majeur pour toute organisation. Celle-ci a pour but de capitaliser et de rendre accessible à ses acteurs la connaissance détenue par l'organisation. Cet article s'intéresse particulièrement à la visualisation à deux niveaux de ces connaissances (macroscopique - relatif aux connaissances globales détenues par l'organisation - et microscopique – relatif aux connaissances locales détenues par chaque membre organisationnel). La caractérisation des connaissances détenues par les acteurs repose sur quatre dimensions complémentaires (formelle, conative, cognitive, et socio-cognitive). Les deux types de visualisation proposés s'appuient sur les cartes auto-organisatrices et permettent une navigation dans différentes représentations des connaissances de l'organisation.	Marc Boyer, Marie-Françoise Canut, Max Chevalier, André Péninou, Florence Sedes	http://editions-rnti.fr/render_pdf.php?p1&p=1001437	http://editions-rnti.fr/render_pdf.php?p=1001437	gestion connaissance devenir aujourdhui enjeu majeur organisation Celleci boire capitaliser accessible acteur connaissance détenir lorganisation article sintéresse visualisation niveau connaissance macroscopique   relatif connaissance global détenu lorganisation   microscopique – relatif connaissance local détenu membre organisationnel caractérisation connaissance détenu acteur reposer dimension complémentaire formel conatif cognitif sociocognitive type visualisation proposer sappuient carte autoorganisatrice permettre navigation dan représentation connaissance lorganisation
915	Revue des Nouvelles Technologies de l'Information	EGC	2007	Choix des conclusions et validation des règles issues d'arbres de classification	Cet article traite de la validation de règles dans un contexte de ciblage où il s'agit de déterminer les profils type des différentes valeurs de la variable à prédire. Les concepts de l'analyse statistique implicative fondée sur la différence entre nombre observé de contre-exemples et nombre moyen que produirait le hasard, s'avèrent particulièrement bien adaptés à ce contexte. Le papier montre comment les notions d'indice et d'intensité d'implication de Gras s'appliquent aux règles produites par les arbres de décision et présente des alternatives inspirées de résidus utilisés en modélisation de tables de contingence. Nous discutons ensuite sur un jeu de données réelles deux usages de ces indicateurs de force d'implication pour les règles issues d'arbres. Il s'agit d'une part de l'évaluation individuelle des règles, et d'autre part de leur utilisation comme critère pour le choix de la conclusion de la règle.	Vincent Pisetta, Gilbert Ritschard, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1001424	http://editions-rnti.fr/render_pdf.php?p=1001424	article traire validation règle dan contexte ciblage sagit déterminer profil typer variable prédire concept lanalyse statistique implicatif fonder différence entrer nombre observer contreexemple nombre moyen produire hasard savèrent adapter contexte papier montr notion dindice dintensité dimplication Gras sappliquent règle produire arbre décision présenter alternative inspiré résidu utiliser modélisation table contingence discuter ensuite jeu donnée réel usage indicateur forcer dimplication règle issu darbr sagit dune partir lévaluation individuel règle dautre partir utilisation critère choix conclusion régler
916	Revue des Nouvelles Technologies de l'Information	EGC	2007	Classement des fragments de documents XML par une méthode d'aide à la décision	Vu l'accroissement constant du volume d'information accessible en ligne sous format XML, il devient primordial de proposer des modèles adaptés à la recherche d'information dans les documents XML. Tandis que la recherche d'information classique repose sur l'indexation du contenu des documents, la recherche d'information dans les documents XML tente d'améliorer la qualité des résultats en tirant profit de la sémantique véhiculée par la structure des documents. Dans cet article, nous présentons une méthode de classement des items (éléments XML) retournés lors d'une recherche dans une collection de documents XML. Le classement repose sur la prise en compte d'un ensemble de critères discriminants. La particularité de notre approche réside dans la façon dont nous les utilisons : Nous employons une méthode décisionnelle pour classer les items en les comparant deux-à-deux là où en général une fonction de scoring globale est utilisée.	Faiza Abbaci, Pascal Francq	http://editions-rnti.fr/render_pdf.php?p1&p=1001393	http://editions-rnti.fr/render_pdf.php?p=1001393	voir laccroissement constant volume dinformation accessible ligne sou format xml devenir primordial proposer modèle adapter rechercher dinformation dan document xml rechercher dinformation classique reposer lindexation contenir document rechercher dinformation dan document xml tenter daméliorer qualité résultat tirer profit sémantique véhiculer structurer document Dans article présenter méthode classement item élément xml retourner dune rechercher dan collection document xml classement reposer priser compter dun ensemble critère discriminant particularité approcher résider dan utilison   employer méthode décisionnel classer item comparer deuxàdeux général fonction scoring global utiliser
917	Revue des Nouvelles Technologies de l'Information	EGC	2007	Classification de fonctions continues à l'aide d'une distribution et d'une densité définies dans un espace de dimension infinie	Il n'est pas rare que des données individu soient caractérisées par une distribution continue et non une seule valeur. Ces données fonctionnelles peuvent être utilisées pour classer les individus. Une solution élémentaire est de réduire les distributions à leurs moyennes et variances. Une solution plus riche a été proposée par Diday (2002) et mise en oeuvre par Vrac et al. (2001) et Cuvelier et Noirhomme-Fraiture (2005). Elle utilise des points de coupures dans les distributions et modélise ces valeurs conjointes par une distribution multidimensionnelle construite à l'aide d'une copule. Nous avons montré dans un précédent travail que, si cette technique apporte de bons résultats, la qualité de la classification dépend néanmoins du nombre et de l'emplacement des coupures. Les questions du choix du nombre et de l'emplacement des coupures restaient des questions ouvertes. Nous proposons une solution à ces questions, lorsque le nombre de coupures tend vers l'infini, en proposant une nouvelle distribution de probabilité adaptée à l'espace de dimension infinie que forment les données fonctionnelles. Nous proposons aussi une densité de probabilité adaptée à la nature de cette distribution en utilisant la dérivée directionnelle de Gâteaux. La direction choisie pour cette dérivée est celle de la dispersion des fonctions à classer. Les résultats sont encourageants et offrent des perspectives multiples dans tous les domaines où une distribution de données fonctionnelles est nécessaire.	Etienne Cuvelier, Monique Noirhomme-Fraiture	http://editions-rnti.fr/render_pdf.php?p1&p=1001456	http://editions-rnti.fr/render_pdf.php?p=1001456	nest donnée individu caractériser distribution continuer donnée fonctionnel pouvoir utiliser classer individu solution élémentaire réduire distribution moyenne varianc solution plaire riche proposer diday 2002 mettre oeuvrer Vrac al 2001 Cuvelier NoirhommeFraiture 2005 utiliser point coupure dan distribution modélise conjoint distribution multidimensionnel construire laid dune copule montrer dan précédent travail technique apport résultat qualité classification dépendre nombre lemplacement coupure question choix nombre lemplacement coupure rester question proposer solution question nombre coupure tendre ver linfini proposer distribution probabilité adapté lespace dimension infini former donnée fonctionnel proposer densité probabilité adapté nature distribution utiliser dérivé directionnel gâteau direction choisi dérivé dispersion fonction classer résultat encourageant offrir perspective dan tou domaine distribution donnée fonctionnel nécessaire
918	Revue des Nouvelles Technologies de l'Information	EGC	2007	Classification de grands ensembles de données avec un nouvel algorithme de SVM	Le nouvel algorithme de boosting de Least-Squares Support Vector Machine (LS-SVM) que nous présentons vise à la classification de très grands ensembles de données sur des machines standard. Les méthodes de SVM et de noyaux permettent d'obtenir de bons résultats en ce qui concerne la précision mais la tâche d'apprentissage pour de grands ensembles de données demande une grande capacité mémoire et un temps relativement long. Nous présentons une extension de l'algorithme de LS-SVM proposé par Suykens et Vandewalle pour le boosting de LS-SVM. A cette fin, nous avons ajouté un terme de régularisation de Tikhonov et utilisé la formule de Sherman-Morrison-Woodbury pour traiter des ensembles de données ayant un grand nombre de dimensions. Nous l'avons ensuite étendu par application du boosting de LS-SVM afin de traiter des données ayant simultanément un grand nombre d'individus et de dimensions. Les performances de l'algorithme sont évaluées sur les ensembles de données de l'UCI, Twonorm, Ringnorm, Reuters-21578 et NDC sur une machine standard (PC-P4, 3GHz, 512 Mo RAM).	Thanh-Nghi Do, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1001466	http://editions-rnti.fr/render_pdf.php?p=1001466	nouvel algorithme boosting leastsquare Support Vector Machine LSSVM présenter viser classification grand ensemble donnée machine standard méthode SVM noyau permettre dobtenir résultat concerner précision tâcher dapprentissage grand ensemble donnée demander grand capacité mémoir temps long présenter extension lalgorithm LSSVM proposer suyken Vandewalle boosting LSSVM fin ajouter terme régularisation Tikhonov utiliser formuler shermanmorrisonwoodbury traiter ensemble donnée grand nombre dimension laver ensuite étendre application boosting LSSVM traiter donnée simultanément grand nombre dindividus dimension performance lalgorithme évaluer ensemble donnée luci Twonorm Ringnorm Reuters21578 ndc machiner standard PCP4 3GHz 512 mo ram
919	Revue des Nouvelles Technologies de l'Information	EGC	2007	Classification supervisée de séquences biologiques basée sur les motifs et les matrices de substitution	La classification des séquences biologiques est l'un des importants défis ouverts dans la bioinformatique, tant pour les séquences protéiques que pour les séquences nucléiques. Cependant, la présence de ces données sous la forme de chaînes de caractères ne permet pas de les traiter par les outils standards de classification supervisée, qui utilisent souvent le format relationnel. Pour remédier à ce problème de codage, plusieurs travaux se sont basés sur l'extraction des motifs pour construire une nouvelle représentation des séquences biologiques sous la forme d'un tableau binaire. Nous décrivons une nouvelle approche qui étend les méthodes précédents par l'utilisation de matrices de substitution dans les cas des séquences protéiques. Nous présentons ensuite une étude comparative qui prend en compte l'effet de chaque méthode sur la précision de la classification mais aussi le nombre d'attributs générés et le temps de calcul.	Rabie Saidi, Mondher Maddouri, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1001409	http://editions-rnti.fr/render_pdf.php?p=1001409	classification séquence biologique lun important défi ouvrir dan bioinformatiqu séquence protéique séquence nucléique présence donnée sou former chaîne caractère permettre traiter outil standard classification superviser utiliser format relationnel Pour remédier problème codage travail baser lextraction motif construire représentation séquence biologique sou former dun tableau binaire décrire approcher étendre méthode précédent lutilisation matrice substitution dan cas séquence protéique présenter ensuite étude comparatif prendre compter leffet méthode précision classification nombre dattributs généré temps calcul
920	Revue des Nouvelles Technologies de l'Information	EGC	2007	Clustering : from model-based approaches to heuristic algorithms	Les méthodes du 'clustering' ont pour but de diviser un ensemble (large) d'objets dans un petit nombre de groupes homogènes (clusters), basé sur des données relevées ou observées qui décrivent les (dis-)similarités qui existent entre les objets – en espérant que ces clusters soient utiles pour l'application concernée. Il existe une multitude d'approches, et cette contribution présente quelques-unes qui sont les plus importantes ou actuelles.	Hans-Hermann Bock	http://editions-rnti.fr/render_pdf.php?p1&p=1001287	http://editions-rnti.fr/render_pdf.php?p=1001287	méthode clustering boire diviser ensemble large dobjet dan petit nombre groupe homogène cluster baser donnée relever observer décrire dissimilarité exister entrer objet – espérer cluster utile lapplication concerner exister multitude dapproche contribution présenter quelquesunes plaire important actuel
921	Revue des Nouvelles Technologies de l'Information	EGC	2007	Combinaison des cartes topologiques mixtes et des machines à vecteurs de support : une application pour la prédiction de perte de poids chez les obèses	Cet article présente un modèle pour aborder les problèmes de classement difficiles, en particulier dans le domaine médical. Ces problèmes ont souvent la particularité d'avoir des taux d'erreurs en généralisations très élevés et ce quelles que soient les méthodes utilisées. Pour ce genre de problèmes, nous proposons d'utiliser un modèle de classement combinant le modèle de partitionnement des cartes topologiques mixtes et les machines à vecteurs de support (SVM). Le modèle non supervisé est dédié à la visualisation et au partitionnement des données composées de variables quantitatives et/ou qualitatives. Le deuxième modèle supervisé, est dédié au classement. La combinaison de ces deux modèles permet non seulement d'améliorer la visualisation des données mais aussi en les performances en généralisation. Ce modèle (CT-SVM) consiste à entraîner des cartes auto-organisatrices pour construire une partition organisée des données, constituée de plusieurs sous-ensembles qui vont servir à reformuler le problème de classement initial en sous-problème de classement. Pour chaque sous-ensemble, on entraîne un classeur SVM spécifique. Pour la validation expérimentale de notre modèle (CT-SVM), nous avons utilisé quatre jeux de données. La première base est un extrait d'une grande base médicale sur l'étude de l'obésité réalisée à l'Hôpital Hôtel-Dieu de Paris, et les trois dernières bases sont issues de la littérature.	Mohamed Ramzi Temanni, Mustapha Lebbah, Christine Poitou-Bernert, Karine Clément, Jean-Daniel Zucker	http://editions-rnti.fr/render_pdf.php?p1&p=1001298	http://editions-rnti.fr/render_pdf.php?p=1001298	article présenter modeler aborder problème classement difficile dan domaine médical problème particularité davoir taux derreur généralisation élevé méthode utiliser Pour genre problème proposer dutiliser modeler classement combiner modeler partitionnement carte topologique mixte machine vecteur support svm modeler superviser dédier visualisation partitionnemer donnée composer variable quantitatif etou qualitatif modeler superviser dédier classement combinaison modèle permettre daméliorer visualisation donnée performance généralisation modeler ctsvm consister entraîner carte autoorganisatrice construire partition organiser donnée constituer sousensemble aller servir reformuler problème classement initial sousproblème classement Pour sousensemble entraîner classeur svm spécifique Pour validation expérimental modeler ctsvm utiliser jeu donnée baser extraire dune grand baser médical létude lobésité réaliser lhôpital hôteldieu Paris base issu littérature
922	Revue des Nouvelles Technologies de l'Information	EGC	2007	Construction coopérative de carte de thèmes : vers une modélisation de l'activité socio-sémantique	Nous présentons dans cette contribution un cadre de modélisation recourant conjointement au modèle Hypertopic (Cahier et al., 2004) pour la représentation des connaissances de domaine et au modèle SeeMe (Herrmann et al., 1999) pour la représentation de l'activité. Ces deux approches apparaissent complémentaires, et nous montrons comment elles peuvent être combinées, pour mieux ancrer, sur les plans formel et méthodologique, les approches de cartographie collective des connaissances.	L'Hédi Zaher, Jean-Pierre Cahier, Christophe Lejeune, Manuel Zacklad	http://editions-rnti.fr/render_pdf.php?p1&p=1001304	http://editions-rnti.fr/render_pdf.php?p=1001304	présenter dan contribution cadrer modélisation recourir conjointement modeler Hypertopic Cahier al 2004 représentation connaissance domaine modeler seeme Herrmann al 1999 représentation lactivité approche apparaître complémentaire montrer pouvoir combiner mieux ancrer plan formel méthodologique approche cartographie collectif connaissance
923	Revue des Nouvelles Technologies de l'Information	EGC	2007	Construction d'ontologie à partir de corpus de textes	"Cet article présente une méthode semi-automatique de construction d'ontologie à partir de corpus de textes sur un domaine spécifique. Cette méthode repose en premier lieu sur un analyseur syntaxique partiel et robuste des textes, et en second lieu, sur l'utilisation de l'analyse formelle de concepts ""FCA"" pour la construction de classes d'objets en un treillis de Galois. La construction de l'ontologie, c'est à dire d'une hiérarchie de concepts et d'instances, est réalisée par une transformation formelle de la structure du treillis. Cette méthode s'applique dans le domaine de l'astronomie."	Amedeo Napoli, Yannick Toussaint, Rokia Bendaoud	http://editions-rnti.fr/render_pdf.php?p1&p=1001364	http://editions-rnti.fr/render_pdf.php?p=1001364	article présenter méthode semiautomatique construction dontologie partir corpus texte domaine spécifique méthode reposer lieu analyseur syntaxique partiel robuste texte second lieu lutilisation lanalyse formel concept FCA construction classe dobjet treillis Galois construction lontologie cest dune hiérarchie concept dinstance réaliser transformation formel structurer treillis méthode sappliqu dan domaine lastronomie
924	Revue des Nouvelles Technologies de l'Information	EGC	2007	Construction et analyse de résumés de données évolutives : application aux données d'usage du Web	La manière dont une visite est réalisée sur un site Web peut changer en raison de modifications liées à la structure et au contenu du site lui-même, ou bien en raison du changement de comportement de certains groupes d'utilisateurs ou de l'émergence de nouveaux comportements. Ainsi, les modèles associés à ces comportements dans la fouille d'usage du Web doivent être mis à jour continuellement afin de mieux refléter le comportement actuel des internautes. Une solution, proposée dans cet article, est de mettre à jour ces modèles à l'aide des résumés obtenus par une approche évolutive des méthodes de classification.	Alzennyr Da Silva, Yves Lechevallier, Fabrice Rossi, Francisco de Assis Tenório de Carvalho	http://editions-rnti.fr/render_pdf.php?p1&p=1001434	http://editions-rnti.fr/render_pdf.php?p=1001434	manière visiter réaliser site Web pouvoir changer raison modification lier structurer contenir site luimêm raison changement comportement groupe dutilisateur lémergence comportement modèle associer comportement dan fouiller dusage web devoir mettre jour continuellemer mieux refléter comportement actuel internaute solution proposer dan article mettre jour modèle laid résumé obtenir approcher évolutif méthode classification
925	Revue des Nouvelles Technologies de l'Information	EGC	2007	Construction incrémentale et visualisation de graphes de voisinage par des fourmis artificielles	Cet article décrit un nouvel algorithme incrémental nommé AntGraph pour la construction de graphes de voisinage. Il s'inspire du comportement d'autoassemblage observé chez des fourmis réelles où ces dernières se fixent progressivement à un support fixe puis successivement aux fourmis déjà fixées afin de créer une structure vivante. Nous utilisons ainsi une approche à base de fourmis artificielles où chaque fourmi représente une donnée. Nous indiquons comment ce comportement peut être utilisé pour construire de manière incrémentale un graphe à partir d'une mesure de similarité entre les données. Nous montrons finalement que notre algorithme obtient de meilleurs résultats en comparaison avec le graphe de Voisins Relatifs, notamment en terme de temps de calcul.	Julien Lavergne, Hanane Azzag, Christiane Guinot, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1001323	http://editions-rnti.fr/render_pdf.php?p=1001323	article décrire nouvel algorithme incrémental nommer AntGraph construction graphe voisinage sinspire comportement dautoassemblage observer fourmi réel dernière fixer progressivement support fixer pouvoir successivement fourmi déjà fixer créer structurer vivant utiliser approcher baser fourmi artificiel fourmi représenter donner indiquer comportement pouvoir utiliser construire manière incrémental graphe partir dune mesurer similarité entrer donnée montrer finalement algorithme obtenir meilleur résultat comparaison graphe voisin Relatifs terme temps calcul
926	Revue des Nouvelles Technologies de l'Information	EGC	2007	Découverte de chroniques à partir de séquences d'événements pour la supervision de processus dynamiques	Ce papier adresse le problème de la découverte de connaissances temporelles à partir des données datées, générées par le système de supervision d'un processus de fabrication. Par rapport aux approches existantes qui s'appliquent directement aux données, notre méthode d'extraction des connaissances se base sur un modèle global construit à partir des données. L'approche de modélisation adoptée, dite stochastique, considère les données datées comme une séquence d'occurrences de classes d'événements discrets. Cette séquence est représentée sous les formes duales d'une chaîne de Markov homogène et d'une superposition de processus de Poisson. L'algorithme proposé, appelé BJT4R, permet d'identifier les motifs séquentiels, les plus probables entre deux classes d'événements discrets et les représentent sous la forme de modèles de chroniques. Ce papier présente les premiers résultats de l'application de cet algorithme sur des données générées par un processus de fabrication de semi-conducteur d'un site de production du groupe STMicroelectronics.	Nabil Benayadi, Marc Le Goc, Philippe Bouché	http://editions-rnti.fr/render_pdf.php?p1&p=1001389	http://editions-rnti.fr/render_pdf.php?p=1001389	papier adresser problème découvrir connaissance temporel partir donnée dater générer système supervision dun processus fabrication Par rapport approche existant sappliquent donnée méthode dextraction connaissance baser modeler global construire partir donnée Lapproche modélisation adopter stochastique considérer donnée dater séquence doccurrence classe dévénement discret séquence représenter sou forme dual dune chaîner Markov homogène dune superposition processus Poisson Lalgorithme proposer appeler bjt4r permettre didentifier motif séquentiel plaire entrer classe dévénement discret représenter sou former modèle chronique papier présenter résultat lapplication algorithme donnée générer processus fabrication semiconducteur dun site production grouper stmicroelectronic
927	Revue des Nouvelles Technologies de l'Information	EGC	2007	Des fonctions d'oubli intelligentes dans les entrepôts de données	"Les entrepôts de données stockent des quantités de données de plus en plus massives et arrivent vite à saturation. Un langage de spécifications de fonctions d'oubli est défini pour résoudre ce problème. Dans le but d'offrir la possibilité d'effectuer des analyses sur l'historique des données, les spécifications définissent des résumés par agrégation et par échantillonnage à conserver parmi les données à ""oublier"". Cette communication présente le langage de spécifications ainsi que les principes et les algorithmes pour assurer de façon mécanique la gestion des fonctions d'oubli."	Aliou Boly, Sabine Goutier, Georges Hébrail	http://editions-rnti.fr/render_pdf.php?p1&p=1001380	http://editions-rnti.fr/render_pdf.php?p=1001380	entrepôt donnée stocker quantité donnée plaire plaire massif arriver vite saturation langage spécification fonction doubli définir résoudre problème Dans boire doffrir possibilité deffectuer analyse lhistorique donnée spécification définir résumé agrégation échantillonnage conserver donnée oublier communication présenter langage spécification principe algorithme mécanique gestion fonction doubli
928	Revue des Nouvelles Technologies de l'Information	EGC	2007	Détermination du niveau de consommation des abonnés en téléphonie mobile par la théorie des ensembles flous	La détermination du niveau de consommation chez les clients est essentielle pour tout objectif de segmentation stratégique et de churn. Nous présentons sur un cas réel l'utilisation de la théorie des ensembles flous pour la définition d'une fonction d'appartenance permettant d'évaluer, de manière précise, le niveau de consommation, des abonnés en téléphonie mobile.	Rachid El Meziane, Ilham Berrada, Ismail Kassou, Karim Baïna	http://editions-rnti.fr/render_pdf.php?p1&p=1001378	http://editions-rnti.fr/render_pdf.php?p=1001378	détermination niveau consommation client essentiel objectif segmentation stratégique churn présenter cas réel lutilisation théorie ensemble flou définition dune fonction dappartenanc permettre dévaluer manière préciser niveau consommation abonné téléphonie mobile
929	Revue des Nouvelles Technologies de l'Information	EGC	2007	Ensemble prédicteur fondé sur les cartes auto-organisatrices adapté aux données volumineuses	Le stockage massif des données noie l'information pertinente et engendre des problèmes théoriques liés à la volumétrie des données disponibles. Ces problèmes dégradent la capacité prédictive des algorithmes d'extraction des connaissances à partir des données. Dans cet article, nous proposons une méthodologie adaptée à la représentation et à la prédiction des données volumineuses. A cette fin, suite à un partitionnement des attributs, des groupes d'attributs non-corrélés sont créés qui permettent de contourner les problèmes liés aux espaces de grandes dimensions. Un Ensemble est alors mis en place, apprenant chaque groupe par une carte auto-organisatrice. Outre la prédiction, ces cartes ont pour objectif une représentation pertinente des données. Enfin, la prédiction est réalisée par un vote des différentes cartes. Une expérimentation est menée qui confirme le bien-fondé de cette approche.	Elie Prudhomme, Stéphane Lallich	http://editions-rnti.fr/render_pdf.php?p1&p=1001422	http://editions-rnti.fr/render_pdf.php?p=1001422	stockage massif donnée noyer linformation pertinent engendrer problème théorique lier volumétrie donnée disponible problème dégrader capacité prédictif algorithme dextraction connaissance partir donnée Dans article proposer méthodologie adapté représentation prédiction donnée volumineux fin suite partitionnement attribut groupe dattribut noncorréler créer permettre contourner problème lier espace grand dimension ensemble mettre placer apprendre grouper carte autoorganisatrice Outre prédiction carte objectif représentation pertinent donnée Enfin prédiction réaliser voter carte expérimentation mener confirmer bienfondé approcher
930	Revue des Nouvelles Technologies de l'Information	EGC	2007	Evaluation d'une approche de classification conceptuelle	L'objectif de ce travail est d'évaluer la perte d'information au sens de l'inertie entre des méthodes de partitionnement ou de classification hiérarchiques et une approche de classification conceptuelle. Nous voulons répondre à la question suivante : l'aspect simpliste du processus monothétique d'une méthode conceptuelle implique-t-il des partitions de moins bonne qualité au sens du critère de l'inertie ? Nous proposons de réaliser cette expérience sur 6 bases de l'UCI, trois de ces bases sont des tableaux de données quantitatives, les trois autres sont des tableaux de données qualitatives.	Marie Chavent, Yves Lechevallier	http://editions-rnti.fr/render_pdf.php?p1&p=1001455	http://editions-rnti.fr/render_pdf.php?p=1001455	Lobjectif travail dévaluer perte dinformation sens linertie entrer méthode partitionnement classification hiérarchique approcher classification conceptuel vouloir répondre question   laspect simpliste processus monothétiqu dune méthode conceptuel impliquetil partition qualité sentir critère linertie   proposer réaliser expérience 6 base luci base tableau donnée quantitatif tableau donnée qualitatif
931	Revue des Nouvelles Technologies de l'Information	EGC	2007	Evaluation supervisée de métrique : application à la préparation de données séquentielles	De nos jours, le statisticien n'a plus nécessairement le contrôle sur la récolte des données. Le besoin d'une analyse statistique vient dans un second temps, une fois les données récoltées. Par conséquent, un travail est à fournir lors de la phase de préparation des données afin de passer d'une représentation informatique à une représentation statistique adaptée au problème considéré. Dans cet article, nous étudions un procédé de sélection d'une bonne représentation en nous basant sur des travaux antérieurs. Nous proposons un protocole d'évaluation de la pertinence d'une représentation par l'intermédiaire d'une métrique, dans le cas de la classification supervisée. Ce protocole exploite une méthode de classification non paramétrique régularisée, garantissant l'automaticité et la fiabilité de l'évaluation. Nous illustrons le fonctionnement et les apports de ce protocole par un problème réel de préparation de données de consommation téléphonique. Nous montrons également la fiabilité et l'interprétabilité des décisions qui en résultent.	Sylvain Ferrandiz, Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001392	http://editions-rnti.fr/render_pdf.php?p=1001392	De jour statisticien plaire nécessairement contrôler récolter donnée besoin dune analyser statistique venir dan second temps donnée récolter Par conséquent travail fournir phase préparation donnée prendre dune représentation informatique représentation statistique adapté problème considérer Dans article étudier procéder sélection dune représentation baser travail antérieur proposer protocole dévaluation pertinence dune représentation lintermédiair dune métrique dan cas classification superviser protocole exploiter méthode classification paramétrique régulariser garantir lautomaticité fiabilité lévaluation illustrer fonctionnement apport protocole problème réel préparation donnée consommation téléphonique montrer également fiabilité linterprétabilité décision résulter
932	Revue des Nouvelles Technologies de l'Information	EGC	2007	Evolution de l'ontologie et gestion des annotations sémantiques inconsistantes	Les ontologies et les annotations sémantiques sont deux composants importants dans un système de gestion des connaissances basé sur le Web sémantique. Dans l'environement dynamique et distribué du Web sémantique, les ontologies et les annotations pourraient être changées pour s'adapter à l'évolution de l'organisation concernée. Ces changements peuvent donc entraîner des inconsistances à détecter et traiter. Dans cet article, nous nous focalisons principalement sur l'évolution des annotations sémantiques en soulignant le contexte où les modifications de l'ontologie entraînent des inconsistances sur ces annotations. Nous présentons une approche basée sur des règles permettant de détecter les inconsistances dans les annotations sémantiques devenues obsolètes par rapport à l'ontologie modifiée. Nous décrivons aussi les stratégies d'évolution nécessaires pour guider le processus de résolution de ces inconsistances grâce à des règles correctives.	Phuc-Hiep Luong, Rose Dieng-Kuntz, Alain Boucher	http://editions-rnti.fr/render_pdf.php?p1&p=1001450	http://editions-rnti.fr/render_pdf.php?p=1001450	ontologie annotation sémantique composant important dan système gestion connaissance baser web sémantique Dans lenvironement dynamique distribuer web sémantique ontologie annotation pouvoir changer sadapter lévolution lorganisation concerner changement pouvoir entraîner inconsistance détecter traiter Dans article focaliser principalement lévolution annotation sémantique souligner contexte modification lontologie entraîner inconsistance annotation présenter approcher basé règle permettre détecter inconsistance dan annotation sémantique devenir obsolète rapport lontologie modifier décrire stratégie dévolution nécessaire guider processus résolution inconsistance grâce règle corrective
933	Revue des Nouvelles Technologies de l'Information	EGC	2007	Extension sémantique du modèle de similarité basé sur la proximité floue des termes	Le modèle flou de proximité repose sur l'hypothèse que plus les occurrences des termes d'une requête se trouvent proches dans un document, plus ce dernier est pertinent. Cette mesure floue est très avantageuse dans le traitement des documents à textes courts, toutefois elle ne tient pas compte de la sémantique des termes. Nous présentons dans cet article l'intégration d'une métrique conceptuelle au modèle de proximité floue des termes pour la formalisation de notre propre modèle.	Zoulikha Heddadji, Nicole Vincent, Séverine Kirchner, Georges Stamon	http://editions-rnti.fr/render_pdf.php?p1&p=1001399	http://editions-rnti.fr/render_pdf.php?p=1001399	modeler flou proximité reposer lhypothèse plaire occurrence terme dune requête trouver dan document plaire pertiner mesurer flou avantageux dan traitement document texte court compter sémantique terme présenter dan article lintégration dune métrique conceptuel modeler proximité flouer terme formalisation propre modeler
934	Revue des Nouvelles Technologies de l'Information	EGC	2007	Extraction d'entités dans des collections évolutives	Nous nous intéressons à l'extraction d'entités nommées avec comme but d'exploiter un ensemble de rapports pour en extraire une liste de partenaires. À partir d'une liste initiale, nous utilisons un premier ensemble de documents pour identifier des schémas de phrase qui sont ensuite validés par apprentissage supervisé sur des documents annotés pour en mesurer l'efficacité avant d'être utilisés sur l'ensemble des documents à explorer. Cette approche est inspirée de celle utilisée pour l'extraction de données dans les documents semi-structurés (wrappers) et ne nécessite pas de ressources linguistiques particulières ni de larges collections de tests. Notre collection de documents évoluant annuellement, nous espérons de plus une amélioration de notre extraction dans le temps.	Thierry Despeyroux, Eduardo Fraschini, Anne-Marie Vercoustre	http://editions-rnti.fr/render_pdf.php?p1&p=1001433	http://editions-rnti.fr/render_pdf.php?p=1001433	intéresser lextraction dentité nommer boire dexploiter ensemble rapport extraire liste partenaire À partir dune liste initial utiliser ensemble document identifier schéma phraser ensuite valider apprentissage superviser document annoter mesurer lefficacité dêtre utiliser lensembl document explorer approcher inspirer utiliser lextraction donnée dan document semistructuré wrapper nécessiter ressource linguistique large collection test collection document évoluer annuellement espérer plaire amélioration extraction dan temps
935	Revue des Nouvelles Technologies de l'Information	EGC	2007	Extraction de connaissances d'adaptation par analyse de la base de cas	En raisonnement à partir de cas, l'adaptation d'un cas source pour résoudre un problème cible est une étape à la fois cruciale et difficile à réaliser. Une des raisons de cette difficulté tient au fait que les connaissances d'adaptation sont généralement dépendantes du domaine d'application. C'est ce qui motive la recherche sur l'acquisition de connaissances d'adaptation (ACA). Cet article propose une approche originale de l'ACA fondée sur des techniques d'extraction de connaissances dans des bases de données (ECBD). Nous présentons CABAMAKA, une application qui réalise l'ACA par analyse de la base de cas, en utilisant comme technique d'apprentissage l'extraction de motifs fermés fréquents. L'ensemble du processus d'extraction des connaissances est détaillé, puis nous examinons comment organiser les résultats obtenus de façon à faciliter la validation des connaissances extraites par l'analyste.	Amedeo Napoli, Jean Lieber, Fadi Badra	http://editions-rnti.fr/render_pdf.php?p1&p=1001464	http://editions-rnti.fr/render_pdf.php?p=1001464	En raisonnement partir cas ladaptation dun cas source résoudre problème cibl étape crucial difficile réaliser raison difficulté faire connaissance dadaptation généralement dépendant domaine dapplication cest motiver rechercher lacquisition connaissance dadaptation acer article proposer approcher original laca fonder technique dextraction connaissance dan base donnée ecbd présenter cabamaker application réaliser lACA analyser baser cas utiliser technique dapprentissage lextraction motif fermer fréquent Lensemble processus dextraction connaissance détailler pouvoir examiner organiser résultat obtenir faciliter validation connaissance extraire lanalyste
936	Revue des Nouvelles Technologies de l'Information	EGC	2007	Extraction de données sur Internet avec Retroweb	Ce document décrit Retroweb, une boite à outils qui permet l'extraction de données structurées à partir de pages Web. Notre solution est semi-automatique car les données à extraire sont préalablement dénies par l'utilisateur. L'intérêt de cette approche est qu'elle permet l'extraction de données ciblées et conformes aux besoins de l'application utilisatrice (migrateur, moteur de recherche, outil de veille). Retroweb se caractérise aussi par une grande facilité d'utilisation car il ne nécessite aucune connaissance de langage particulier, la définition des règles d'extraction se faisant directement de manière interactive dans le navigateur Internet. Ce document décrit les trois principaux processus de notre méthode.	Fabrice Estievenart, Jean-Roch Meurisse	http://editions-rnti.fr/render_pdf.php?p1&p=1001338	http://editions-rnti.fr/render_pdf.php?p=1001338	document décrire Retroweb boiter outil permettre lextraction donnée structurer partir page Web solution semiautomatiqu donnée extraire préalablement dénier lutilisateur Lintérêt approcher permettre lextraction donnée cibler conforme besoin lapplication utilisateur migrateur moteur rechercher outil veiller Retroweb caractériser grand faciliter dutilisation nécessiter connaissance langage définition règle dextraction faire manière interactif dan navigateur Internet document décrire principal processus méthode
937	Revue des Nouvelles Technologies de l'Information	EGC	2007	Extraction de séquences multidimensionnelles convergentes et divergentes	"Les motifs séquentiels sont un domaine de la fouille de données très étudié depuis leur introduction par Agrawal et Srikant.Même s'il existe de nombreux travaux (algorithmes, domaines d'application), peu d'entre eux se situent dans un contexte multidimensionnel avec la prise en compte de ses spécificités : plusieurs dimensions, relations hiérarchiques entre les éléments de chaque dimension, etc. Dans cet article, nous proposons une méthode originale pour extraire des connaissances multidimensionnelles définies sur plusieurs niveaux de hiérarchies mais selon un certain point de vue : du général au particulier ou vice et versa. Nous définissons ainsi le concept de séquences multidimensionnelles convergentes ou divergentes ainsi que l'algorithme associé, M2S_CD, basé sur le paradigme ""pattern growth"". Des expérimentations, sur des jeux de données synthétiques et réelles, montrent l'intérêt de notre approche aussi bien en terme de robustesse des algorithmes que de pertinence des motifs extraits."	Marc Plantevit, Anne Laurent, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001388	http://editions-rnti.fr/render_pdf.php?p=1001388	motif séquentiel domaine fouiller donnée étudier introduction Agrawal srikantmêm sil exister travail algorithm domaine dapplication dentre situer dan contexte multidimensionnel priser compter spécificité   dimension relation hiérarchique entrer élément dimension Dans article proposer méthode original extraire connaissance multidimensionnel définie niveau hiérarchie poindre   général vice verser définir concept séquence multidimensionnel convergente divergente lalgorithme associer M2SCD baser paradigme pattern growth expérimentation jeu donnée synthétique réel montrer lintérêt approcher terme robustesse algorithme pertinence motif extrait
938	Revue des Nouvelles Technologies de l'Information	EGC	2007	Extraction des Top-k Motifs par Approximer-et-Pousser	Cet article porte sur l'extraction de motifs sous contraintes globales. Contrairement aux contraintes usuelles comme celle de fréquence minimale, leur vérification est problématique car elle entraine de multiples comparaisons entre les motifs. Typiquement, la localisation des k motifs maximisant une mesure d'intérêt, i.e. satisfaisant la contrainte top-k, est difficile. Pourtant, cette contrainte globale se révèle très utile pour trouver les motifs les plus significatifs au regard d'un critère choisi par l'utilisateur. Dans cet article, nous proposons une méthode générale d'extraction de motifs sous contraintes globales, appelée Approximer-et-Pousser. Cette méthode peut être vue comme une méthode de relaxation d'une contrainte globale en une contrainte locale évolutive. Nous appliquons alors cette approche à l'extraction des top-k motifs selon une mesure d'intérêt. Les expérimentations montrent l'efficacité de l'approche Approximer-et-Pousser.	Arnaud Soulet, Bruno Crémilleux	http://editions-rnti.fr/render_pdf.php?p1&p=1001387	http://editions-rnti.fr/render_pdf.php?p=1001387	article porter lextraction motif sou contraint global contrairement contraint usuel fréquence minimal vérification problématique entraine comparaison entrer motif typiquement localisation motif maximiser mesurer dintérêt ie satisfaire contraint topk difficile pourtant contraint global révéler utile trouver motif plaire significatif regard dun critèr choisir lutilisateur Dans article proposer méthode général dextraction motif sou contraint global appeler approximeretpousser méthode pouvoir voir méthode relaxation dune contraint global contraint local évolutif appliquer approcher lextraction topk motif mesurer dintérêt expérimentation montrer lefficacité lapproche approximeretpousser
939	Revue des Nouvelles Technologies de l'Information	EGC	2007	Filtrage des sites Web à caractère violent par analyse du contenu textuel et structurel	"Dans cet article, nous proposons une solution pour la classification et le filtrage des sites Web à caractère violent. A la différence de la majorité de systèmes commerciaux basés essentiellement sur la détection de mots indicatifs ou l'utilisation d'une liste noire manuellement collectée, notre solution baptisée, ""WebAngels Filter"", s'appuie sur un apprentissage automatique par des techniques de data mining et une analyse conjointe du contenu textuel et structurel de la page Web. Les résultats expérimentaux obtenus lors de l'évaluation de notre approche sur une base de test sont assez bons. Comparé avec des logiciels, parmi les plus populaires, ""WebAngels Filter"" montre sa performance en terme de classification."	Abdelmajid Ben Hamadou, Mohamed Hammami, Radhouane Guermazi	http://editions-rnti.fr/render_pdf.php?p1&p=1001395	http://editions-rnti.fr/render_pdf.php?p=1001395	Dans article proposer solution classification filtrage site Web caractère violer A différence majorité système commercial baser essentiellement détection indicatif lutilisation dune liste noir manuellement collecter solution baptiser webangel filter sappuie apprentissage automatique technique dater mining analyser conjoindre contenir textuel structurel page Web résultat expérimental obtenir lévaluation approcher baser test comparer logiciel plaire populaire webangel filter montrer performance terme classification
940	Revue des Nouvelles Technologies de l'Information	EGC	2007	Finding interesting queries in relational databases	La découverte de motifs dans des bases de données relationnelles quelconques est un problème intéressant pour lequel il existe très peu de méthodes efficaces. Nous présentons un cadre dans lequel des paires de requêtes sur les données sont utilisées comme des motifs et nous discutons du problème de la découverte d'associations utiles entre elles. Plus spécifiquement, nous considérons des petites sous-classes de requêtes conjonctives qui nous permettent de découvrir des motifs intéressants de manière efficace.	Bart Goethals	http://editions-rnti.fr/render_pdf.php?p1&p=1001285	http://editions-rnti.fr/render_pdf.php?p=1001285	découvrir motif dan base donnée relationnel problème intéresser exister méthode efficace présenter cadrer dan paire requête donnée utiliser motif discuter problème découvrir dassociation utile entrer plaire spécifiquement considérer petit sousclasse requêt conjonctive permettre découvrir motif intéressant manière efficace
941	Revue des Nouvelles Technologies de l'Information	EGC	2007	Fusion des approches visuelles et contextuelles pour l'annotation des images médicales	Dans le contexte de la recherche d'information sur Internet, nous proposons une architecture d'annotation automatique des images médicales, extraites à partir des documents de santé en ligne. Notre système est conçu pour extraire des informations médicales spécifiques (i.e. modalité médicale, région anatomique) à partir du contenu et du contexte des images. Nous proposons une architecture de fusion des approches contenu/contexte adaptée aux images médicales. L'approche orientée sur le contenu des images, consiste à annoter des images inconnues par la catégorisation des représentations visuelles compactes. Nous utilisons en même temps le contexte des images (les régions textuelles) ainsi que des ontologies médicales spécialement adaptées aux informations recherchées. Finalement, nous démontrons qu'en fusionnant les décisions des deux approches, nous améliorons les performances globales du système d'annotation.	Filip Florea, Valeriu Cornea, Alexandrina Rogozan, Abdelaziz Bensrhair, Stéfan Jacques Darmoni	http://editions-rnti.fr/render_pdf.php?p1&p=1001416	http://editions-rnti.fr/render_pdf.php?p=1001416	Dans contexte rechercher dinformation Internet proposer architecturer dannotation automatique image médical extrait partir document santé ligne système concevoir extraire information médical spécifique ie modalité médical région anatomique partir contenir contexte image proposer architecturer fusion approche contenucontexte adapter image médical Lapproche orienter contenir image consister annoter image inconnu catégorisation représentation visuel compacte utiliser temps contexte image région textuel ontologie médical spécialement adapter information rechercher finalement démontrer quen fusionner décision approche améliorer performance global système dannotation
942	Revue des Nouvelles Technologies de l'Information	EGC	2007	Génération et enrichissement automatique de listes de patrons de phrases pour les moteurs de questions-réponses	Nous utilisons un algorithme d'amorce mutuelle (Riloff et Jones 99), entre des couples de termes d'une relation et des patrons de phrase. À partir de couples d'amorce, le système génère des listes de patrons qui sont ensuite enrichies de façon semi-supervisée, puis utilisées pour trouver de nouveaux couples. Ces couples sont à leur tour réutilisés pour générer, par itérations successives, de nouveaux patrons. L'originalité de l'étude réside dans l'interprétation du rappel, estimé comme la couverture d'un patron sur l'ensemble des exemples auxquels il s'applique	Cédric Vidrequin, Juan-Manuel Torres-Moreno, Jean-Jacques Schneider, Marc El-Bèze	http://editions-rnti.fr/render_pdf.php?p1&p=1001363	http://editions-rnti.fr/render_pdf.php?p=1001363	utiliser algorithme damorc mutuel Riloff Jones 99 entrer couple terme dune relation patron phraser À partir couple damorc système génèr liste patron ensuite enrichie semisupervisé pouvoir utiliser trouver couple couple tour réutiliser générer itération successif patron Loriginalité létude résider dan linterprétation rappel estimer couverture dun patron lensembl exemple auxquel sappliqu
943	Revue des Nouvelles Technologies de l'Information	EGC	2007	Intégration des connaissances utilisateurs pour des analyses personnalisées dans les entrepôts de données évolutifs	"Dans cet article, nous proposons une approche d'évolution de schéma dans les entrepôts de données qui permet aux utilisateurs d'intégrer leurs propres connaissances du domaine afin d'enrichir les possibilités d'analyse de l'entrepôt. Nous représentons cette connaissance sous la forme de règles de type ""si-alors"". Ces règles sont utilisées pour créer de nouveaux axes d'analyse en générant de nouveaux niveaux de granularité dans les hiérarchies de dimension. Notre approche est fondée sur un modèle formel d'entrepôts de données évolutif qui permet de gérer la mise à jour des hiérarchies de dimension."	Cécile Favre, Fadila Bentayeb, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1001379	http://editions-rnti.fr/render_pdf.php?p=1001379	Dans article proposer approcher dévolution schéma dan entrepôt donnée permettre utilisateur dintégrer propre connaissance domaine denrichir possibilité danalyse lentrepôt représenter connaissance sou former règle typer sialors règle utiliser créer ax danalyse générer niveau granularité dan hiérarchie dimension approcher fonder modeler formel dentrepôts donnée évolutif permettre gérer miser jour hiérarchie dimension
944	Revue des Nouvelles Technologies de l'Information	EGC	2007	Interestingness in Data Mining	Interestingness measures play an important role in data mining regardless of the kind of patterns being mined. These measures are intended for selecting and ranking patterns according to their potential interest to the user. Good measures also allow the time and space cost of the mining process to be reduced. Measuring the interestingness of discovered patterns is an active and important area of data mining research. Although much work has been conducted in this area, so far there is no widespread agreement on a formal definition of interestingness in this context. Based on the diversity of definitions presented to date, interestingness is perhaps best treated as a broad concept, which emphasizes conciseness, coverage, reliability, peculiarity, diversity, novelty, surprisingness, utility, and actionability. This presentation reviews interestingness measures for rules and summaries, classifies them from several perspectives, compares their properties, identifies their roles in the data mining process, gives strategies for selecting appropriate measures for applications, and identifies opportunities for future research in this area.	Howard J. Hamilton	http://editions-rnti.fr/render_pdf.php?p1&p=1001283	http://editions-rnti.fr/render_pdf.php?p=1001283	interestingnes measur play an importer role in dater mining regardless of the kind of pattern being mined these measur are intended for selecting and ranking patterns according to their potential interest to the user Good measur also allow the time and space cost of the mining process to be reduced Measuring the interestingness of discovered patterns is an activer and importer areer of dater mining research Although much work has been conducted in this areer so far there is no widespread agreement formal definition of interestingness in this context Based the diversity of definition presented to dater interestingnes is perhap best treated broad concept which emphasizer concisenes coverage reliability peculiarity diversity novelty surprisingness utility and actionability This presentation review interestingnes measur for ruler and summari classifier them from several perspective compar their propertie identifie their rol in the dater mining process giv strategier for selecting appropriat measur for application and identifie opportunitie for futur research in this areer
945	Revue des Nouvelles Technologies de l'Information	EGC	2007	L'émergence de connaissances dans les communautés de pratique	Cet article est le résultat d'une recherche sur le processus, peu explicité dans la littérature, de création de connaissances dans les communautés de pratique. Nous commençons par établir une définition de travail pour ce concept de communauté de pratique qui permet l'échange et le partage de connaissances au sein de groupes de plus en plus virtuels. Nous analysons ensuite les communautés de pratique sous l'angle de la théorie de l'émergence. Nous proposons, alors, la modélisation d'un outil de support pour ces communautés qui améliore les échanges entre les membres et favorise l'émergence de nouvelles connaissances. Cet outil manipule les connaissances implicites ainsi qu'explicites et propose des possibilités pour la publication et la recherche d'informations. De plus, il s'adapte à chaque membre de la communauté par un processus de personnalisation.	Caroline Wintergerst, Thomas Ludwig, Danielle Boulanger	http://editions-rnti.fr/render_pdf.php?p1&p=1001442	http://editions-rnti.fr/render_pdf.php?p=1001442	article résultat dune rechercher processus expliciter dan littérature création connaissance dan communauté pratiquer commencer établir définition travail concept communauté pratiquer permettre léchange partager connaissance groupe plaire plaire virtuel analyser ensuite communauté pratiquer sou langle théorie lémergence proposer modélisation dun outil support communauté améliorer échange entrer membre favoriser lémergence connaissance outil manipuler connaissance implicite quexplicit proposer possibilité publication rechercher dinformation De plaire sadapte membre communauté processus personnalisation
946	Revue des Nouvelles Technologies de l'Information	EGC	2007	L'outil SDET pour le complètement des données descriptives liées aux bases de données géographiques	L'enrichissement des bases de données est un moyen visant à offrir un supplément informationnel aux utilisateurs. Dans le cas des données géographiques, cette activité représente de nos jours un problème crucial. Sa résolution permettrait de meilleures prises de décisions ne reposant pas uniquement sur les informations limitées. Notre outil SDET (Semantic Data Enrichment Tool) vient proposer une solution d'enrichissement faisant du Système d'Information Géographiques (SIG) initial une source riche d'informations.	Khaoula Mahmoudi, Sami Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1001337	http://editions-rnti.fr/render_pdf.php?p=1001337	lenrichissement base donnée moyen viser offrir supplément informationnel utilisateur Dans cas donnée géographique activité représenter jour problème crucial résolution permettre meilleure prendre décision reposer uniquement information limiter outil SDET Semantic Data Enrichment Tool venir proposer solution denrichissemer faire système dinformation Géographiques SIG initial source riche dinformation
947	Revue des Nouvelles Technologies de l'Information	EGC	2007	Les itemsets essentiels fermés : une nouvelle représentation concise	Devant l'accroissement constant des grandes bases de données, plusieurs travaux de recherche en fouille de données s'orientent vers le développement de techniques de représentation compacte. Ces recherches se développent suivant deux axes complémentaires : l'extraction de bases génériques de règles d'association et l'extraction de représentations concises d'itemsets fréquents.Dans ce papier, nous introduisons une nouvelle représentation concise exacte des itemsets fréquents. Elle se situe au croisement de chemins de deux autres représentations concises, à savoir les itemsets fermés et ceux dits essentiels. L'idée intuitive est de profiter du fait que tout opérateur de fermeture induit une fonction surjective. Dans ce contexte, nous introduisons un nouvel opérateur de fermeture permettant de calculer les fermetures des itemsets essentiels. Ceci a pour but d'avoir une représentation concise de taille réduite tout en permettant l'extraction des supports négatif et disjonctif d'un itemset en plus de son support conjonctif. Un nouvel algorithme appelé D-CLOSURE permettant d'extraire les itemsets essentiels fermés est aussi présenté. L'étude expérimentale que nous avons menée a permis de confirmer que la nouvelle approche présente un bon taux de compacité comparativement aux autres représentations concises exactes.	Tarek Hamrouni, Islem Denden, Sadok Ben Yahia, Engelbert Mephu Nguifo, Yahya Slimani	http://editions-rnti.fr/render_pdf.php?p1&p=1001384	http://editions-rnti.fr/render_pdf.php?p=1001384	Devant laccroissement constant grand base donnée travail rechercher fouiller donnée sorientent ver développement technique représentation compacter recherche développer axe complémentaire   lextraction base générique règle dassociation lextraction représentation concis ditemset fréquentsDans papier introduire représentation concis exact itemset fréquent situer croisemer chemin représentation concis savoir itemset fermer essentiel Lidée intuitif profiter faire opérateur fermeture induire fonction surjectif Dans contexte introduire nouvel opérateur fermeture permettre calculer fermeture itemset essentiel boire davoir représentation concis tailler réduire permettre lextraction support négatif disjonctif dun itemset plaire support conjonctif nouvel algorithme appeler DCLOSURE permettre dextraire itemset essentiel fermer présenter Létude expérimental mener permettre confirmer approcher présenter taux compacité comparativement représentation concis exact
948	Revue des Nouvelles Technologies de l'Information	EGC	2007	Logiciel d'aide à l'évaluation des catégorisations	"Les méthodes de classification automatique sont employées dans des domaines variés et de nombreux algorithmes ont été proposés dans la littérature. Au milieu de cette ""jungle"", il semble parfois difficile à un simple utilisateur de choisir quel algorithme est le plus adapté à ses besoins. Depuis le milieu des années 90, une nouvelle thématique de recherches, appelée clustering validity, tente de répondre à ce genre d'interrogation en proposant des indices pour juger de la qualité des catégorisations obtenues. Mais le choix est parfois difficile entre ces indices et il peut s'avérer délicat de prendre la bonne décision. C'est pourquoi nous proposons un logiciel adapté à cette problématique d'évaluation."	Julien Velcin, William Vacher, Jean-Gabriel Ganascia	http://editions-rnti.fr/render_pdf.php?p1&p=1001333	http://editions-rnti.fr/render_pdf.php?p=1001333	méthode classification automatique employer dan domaine varié algorithme proposer dan littérature milieu jungle sembler difficile simple utilisateur choisir algorithme plaire adapter besoin Depuis milieu année 90 thématique recherche appeler clustering validity tenter répondre genre dinterrogation proposer indice juger qualité catégorisation obtenu Mais choix difficile entrer indice pouvoir savérer délicat prendre décision cest proposer logiciel adapter problématique dévaluation
949	Revue des Nouvelles Technologies de l'Information	EGC	2007	Mesure d'entropie asymétrique et consistante	Les mesures d'entropie, dont la plus connue est celle de Shannon, ont été proposées dans un contexte de codage et de transmission d'information. Néanmoins, dès le milieu des années soixante, elles ont été utilisées dans d'autres domaines comme l'apprentissage et plus particulièrement pour construire des graphes d'induction et des arbres de décision. L'usage brut de ces mesures n'est cependant pas toujours bien approprié pour engendrer des modèles de prédiction ou d'explication pertinents. Cette faiblesse résulte des propriétés des entropies, en particulier le maximum nécessairement atteint pour la distribution uniforme et l'insensibilité à la taille de l'échantillon. Nous commençons par rappeler ces propriétés classiques. Nous définissons ensuite une nouvelle axiomatique mieux adaptée à nos besoins et proposons une mesure empirique d'entropie plus flexible vérifiant ces axiomes.	Djamel Abdelkader Zighed, Simon Marcellin, Gilbert Ritschard	http://editions-rnti.fr/render_pdf.php?p1&p=1001309	http://editions-rnti.fr/render_pdf.php?p=1001309	mesure dentropie plaire connaître Shannon proposer dan contexte codage transmission dinformation milieu année soixant utiliser dan dautre domaine lapprentissage plaire construire graphe dinduction arbre décision Lusage brut mesure nest approprier engendrer modèle prédiction dexplication pertinent faiblesse résulter propriété entropie maximum nécessairement atteindre distribution uniforme linsensibilité tailler léchantillon commencer rappeler propriété classique définir ensuite axiomatique mieux adapter besoin proposon mesurer empirique dentropie plaire flexible vérifier axiome
950	Revue des Nouvelles Technologies de l'Information	EGC	2007	Mesure non symétrique pour l'évaluation de modèles, utilisation pour les jeux de données déséquilibrés	Les critères servant à l'évaluation de modèles d'apprentissage supervisé ainsi que ceux utilisés pour bâtir des arbres de décision sont, pour la plupart, symétriques. De manière pragmatique, cela signifie que chacune des modalités de la variable endogène se voit assigner une importance identique. Or, dans nombre de cas pratiques cela n'est pas le cas. Ainsi, on peut notamment prendre l'exemple de jeux de données fortement déséquilibrés pour lesquels l'objectif principal est l'identification des objets représentatifs de la modalité minoritaire (Aide au diagnostic, identification de phénomènes inhabituels : fraudes, pannes...). Dans ce type de situation il apparaît clairement qu'assigner une importance identique aux erreurs de prédiction ne constitue pas la meilleure des solutions. Nous proposons dans cet article un critère (pouvant servir à la fois pour l'évaluation de modèles d'apprentissage supervisé ou encore de critère utilisé pour bâtir des arbres de décision) prenant en compte cet aspect non symétrique de l'importance associée à chacune des modalités de la variable endogène. Nous proposons ensuite une évolution des modèles de type forêts aléatoires utilisant ce critère pour les jeux de données fortement déséquilibrés.	Julien Thomas, Pierre-Emmanuel Jouve, Nicolas Nicoloyannis	http://editions-rnti.fr/render_pdf.php?p1&p=1001426	http://editions-rnti.fr/render_pdf.php?p=1001426	critère servir lévaluation modèle dapprentissage superviser utiliser bâtir arbre décision symétrique De manière pragmatique celer signifier modalité variable endogène voir assigner importance identique Or dan nombre cas pratique celer nest cas pouvoir prendre lexemple jeu donnée fortement déséquilibrer lobjectif principal lidentification objet représentatif modalité minoritaire aider diagnostic identification phénomène inhabituel   fraude panner Dans typer situation apparaître clairement quassigner importance identique erreur prédiction constituer meilleur solution proposer dan article critère pouvoir servir lévaluation modèle dapprentissage superviser critère utiliser bâtir arbre décision prendre compter aspect symétrique limportance associer modalité variable endogène proposer ensuite évolution modèle typer forêt aléatoire utiliser critère jeu donnée fortement déséquilibrer
951	Revue des Nouvelles Technologies de l'Information	EGC	2007	Méthodes statistiques et modèles thermiques compacts	Dans le domaine thermique, la plupart des études reposent sur des modèles à éléments finis. Cependant, le coût en calcul et donc en temps de ces méthodes ont renforcé le besoin de modèles plus compacts. Le réseau RC équivalent est la solution la plus souvent utilisée. Toutefois, ses paramètres doivent souvent être ajustés à l'aide de mesures ou de simulation. Dans ce contexte d'identification de système, les méthodes statistiques seront comparées aux méthodes classiquement utilisées pour la prédiction thermique.	Hubert Polaert, Philippe Leray, Grégory Mallet	http://editions-rnti.fr/render_pdf.php?p1&p=1001377	http://editions-rnti.fr/render_pdf.php?p=1001377	Dans domaine thermique étude reposer modèle élément finir coût calcul temps méthode renforcer besoin modèle plaire compact réseau RC équivaloir solution plaire utiliser paramètre devoir ajuster laid mesure simulation Dans contexte didentification système méthode statistique comparer méthode classiquement utiliser prédiction thermique
952	Revue des Nouvelles Technologies de l'Information	EGC	2007	Navigation et appariement d'objets géographiques dans une ontologie	L'ACI FoDoMuSt se propose d'élaborer un processus de fouille de données multi-stratégies pour la reconnaissance automatique d'objets géographiques sur des images satellitaires ou aériennes. Ces dernières sont segmentées afin d'isoler des polygones définis par un ensemble de descripteurs de bas niveaux. Afin de leur affecter une sémantique, on applique dans un premier temps une classification. Si aucun objet géographique n'est identifié, on tente alors un appariement du polygone avec les concepts d'une ontologie d'objets géographiques. Un algorithme de navigation dans l'ontologie et une mesure de comparaison sémantique ont ainsi été développés, paramétrables selon le contexte d'appariement. Cette mesure évalue la pertinence d'un appariement et comprend une composante locale (comparaison au niveau du concept) et une composante globale (combinaison linéaire de mesures locales). La méthode proposée a été développée en JAVA et intégrée à la plate-forme FoDoMuSt. Les premières expérimentations et évaluations humaines sont très encourageantes.	Rémy Brisson, Omar Boussaid, Pierre Gançarski, Anne Puissant, Nicolas Durand	http://editions-rnti.fr/render_pdf.php?p1&p=1001402	http://editions-rnti.fr/render_pdf.php?p=1001402	laci fodomust proposer délaborer processus fouiller donnée multistratégi reconnaissance automatique dobjet géographique image satellitaire aérien dernière segmenter disoler polygone définir ensemble descripteur niveau Afin sémantique appliquer dan temps classification Si objet géographique nest identifier tenter appariement polygone concept dune ontologie dobjet géographique algorithme navigation dan lontologie mesurer comparaison sémantique développer paramétrable contexte dappariemer mesurer évaluer pertinence dun appariement comprendre composant local comparaison niveau concept composant global combinaison linéaire mesure local méthode proposer développer JAVA intégrer plateforme fodomust expérimentation évaluation humain encourageant
953	Revue des Nouvelles Technologies de l'Information	EGC	2007	Notion de conversation dans les communications interpersonnelles instantanées sur IP	Dans cet article nous étudions la contribution des techniques de fouille de données à l'amélioration des services de communications instantanées sur IP tel que la messagerie instantanée (IM) et la téléphonie sur IP (ToIP).	Alexandre Bouchacourt, Luigi Lancieri	http://editions-rnti.fr/render_pdf.php?p1&p=1001359	http://editions-rnti.fr/render_pdf.php?p=1001359	Dans article étudier contribution technique fouiller donnée lamélioration service communication instantaner ip messagerie instantané im téléphonie ip toip
954	Revue des Nouvelles Technologies de l'Information	EGC	2007	OKM : une extension des k-moyennes pour la recherche de classes recouvrantes	Dans cet article nous abordons le problème de la classification (ou clustering) dans le but de découvrir des classes avec recouvrements. Malgré quelques avancées récentes dans ce domaines, motivées par des besoins applicatifs importants (traitements des données multimédia par exemple), nous constatons l'absence de solutions théoriques à ce problème. Notre étude consiste alors à proposer une nouvelle formulation du problème de classification par partitionnement, adaptée à la recherche d'un recouvrement des données en classes d'objets similaires. Cette approche se fonde sur la dénition d'un critère objectif de qualité d'un recouvrement et d'une solution algorithmique visant à optimiser ce critère. Nous proposons deux évaluations de ce travail permettant d'une part d'appréhender le fonctionnement global de l'algorithme sur des données simples (vitesse de convergence, visualisation des résultats) et d'autre part d'évaluer quantitativement le bénéfice d'une telle approche sur une application de classification de documents textuels.	Guillaume Cleuziou	http://editions-rnti.fr/render_pdf.php?p1&p=1001458	http://editions-rnti.fr/render_pdf.php?p=1001458	Dans article aborder problème classification clustering dan boire découvrir classe recouvrement Malgré avancée récent dan domaine motiver besoin applicatif important traitement donnée multimédier exemple constater labsence solution théorique problème étude consister proposer formulation problème classification partitionnement adapter rechercher dun recouvrement donnée classe dobjet similaire approcher fondre dénition dun critère objectif qualité dun recouvrement dune solution algorithmique viser optimiser critère proposer évaluation travail permettre dune partir dappréhender fonctionnement global lalgorithme donnée simple vitesse convergence visualisation résultat dautre partir dévaluer quantitativement bénéfice dune approcher application classification document textuel
955	Revue des Nouvelles Technologies de l'Information	EGC	2007	Optimal histogram representation of large data sets: Fisher vs piecewise linear approximation	Histogram representation of a large set of data is a good way to summarize and visualize data and is frequently performed in order to optimize query estimation in DBMS. In this paper, we show the performance and the properties of two strategies for an optimal construction of histograms on a single real valued descriptor on the base of a prior choice of the number of buckets. The first one is based on the Fisher algorithm, while the second one is based on a geometrical procedure for the interpolation of the empirical distribution function by a piecewise linear function. The goodness of fit is computed using the Wasserstein metric between distributions. We compare the proposed method performances against some existing ones on artificial and real datasets.	Antonio Irpino, Elvira Romano	http://editions-rnti.fr/render_pdf.php?p1&p=1001314	http://editions-rnti.fr/render_pdf.php?p=1001314	Histogram representation of large set of dater is good way to summarize and visualize dater and is frequently performed in order to optimize query estimation in DBMS In this paper we show the performance and the propertie of two strategier for an optimal construction of histogram single real valued descriptor the baser of prior choice of the number of bucket The first one is based the Fisher algorithm while the second one is based geometrical procedur for the interpolation of the empirical distribution function by piecewise linear function The goodness of faire is computed using the Wasserstein metric between distributer we comparer the proposed method performance against some existing artificial and real dataset
956	Revue des Nouvelles Technologies de l'Information	EGC	2007	Partitionnement d'un réseau de sociabilité à fort coefficient de clustering	Afin de comparer l'organisation sociale d'une paysannerie médiévale avant et après la guerre de Cent Ans nous étudions la structure de réseaux sociaux construits à partir d'un corpus de contrats agraires. Faibles diamètres et fort clustering révèlent des graphes en petit monde. Comme beaucoup de grands réseaux d'interaction étudiés ces dernières années ces graphes sont sans échelle typique. Les distributions des degrés de leurs sommets sont bien ajustées par une loi de puissance tronquée par une coupure exponentielle. Ils possèdent en outre un club-huppé, c'est à dire un noyau dense et de faible diamètre regroupant les individus à forts degrés. La forme particulière des éléments propres du laplacien permet d'extraire des communautés qui se répartissent en étoile autour du club huppé.	Romain Boulet, Bertrand Jouve	http://editions-rnti.fr/render_pdf.php?p1&p=1001438	http://editions-rnti.fr/render_pdf.php?p=1001438	Afin comparer lorganisation social dune paysannerie médiéval guerre Cent an étudier structurer réseau social construit partir dun corpu contrat agraire Faibles diamètr fort clustering révéler graphe petit monder Comme grand réseau dinteraction étudier année graphe échelle typique distribution degré sommet ajuster loi puissance tronquer coupure exponentiel posséder outrer clubhuppé cest noyau dense faible diamètre regrouper individu fort degrer former élément propre laplacien permettre dextraire communauté répartir étoiler autour club huppé
957	Revue des Nouvelles Technologies de l'Information	EGC	2007	Peut-on capturer la sémantique à travers la syntaxe ? - découverte des règles d'exception simultanée	L'objectif de la fouille de données est la découverte sophistiquée de connaissances lisibles, surprenantes et possiblement utiles. Les aspects surprenant et utile font partie de la sémantique et nécessitent l'utilisation des connaissances du domaine, ce qui cause souvent le problème d'acquisition de la connaissance. Notre découverte des règles d'exception simultanée peut être une réponse à ce problème. Nous envisageons de trouver les connaissances surprenantes et possiblement utiles à travers notre forme de paire de règles d'exception. Les autres méthodes inventées concernent l'index d'évaluation et la recherche exhaustive. Plusieurs applications médicales seront présentées sur lesquelles nos propositions ont été appliquées.	Einoshin Suzuki	http://editions-rnti.fr/render_pdf.php?p1&p=1001280	http://editions-rnti.fr/render_pdf.php?p=1001280	Lobjectif fouiller donnée découvrir sophistiquer connaissance lisible surprenante possiblemer utile aspect surprendre utile faire partir sémantique nécessiter lutilisation connaissance domaine causer problème dacquisition connaissance découvrir règle dexception simultané pouvoir réponse problème envisager trouver connaissance surprenant possiblemer utile travers former pair règle dexception méthode inventé concerner lindex dévaluation rechercher exhaustif application médical présenter proposition appliquer
958	Revue des Nouvelles Technologies de l'Information	EGC	2007	Préservation de l'Intimité dans les Protocoles de Conversations	Le travail présenté dans cet article, rentre dans le cadre de la gestion des données privées en vue de la substitution, appelée remplaçabilité, dynamique des services Web. Trois contributions sont apportées, (1) modélisation des politiques privées spécifiant les règles d'utilisation des données privées, prenant en compte des aspects se rapportant aux services Web, (2) étendre les protocoles de conversations des services Web par le modèle proposé, afin d'apporter les primitives nécessaires pour l'analyse des protocoles en présence de ces règles, (3) définition d'un mécanisme d'analyse de la remplaçabilité d'un service par un autre en vue de ses politiques privées.	Nawal Guermouche, Salima Benbernou, Emmanuel Coquery, Mohand-Said Hacid	http://editions-rnti.fr/render_pdf.php?p1&p=1001360	http://editions-rnti.fr/render_pdf.php?p=1001360	travail présenter dan article rentrer dan cadrer gestion donnée privé substitution appeler remplaçabilité dynamique service Web Trois contribution apporter 1 modélisation politique privé spécifier règle dutilisation donnée privé prendre compter aspect rapporter service Web 2 étendre protocole conversation service Web modeler proposer dapporter primitive nécessaire lanalyse protocole présence règle 3 définition dun mécanisme danalyse remplaçabilité dun service politique privé
959	Revue des Nouvelles Technologies de l'Information	EGC	2007	RAS : Un outil pour l'annotation de documents basée sur les liens de citation	RAS (Reference Annotation System) est un outil d'annotation de documents. Cet outil est le résultat de l'implémentation de notre approche d'annotation basée sur le contexte de citation. L'approche est indépendante du contenu et utilise un regroupement thématique des références construit à partir d'une classification floue non-supervisée. L'outil présenté dans cet article a été expérimentée et évaluée avec la base de documents scientifiques Citeseer.	Lylia Abrouk, Danièle Hérin	http://editions-rnti.fr/render_pdf.php?p1&p=1001341	http://editions-rnti.fr/render_pdf.php?p=1001341	RAS Reference Annotation System outil dannotation document outil résultat limplémentation approcher dannotation basé contexte citation Lapproche indépendant contenir utiliser regroupement thématique référence construire partir dune classification flou nonsupervisée Loutil présenter dan article expérimenter évaluer baser document scientifique Citeseer
960	Revue des Nouvelles Technologies de l'Information	EGC	2007	Ré-ordonnancement pour l'apprentissage de transformations de documents HTML	Notre objectif est de transformer les documents Web vers un schéma médiateur XML défini a priori. C'est une étape nécessaire pour de nombreuses tâches de recherche d'information concernant le Web Sémantique, les documents semi-structurés, le traitement de sources hétérogènes, etc. Elle permet d'associer une structure sémantiquement riche à des documents dont le formats ne contient que des informations de présentation. Nous proposons de traiter ce problème comme un problème d'apprentissage structuré en le formalisant comme une transformation d'arbre en arbre.Notre méthode de transformation comporte deux étapes. Dans une première étape, une grammaire hors-contexte probabiliste permet de générer un ensemble de solutions candidates. Dans une deuxième étape, ces solutions candidates sont ordonnées grâce à un algorithme de ré-ordonnancement à base de perceptron à noyau. Cette étape d'ordonnancement nous permet d'utiliser de manière efficace des caractéristiques complexes définies à partir du document d'entrée et de la solution candidate.	Guillaume Wisniewski, Patrick Gallinari	http://editions-rnti.fr/render_pdf.php?p1&p=1001461	http://editions-rnti.fr/render_pdf.php?p=1001461	objectif transformer document Web ver schéma médiateur xml définir priori cest étape nécessaire tâche rechercher dinformation concerner web Sémantique document semistructuré traitement source hétérogène permettre dassocier structurer sémantiquement riche document format contenir information présentation proposer traiter problème problème dapprentissage structurer formaliser transformation darbr arbreNotre méthode transformation comporter étape Dans étape grammair horscontexte probabiliste permettre générer ensemble solution candidater Dans étape solution candidat ordonner grâce algorithme réordonnancement baser perceptron noyau étape dordonnancement permettre dutiliser manière efficace caractéristique complexe définir partir document dentré solution candidat
961	Revue des Nouvelles Technologies de l'Information	EGC	2007	Réduction de dimension pour l'analyse de données vidéo	Les données vidéo ont la particularité d'être très volumineuses alors qu'elles contiennent peu d'information sémantique. Pour les analyser, il faut réduire la quantité d'information dans l'espace de recherche. Les données vidéo sont souvent considérées comme l'ensemble des pixels d'une succession d'images analysées séquentiellement. Dans cet article, nous proposons d'utiliser une analyse en composantes principales (ACP) pour réduire la dimensionnalité des informations sans perdre la nature tridimensionnelle des données initiales. Nous commençons par considérer des sous-séquences, dont le nombre de trames est le nombre de dimensions dans l'espace de représentation. Nous appliquons une ACP pour obtenir un espace de faible dimension où les points similaires sémantiquement sont proches. La sous-séquence est ensuite divisée en blocs tridimensionnels dont on projette l'ellipsoïde d'inertie dans le premier plan factoriel. Nous déduisons enfin le mouvement présent dans les blocs à partir des ellipses ainsi obtenues. Nous présenterons les résultats obtenus pour un problème de vidéosurveillance.	Nicolas Verbeke, Nicole Vincent	http://editions-rnti.fr/render_pdf.php?p1&p=1001404	http://editions-rnti.fr/render_pdf.php?p=1001404	donnée vidéo particularité dêtre volumineux contenir dinformation sémantique Pour analyser falloir réduire quantité dinformation dan lespace rechercher donnée vidéo considérer lensembl pixel dune succession dimag analyser séquentiellemer Dans article proposer dutiliser analyser composante principal acp réduire dimensionnalité information perdre nature tridimensionnel donnée initial commencer considérer sousséquence nombre trame nombre dimension dan lespace représentation appliquer ACP obtenir espacer faible dimension point similaire sémantiquement sousséquence ensuite diviser bloc tridimensionnel projeter lellipsoïde dinertie dan plan factoriel déduire mouvement présent dan bloc partir ellipse obtenu présenter résultat obtenir problème vidéosurveillance
962	Revue des Nouvelles Technologies de l'Information	EGC	2007	Régression floue et crédibiliste par SVM pour la classification des images sonar	La classification des images sonar est d'une grande importance par exemple pour la navigation sous-marine ou pour la cartographie des fonds marins. En effet, le sonar offre des capacités d'imagerie plus performantes que les capteurs optiques en milieu sous-marin. La classification de ce type de données rencontre plusieurs difficultés en raison des imprécisions et incertitudes liées au capteur et au milieu. De nombreuses approches ont été proposées sans donner de bons résultats, celles-ci ne tenant pas compte des imperfections des données. Pour modéliser ce type de données, il est judicieux d'utiliser les théories de l'incertain comme la théorie des sous-ensembles flous ou la théorie des fonctions de croyance. Les machines à vecteurs de supports sont de plus en plus utilisées pour la classification automatique aux vues leur simplicité et leurs capacités de généralisation. Il est ainsi possible de proposer une approche qui tient compte de ces imprécisions et de ces incertitudes au coeur même de l'algorithme de classification. L'approche de la régression par SVM que nous avons introduite permet cette modélisation des imperfections. Nous proposons ici une application de cette nouvelle approche sur des données réelles particulièrement complexes, dans le cadre de la classification des images sonar.	Hicham Laanaya, Arnaud Martin, Driss Aboutajdine, Ali Khenchaf	http://editions-rnti.fr/render_pdf.php?p1&p=1001295	http://editions-rnti.fr/render_pdf.php?p=1001295	classification image sonar dune grand importance exemple navigation sousmarin cartographie fonds marin En sonar offrir capacité dimagerie plaire performant capteur optique milieu sousmarin classification typer donnée rencontrer difficulté raison imprécision incertitude lier capteur milieu approche proposer donner résultat cellesci compter imperfection donnée Pour modéliser typer donnée judicieux dutiliser théorie lincertain théorie sousensemble flou théorie fonction croyance machine vecteur support plaire plaire utiliser classification automatique simplicité capacité généralisation proposer approcher compter imprécision incertitude coeur lalgorithme classification Lapproche régression svm introduire permettre modélisation imperfection proposer application approcher donnée réel complexe dan cadrer classification image sonar
963	Revue des Nouvelles Technologies de l'Information	EGC	2007	Segmentation thématique par calcul de distance thématique	Dans cet article, nous présentons une approche de la segmentation thématique fondée sur une représentation en vecteurs sémantiques des phrases et des calculs de distance entre ces vecteurs. Les vecteurs sémantiques sont générés par le système SYGFRAN, un analyseur morpho-syntaxique et conceptuel de la langue française. La segmentation thématique s'effectue elle en recherchant des zones de transition au sein du texte grâce aux vecteurs sémantiques. L'évaluation de cette méthode s'est faite sur les données du défi DEFT'06.	Jacques Chauché, Alexandre Labadié	http://editions-rnti.fr/render_pdf.php?p1&p=1001398	http://editions-rnti.fr/render_pdf.php?p=1001398	Dans article présenter approcher segmentation thématique fonder représentation vecteur sémantique phrase calcul distancer entrer vecteur vecteur sémantique générer système sygfran analyseur morphosyntaxique conceptuel langue français segmentation thématique seffectue rechercher zone transition texte grâce vecteur sémantique lévaluation méthode sest faire donnée défi DEFT06
964	Revue des Nouvelles Technologies de l'Information	EGC	2007	Sémantique et contextes conceptuels pour la recherche d'information	Cet article propose une méthodologie de recherche d'information qui utilise l'analyse conceptuelle conjointement avec la sémantique dans le but de fournir des réponses contextuelles à des requêtes sur le web. Le contexte conceptuel défini dans cet article peut être global – c'est-à-dire stable – ou instantané – c'est-à-dire borné par le contexte global. Notre méthodologie consiste en une première phase de pré traitement permettant de construire le contexte global, et une seconde phase de traitement en ligne des requêtes des utilisateurs, associées au contexte instantané. Notre processus de recherche d'information est illustré à travers une expérimentation dans le domaine du tourisme.	Michel Soto, Bénédicte Le Grand, Marie-Aude Aufaure	http://editions-rnti.fr/render_pdf.php?p1&p=1001440	http://editions-rnti.fr/render_pdf.php?p=1001440	article proposer méthodologie rechercher dinformation utiliser lanalys conceptuel conjointement sémantique dan boire fournir réponse contextuel requête web contexte conceptuel définir dan article pouvoir global – cestàdir stable – instantané – cestàdir borner contexte global méthodologie consister phase pré traitement permettre construire contexte global second phase traitement ligne requête utilisateur associé contexte instantané processus rechercher dinformation illustrer travers expérimentation dan domaine tourisme
965	Revue des Nouvelles Technologies de l'Information	EGC	2007	Sous-bases k-faibles pour des règles d'association valides au sens de la confiance	Nous introduisons la notion de sous-base k-faible pour les règles d'association valides au sens de la confiance. Ces sous-bases k-faibles sont caractérisées en termes d'opérateurs de fermeture correspondant à des familles de Moore k-faiblement hiérarchiques.	Jean Diatta, Régis Girard	http://editions-rnti.fr/render_pdf.php?p1&p=1001383	http://editions-rnti.fr/render_pdf.php?p=1001383	introduire notion sousbase kfaibl règle dassociation valide sens confiance sousbase kfaibl caractériser terme dopérateur fermeture correspondre famille Moore kfaiblement hiérarchique
966	Revue des Nouvelles Technologies de l'Information	EGC	2007	SPoID : Extraction de motifs séquentiels pour les bases de données incomplètes	Les bases de données issues du monde réel contiennent souvent de nombreuses informations non renseignées. Durant le processus d'extraction de connaissances dans les bases de données, une phase de traitement spécifique de ces données est souvent nécessaire, permettant de les supprimer ou de les compléter. Lors de l'extraction de séquences fréquentes, ces données incomplètes sont la plupart du temps occultées. Ceci conduit parfois à l'élimination de plus de la moitié de la base et l'information extraite n'est plus représentative. Nous proposons donc de ne plus éliminer les enregistrements incomplets, mais d'utiliser l'information partielle qu'ils contiennent. La méthode proposée ignore en fait temporairement certaines données incomplètes pour les séquences recherchées. Les expérimentations sur jeux de données synthétiques montrent la validité de notre proposition aussi bien en terme de qualité des motifs extraits que de robustesse aux valeurs manquantes.	Céline Fiot, Anne Laurent, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1001460	http://editions-rnti.fr/render_pdf.php?p=1001460	base donnée issu monder réel contenir information renseigner Durant processus dextraction connaissance dan base donnée phase traitement spécifique donnée nécessaire permettre supprimer compléter lextraction séquence fréquent donnée incomplet temps occulter conduire lélimination plaire moitié baser linformation extraire nest plaire représentatif proposer plaire éliminer enregistrement incomplet dutiliser linformation partiel quils contenir méthode proposer ignorer faire temporairement donnée incomplet séquence rechercher expérimentation jeu donnée synthétique montrer validité proposition terme qualité motif extrait robustesse manquant
967	Revue des Nouvelles Technologies de l'Information	EGC	2007	SyRQuS - Recherche par combinaison de graphes RDF	Nous nous intéressons à un mécanisme permettant la construction de réponses combinés à partir de plusieurs graphes RDF. Nous imposons, par souci de cohérence, que cette combinaison soit réalisée uniquement si les graphes RDF ne se contredisent pas. Pour déterminer la non-contradiction entre deux graphes RDF nous utilisons une mesure de similarité, calculée au moment de l'ajout de documents RDF dans la base de documents.	Adrian Tanasescu	http://editions-rnti.fr/render_pdf.php?p1&p=1001347	http://editions-rnti.fr/render_pdf.php?p=1001347	intéresser mécanisme permettre construction réponse combiner partir graphe RDF imposer souci cohérence combinaison réaliser uniquement graphe RDF contredire Pour déterminer noncontradiction entrer graphe RDF utiliser mesurer similarité calculer moment lajout document RDF dan baser document
968	Revue des Nouvelles Technologies de l'Information	EGC	2007	Traitement de données de consommation électrique par un Système de Gestion de Flux de Données	Avec le développement de compteurs communicants, les consommations d'énergie électrique pourront à terme être télérelevées par les fournisseurs d'électricité à des pas de temps pouvant aller jusqu'à la seconde. Ceci générera des informations en continu, à un rythme rapide et en quantité importante. Les Systèmes de Gestion de Flux de Données (SGFD), aujourd'hui disponibles sous forme de prototypes, ont vocation à faciliter la gestion de tels flux. Cette communication décrit une étude expérimentale pour analyser les avantages et limites de l'utilisation de deux prototypes de SGFD (STREAM et TelegraphCQ) pour la gestion de données de consommation électrique.	Talel Abdessalem, Raja Chiky, Georges Hébrail, Jean Louis Vitti	http://editions-rnti.fr/render_pdf.php?p1&p=1001432	http://editions-rnti.fr/render_pdf.php?p=1001432	Avec développement compteur communicant consommation dénergie électrique pouvoir terme télérelever fournisseur délectricité temps pouvoir aller jusquà second générer information continu rythmer rapide quantité important système gestion flux donnée SGFD aujourdhui disponible sou former prototype vocation faciliter gestion flux communication décrire étude expérimental analyser avantage limite lutilisation prototype sgfd stream TelegraphCQ gestion donnée consommation électrique
969	Revue des Nouvelles Technologies de l'Information	EGC	2007	Traitement et exploration du fichier Log du Serveur Web pour l'extraction des connaissances : Web Usage Mining	Le but dans ce travail consiste à concevoir et réaliser un Outil Logiciel, en utilisant les concepts du Web Usage Mining pour offrir aux web masters l'ensemble des connaissances, y inclut les statistiques sur leurs sites, afin de prendre les décisions adéquates. Il s'agit en fait, d'extraire de l'information à partir du fichier log du serveur Web, hébergeant le site Web, et de prendre les décisions pour découvrir les habitudes des internautes, et de répondre à leurs besoins en adaptant le contenu, la forme et l'agencement des pages web.	Mostafa Hanoune, Faouzia Benabbou	http://editions-rnti.fr/render_pdf.php?p1&p=1001344	http://editions-rnti.fr/render_pdf.php?p=1001344	boire dan travail consister concevoir réaliser Outil Logiciel utiliser concept web Usage Mining offrir web master lensembl connaissance yu inclure statistique site prendre décision adéquat sagit faire dextraire linformation partir fichier log serveur Web héberger site Web prendre décision découvrir habitude internaute répondre besoin adapter contenir former lagencement page web
970	Revue des Nouvelles Technologies de l'Information	EGC	2007	Un algorithme multi-agent de classification pour la construction d'ontologies dynamiques	La construction d'ontologies à partir de textes reste une tâche coûteuse en temps qui justifie l'émergence de l'Ontology Learning. Notre système, Dynamo, s'inscrit dans cette mouvance, en apportant une approche originale basée sur une architecture multi-agent adaptative. En particulier, l'article présente le coeur de notre approche, un algorithme distribué de classification hiérarchique qui s'applique sur les résultats d'un analyseur syntaxique. Cet algorithme est évalué et comparé à un algorithme centralisé plus conventionnel. Forts de ces résultats, nous discutons ses limites et dressons en perspective les aménagements à effectuer pour aller vers une solution complète de construction d'ontologies.	Kévin Ottens, Nathalie Aussenac-Gilles	http://editions-rnti.fr/render_pdf.php?p1&p=1001452	http://editions-rnti.fr/render_pdf.php?p=1001452	construction dontologier partir texte rester tâcher coûteux temps justifier lémergence lOntology Learning système Dynamo sinscrit dan mouvance apporter approcher original baser architecturer multiagent adaptative En larticle présenter coeur approcher algorithme distribuer classification hiérarchique sapplique résultat dun analyseur syntaxique algorithme évaluer comparer algorithme centraliser plaire conventionnel Forts résultat discuter limite dresson perspectif aménagement effectuer aller ver solution complet construction dontologier
971	Revue des Nouvelles Technologies de l'Information	EGC	2007	Un cadre théorique pour la gestion de grandes bases de motifs	Les algorithmes de fouille de données sont maintenant capables de traiter de grands volumes de données mais les utilisateurs sont souvent submergés par la quantité de motifs générés. En outre, dans certains cas, que ce soit pour des raisons de confidentialité ou de coûts, les utilisateurs peuvent ne pas avoir accès directement aux données et ne disposer que des motifs. Les utilisateurs n'ont plus alors la possibilité d'approfondir à partir des données initiales le processus de fouille de façon à extraire des motifs plus spécifiques. Pour remédier à cette situation, une solution consiste à gérer les motifs. Ainsi, dans cet article, nous présentons un cadre théorique permettant à un utilisateur de manipuler, en post-traitement, une collection de motifs préalablement extraite. Nous proposons de représenter la collection sous la forme d'un graphe qu'un utilisateur pourra ensuite exploiter à l'aide d'opérateurs algébriques pour y retrouver des motifs ou en chercher de nouveaux.	François Jacquenet, Baptiste Jeudy, Christine Largeron	http://editions-rnti.fr/render_pdf.php?p1&p=1001385	http://editions-rnti.fr/render_pdf.php?p=1001385	algorithme fouiller donnée maintenir capabler traiter grand volume donnée utilisateur submerger quantité motif générer En outrer dan cas raison confidentialité coût utilisateur pouvoir accès donnée disposer motif utilisateur nont plaire possibilité dapprofondir partir donnée initial processus fouiller extraire motif plaire spécifique Pour remédier situation solution consister gérer motif dan article présenter cadrer théorique permettre utilisateur manipuler posttraitement collection motif préalablement extraire proposer représenter collection sou former dun graph quun utilisateur pouvoir ensuite exploiter laid dopérateur algébrique yu retrouver motif chercher
972	Revue des Nouvelles Technologies de l'Information	EGC	2007	Un outil pour la visualisation de relations entre gènes	La reconstruction de réseaux de gènes est un des défis majeurs de la post-génomique. A partir de données d'expression issues de puces à ADN, différentes techniques existent pour inférer des réseaux de gènes. Nous proposons dans ce papier une approche pour la visualisation de réseaux d'interactions entre gènes à partir de données d'expression. L'originalité de notre approche est de superposer des règles avec des sémantiques différentes au sein d'un même support visuel et de ne générer que les règles qui impliquent des gènes dits centraux. Ceux-ci sont spécifiés en amont par les experts et permettent de limiter la génération des règles aux seuls gènes qui intéressent les spécialistes. Une implémentation a été réalisée dans le logiciel libre MeV de l'institut TIGR.	Jean-Marc Petit, Marie Agier	http://editions-rnti.fr/render_pdf.php?p1&p=1001342	http://editions-rnti.fr/render_pdf.php?p=1001342	reconstruction réseau gène défi majeur postgénomique partir donnée dexpression issu puce ADN technique exister inférer réseau gène proposer dan papier approcher visualisation réseau dinteraction entrer gène partir donnée dexpression Loriginalité approcher superposer règle sémantique dun support visuel générer règle impliquer gène central Ceuxci spécifier amont expert permettre limiter génération règle gène intéresser spécialiste implémentation réaliser dan logiciel libre mev linstitut tigr
973	Revue des Nouvelles Technologies de l'Information	EGC	2007	Un segmenteur de texte en phrases guidé par l'utilisateur	Ce programme effectue une segmentation en phrases d'un texte. Contrairement aux procédures classiques, nous n'utilisons pas d'annotations préliminaires et tirons parti d'un apprentissage guidé par l'utilisateur.	Thomas Heitz	http://editions-rnti.fr/render_pdf.php?p1&p=1001334	http://editions-rnti.fr/render_pdf.php?p=1001334	programmer effectuer segmentation phrase dun texte contrairement procédure classique nutiliser dannotation préliminaire tiron partir dun apprentissage guider lutilisateur
974	Revue des Nouvelles Technologies de l'Information	EGC	2007	Une approche de classification non supervisée basée sur la détection de singularités et la corrélation de séries temporelles pour la recherche d'états : application à un bioprocédé fed-batch	Nous proposons dans cet article une méthode de clustering qui combine l'analyse dynamique et l'analyse statistique pour caractériser des états. Il s'agit d'une méthode de fouille de données qui travaille sur des ensembles de séries temporelles pour détecter des états; ces états représentent les informations les plus significatives du système. L'objectif de cette méthode non supervisée est d'extraire de la connaissance à partir de l'analyse des séries temporelles multiples. Elle s'appuie sur la détection de singularités dans les séries temporelles et sur l'analyse des corrélations des séries entre les intervalles définis par ces singularités. Pour l'application présentée, les séries temporelles sont des signaux biochimiques mesurés durant un bioprocédé. Cette approche est donc utilisée pour confirmer et enrichir la connaissance des experts du domaine des bioprocédés sans utiliser la connaissance a priori de ces experts. Elle est appliquée à la recherche d'états physiologiques dans un bioprocédé de type fed-batch.	Sébastien Régis	http://editions-rnti.fr/render_pdf.php?p1&p=1001453	http://editions-rnti.fr/render_pdf.php?p=1001453	proposer dan article méthode clustering combiner lanalyse dynamique lanalyse statistique caractériser sagit dune méthode fouiller donnée travailler ensemble série temporel détecter représenter information plaire significatif système Lobjectif méthode superviser dextrair connaissance partir lanalyse série temporel sappuie détection singularité dan série temporel lanalyse corrélation série entrer intervalle définir singularité Pour lapplication présenter série temporel signal biochimique mesurer durer bioprocédé approcher utiliser confirmer enrichir connaissance expert domaine bioprocédé utiliser connaissance priori expert appliquer rechercher détat physiologique dan bioprocédé typer fedbatch
975	Revue des Nouvelles Technologies de l'Information	EGC	2007	Une approche non paramétrique Bayésienne pour l'estimation de densité conditionnelle sur les rangs	Nous nous intéressons à l'estimation de la distribution des rangs d'une variable cible numérique conditionnellement à un ensemble de prédicteurs numériques. Pour cela, nous proposons une nouvelle approche non paramétrique Bayesienne pour effectuer une partition rectangulaire optimale de chaque couple (cible, prédicteur) uniquement à partir des rangs des individus. Nous montrons ensuite comment les effectifs de ces grilles nous permettent de construire un estimateur univarié de la densité conditionnelle sur les rangs et un estimateur multivarié utilisant l'hypothèse Bayesienne naïve. Ces estimateurs sont comparés aux meilleures méthodes évaluées lors d'un récent Challenge sur l'estimation d'une densité prédictive. Si l'estimateur Bayésien naïf utilisant l'ensemble des prédicteurs se révèle peu performant, l'estimateur univarié et l'estimateur combinant deux prédicteurs donne de très bons résultats malgré leur simplicité.	Marc Boullé, Carine Hue	http://editions-rnti.fr/render_pdf.php?p1&p=1001317	http://editions-rnti.fr/render_pdf.php?p=1001317	intéresser lestimation distribution rang dune variable cibl numérique conditionnellement ensemble prédicteur numérique Pour celer proposer approcher paramétrique Bayesienne effectuer partition rectangulaire optimal coupler cibl prédicteur uniquement partir rang individu montrer ensuite effectif grille permettre construire estimateur univarié densité conditionnel rang estimateur multivarié utiliser lhypothèse Bayesienne naïf estimateur comparer meilleur méthode évalué dun récent Challenge lestimation dune densité prédictif Si lestimateur Bayésien naïf utiliser lensembl prédicteur révéler performer lestimateur univarié lestimateur combiner prédicteur donner résultat simplicité
976	Revue des Nouvelles Technologies de l'Information	EGC	2007	Une approche sociotechnique pour le Knowledge Management (KM)	Cet article présente un cadre sociotechnique pour le KM. Cette vision sociotechnique du KM permet : (1) d'écarter le KM d'un souci commercial ; (2) faire le clivage des différentes technologies du KM ; et (3) de s'interroger sur les paradigmes associés aux composants social et technique du KM. C'est précisément ce dernier point que cet article développe afin d'identifier les mécanismes génériques du KM. Plus précisément, l'aspect social est décrit à travers l'approche organisationnelle du KM, l'approche managériale du KM, et l'approche biologique du KM, alors que l'aspect technique est décrit à travers l'approche ingénierie des connaissances et compétences du KM. Ces approches nous conduisent aussi à donner un tableau comparatif entre ces visions organisationnelles, managériales et biologiques du KM.	Leoncio Jiménez	http://editions-rnti.fr/render_pdf.php?p1&p=1001436	http://editions-rnti.fr/render_pdf.php?p=1001436	article présenter cadrer sociotechnique KM vision sociotechnique km permettre   1 décarter KM dun souci commercial   2 faire clivage technologie km   3 sinterroger paradigme associé composant social technique KM cest précisément poindre article développer didentifier mécanisme générique KM plaire précisément laspect social décrire travers lapproch organisationnel km lapproch managérial km lapproche biologique KM laspect technique décrire travers lapproch ingénierie connaissance compétence KM approche conduire donner tableau comparatif entrer vision organisationnel managériale biologique KM
977	Revue des Nouvelles Technologies de l'Information	EGC	2007	Une étude des algorithmes de construction d'architecture des réseaux de neurones multicouches	Le problème de choix d'architecture d'un réseau de neurones multicouches reste toujours très difficile à résoudre dans un processus de fouille de données. Ce papier recense quelques algorithmes de recherche d'architectures d'un réseau de neurones pour les tâches de classification. Il présente également une analyse théorique et expérimentale de ces algorithmes. Ce travail confirme les difficultés de choix des paramètres d'apprentissage (modèle, nombre de couches, nombre de neurones par couches, taux d'apprentissage, algorithme d'apprentissage,...) communs à tout processus de construction de réseaux de neurones et les difficultés de choix de paramètres propres à certains algorithmes.	Norbert Tsopzé, Engelbert Mephu Nguifo, Gilbert Tindo	http://editions-rnti.fr/render_pdf.php?p1&p=1001288	http://editions-rnti.fr/render_pdf.php?p=1001288	problème choix darchitectur dun réseau neuron multicouche rester difficile résoudre dan processus fouiller donnée papier recenser algorithme rechercher darchitectur dun réseau neurone tâche classification présenter également analyser théorique expérimental algorithme travail confirmer difficulté choix paramètre dapprentissage modeler nombre couche nombre neurone couche taux dapprentissage algorithm dapprentissage commun processus construction réseau neurone difficulté choix paramètre propre algorithm
978	Revue des Nouvelles Technologies de l'Information	EGC	2007	Une extension de XQuery pour la recherche textuelle d'information dans des documents XML	Nous présentons dans cet article une extension de XQuery que nous avons développée pour interroger le contenu et la structure de documents XML. Cette extension consiste à intégrer dans XQuery le langage NEXI, un sous-ensemble de XPath, défini dans le cadre de l'initiative INEX. Notre proposition est double : (i) équiper NEXI d'une sémantique floue, (ii) intégrer NEXI dans XQuery au moyen d'une métafonction appelée nexi, ayant une requête NEXI comme paramètre, et d'une extension de la clause for de l'opérateur FLWOR de XQuery. De plus, nous décrivons le prototype paramétrable que nous avons développé au dessus de deux moteurs XQuery classiques : Galax et Saxon.	Jacques Le Maitre, Nicolas Faessel	http://editions-rnti.fr/render_pdf.php?p1&p=1001401	http://editions-rnti.fr/render_pdf.php?p=1001401	présenter dan article extension xquery développer interroger contenir structurer document xml extension consister intégrer dan xquery langage nexi sousensemble XPath définir dan cadrer linitiativ INEX proposition double   ie équiper nexi dune sémantique flou ii intégrer nexi dan xquery moyen dune métafonction appeler nexi requête nexi paramétrer dune extension clause for lopérateur FLWOR xquery De plaire décrire prototype paramétrable développer moteur xquery classique   Galax Saxon
979	Revue des Nouvelles Technologies de l'Information	EGC	2007	Une méthode d'interprétation de scores	Cet article présente une méthode permettant d'interpréter la sortie d'un modèle de classification ou de régression. L'interprétation se base sur l'importance de la variable et l'importance de la valeur de la variable. Cette approche permet d'interpréter la sortie du modèle pour chaque instance.	Raphaël Feraud, Vincent Lemaire	http://editions-rnti.fr/render_pdf.php?p1&p=1001348	http://editions-rnti.fr/render_pdf.php?p=1001348	article présenter méthode permettre dinterpréter sortir dun modeler classification régression linterprétation baser limportance variable limportance variable approcher permettre dinterpréter sortir modeler instance
980	Revue des Nouvelles Technologies de l'Information	EGC	2007	Une méthode optimale d'évaluation bivariée pour la classification supervisée	En préparation des données pour la classification supervisée, les méthodes filtres usuellement utilisées pour la sélection de variables sont efficaces en temps de calcul. Néanmoins, leur nature univariée ne permet pas de détecter les redondances ou les interactions constructives entre variables. Cet article présente une nouvelle méthode permettant d'évaluer l'importance prédictive jointe d'une paire de variables de façon automatique, rapide et fiable. Elle est basée sur un partitionnement de chaque variable exogène, en intervalles dans le cas numérique et groupes de valeurs dans le cas catégoriel. La grille de données exogène résultante permet alors d'évaluer la corrélation entre la paire de variables exogènes et la variable endogène. Le meilleur partitionnement bivarié est recherché au moyen d'une approche Bayésienne de la sélection de modèle. Les expérimentations démontrent les apports de la méthode, notamment une amélioration significative des performances en classification.	Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001420	http://editions-rnti.fr/render_pdf.php?p=1001420	En préparation donnée classification superviser méthode filtr usuellement utiliser sélection variable efficacer temps calcul nature univarier permettre détecter redondance interaction constructif entrer variable article présenter méthode permettre dévaluer limportance prédictif joint dune pair variable automatique rapide fiable baser partitionnement variable exogène intervalle dan cas numérique groupe dan cas catégoriel griller donnée exogène résultant permettre dévaluer corrélation entrer pair variable exogène variable endogène meilleur partitionnement bivarié rechercher moyen dune approcher Bayésienne sélection modeler expérimentation démontrer apport méthode amélioration significatif performance classification
981	Revue des Nouvelles Technologies de l'Information	EGC	2007	Une nouvelle approche de la programmation DC et DCA pour la classification floue	Dans cet article, nous nous intéressons à Fuzzy C-Means (FCM), une technique très connue pour la classification floue. Nous proposons un algorithme efficace basé sur la programmation DC (Difference of Convexe functions) et DCA (DC Algorithm) pour résoudre ce problème. Les expériences numériques comparatives avec l'algorithme standard FCM sur les données réelles montrent la robustesse, la performance de cet nouvel algorithme DCA et sa supériorité par rapport à FCM.	Le Thi Hoai An, Le Hoai Minh, Pham Dinh Tao	http://editions-rnti.fr/render_pdf.php?p1&p=1001459	http://editions-rnti.fr/render_pdf.php?p=1001459	Dans article intéresser Fuzzy CMeans FCM technique connaître classification flou proposer algorithme efficace baser programmation DC difference of Convexe function DCA DC algorithm résoudre problème expérience numérique comparatif lalgorithme standard FCM donnée réel montrer robustesse performance nouvel algorithme DCA supériorité rapport FCM
982	Revue des Nouvelles Technologies de l'Information	EGC	2007	Une nouvelle méthode d'alignement et de visualisation d'ontologies OWL-Lite	Dans ce papier, une nouvelle plate-forme d'alignement et de visualisation des ontologies, appelée POVA (Prototype OWL-Lite Visual Alignment), est décrite. Le module d'alignement implémente une nouvelle approche d'alignement d'ontologies remédiant au problème de la circularité et de l'intervention de l'utilisateur.	Sami Zghal, Karim Kamoun, Sadok Ben Yahia, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1001355	http://editions-rnti.fr/render_pdf.php?p=1001355	Dans papier plateforme dalignement visualisation ontologie appeler pover Prototype owllite Visual Alignment décrire moduler dalignemer implément approcher dalignement dontologie remédier problème circularité lintervention lutilisateur
983	Revue des Nouvelles Technologies de l'Information	EGC	2007	Une règle d'exception en Analyse Statistique Implicative	En fouille de règles, certaines situations exceptionnelles défient le bon sens. C'est le cas de la règle R : a --> c et b --> c et (a et b) --> non c. Une telle règle, que nous étudions dans l'article, est appelée règle d'exception. A la suite des travaux précurseurs de E. Suzuki et Y. Kodratoff (1999), qui ont étudié un autre type de règle d'exception, nous cherchons ici à caractériser les conditions d'apparition de la règle R dans le cadre de l'Analyse Statistique Implicative.	Régis Gras, Pascale Kuntz, Einoshin Suzuki	http://editions-rnti.fr/render_pdf.php?p1&p=1001312	http://editions-rnti.fr/render_pdf.php?p=1001312	En fouiller règle situation exceptionnel défier sens cest cas régler         régler étudier dan larticle appeler régler dexception A suite travail précurseur E Suzuki Y Kodratoff 1999 étudier typer régler dexception chercher caractériser condition dapparition régler dan cadrer lanalyse Statistique Implicative
984	Revue des Nouvelles Technologies de l'Information	EGC	2007	Utilisation de WordNet dans la catégorisation de textes multilingues	Cet article est consacré au problème de la catégorisation multilingue qui consiste à catégoriser des documents de différentes langues en utilisant le même classifieur. L'approche que nous proposons est basée sur l'idée d'étendre l'utilisation de WordNet dans la catégorisation monolingue vers la catégorisation multilingue.	Mohamed Amine Bentaallah, Mimoun Malki	http://editions-rnti.fr/render_pdf.php?p1&p=1001353	http://editions-rnti.fr/render_pdf.php?p=1001353	article consacrer problème catégorisation multilingue consister catégoriser document langu utiliser classifieur Lapproche proposer baser lidée détendre lutilisation WordNet dan catégorisation monolingue ver catégorisation multilingue
985	Revue des Nouvelles Technologies de l'Information	EGC	2007	Validation des visualisations par axes principaux de données numériques et textuelles	Parmi les outils de visualisation de données multidimensionnelles figurent d'une part les méthodes fondées sur la décomposition aux valeurs singulières, et d'autre part les méthodes de classification, incluant les cartes auto-organisées de Kohonen. Comment valider ces visualisations ? On présente sept procédures de validation par bootstrap qui dépendent des données, des hypothèses, des outils : a) le bootstrap partiel, qui considère les réplications comme des variables supplémentaires; b) le bootstrap total de type 1, qui réanalyse les réplications avec changements éventuels de signes des axes; c) le bootstrap total de type 2 qui corrige aussi les interversions d'axes; d) le bootstrap total de type 3, sur lequel on insistera, qui corrige les réplications par rotations procrustéenne; e) le bootstrap spécifique (cas des hiérarchies d'individus statistiques et des données textuelles). f) le bootstrap sur variables. g) les extensions des procédures précédentes à certaines cartes auto-organisées.	Ludovic Lebart	http://editions-rnti.fr/render_pdf.php?p1&p=1001332	http://editions-rnti.fr/render_pdf.php?p=1001332	Parmi outil visualisation donnée multidimensionnel figurer dune partir méthode fonder décomposition singulier dautre partir méthode classification inclure carte autoorganiser Kohonen valider visualisation   présenter procédure validation bootstrap dépendre donnée hypothèse outil   bootstrap partiel considérer réplication variable supplémentaire bootstrap total typer 1 réanalyse réplication changement éventuel signe axe bootstrap total typer 2 corriger interversion dax bootstrap total typer 3 insister corriger réplication rotation procrustéenne 7e bootstrap spécifique cas hiérarchie dindividus statistique donnée textuel bootstrap variable extension procédure précédent carte autoorganiser
986	Revue des Nouvelles Technologies de l'Information	EGC	2007	Vers un algorithme multi-agents de clustering dynamique	Dans cet article, nous présentons un algorithme multi-agents de clustering dynamique. Ce type de clustering doit permettre de gérer des données évolutives et donc être capable d'adapter en permanence les clusters construits.	Bruno Mermet, Gaële Simon, Dominique Fournier	http://editions-rnti.fr/render_pdf.php?p1&p=1001357	http://editions-rnti.fr/render_pdf.php?p=1001357	Dans article présenter algorithme multiagent clustering dynamique typer clustering devoir permettre gérer donnée évolutif capable dadapter permanence cluster construit
987	Revue des Nouvelles Technologies de l'Information	EGC	2007	Vers un système hybride pour l'annotation sémantique d'images IRM du cerveau	Cet article montre l'intérêt de combiner des méthodes numériques et symboliques pour obtenir une annotation sémantique des images IRM du cerveau humain. Il s'agit d'identifier des structures anatomiques du cortex cérébral humain, en utilisant conjointement des connaissances a priori de nature numérique et une ontologie des structures corticales du cerveau représentée en OWL DL, étendue par des règles SWRL. Ces connaissances symboliques a priori représentées dans des langages standards du Web deviennent non seulement partageables mais permettent aussi un raisonnement automatique qui aide l'utilisateur à la labellisation des structures anatomiques mises en évidence dans des images IRM du cerveau d'un individu donné.	Ammar Mechouche, Christine Golbreich, Bernard Gibaud	http://editions-rnti.fr/render_pdf.php?p1&p=1001417	http://editions-rnti.fr/render_pdf.php?p=1001417	article montr lintérêt combiner méthode numérique symbolique obtenir annotation sémantique image irm cerveau humain sagit didentifier structure anatomique cortex cérébral humain utiliser conjointemer connaissance priori nature numérique ontologie structure cortical cerveau représenter OWL DL étendu règle swrl connaissance symbolique priori représenter dan langage standard web devenir partageable permettre raisonnement automatique aider lutilisateur labellisation structure anatomique mise évidence dan image irm cerveau dun individu donner
988	Revue des Nouvelles Technologies de l'Information	EGC	2007	Vers une base de connaissances biographique : extraction d'information et ontologie	Le projet B-Ontology a pour but l'extraction, l'organisation et l'exploitation de connaissances biographiques à partir de dépêches de presse. Sa réalisation requiert l'intégration de diverses technologies, principalement l'extraction d'information, les ontologies et bases de connaissances, les techniques de data mining. Cet article propose un aperçu des choix réalisés dans le cadre du projet. Cette démarche permet également de définir un environnement d'outils utiles pour les applications d'extraction et de gestion de connaissances.	Laurent Kevers, Cédrick Fairon	http://editions-rnti.fr/render_pdf.php?p1&p=1001400	http://editions-rnti.fr/render_pdf.php?p=1001400	projet bontology boire lextraction lorganisation lexploitation connaissance biographique partir dépêche presser réalisation requérir lintégration technologi principalement lextraction dinformation ontologie base connaissance technique dater mining article proposer apercevoir choix réaliser dan cadrer projet démarcher permettre également définir environnement doutil utile application dextraction gestion connaissance
989	Revue des Nouvelles Technologies de l'Information	EGC	2007	Vers une nouvelle approche d'extraction des motifs séquentiels non-dérivables	L'extraction de motifs séquentiels est un défi important pour la communauté fouille de données. Même si les représentations condensées ont montré leur intérêt dans le domaine des itemsets, à l'heure actuelle peu de travaux considèrent ce type de représentation pour extraire des motifs. Cet article propose d'établir les premières bases formelles pour obtenir les bornes inférieures et supérieures du support d'une séquence S. Nous démontrons que ces bornes peuvent être dérivées à partir des sous-séquences de S et prouvons que ces règles de dérivation permettent la construction d'une nouvelle représentation condensée de l'ensemble des motifs fréquents. Les différentes expérimentations menées montrent que notre approche offre une meilleure représentation condensée que celles des motifs clos et cela sans perte d'information.	Pascal Poncelet, Chedy Raïssi	http://editions-rnti.fr/render_pdf.php?p1&p=1001391	http://editions-rnti.fr/render_pdf.php?p=1001391	lextraction motif séquentiel défi importer communauté fouill donnée représentation condenser montrer intérêt dan domaine itemset lheure actuel travail considérer typer représentation extraire motif article proposer détablir base formel obtenir borne inférieur supérieur support dune séquence démontrer borne pouvoir dériver partir sousséquence prouvon règle dérivation permettre construction dune représentation condenser lensemble motif fréquent expérimentation mener montrer approcher offrir meilleur représentation condenser motif clore celer perte dinformation
990	Revue des Nouvelles Technologies de l'Information	EGC	2007	Vers une plate-forme interactive pour la visualisation de grands ensembles de règles d'association	"La recherche de règles d'association est une question centrale en Extraction de Connaissances dans les Données (ECD). Dans cet article, nous nous intéressons plus particulièrement à la restitution visuelle de règles pertinentes dans un corpus très important. Nous proposons ainsi un prototype basé sur une approche de type ""wrapper"" par intégration des phases d'extraction et de visualisation de l'ECD. Tout d'abord, le processus d'extraction génère une base générique de règles et dans un second temps, la tâche de visualisation s'appuie sur un processus de regroupement (""clustering"") permettant de grouper et de visualiser un sous-ensemble de règles d'association génériques. Le rendu visuel à l'écran exploite une représentation de type ""Fisheye view"" de manière à obtenir simultanément une représentation globale des différents groupes de règles et une vue détaillée du groupe sélectionné."	Olivier Couturier, Tarek Hamrouni, Sadok Ben Yahia, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1001381	http://editions-rnti.fr/render_pdf.php?p=1001381	rechercher règle dassociation question central extraction connaissance dan donnée ecd Dans article intéresser plaire restitution visuel règle pertinent dan corpus importer proposer prototype baser approcher typer wrapper intégration phase dextraction visualisation lecd dabord processus dextraction génèr baser générique règle dan second temps tâcher visualisation sappui processus regroupement clustering permettre grouper visualiser sousensemble règle dassociation générique visuel lécran exploiter représentation typer fisheye view manière obtenir simultanément représentation global groupe règle détailler grouper sélectionner
991	Revue des Nouvelles Technologies de l'Information	EGC	2007	Visualisation de graphes avec Tulip : exploration interactive de grandes masses de données en appui à la fouille de données et à l'extraction de connaissances	Cet article décrit une étude de cas exhibant les qualités de la plateforme de visualisation de graphes Tulip, démontrant l'apport de la visualisation à la fouille de données interactive et à l'extraction de connaissances. Le calcul d'un graphe à partir d'indices de similarité est un exemple typique où l'exploration visuelle et interactive de graphes vient en appui au travail de fouille de données. Nous penchons sur le cas où l'on souhaite étudier une collection de documents afin d'avoir une idée des thématiques abordées dans la collection.	David Auber, Yves Chiricota, Maylis Delest, Jean-Philippe Domenger, Patrick Mary, Guy Melançon	http://editions-rnti.fr/render_pdf.php?p1&p=1001330	http://editions-rnti.fr/render_pdf.php?p=1001330	article décrire étude cas exhiber qualité plateforme visualisation graphe Tulip démontrer lapport visualisation fouiller donnée interactif lextraction connaissance calcul dun graph partir dindice similarité exemple typique lexploration visuel interactif graphe venir appui travail fouiller donnée pencher cas lon souhaiter étudier collection document davoir idée thématique aborder dan collection
992	Revue des Nouvelles Technologies de l'Information	EGC	2007	Visualisation exploratoire des résultats d'algorithmes d'arbre de décision	Nous présentons une méthode d'exploration des résultats des algorithmes d'apprentissage par arbre de décision (comme C4.5). La méthode présentée utilise simultanément une visualisation radiale, focus+context, fisheye et hiérarchique pour la représentation et l'exploration des résultats des algorithmes d'arbre de décision. L'utilisateur peut ainsi extraire facilement des règles d'induction et élaguer l'arbre obtenu dans une phase de post-traitement. Cela lui permet d'avoir une meilleure compréhension des résultats obtenus. Les résultats des tests numériques avec des ensembles de données réelles montrent que la méthode proposée permet une bien meilleure compréhension des résultats des arbres de décision.	Thanh-Nghi Do, Nguyen-Khang Pham, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1001331	http://editions-rnti.fr/render_pdf.php?p=1001331	présenter méthode dexploration résultat algorithme dapprentissage arbre décision c45 méthode présenter utiliser simultanément visualisation radial focuscontext fishey hiérarchique représentation lexploration résultat algorithme darbre décision Lutilisateur pouvoir extraire facilement règle dinduction élaguer larbre obtenir dan phase posttraitement celer luire permettre davoir meilleur compréhension résultat obtenu résultat test numérique ensemble donnée réel montrer méthode proposer permettre meilleur compréhension résultat arbre décision
993	Revue des Nouvelles Technologies de l'Information	EGC	2007	WebDocEnrich : enrichissement sémantique flexible de documents semi-structurés	WebdocEnrich est une approche d'enrichissement sémantique automatique de documents HTML hétérogènes qui exploite une description du domaine pour enrichir le contenu des documents et les représenter en XML.	Mouhamadou Thiam, Nacéra Bennacer, Nathalie Pernelle	http://editions-rnti.fr/render_pdf.php?p1&p=1001365	http://editions-rnti.fr/render_pdf.php?p=1001365	webdocenrich approcher denrichissement sémantique automatique document HTML hétérogène exploiter description domaine enrichir contenir document représenter xml
994	Revue des Nouvelles Technologies de l'Information	EGC	2006	Accès aux connaissances orales par le résumé automatique	Le temps nécessaire pour écouter un flux audio est un facteur réduisant l'accès efficace àde grandes archives de parole. Une première approche, la structuration automatique des données,permet d'utiliser un moteur de recherche pour cibler plus rapidement l'information. Leslistes de résultats générées sont longues dans un souci d'exhaustivité. Alors que pour des documentstextuels, un coup d'oeil discrimine un résultat interessant d'un résultat non pertinant,il faut écouter l'audio dans son intégralité pour en capturer le contenu. Nous proposons doncd'utiliser le résumé automatique afin de structurer les résultats des recherches et d'en réduirela redondance.	Benoît Favre, Jean-François Bonastre, Patrice Bellot, François Capman	http://editions-rnti.fr/render_pdf.php?p1&p=1000358	http://editions-rnti.fr/render_pdf.php?p=1000358	temps nécessaire écouter flux audio facteur réduire laccè efficace àd grand archive approcher structuration automatique donnéespermet dutiliser moteur rechercher cibler plaire rapidement linformation leslist résultat généré longuer dan souci dexhaustivité documentstextuel coup doeil discrimin résultat interessant dun résultat pertinantil falloir écouter laudio dan intégralité capturer contenir proposer doncdutiliser résumer automatique structurer résultat recherche den réduirela redondance
995	Revue des Nouvelles Technologies de l'Information	EGC	2006	Affectation pondérée sur des données de type intervalle	On s'intéresse à la construction d'arbres de décision sur des données symboliques de type intervalle en utilisant le critère de découpage binaire de Kolmogorov-Smirnov. Nous proposons une approche permettant d'affecter un individu à la fois aux deux noeuds fils générés par le partitionnement d'un noeud non terminal. Le but de cette méthode est de prendre en compte le positionnement de la donnée à classer par rapport à la donnée seuil de coupure.	Edwin Diday, Chérif Mballo	http://editions-rnti.fr/render_pdf.php?p1&p=1000373	http://editions-rnti.fr/render_pdf.php?p=1000373	sintéresse construction darbr décision donnée symbolique typer intervall utiliser critère découpage binaire KolmogorovSmirnov proposer approcher permettre daffecter individu noeud fils générer partitionnement dun noeud terminal boire méthode prendre compter positionnement donner classer rapport donner seuil coupure
996	Revue des Nouvelles Technologies de l'Information	EGC	2006	Aide en gestion hospitalière par visualisation des composantes de non-pertinence		Bernard Huet	http://editions-rnti.fr/render_pdf.php?p1&p=1000431	http://editions-rnti.fr/render_pdf.php?p=1000431	
997	Revue des Nouvelles Technologies de l'Information	EGC	2006	Algorithme semi-interactif pour la sélection de dimensions	"Nous présentons un algorithme génétique semi-interactif de sélectionde dimensions dans les grands ensembles de données pour la détectiond'individus atypiques (outliers). Les ensembles de données possédant unnombre élevé de dimensions posent de nombreux problèmes aux algorithmesde fouille de données, une solution est d'effectuer un pré-traitement afin de neretenir que les dimensions ""intéressantes"". Nous utilisons un algorithmegénétique pour le choix du sous-ensemble de dimensions à retenir. Par ailleursnous souhaitons donner un rôle plus important à l'utilisateur dans le processusde fouille, nous avons donc développé un algorithme génétique semi-interactifoù l'évaluation des solutions n'élimine pas complètement la fonctiond'évaluation mais la couple avec une évaluation de l'utilisateur. Enfin,l'importante réduction du nombre de dimensions nous permet de visualiser lesrésultats de l'algorithme de détection d'outlier. Cette visualisation permet àl'expert des données d'étiqueter les éléments atypiques (erreurs ou simplementdes individus différents de la masse)."	Lydia Boudjeloud, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1000366	http://editions-rnti.fr/render_pdf.php?p=1000366	présenter algorithme génétique semiinteractif sélectionde dimension dan grand ensemble donnée détectiondindividu atypique outlier ensemble donnée posséder unnombre élever dimension poser problème algorithmesde fouiller donnée solution deffectuer prétraitement neretenir dimension intéressant utiliser algorithmegénétiqu choix sousensemble dimension retenir Par ailleursnou souhaiter donner rôle plaire importer lutilisateur dan processusde fouiller développer algorithme génétique semiinteractifoù lévaluation solution nélimine complètement fonctiondévaluation coupler évaluation lutilisateur enfinlimportant réduction nombre dimension permettre visualiser lesrésultat lalgorithme détection doutlier visualisation permettre àlexpert donnée détiqueter élément atypique erreur simplementde individu masser
998	Revue des Nouvelles Technologies de l'Information	EGC	2006	Alignement extensionnel et asymétrique de hiérarchies conceptuelles par découverte d'implications entre concepts	Dans la littérature, de nombreux travaux traitent de méthodes d'alignementd'ontologies. Ils utilisent, pour la plupart, des relations basées sur desmesures de similarité qui ont la particularité d'être symétriques. Cependant, peude travaux évaluent l'intérêt d'utiliser des mesures d'appariement asymétriquesdans le but d'enrichir l'alignement produit. Ainsi, nous proposons dans ce papierune méthode d'alignement extensionnelle et asymétrique basée sur la découvertedes implications significatives entre deux ontologies. Notre approche,basée sur le modèle probabiliste d'écart à l'indépendance appelé intensité d'implication,est divisée en deux parties consécutives : (1) l'extraction, à partir ducorpus textuel associé à l'ontologie, et l'association des termes aux concepts;(2) la découverte et sélection des implications génératrices les plus significativesentre les concepts. La méthode proposée est évaluée sur deux jeux de donnéesréels portant respectivement sur des profils d'entreprises et sur des cataloguesde cours d'universités. Les résultats obtenus montrent que l'on peut trouver desrelations pertinentes qui sont ignorées par un alignement basé seulement sur desmesures de similarité.	Jérôme David, Fabrice Guillet, Régis Gras, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1000337	http://editions-rnti.fr/render_pdf.php?p=1000337	Dans littérature travail traiter méthode dalignementdontologier utiliser relation baser desmesure similarité particularité dêtre symétrique peude travail évaluer lintérêt dutiliser mesure dappariemer asymétriquesdans boire denrichir lalignement produire proposer dan papierune méthod dalignement extensionnel asymétrique baser découvertede implication significatif entrer ontologie approchebasée modeler probabiliste décart lindépendance appeler intensité dimplicationest diviser party consécutif   1 lextraction partir ducorpu textuel associer lontologie lassociation terme concepts2 découvrir sélection implication générateur plaire significativesentre concept méthode proposer évaluer jeu donnéesréel porter respectivement profil dentrepris cataloguesde courir duniversité résultat obtenir montrer lon pouvoir trouver desrelation pertinent ignorer alignement baser desmesure similarité
999	Revue des Nouvelles Technologies de l'Information	EGC	2006	Amélioration des indicateurs techniques pour l'analyse du marché financier	La technique des motifs fréquents a été utilisée pour améliorer lepouvoir prédictif des stratégies quantitatives. Innovant dans le contexte desmarchés financiers, notre méthode associe une signature aux configurations demarché fréquentes. Un système de « trading » automatique sélectionne lesmeilleures signatures par une procédure de « back testing » itérative et les utiliseen combinaison avec l'indicateur technique pour améliorer sa performance.L'application des motifs fréquents à cette problématique des indicateurstechniques est une contribution originale. Au sens du test t de Student,notre méthode améliore nettement les approches sans signatures. La techniquea été testé sur des données journalières type taux d'intérêt et actions. Notreanalyse des indicateurs (Williams%R, BN et croisement des moments) a montréque qu'une approche par signatures est particulièrement bien adaptée auxstratégies à mémoire courte.	Hunor Albert-Lorincz, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1000428	http://editions-rnti.fr/render_pdf.php?p=1000428	technique motif fréquent utiliser améliorer lepouvoir prédictif stratégie quantitatif innover dan contexte desmarché financier méthode associer signature configuration demarché fréquent système « trading » automatique sélectionner lesmeilleur signatur procédure « back testing » itératif utiliseen combinaison lindicateur technique améliorer performancelapplication motif fréquent problématique indicateurstechnique contribution original sens test studentnotre méthod améliorer nettement approche signatur techniquea tester donnée journalier typer taux dintérêt action Notreanalyse indicateur WilliamsR BN croisemer moment montréqu quune approcher signature adapter auxstratégier mémoire court
1000	Revue des Nouvelles Technologies de l'Information	EGC	2006	Analyse du Comportement des utilisateurs exploitant une base de données vidéo	Dans cet article, nous présentons un modèle de fouille des usages dela vidéo pour améliorer la qualité de l'indexation. Nous proposons une approchebasée sur un modèle à deux niveaux représentant le comportement des utilisateursexploitant un moteur de recherche vidéo. Le premier niveau consiste àmodéliser le comportement lors de la lecture d'une vidéo unique (comportementintra vidéo), le second à modéliser le comportement sur l'ensemble d'une session(comportement inter video). A partir de cette représentation, nous avonsdéveloppé un algorithme de regroupement, adapté à la nature particulière de cesdonnées. L'analyse des usages de la vidéo nous permet d'affiner l'indexationvidéo sur la base de l'intérêt des utilisateurs.	Sylvain Mongy	http://editions-rnti.fr/render_pdf.php?p1&p=1000376	http://editions-rnti.fr/render_pdf.php?p=1000376	Dans article présenter modeler fouiller usage dela vidéo améliorer qualité lindexation proposer approchebasée modeler niveau représenter comportement utilisateursexploitant moteur rechercher vidéo niveau consister àmodéliser comportement lecture dune vidéo comportementintra vidéo second modéliser comportement lensembl dune sessioncomportement inter video A partir représentation avonsdévelopper algorithme regroupement adapter nature cesdonnée Lanalyse usage vidéo permettre daffiner lindexationvidéo baser lintérêt utilisateur
1001	Revue des Nouvelles Technologies de l'Information	EGC	2006	Annotation sémantique de pages web	Cet article présente un système automatique d'annotation sémantiquede pages web. Les systèmes d'annotation automatique existants sont essentiellementsyntaxiques, même lorsque les travaux visent à produire une annotationsémantique. La prise en compte d'informations sémantiques sur le domaine pourl'annotation d'un élément dans une page web à partir d'une ontologie supposed'aborder conjointement deux problèmes : (1) l'identification de la structuresyntaxique caractérisant cet élément dans la page web et (2) l'identification duconcept le plus spécifique (en termes de subsumption) dans l'ontologie dontl'instance sera utilisée pour annoter cet élément. Notre démarche repose sur lamise en oeuvre d'une technique d'apprentissage issue initialement des wrappersque nous avons articulée avec des raisonnements exploitant la structure formellede l'ontologie.	Sylvain Tenier, Amedeo Napoli, Xavier Polanco, Yannick Toussaint	http://editions-rnti.fr/render_pdf.php?p1&p=1001499	http://editions-rnti.fr/render_pdf.php?p=1001499	article présenter système automatique dannotation sémantiqued pag web système dannotation automatique existant essentiellementsyntaxiqu travail viser produire annotationsémantiqu priser compter dinformation sémantique domaine pourlannotation dun élément dan page web partir dune ontologie supposedaborder conjointement problème   1 lidentification structuresyntaxique caractériser élément dan page web 2 lidentification duconcept plaire spécifique terme subsumption dan lontologie dontlinstance utiliser annoter élément démarcher reposer lamise oeuvrer dune technique dapprentissage issu initialement wrappersque articuler raisonnement exploiter structurer formellede lontologie
1002	Revue des Nouvelles Technologies de l'Information	EGC	2006	Apprentissage de la structure des réseaux bayésiens à partir des motifs fréquents corrélés : application à l'identification des facteurs environnementaux du cancer du Nasopharynx	L'apprentissage de structure des réseaux bayésien à partir de donnéesest un problème NP-difficile pour lequel de nombreuses heuristiques ont été proposées.Dans cet article, nous proposons une nouvelle méthode inspirée des travauxsur la recherche de motifs fréquents corrélés pour identifier les causalitésentre les variables. L'algorithme opère en quatre temps : (1) la découvertepar niveau des motifs fréquents corrélés minimaux ; (2) la construction d'ungraphe non orienté à partir de ces motifs ; (3) la détection des V_structures etl'orientation partielle du graphe ; (4) l'élimination des arêtes superflues par destests d'indépendance conditionnelle. La méthode, appliquée au réseau Asia, permetde retrouver la structure du graphe initial. Nous l'appliquons ensuite auxdonnées d'une étude épidémiologique cas-témoins du cancer du nasopharynx(NPC). L'objectif est de dresser un profil statistique type de la population étudiéeet d'apporter un éclairage utile sur les différents facteurs impliqués dans leNPC.	Alexandre Aussem, Zahra Kebaili, Marilys Corbex, Fabien De Marchi	http://editions-rnti.fr/render_pdf.php?p1&p=1000420	http://editions-rnti.fr/render_pdf.php?p=1000420	lapprentissage structurer réseau bayésien partir donnéesest problème npdifficil heuristique proposéesDans article proposer méthode inspirer travauxsur rechercher motif fréquent corrélé identifier causalitésentre variable Lalgorithme opèr temps   1 découvertepar niveau motif fréquent corrélé minimal   2 construction dungraph orienter partir motif   3 détection vstructure etlorientation partiel graphe   4 lélimination arête superflu destest dindépendance conditionnel méthode appliquer réseau Asia permetde retrouver structurer graphe initial lappliquer ensuite auxdonner dune étude épidémiologique castémoins cancer nasopharynxnpc Lobjectif dresser profil statistique typer population étudiéeet dapporter éclairage utile facteur impliquer dan lenpc
1003	Revue des Nouvelles Technologies de l'Information	EGC	2006	Approche entropique pour l'analyse de modèle de chroniques	Cet article propose d'utiliser l'entropie informationnelle pouranalyser des modèles de chroniques découverts selon une approchestochastique (Bouché et Le Goc, 2005). Il décrit une adaptation de l'algorithmeTemporalID3 (Console et Picardi, 2003) permettant de découvrir des modèlesde chroniques à partir d'un ensemble d'apprentissage contenant des séquencesd'occurrences d'événements discrets. Ces séquences représentent des suitesd'alarmes générées par un système à base de connaissance de monitoring et dediagnostic de systèmes dynamiques. On montre sur un exemple que l'approcheentropique complète l'approche stochastique en identifiant les classesd'événements qui contribuent le plus significativement à la prédiction d'uneoccurrence d'une classe particulière.	Nabil Benayadi, Marc Le Goc, Philippe Bouché	http://editions-rnti.fr/render_pdf.php?p1&p=1000397	http://editions-rnti.fr/render_pdf.php?p=1000397	article proposer dutiliser lentropie informationnel pouranalyser modèle chronique découvrir approchestochastique Bouché Goc 2005 décrire adaptation lalgorithmetemporalid3 Console Picardi 2003 permettre découvrir modèlesde chronique partir dun ensemble dapprentissage contenir séquencesdoccurrence dévénement discret séquence représenter suitesdalarme générer système baser connaissance monitoring dediagnostic système dynamique montrer exemple lapprocheentropiqu complet lapproche stochastique identifier classesdévénement contribuer plaire significativement prédiction duneoccurrence dune classer
1004	Revue des Nouvelles Technologies de l'Information	EGC	2006	ARABASE : Base de données Web pour l'exploitation en reconnaissance optique de l'écriture Arabe		Noura Bouzrara, Nacéra Madani Aissaoui, Najoua Essoukri Ben Amara	http://editions-rnti.fr/render_pdf.php?p1&p=1000448	http://editions-rnti.fr/render_pdf.php?p=1000448	
1005	Revue des Nouvelles Technologies de l'Information	EGC	2006	Arbres de décision multi modes et multi cibles.	Nous présentons une nouvelle méthode d'induction d'arbre de décision appelée MuMTree (pour Multi Models Tree) utilisable pour les modes d'apprentissage supervisé, non supervisé, supervisé à plusieurs variables cibles. Nous présentons les différents principes nécessaires pour réaliser un tel arbre de décision. Nous illustrons ensuite, sur un cas de modélisation multi-cibles, les avantages de cette méthode par rapport à un arbre de décision classique.	Frank Meyer, Fabrice Clérot	http://editions-rnti.fr/render_pdf.php?p1&p=1000401	http://editions-rnti.fr/render_pdf.php?p=1000401	présenter méthod dinduction darbr décision appeler MuMTree multi Models Tree utilisable mode dapprentissage superviser superviser superviser variable cibl présenter principe nécessaire réaliser arbre décision illustrer ensuite cas modélisation multicibl avantage méthode rapport arbre décision classique
1006	Revue des Nouvelles Technologies de l'Information	EGC	2006	Archiview, un outil de visualisation topographique des paramètres d'un hôpital		Pierre P. Lévy, Jean-Philippe Villaréal, Pierre-Paul Couka, Fabrice Gallois, Laurence Herbin, Antoine Flahault	http://editions-rnti.fr/render_pdf.php?p1&p=1000449	http://editions-rnti.fr/render_pdf.php?p=1000449	
1007	Revue des Nouvelles Technologies de l'Information	EGC	2006	Biclustering of Gene Expression Data Based on Local Nearness	The analysis of gene expression data in DNA chips is an importanttool used in genomic research whose main objectives range from the study ofthe functionality of specific genes and their participation in biological processto the reconstruction of diseases's conditions and their subsequent prognosis.Gene expression data are arranged in matrices where each gene corresponds toone row and every column represents one specific experimental condition. Thebiclustering techniques have the purpose of finding subsets of genes that showsimilar activity patterns under a subset of conditions. Our approach consists ofa biclustering algorithm based on local nearness. The algorithm searches forbiclusters in a greedy fashion, starting with two–genes biclusters and includingas much as possible depending on a distance threshold which guarantees thesimilarity of gene behaviors.	Jesús S. Aguilar-Ruiz, Domingo S. Rodríguez, Dan A. Simovici	http://editions-rnti.fr/render_pdf.php?p1&p=1000427	http://editions-rnti.fr/render_pdf.php?p=1000427	The analysis of gene expression dater in DNA chip is an importanttool used in genomic research whose main objective rang from the study ofthe functionality of specific gen and their participation in biological processto the reconstruction of diseasess condition and their subsequent prognosisgene expression dater are arranged in matrice where each gene correspond toone row and every column represents one specific experimental condition thebiclustering technique hav the purpose of finding subset of gene that showsimilar activity pattern under subset of condition Our approach consist ofa biclustering algorithm based local nearnes The algorithm search forbicluster in greedy fashion starting with two – gene bicluster and includinga much depending distancer threshold which guarante thesimilarity of gene behaviors
1008	Revue des Nouvelles Technologies de l'Information	EGC	2006	Bordures statistiques pour la fouille incrémentale de données dans les Data Streams	Récemment la communauté Extraction de Connaissances s'est intéressée à de nouveaux modèles où les données arrivent séquentiellement sous la forme d'un flot rapide et continu, i.e. les data streams. L'une des particularités importantes de ces flots est que seule une quantité d'information partielle est disponible au cours du temps. Ainsi après différentes mises à jour successives, il devient indispensable de considérer l'incertitude inhérente à l'information retenue. Dans cet article, nous introduisons une nouvelle approche statistique en biaisant les valeurs supports pour les motifs fréquents. Cette dernière a l'avantage de maximiser l'un des deux paramètres (précision ou rappel) déterminés par l'utilisateur tout en limitant la dégradation sur le paramètre non choisi. Pour cela, nous définissons les notions de bordures statistiques. Celles-ci constituent les ensembles de motifs candidats qui s'avèrent très pertinents à utiliser dans le cas de la mise à jour incrémentale des streams. Les différentes expérimentations effectuées dans le cadre de recherche de motifs séquentiels ont montré l'intérêt de l'approche et le potentiel des techniques utilisées.	Jean-Emile Symphor, Pierre-Alain Laur	http://editions-rnti.fr/render_pdf.php?p1&p=1000417	http://editions-rnti.fr/render_pdf.php?p=1000417	récemment communauté extraction Connaissances sest intéressé modèle donnée arriver séquentiellemer sou former dun flot rapide continu ie dater stream lune particularité important flot quantité dinformation partiel disponible cours temps mise jour successif devenir indispensable considérer lincertitude inhérent linformation retenir Dans article introduire approcher statistique biaiser support motif fréquent lavantage maximiser lun paramètre précision rappel déterminé lutilisateur limiter dégradation paramètre choisir Pour celer définir notion bordure statistique Cellesci constituer ensemble motif candidat saver pertinent utiliser dan cas miser jour incrémentale stream expérimentation effectuer dan cadrer rechercher motif séquentiel montrer lintérêt lapproche potentiel technique utiliser
1009	Revue des Nouvelles Technologies de l'Information	EGC	2006	Carte auto-organisatrice probabiliste sur données binaires	Lesméthodes factorielles d'analyse exploratoire statistique définissentdes directions orthogonales informatives à partir d'un ensemble de données.Elles conduisent par exemple à expliquer les proximités entre individus à l'aided'un groupe de variables caractéristiques.Dans le contexte du datamining lorsqueles tableaux de données sont de grande taille, une méthode de cartographie synthétiques'avère intéressante. Ainsi une carte auto-organisatrice (SOM) est uneméthode de partitionnement munie d'une structure de graphe de voisinage -surles classes- le plus souvent planaire. Des travaux récents sont développés pourétendre le SOM probabiliste Generative Topographic Mapping (GTM) aux modèlesde mélanges classiques pour données discrètes. Dans ce papier nous présentonset étudions un modèle génératif symétrique de carte auto-organisatricepour données binaires que nous appelons Bernoulli Aspect Topological Model(BATM). Nous introduisons un nouveau lissage et accélérons la convergence del'estimation par une initialisation originale des probabilités en jeu.	Mohamed Nadif, Rodolphe Priam	http://editions-rnti.fr/render_pdf.php?p1&p=1000386	http://editions-rnti.fr/render_pdf.php?p=1000386	lesméthod factoriel danalyse exploratoire statistique définissentdes direction orthogonal informatif partir dun ensemble donnéeselle conduire exemple expliquer proximité entrer individu laidedun grouper variable caractéristiquesdans contexte datamining lorsquel tableau donnée grand tailler méthode cartographie synthétiquesavèr intéressant carte autoorganisatrice SOM uneméthode partitionnement munir dune structurer graphe voisinage surl classe plaire planaire travail récent développer pourétendre som probabiliste Generative Topographic Mapping gtm modèlesde mélange classique donnée discret Dans papier présentonset étudier modeler génératif symétrique carte autoorganisatricepour donnée binaire appeler Bernoulli Aspect Topological ModelBATM introduire lissage accéléron convergence delestimation initialisation original probabilité jeu
1010	Revue des Nouvelles Technologies de l'Information	EGC	2006	Champs de Markov conditionnels pour le traitement de séquences	Les modèles conditionnels du type modèles de Markov d'entropiemaximale et champs de Markov conditionnels apportent des réponses auxlacunes des modèles de Markov cachés traditionnellement employés pour laclassification et la segmentation de séquences. Ces modèles conditionnels ontété essentiellement utilisés jusqu'à présent dans des tâches d'extractiond'information ou d'étiquetage morphosyntaxique. Cette contribution explorel'emploi de ces modèles pour des données de nature différente, de type« signal », telles que la parole ou l'écriture en ligne. Nous proposons desarchitectures de modèles adaptées à ces tâches pour lesquelles nous avonsdérivé les algorithmes d'inférence et d'apprentissage correspondant. Nousfournissons des résultats expérimentaux pour deux tâches de classification etd'étiquetage de séquences.	Thierry Artières, Trinh Minh Tri Do	http://editions-rnti.fr/render_pdf.php?p1&p=1000419	http://editions-rnti.fr/render_pdf.php?p=1000419	modèle conditionnel typer modèle Markov dentropiemaximal champ Markov conditionnel apporter réponse auxlacun modèle Markov cacher traditionnellement employer laclassification segmentation séquence modèle conditionnel ontéter essentiellement utiliser jusquà présent dan tâche dextractiondinformation détiquetage morphosyntaxique contribution explorelemploi modèle donnée nature typer « signal » lécriture ligne proposon desarchitectur modèle adapter tâche avonsdériver algorithme dinférence dapprentissage correspondre nousfournisson résultat expérimental tâche classification etdétiquetage séquence
1011	Revue des Nouvelles Technologies de l'Information	EGC	2006	Choix du taux d'élagage pour l'extraction de la terminologie. Une approche fondée sur les courbes ROC	Le choix du taux d'élagage est crucial dans le but d'acquérir une terminologiede qualité à partir de corpus de spécialité. Cet article présente uneétude expérimentale consistant à déterminer le taux d'élagage le plus adapté.Plusieurs mesures d'évaluation peuvent être utilisées pour déterminer ce tauxtels que la précision, le rappel et le Fscore. Cette étude s'appuie sur une autremesure d'évaluation qui semble particulièrement bien adaptée pour l'extractionde la terminologie : les courbes ROC (Receiver Operating Characteristics).	Mathieu Roche, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1000347	http://editions-rnti.fr/render_pdf.php?p=1000347	choix taux délagage crucial dan boire dacquérir terminologiede qualité partir corpus spécialité article présenter uneétude expérimental consister déterminer taux délagage plaire adaptéplusieurs mesure dévaluation pouvoir utiliser déterminer tauxtel précision rappel Fscore étude sappuie autremesure dévaluation sembler adapter lextractionde terminologie   courbe ROC Receiver Operating Characteristics
1012	Revue des Nouvelles Technologies de l'Information	EGC	2006	Classification d'un tableau de contingence et modèle probabiliste	Ces dernières années, la classification croisée ou classification parblocs, c'est-à-dire la recherche simultanée d'une partition des lignes et d'unepartition des colonnes d'un tableau de données, est devenue un outil très utiliséen fouille de données. Dans ce domaine, l'information se présente souvent sousforme de tableaux de contingence ou tableaux de co-occurrence croisant les modalitésde deux variables qualitatives. Dans cet article, nous étudions le problèmede la classification croisée de ce type de données en nous appuyant sur un modèlede mélange probabiliste. En utilisant l'approche vraisemblance classifiante,nous proposons un algorithme de classification croisée basé sur la maximisationalternée de la vraisemblance associée à deux mélanges multinomiaux classiqueset nous montrons alors que sous certaines contraintes restrictives, on retrouveles critères du Chi2 et de l'information mutuelle. Des résultats sur des donnéessimulées et des données réelles illustrent et confirment l'efficacité et l'intérêt decette approche.	Gérard Govaert, Mohamed Nadif	http://editions-rnti.fr/render_pdf.php?p1&p=1000387	http://editions-rnti.fr/render_pdf.php?p=1000387	dernière année classification croisé classification parbloc cestàdire rechercher simultané dune partition ligne dunepartition colonne dun tableau donnée devenir outil utiliséen fouiller donnée Dans domaine linformation présenter sousform tableau contingence tableau cooccurrence croiser modalitésde variable qualitatif Dans article étudier problèmede classification croiser typer donnée appuyer modèlede mélang probabiliste En utiliser lapproch vraisemblance classifiantenous proposer algorithme classification croiser baser maximisationalternée vraisemblance associer mélange multinomial classiqueset montrer sou contrainte retrouvele critère Chi2 linformation mutuel résultat donnéessimulée donnée réel illustrer confirmer lefficaciter lintérêt decett approcher
1013	Revue des Nouvelles Technologies de l'Information	EGC	2006	Classification de documents XML à partir d'une représentation linéaire des arbres de ces documents	Cet article présente un nouveau modèle de représentation pour la classificationde documents XML. Notre approche permet de prendre en compte soitla structure seule, soit la structure et le contenu de ces documents. L'idée estde représenter un document par l'ensemble des sous-chemins de l'arbre XMLde longueur comprise entre n et m, deux valeurs fixées a priori. Ces cheminssont ensuite considérés comme de simples mots sur lesquels on peut appliquerdes méthodes standards de classification, par exemple K-means. Nous évaluonsnotre méthode sur deux collections: la collection INEX et les rapports d'activitéde l'INRIA. Nous utilisons un ensemble de mesures bien connues dans le domainede la recherche d'information lorsque les classes sont connues a priori.Lorsqu'elles ne sont pas connues, nous proposons une analyse qualitative desrésultats qui s'appuie sur les mots (chemins) les plus caractéristiques des classesgénérées.	Anne-Marie Vercoustre, Mounir Fegas, Yves Lechevallier, Thierry Despeyroux	http://editions-rnti.fr/render_pdf.php?p1&p=1000384	http://editions-rnti.fr/render_pdf.php?p=1000384	article présenter modeler représentation classificationde document xml approcher permettre prendre compter soitla structurer structurer contenir document Lidée estde représenter document lensembl souschemin larbre XMLde longueur comprendre entrer fixer priori cheminssont ensuite considérer simple pouvoir appliquerdes méthode standard classification exemple Kmeans évaluonsnotre méthode collection collection inex rapport dactivitéde linria utiliser ensemble mesure connu dan domainede rechercher dinformation classe connu priorilorsquell connu proposer analyser qualitatif desrésultat sappuie chemin plaire caractéristique classesgénérée
1014	Revue des Nouvelles Technologies de l'Information	EGC	2006	Classification des comptes-rendus mammographiques à partir d'une ontologie radiologique en OWL	Dans cet article, nous proposons un système de classification descomptes-rendus mammographiques, reposant sur une ontologie radiologiquedécrivant les signes radiologiques et les différentes classes de la classificationACR des systèmes BIRADS dans le langage OWL. Le système est conçu pour,extraire les faits issus des textes libres de comptes-rendus en étant dirigé parl'ontologie, puis inférer la classe correspondante et en déduire l'attitude à tenirà partir de la classification ACR. Ce travail présente la construction d'une ontologieradiologique mammaire dans le langage OWL et son intérêt pour classerautomatiquement les comptes-rendus de mammographies.	Amel Boustil, Zaïdi Sahnoun, Ziad Mansouri, Christine Golbreich	http://editions-rnti.fr/render_pdf.php?p1&p=1000346	http://editions-rnti.fr/render_pdf.php?p=1000346	Dans article proposer système classification descomptesrendu mammographiqu reposer ontologie radiologiquedécriver signe radiologique classe classificationacr système BIRADS dan langage owl système concevoir pourextraire issu texte libre comptesrendu diriger parlontologie pouvoir inférer classer correspondant déduir lattitude tenirà partir classification ACR travail présenter construction dune ontologieradiologiqu mammaire dan langage OWL intérêt classerautomatiquemer comptesrendu mammographie
1015	Revue des Nouvelles Technologies de l'Information	EGC	2006	Classification non-supervisée de données relationnelles		Jérôme Maloberti, Shin Ando, Einoshin Suzuki	http://editions-rnti.fr/render_pdf.php?p1&p=1000375	http://editions-rnti.fr/render_pdf.php?p=1000375	
1016	Revue des Nouvelles Technologies de l'Information	EGC	2006	Classifications hiérarchiques factorielles de variables	On présente deux méthodes de classification hiérarchique ascendantede variables quantitatives et de fréquences. Chaque noeud de ces hiérarchiesregroupe deux classes de variables à partir d'une analyse factorielle particulièrebasée sur les variables représentatives de ces deux classes. Par cette méthode,on dispose, à chaque pas, d'un plan factoriel permettant de représenter àla fois les variables des deux classes fusionnées et l'ensemble des individus.Ces derniers se positionnent dans ce plan suivant leurs valeurs pour les variablesconsidérées. Ainsi, l'interprétation des noeuds obtenus s'effectue facilementà partir de l'examen de ces représentations factorielles. La répartition desindividus observée dans chacun de ces plans factoriels permet également dedéfinir une segmentation des individus en total accord avec la hiérarchie desvariables obtenues. On montre le fonctionnement des méthodes sur des exemplesréels.	Sergio Camiz, Jean-Jacques Denimal 	http://editions-rnti.fr/render_pdf.php?p1&p=1000374	http://editions-rnti.fr/render_pdf.php?p=1000374	présenter méthode classification hiérarchique ascendantede variable quantitatif fréquence noeud hiérarchiesregroupe classe variable partir dune analyser factoriel particulièrebaser variable représentatif classe Par méthodeon disposer dun plan factoriel permettre représenter àla variable classe fusionné lensembl individusce positionner dan plan variablesconsidérée linterprétation noeud obtenir seffectue facilementà partir lexamen représentation factoriel répartition desindividus observer dan plan factoriel permettre également dedéfinir segmentation individu total accord hiérarchie desvariabl obtenu montrer fonctionnement méthode exemplesréel
1017	Revue des Nouvelles Technologies de l'Information	EGC	2006	Clustering dynamique d'un flot de données : un algorithme incrémental et optimal de détection des maxima de densité	L'extraction non supervisée et incrémentale de classes sur un flot dedonnées (data stream clustering) est un domaine en pleine expansion. La plupartdes approches visent l'efficacité informatique. La nôtre, bien que se prêtantà un passage à l'échelle en mode distribué, relève d'une problématiquequalitative, applicable en particulier au domaine de la veille informationnelle :faire apparaître les évolutions fines, les « signaux faibles », à partir des thématiquesextraites d'un flot de documents. Notre méthode GERMEN localise defaçon exhaustive les maxima du paysage de densité des données à l'instant t,en identifiant les perturbations locales du paysage à t-1 et modifications defrontières induites par le document présenté. Son caractère optimal provient deson exhaustivité (à une valeur du paramètre de localité correspond un ensembleunique de maxima, et un découpage unique des classes qui la rend indépendantede tout paramètre d'initialisation et de l'ordre des données.	Alain Lelu	http://editions-rnti.fr/render_pdf.php?p1&p=1000320	http://editions-rnti.fr/render_pdf.php?p=1000320	lextraction superviser incrémental classe flot dedonner dater stream clustering domaine expansion plupartde approch viser lefficacité informatique prêtantà passage léchelle mode distribuer relever dune problématiquequalitative applicable domaine veiller informationnel faire apparaître évolution fin « signal faible » partir thématiquesextraite dun flot document méthode germen localiser defaçon exhaustif maximum paysage densité donnée linster ten identifier perturbation local paysage t1 modification defrontièr induit document présenter caractère optimal provenir deson exhaustivité paramètre localité correspondre ensembleunique maximum découpage classe indépendantede paramètr dinitialisation lordre donnée
1018	Revue des Nouvelles Technologies de l'Information	EGC	2006	Combinaison de l'approche inductive (progressive) et linguistique pour l'étiquetage morphosyntaxique des corpus de spécialité	Les étiqueteurs morphosyntaxiques sont de plus en plus performantset cependant, un véritable problème apparaît lorsque nous voulons étiqueterdes corpus de spécialité pour lesquels nous n'avons pas de corpus annotés. Lacorrection des ambiguïtés difficiles est une étape importante pour obtenir uncorpus de spécialité parfaitement étiqueté. Pour corriger ces ambiguïtés et diminuerle nombre de fautes, nous utilisons une approche itérative appelée InductionProgressive. Cette approche est une combinaison d'apprentissage automatique,de règles rédigées par l'expert et de corrections manuelles qui secombinent itérativement afin d'obtenir une amélioration de l'étiquetage tout enrestreignant les actions de l'expert à la résolution de problèmes de plus en plusdélicats. L'approche proposée nous a permis d'obtenir un corpus de biologiemoléculaire « correctement » étiqueté. En utilisant ce corpus, nous avons effectuéune étude comparative de quatre étiqueteurs supervisés.	Ahmed Amrani, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1000354	http://editions-rnti.fr/render_pdf.php?p=1000354	étiqueteur morphosyntaxique plaire plaire performantset véritable problème apparaître vouloir étiqueterde corpu spécialité naver corpus annoter lacorrection ambiguïté difficile étape important obtenir uncorpu spécialité parfaitement étiqueter Pour corriger ambiguïté diminuerle nombre faute utiliser approcher itératif appeler inductionprogressiv approcher combinaison dapprentissage automatiquede régler rédiger lexpert correction manuel secombinent itérativemer dobtenir amélioration létiquetage enrestreigner action lexpert résolution problème plaire plusdélicat Lapproche proposer permettre dobtenir corpus biologiemoléculaire « correctement » étiqueter En utiliser corpus effectuéune étude comparatif étiqueteur superviser
1019	Revue des Nouvelles Technologies de l'Information	EGC	2006	Comment formaliser les connaissances tacites d'une organisation ? Le cas de la conduite du changement à la SNCF		Anne Remillieux, Christian Blatter	http://editions-rnti.fr/render_pdf.php?p1&p=1000440	http://editions-rnti.fr/render_pdf.php?p=1000440	
1020	Revue des Nouvelles Technologies de l'Information	EGC	2006	Comparaison de deux modes de représentation de données faiblement structurées en sciences du vivant	Cet article présente deux modes de représentation de l'informationdans le cadre d'une problématique en sciences du vivant. Le premier, appliqué àla microbiologie prévisionnelle, s'appuie sur deux formalismes, le modèle relationnelet les graphes conceptuels, interrogés uniformément via une même interface.Le second, appliqué aux technologies des céréales, utilise le seul modèlerelationnel. Cet article décrit les caractéristiques des données et compare les solutionsde représentation adoptées dans les deux systèmes.	Rallou Thomopoulos, Patrice Buche, Ollivier Haemmerlé, Frédéric Mabille, Nongyao Mueangdee	http://editions-rnti.fr/render_pdf.php?p1&p=1000332	http://editions-rnti.fr/render_pdf.php?p=1000332	article présenter mode représentation linformationdans cadrer dune problématique science vivre appliquer àla microbiologi prévisionnel sappuie formalisme modeler relationnelet graphe conceptuel interrogé uniformément interfacele second appliquer technologi céréale utiliser modèlerelationnel article décrire caractéristique donnée comparer solutionsde représentation adopter dan système
1021	Revue des Nouvelles Technologies de l'Information	EGC	2006	Comparaison de dissimilarités pour l'analyse de l'usage d'un site web	"L'obtention d'une classification des pages d'un site web en fonctiondes navigations extraites des fichiers ""logs"" du serveur peut s'avérer très utilepour évaluer l'adéquation entre la structure du site et l'attente des utilisateurs. Onconstruit une telle typologie en s'appuyant une mesure de dissimilarité entre lespages, définie à partir des navigations. Le choix de la mesure la plus appropriéeà l'analyse du site est donc fondamental. Dans cet article, nous présentons unsite de petite taille dont les pages sont classées en catégories sémantiques parun expert. Nous confrontons ce classement aux partitions obtenues à partir dediverses dissimilarités afin d'en étudier les avantages et inconvénients."	Fabrice Rossi, Francisco de Assis Tenório de Carvalho, Yves Lechevallier, Alzennyr Da Silva	http://editions-rnti.fr/render_pdf.php?p1&p=1000378	http://editions-rnti.fr/render_pdf.php?p=1000378	lobtention dune classification page dun site web fonctionde navigation extrait fichier log serveur pouvoir savérer utilepour évaluer ladéquation entrer structurer site lattente utilisateur onconstruit typologie sappuyer mesurer dissimilarité entrer lespage définir partir navigation choix mesurer plaire appropriéeà lanalyse site fondamental Dans article présenter unsite petit tailler page classer catégorie sémantique parun expert confronter classement partition obtenu partir dediverse dissimilariter den étudier avantage inconvénient
1022	Revue des Nouvelles Technologies de l'Information	EGC	2006	Comparaison des mammographies par des méthodes d'apprentissage		Irina Diana Coman, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1000433	http://editions-rnti.fr/render_pdf.php?p=1000433	
1023	Revue des Nouvelles Technologies de l'Information	EGC	2006	Comparaison des mesures d'intérêt de règles d'association : une approche basée sur des graphes de corrélation	Le choix des mesures d'intérêt (MI) afin d'évaluer les règles d'associationest devenu une question importante pour le post-traitement des connaissanceen ECD. Dans la littérature, de nombreux auteurs ont discuté et comparéles propriétés des MI afin d'améliorer le choix des meilleures mesures. Cependant,il s'avère que la qualité d'une règle est contextuelle : elle dépend à la fois dela structure de données et des buts du décideur. Ainsi, certaines mesures peuventêtre appropriées dans un certain contexte, mais pas dans d'autres. Dans cet article,nous présentons une nouvelle approche contextuelle mise en applicationpar un nouvel outil, ARQAT, permettant à un décideur d'évaluer et de comparerle comportement des MI sur ses jeux de données spécifiques. Cette approche estbasée sur l'analyse visuelle d'un graphe de corrélation entre des MI objectives.Nous employons ensuite cette approche afin de comparer et de discuter le comportementde trente-six mesures d'intérêt sur deux ensembles de données a prioritrès opposés : un premier dont les données sont fortement corrélées et un secondaux données faiblement corrélées. Alors que nous attendions des différences importantesentre les graphes de corrélation de ces deux jeux d'essai, nous avonspu observer des stabilités de corrélation entre certaines MI qui sont révélatricesde propriétés indépendantes de la nature des données observées. Ces stabilitéssont récapitulées et analysées.	Xuan-Hiep Huynh, Fabrice Guillet, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1000404	http://editions-rnti.fr/render_pdf.php?p=1000404	choix mesure dintérêt mi dévaluer règle dassociationest devenir question important posttraitement connaissanceen ecd Dans littérature auteur discuter comparél propriété mi daméliorer choix meilleure mesure Cependantil savère qualité dune régler contextuel   dépendre dela structurer donnée but décideur mesure peuventêtre approprier dan contexte dan dautre Dans articlenou présenter approcher contextuel mettre applicationpar nouvel outil arqat permettre décideur dévaluer comparerle comportemer mi jeu donnée spécifique approcher estbaser lanalyse visuel dun graph corrélation entrer mi objectivesnou employer ensuite approcher comparer discuter comportementde trentesix mesure dintérêt ensemble donnée prioritrès opposer   donnée fortement corréler secondaux donnée faiblement corréler attendre différence importantesentre graphe corrélation jeu dessai avonspu observer stabilité corrélation entrer mi révélatricesde propriété indépendant nature donnée observer stabilitéssont récapituler analyser
1024	Revue des Nouvelles Technologies de l'Information	EGC	2006	Confrontation de Points de Vue dans le système Porphyry		Samuel Gesche, Sylvie Calabretto, Guy Caplat	http://editions-rnti.fr/render_pdf.php?p1&p=1000444	http://editions-rnti.fr/render_pdf.php?p=1000444	
1025	Revue des Nouvelles Technologies de l'Information	EGC	2006	Credit scoring, statistique et apprentissage	Basel 2 regulations brought new interest in supervised classification methodologies for predicting default probability for loans. An important feature of consumer credit is that predictors are generally categorical. Logistic regression and linear discriminant analysis are the most frequently used techniques but are often unduly opposed. Vapnik's statistical learning theory explains why a prior dimension reduction (eg by means of multiple correspondence analysis) improves the robustness of the score function. Ridge regression, linear SVM, PLS regression are also valuable competitors. Predictive capability is measured by AUC or Gini's index which are related to the well known non-parametric Wilcoxon-Mann-Whitney test. Among methodological problems, reject inference is an important one, since most samples are subject to a selection bias. There are many methods, none being satisfactory. Distinguish between good and bad customers is not enough, especially for long-term loans. The question is then not only “if”, but “when” the customers default. Survival analysis provides new types of scores.	Gilbert Saporta	http://editions-rnti.fr/render_pdf.php?p1&p=1000316	http://editions-rnti.fr/render_pdf.php?p=1000316	Basel 2 regulation brought new interest in supervised classification methodologier for predicting default probability for loans an importer feature of consumer credit is that predictor are generally categorical Logistic regression and linear discriminer analysis are the most frequently used technique boire are often unduly opposed Vapniks statistical learning theory explain why prior dimension reduction eg by means of correspondence analysis improv the robustness of the score function Ridge regression linear svm pl regression are also valuabl competitors predictive capability is measured by auc or Ginis index which are related to the well known nonparametric WilcoxonMannWhitney test Among methodological problems reject inference is an importer one since most sampl are subject to selection bia There are many method being satisfactory distinguish between good and bad customer is not enough especially for longterm loan The question is then not only “ if ” boire “ when ” the customer default Survival analysis provider new type of score
1026	Revue des Nouvelles Technologies de l'Information	EGC	2006	Critère VT100 de sélection des règles d'association	L'extraction de règles d'association génère souvent un grand nombrede règles. Pour les classer et les valider, de nombreuses mesures statistiquesont été proposées ; elles permettent de mettre en avant telles ou telles caractéristiquesdes règles extraites. Elles ont pour point commun d'être fonctioncroissante du nombre de transactions et aboutissent bien souvent àl'acceptation de toutes les règles lorsque la base de données est de grandetaille. Dans cet article, nous proposons une mesure inspirée de la notion de valeur-test. Elle présente comme principale caractéristique d'être insensible à lataille de la base, évitant ainsi l'écueil des règles fallacieusement significatives.Elle permet également de mettre sur un même pied, et donc de les comparer,des règles qui auront été extraites de bases de données différentes. Elle permetenfin de gérer différents seuils de signification des règles. Le comportement dela mesure est détaillé sur un exemple.	Alain Morineau, Ricco Rakotomalala	http://editions-rnti.fr/render_pdf.php?p1&p=1000409	http://editions-rnti.fr/render_pdf.php?p=1000409	lextraction règle dassociation génèr grand nombrede régler Pour classer valider mesure statistiquesont proposer   permettre mettre caractéristiquesde règle extrait poindre commun dêtre fonctioncroissant nombre transaction aboutir àlacceptation règle baser donnée grandetaille Dans article proposer mesurer inspiré notion valeurtest présenter principal caractéristique dêtre insensible lataille baser éviter lécueil règle fallacieusement significativesell permettre également mettre pied comparerde règle extraire base donnée permetenfin gérer seuil signification règle comportement dela mesurer détailler exemple
1027	Revue des Nouvelles Technologies de l'Information	EGC	2006	De l'analyse didactique à la modélisation informatique pour la conception d'un EIAH en chirurgie orthopédique	L'objet de la recherche présentée est de concevoir un environnementinformatique d'apprentissage qui permette de réduire l'écart entre la formationthéorique des chirurgiens et leur formation pratique, qui se dérouleprincipalement sur le mode du compagnonnage. L'article expose laméthodologie et quelques illustrations du travail didactique d'analyse desconnaissances et du système d'enseignement / apprentissage en milieuhospitalier (chirurgie orthopédique) ainsi que partie de la formalisationinformatique de cette connaissance. Cette modélisation permet la prise encompte dans l'environnement informatique de connaissances pragmatiquespour le diagnostic des connaissances de l'utilisateur en fonction des actionsqu'il effectue à l'interface pendant la résolution d'un problème (pose de visdans le bassin), et la prise de décision didactique qui suit : quelle rétroactionfournir pour affiner le diagnostic, et/ou permettre l'apprentissage souhaité.	Vanda Luengo, Lucile Vadcard, Dima Mufti-Alchawafa	http://editions-rnti.fr/render_pdf.php?p1&p=1000422	http://editions-rnti.fr/render_pdf.php?p=1000422	lobjet rechercher présenter concevoir environnementinformatiqu dapprentissage permettre réduire lécart entrer formationthéorique chirurgien formation pratique dérouleprincipalemer mode compagnonnage Larticle exposer laméthodologie illustration travail didactique danalyse desconnaissances système denseignemer   apprentissage milieuhospitalier chirurgie orthopédique partir formalisationinformatique connaissance modélisation permettre priser encompte dan lenvironnement informatique connaissance pragmatiquespour diagnostic connaissance lutilisateur fonction actionsquil effectuer linterface pendre résolution dun problème pos visdans bassin priser décision didactique   rétroactionfournir affiner diagnostic etou permettre lapprentissage souhaiter
1028	Revue des Nouvelles Technologies de l'Information	EGC	2006	Définition et diffusion de signatures sémantiques dans les systèmes pair-à-pair	Les systèmes pair-à-pair (peer-to-peer, P2P, égal-à-égal) se sont popularisésces dernières années avec les systèmes de partage de fichiers sur Internet.De nombreuses recherches concernant l'optimisation de la localisationdes données ont émergé et constituent un axe de recherche très actif. La priseen compte de la sémantique du contenu des pairs dans le routage des requêtespermet d'améliorer considérablement la localisation des données. Nous nousconcentrons sur l'approche PlanetP, faisant usage de la notion de filtre de Bloom,qui consiste à propager une signature sémantique des pairs (filtres de Bloom) àtravers le réseau. Nous présentons cette approche et en proposons une amélioration: la création de filtres de Bloom dynamiques, dans le sens où leur tailledépend de la charge des pairs (nombre de documents partagés).	Raja Chiky, Bruno Defude, Georges Hébrail	http://editions-rnti.fr/render_pdf.php?p1&p=1000388	http://editions-rnti.fr/render_pdf.php?p=1000388	système pairàpair peertopeer p2p égalàégal popularisésce année système partager fichier internetde recherche concerner loptimisation localisationde donnée émerger constituer axer rechercher actif priseen compter sémantique contenir pair dan routage requêtespermet daméliorer considérablemer localisation donnée nousconcentrer lapproche planetp faire usage notion filtrer Bloomqui consister propager signature sémantique pair filtr Bloom àtraver réseau présenter approcher proposon amélioration création filtre Bloom dynamique dan sens tailledépend charger pair nombre document partagé
1029	Revue des Nouvelles Technologies de l'Information	EGC	2006	Des motifs séquentiels généralisés aux contraintes de temps étendues	Dans de nombreux domaines, la recherche de connaissances temporellesest très appréciée. Des techniques ont été proposées aussi bien en fouille dedonnées qu'en apprentissage, afin d'extraire et de gérer de telles connaissances,en les associant également à la spécification de contraintes temporelles (e.g.: fenêtretemporelle maximale), notamment dans le contexte de la recherche de motifsséquentiels. Cependant, ces contraintes sont souvent trop rigides ou nécessitentune bonne connaissance du domaine pour ne pas extraire des informationserronées. C'est pourquoi nous proposons une approche basée sur la constructionde graphes de séquences afin de prendre en compte des contraintes de tempsplus souples. Ces contraintes sont relâchées par rapport aux contraintes de tempsprécédemment proposées. Elles permettent donc d'extraire plus de motifs pertinents.Afin de guider l'analyse des motifs obtenus, nous proposons égalementun niveau de précision des contraintes temporelles pour les motifs extraits.	Maguelonne Teisseire, Céline Fiot, Anne Laurent	http://editions-rnti.fr/render_pdf.php?p1&p=1000415	http://editions-rnti.fr/render_pdf.php?p=1000415	Dans domaine rechercher connaissance temporellesest apprécier technique proposer fouiller dedonner quen apprentissage dextraire gérer connaissancesen associer également spécification contrainte temporel eg fenêtretemporelle maximal dan contexte rechercher motifsséquentiel contrainte rigide nécessitentun connaissance domaine extraire informationserronée cest proposer approcher basé constructionde graphe séquence prendre compter contrainte tempsplu souple contrainte relâcher rapport contrainte tempsprécédemment proposer permettre dextraire plaire motif pertinentsafin guider lanalyse motif obtenir proposer égalementun niveau précision contrainte temporel motif extrait
1030	Revue des Nouvelles Technologies de l'Information	EGC	2006	EDA : algorithme de désuffixation du langage médical		Didier Nakache, Elisabeth Métais, Annabelle Dierstein	http://editions-rnti.fr/render_pdf.php?p1&p=1000429	http://editions-rnti.fr/render_pdf.php?p=1000429	
1031	Revue des Nouvelles Technologies de l'Information	EGC	2006	Enrichissement d'ontologies dans le secteur de l'eau douce en environnement Internet distribué et multilingue		Lylia Abrouk, Mathieu Lafourcade	http://editions-rnti.fr/render_pdf.php?p1&p=1000432	http://editions-rnti.fr/render_pdf.php?p=1000432	
1032	Revue des Nouvelles Technologies de l'Information	EGC	2006	ESIEA Datalab Logiciel de Nettoyage et Préparation de Données		Christopher Corsia	http://editions-rnti.fr/render_pdf.php?p1&p=1000451	http://editions-rnti.fr/render_pdf.php?p=1000451	
1033	Revue des Nouvelles Technologies de l'Information	EGC	2006	Exploration des paramètres discriminants pour les représentations vectorielles de la sémantique des mots	Les méthodes de représentation sémantique des mots à partir d'une analyse statistique sont basées sur des comptes de co-occurences entre mots et unités textuelles. Ces méthodes ont des paramétrages complexes, notamment le type d'unité textuelle utilisée comme contexte. Ces paramètres déterminent fortement la qualité des résultats obtenus. Dans cet article, nous nous intéressons au paramètrage de la technique dite Hyperspace Analogue to Language (HAL).Nous proposons une nouvelle méthode pour explorer ses paramètres discriminants. Cette méthode est basée sur l'analyse d'un graphe de voisinage d'une liste de mots de référence pré-classés. Nous expérimentons cette méthode et en donnons les premiers résultats qui renforcent et complètent des résultats issus de travaux précédents.	Frank Meyer, Vincent Dubois	http://editions-rnti.fr/render_pdf.php?p1&p=1000360	http://editions-rnti.fr/render_pdf.php?p=1000360	méthode représentation sémantique partir dune analyser statistique baser compte cooccurence entrer unité textuel méthode paramétrage complexe typer dunité textuel utiliser contexte paramètre déterminer fortement qualité résultat obtenir Dans article intéresser paramètrage technique Hyperspace Analogue to language halnous proposer méthode explorer paramètre discriminant méthode baser lanalyse dun graph voisinage dune liste référence préclasser expérimenter méthode donnon résultat renforcer compléter résultat issu travail précédent
1034	Revue des Nouvelles Technologies de l'Information	EGC	2006	Exploration interactive de bases de connaissances : un retour d'expérience	La navigation au sein de bases de connaissances reste un problèmeouvert. S'il existe plusieurs paradigmes de visualisation, peu de travaux sur lesretours d'expérience sont disponibles. Dans le cadre de cet article nous noussommes intéressés aux différents paradigmes de navigation interactive au seinde bases documentaires annotées sémantiquement ; l'accès à la base deconnaissances s'effectuant à travers l'ontologie du domaine d'application. Cesparadigmes ont été évalués dans le cadre d'une application industrielle(mécanique des fluides et échangeurs thermiques) en fonction de critèresdéfinis par les utilisateurs. L'analyse des retours d'expérience1 nous a permisde spécifier et de réaliser un nouveau navigateur dédié à la gestion dedocuments techniques annotés par une ontologie de domaine : le « Eye Tree »,navigateur de type « polar fisheye view ».	Christophe Tricot, Christophe Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1000362	http://editions-rnti.fr/render_pdf.php?p=1000362	navigation base connaissance rester problèmeouvert sil exister paradigme visualisation travail lesretour dexpérience disponible Dans cadrer article noussommes intéresser paradigme navigation interactif seinde base documentaire annoter sémantiquemer   laccè baser deconnaissancer seffectuer travers lontologie domaine dapplication Cesparadigmes évaluer dan cadrer dune application industriellemécaniqu fluide échangeur thermiquer fonction critèresdéfini utilisateur lanalys dexpérience1 permisde spécifier réaliser navigateur dédier gestion dedocument technique annoter ontologie domaine   « Eye Tree » navigateur typer « polar fisheye view »
1035	Revue des Nouvelles Technologies de l'Information	EGC	2006	Extension de l'algorithme CURE aux fouilles de données volumineuses		Jerzy Korczak, Aurélie Bertaux	http://editions-rnti.fr/render_pdf.php?p1&p=1000402	http://editions-rnti.fr/render_pdf.php?p=1000402	
1036	Revue des Nouvelles Technologies de l'Information	EGC	2006	Extraction automatique de champs numériques dans des documents manuscrits	Nous décrivons dans cet article une chaine de traitement complète etgénérique permettant d'extraire automatiquement les champs numériques (numérosde téléphone, codes clients, codes postaux) dans des documents manuscritslibres. Notre chaïne de traitement est constituée des trois étapes suivantes:localisation des champs numériques potentiels selon une approche markoviennesans reconnaissance chiffre ni segmentation, reconnaissance des séquences extraites,et vérification des hypothèses de localisation / reconnaissance en vue delimiter la fausse alarme génerée lors de l'étape de localisation. L'évaluation denotre système sur une base de 300 courriers manuscrits montre des performancesen rappel-précision intéressantes.	Clément Chatelain, Laurent Heutte, Thierry Paquet	http://editions-rnti.fr/render_pdf.php?p1&p=1000319	http://editions-rnti.fr/render_pdf.php?p=1000319	décrire dan article chaine traitement complet etgénériqu permettre dextraire automatiquement champ numérique numérosd téléphoner code client code postal dan document manuscritslibr chaïne traitement constituer étape suivanteslocalisation champ numérique potentiel approcher markoviennesan reconnaissance chiffr segmentation reconnaissance séquence extraiteset vérification hypothèse localisation   reconnaissance delimiter faux alarmer génerer létape localisation lévaluation denotre système baser 300 courrier manuscrit montrer performancesen rappelprécision intéressant
1037	Revue des Nouvelles Technologies de l'Information	EGC	2006	Extraction d'objets vidéo : Une approche combinant les contours actifs et le flot optique	Dans cet article, nous présentons une méthode mixte de segmentationd'objets visuels dans une séquence d'images d'une vidéo combinant à la foisune segmentation basée régions et l'estimation de mouvement par flot optique.L'approche développée est basé sur une minimisation d'une fonctionnelled'énergie (E) qui fait intervenir les probabilités d'appartenance (densité) avecune gaussienne, en tenant compte des informations perceptuelles de couleur etde texture des régions d'intérêt. Pour améliorer la méthode de détection et desuivi, nous avons étendu la formulation énergétique de notre modèle decontour actif en incluant une force supplémentaire issue du calcul du flot optique.Nous montrons l'intérêt de cette approche mixte en terme de temps de calculet d'extraction d'objets vidéo complexes, et nous présentons les résultatsobtenus sur des séquences de corpus vidéo couleur.	Youssef Zinbi, Youssef Chahir, Abder Elmoatz	http://editions-rnti.fr/render_pdf.php?p1&p=1000321	http://editions-rnti.fr/render_pdf.php?p=1000321	Dans article présenter méthode mixte segmentationdobjet visuel dan séquence dimager dune vidéo combiner foisune segmentation basé région lestimation mouvement flot optiquelapproch développer baser minimisation dune fonctionnelledénergie 7e faire intervenir probabilité dappartenanc densité avecun gaussienne compter information perceptuel couleur etde texture région dintérêt Pour améliorer méthode détection desuivi étendre formulation énergétique modeler decontour actif inclure forcer supplémentaire issu calcul flot optiqueNous montrer lintérêt approcher mixte terme temps calculet dextraction dobjet vidéo complexe présenter résultatsobtenu séquence corpus vidéo couleur
1038	Revue des Nouvelles Technologies de l'Information	EGC	2006	Extraction de motifs séquentiels dans les flots de données d'usage du Web	Ces dernières années, de nouvelles contraintes sont apparues pour lestechniques de fouille de données. Ces contraintes sont typiques d'un nouveaugenre de données : les “data streams”. Dans un processus de fouille appliquésur un data stream, l'utilisation de la mémoire est limitée, de nouveaux élémentssont générés en permanence et doivent être traités le plus rapidement possible,aucun opérateur bloquant ne peut être appliqué sur les données et celles-ci nepeuvent être observées qu'une seule fois. A l'heure actuelle, la majorité des travauxrelatifs à l'extraction de motifs dans les data streams ne concernent pas lesmotifs temporels. Nous montrons dans cet article que cela est principalement dûau phénomène combinatoire qui est lié à l'extraction de motifs séquentiels. Nousproposons alors un algorithme basé sur l'alignement de séquences pour extraireles motifs séquentiels dans les data streams. Afin de respecter la contrainte d'unepasse unique sur les données, une heuristique gloutonne est proposée pour segmenterles séquences. Nous montrons enfin que notre proposition est capabled'extraire des motifs pertinents avec un support très faible.	Florent Masseglia, Alice Marascu	http://editions-rnti.fr/render_pdf.php?p1&p=1000418	http://editions-rnti.fr/render_pdf.php?p=1000418	année contrainte apparu lestechniqu fouiller donnée contrainte typiquer dun nouveaugenre donnée   “ dater stream ” Dans processus fouiller appliquésur dater stream lutilisation mémoire limiter élémentssont générer permanence devoir traiter plaire rapidement possibleaucun opérateur bloquer pouvoir appliquer donnée cellesci nepeuvent observer quune lheure actuel majorité travauxrelatif lextraction motif dan dater stream concerner lesmotif temporel montrer dan article celer principalement dûau phénomène combinatoire lier lextraction motif séquentiel nousproposon algorithme baser lalignement séquence extrairel motif séquentiel dan dater stream Afin respecter contraint dunepass donnée heuristique glouton proposer segmenterl séquence montrer proposition capabledextrair motif pertinent support faible
1039	Revue des Nouvelles Technologies de l'Information	EGC	2006	Extraction de relations dans les documents Web	Nous présentons un système pour l'inférence de programmes d'extraction de relations dans les documents Web. Il utilise les vues textuelle et structurelle sur les documents. L'extraction des relations est incrémentale et utilise des méthodes de composition et d'enrichissement. Nous montrons que notre système est capable d'extraire des relations pour les organisations existantes dans les documents Web (listes,  tables, tables tournées, tables croisées).	Rémi Gilleron, Patrick Marty, Marc Tommasi, Fabien Torre	http://editions-rnti.fr/render_pdf.php?p1&p=1000380	http://editions-rnti.fr/render_pdf.php?p=1000380	présenter système linférence programme dextraction relation dan document Web utiliser textuel structurel document lextraction relation incrémental utiliser méthode composition denrichissemer montrer système capable dextraire relation organisation existant dan document Web liste   table tabl tourner tabler croisé
1040	Revue des Nouvelles Technologies de l'Information	EGC	2006	Extraction et identification d'entités complexes à partir de textes biomédicaux	Nous présentons ici un système d'extraction et d'identification d'entitésnommées complexes à l'intention des corpus de spécialité biomédicale. Nousavons développé une méthode qui repose sur une approche mixte à base d'ensemblede règles a priori et de dictionnaires contrôlés. Cet article expose lestechniques que nous avons mises en place pour éviter ou minimiser les problèmesde synonymie, de variabilité des termes et pour limiter la présence denoms ambigus. Nous décrivons l'intégration de ces méthodes au sein du processusde reconnaissance des entités nommées. L'intérêt de cet outil réside dans lacomplexité et l'hétérogénéité des entités extraites. Cette méthode ne se limitepas à la détection des noms des gènes ou des protéines, mais s'adapte à d'autresdescripteurs biomédicaux. Nous avons expérimenté cette approche en mesurantles performances obtenues sur le corpus de référence GENIA.	Julien Lorec, Gérard Ramstein, Yannick Jacques	http://editions-rnti.fr/render_pdf.php?p1&p=1000350	http://editions-rnti.fr/render_pdf.php?p=1000350	présenter système dextraction didentification dentitésnommer complexe lintention corpus spécialité biomédical nousavon développer méthode reposer approcher mixte baser densembled régler priori dictionnaire contrôlé article exposer lestechniqu mettre placer éviter minimiser problèmesde synonymie variabilité terme limiter présence denom ambigu décrivon lintégration méthode processusde reconnaissance entité nommer Lintérêt outil résider dan lacomplexité lhétérogénéité entité extrait méthode limiteper détection nom gène protéine sadapte dautresdescripteur biomédical expérimenter approcher mesurantl performance obtenu corpus référence genia
1041	Revue des Nouvelles Technologies de l'Information	EGC	2006	Extraction multilingue de termes à partir de leur structure morphologique		Delphine Bernhard	http://editions-rnti.fr/render_pdf.php?p1&p=1000356	http://editions-rnti.fr/render_pdf.php?p=1000356	
1042	Revue des Nouvelles Technologies de l'Information	EGC	2006	FaBR-CL : méthode de classification croisée de protéines	Dans cet article, nous proposons une méthode de classification croiséepermettant de classer des protéines, d'une part, et de classer des descripteurs (3-grammes) selon leurs pertinences par rapport aux groupes de protéines obtenus,d'autres part.	Walid Erray, Faouzi Mhamdi	http://editions-rnti.fr/render_pdf.php?p1&p=1000446	http://editions-rnti.fr/render_pdf.php?p=1000446	Dans article proposer méthode classification croiséepermetter classer protéine dune partir classer descripteur 3grammes pertinence rapport groupe protéine obtenusdautr partir
1043	Revue des Nouvelles Technologies de l'Information	EGC	2006	Faire vivre un référentiel métier dans l'industrie : le système de gestion de connaissances ICARE	La gestion des connaissances, enjeu majeur pour l'industrie, est entréedans une phase concrète de déploiement. La conjonction d'une maturitédes organisations dans la maîtrise de leur métier, la consolidation de méthodeset les outils évolutifs pour faire vivre un patrimoine de connaissances favorisentl'émergence de projets significatifs et leur diffusion opérationnelle au seinde grands groupes industriels. ICARE chez PSA Peugeot Citroën réalisé avecl'environnement Ardans Knowledge Maker en est ici l'exemple.	Alain Berger, Pierre Mariot, Christophe Coppens, Julien Laroque Malbert	http://editions-rnti.fr/render_pdf.php?p1&p=1000450	http://editions-rnti.fr/render_pdf.php?p=1000450	gestion connaissance enjeu majeur lindustrie entréedans phase concret déploiemer conjonction dune maturitéde organisater dan maîtriser métier consolidation méthodeset outil évolutif faire vivre patrimoine connaissance favorisentlémergence projet significatif diffusion opérationnel seinde grand groupe industriel ICARE PSA Peugeot Citroën réaliser aveclenvironnement Ardans Knowledge Maker lexemple
1044	Revue des Nouvelles Technologies de l'Information	EGC	2006	Fast-MGB : Nouvelle Base Générique Minimale de Règles Associatives	Le problème de l'exploitation des règles associatives est devenu primordial,puisque le nombre des règles associatives extraites des jeux de donnéesréelles devient très élevé. Une solution possible consiste à ne dériver qu'unebase générique de règles associatives. Cet ensemble de taille réduite permet degénérer toutes les règles associatives via un système axiomatique adéquat. Danscet article, nous proposons une nouvelle approche FAST-MGB qui permet dedériver, directement à partir du contexte d'extraction formel, une base génériqueminimale de règles associatives.	Cherif Chiraz Latiri, Lamia Ben Ghezaiel, Mohamed Ben Ahmed	http://editions-rnti.fr/render_pdf.php?p1&p=1000349	http://editions-rnti.fr/render_pdf.php?p=1000349	problème lexploitation règle associatif devenir primordialpuisqu nombre règle associatif extrait jeu donnéesréelle devenir élever solution consister dériver quunebase générique règle associative ensemble tailler réduit permettre degénérer règle associatif système axiomatique adéquat Danscet article proposer approcher FASTMGB permettre dedériver partir contexte dextraction formel baser génériqueminimal règle associatif
1045	Revue des Nouvelles Technologies de l'Information	EGC	2006	Finding fragments of orders and total orders from 0-1 data	High-dimensional collections of 0-1 data occur in many applications. The attributes insuch data sets are typically considered to be unordered. However, in many cases there is anatural total or partial order underlying the variables of the data set. Examples of variablesfor which such orders exist include terms in documents and paleontological sites in fossil datacollections. We describe methods for finding fragments of total orders from such data, basedon finding frequently occurring patterns. We also discuss techniques for finding good totalorderings (seriation) based on spectral ordering and MCMC methods	Heikki Mannila	http://editions-rnti.fr/render_pdf.php?p1&p=1000315	http://editions-rnti.fr/render_pdf.php?p=1000315	highdimensional collection of 01 dater occur in many application The attribut insuch dater set are typically considered to be unordered However in many cas there is anatural total or partial order underlying the variable of the dater set Examples of variablesfor which such orders exist include term in document and paleontological site in fossil datacollection We describe methods for finding fragment of total orders from such dater basedon finding frequently occurring patterns We also discuss technique for finding good totalorderings seriation based spectral ordering and MCMC method
1046	Revue des Nouvelles Technologies de l'Information	EGC	2006	Fouille de données dans les systèmes Pair-à-Pair pour améliorer la recherche de ressources	La quantité de sources d'information disponible sur Internet fait dessystèmes d'échanges pair-à-pair (P2P) un genre nouveau d'architecture qui offreà une large communauté des applications pour partager des fichiers, des calculs,dialoguer ou communiquer en temps réel. Dans cet article, nous proposonsune nouvelle approche pour améliorer la localisation d'une ressource sur un réseauP2P non structuré. En utilisant une nouvelle heuristique, nous proposonsd'extraire des motifs qui apparaissent dans un grand nombre de noeuds du réseau.Cette connaissance est très utile pour proposer aux utilisateurs des fichierssouvent demandés (en requête ou en téléchargement) et éviter une trop grandeconsommation de la bande passante.	Florent Masseglia, Pascal Poncelet, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000390	http://editions-rnti.fr/render_pdf.php?p=1000390	quantité source dinformation disponible Internet faire dessystèm déchang pairàpair p2p genre darchitecture offreà large communauter application partager fichier calculsdialoguer communiquer temps réel Dans article proposonsune approcher améliorer localisation dune ressource réseaup2p structurer En utiliser heuristique proposonsdextraire motif apparaître dan grand nombre noeud réseaucette connaissance utile proposer utilisateur fichierssouvent demander requête téléchargement éviter grandeconsommation bander passant
1047	Revue des Nouvelles Technologies de l'Information	EGC	2006	Fouille de données spatiales, Approche basée sur la programmation logique inductive	Ce qui caractérise la fouille de données spatiales est la nécessité de prendre en compte les interactions des objets dans l'espace. Les méthodes classiques de fouille de données sont mal adaptées pour ce type d'analyse. Nous proposons dans cet article une approche basée sur la programmation logique inductive. Elle se base sur deux idées. La première consiste à matérialiser ces interactions spatiales dans des tables de distances, ramenant ainsi la fouille de données spatiales à la fouille de fonnées multi-tables. La seconde transforme les données en logique du premier ordre et applique ensuite la programmation logique inductive. Cet article présentera cette approche. Il décrira son application à la classification supervisée par arbre de décision spatial. Il présentera aussi les expérimentations réalisées et les résultats obtenus sur l'analyse de la contamination des coquillages dans la lagune de Thau.	Nadjim Chelghoum, Karine Zeitouni, Thierry Laugier, Annie Fiandrino, Lionel Loubersac	http://editions-rnti.fr/render_pdf.php?p1&p=1000400	http://editions-rnti.fr/render_pdf.php?p=1000400	caractériser fouiller donnée spatial nécessiter prendre compter interaction objet dan lespace méthode classique fouiller donnée mal adapter typer danalyse proposer dan article approcher basé programmation logique inductif baser idée consister matérialiser interaction spatial dan table distance ramener fouiller donnée spatial fouiller fonnée multitabl second transformer donnée logique ordre appliquer ensuite programmation logique inductif article présenter approcher décrire application classification superviser arbre décision spatial présenter expérimentation réaliser résultat obtenir lanalyse contamination coquillage dan lagune Thau
1048	Revue des Nouvelles Technologies de l'Information	EGC	2006	Gestion de connaissances : Compétences et ressources pédagogiques		Olivier Gerbé, Thierno Diarra, Jacques Raynauld	http://editions-rnti.fr/render_pdf.php?p1&p=1000335	http://editions-rnti.fr/render_pdf.php?p=1000335	
1049	Revue des Nouvelles Technologies de l'Information	EGC	2006	Graphes de voisinage pour l'indexation et l'interrogation d'images par le contenu	La découverte d'informations cachées dans les bases de données multimédiasest une tâche difficile à cause de leur structure complexe et à la subjectivitéliée à leur interprétation. Face à cette situation, l'utilisation d'un indexest primordiale. Un index multimédia permet de regrouper les données selondes critères de similarité. Nous proposons dans cet article d'apporter une améliorationà une approche déjà existante d'interrogation d'images par le contenu .Nous proposons une méthode efficace pour mettre à jour, localement, les graphesde voisinage qui constituent notre structure d'index multimédia. Cette méthodeest basée sur une manière intelligente de localisation de points dans un espacemultidimensionnel. Des résultats prometteurs sont obtenus après des expérimentationssur diverses bases de données.	Hakim Hacid, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1000318	http://editions-rnti.fr/render_pdf.php?p=1000318	découvrir dinformation cacher dan base donnée multimédiasest tâcher difficile causer structurer complexe subjectivitéliée interprétation face situation lutilisation dun indexest primordial index multimédia permettre regrouper donnée selond critère similarité proposer dan article dapporter améliorationà approcher déjà existant dinterrogation dimager contenir proposer méthode efficace mettre jour localement graphesde voisinage constituer structurer dindex multimédier méthodeest baser manière intelligent localisation point dan espacemultidimensionnel résultat prometteur obtenir expérimentationssur base donnée
1050	Revue des Nouvelles Technologies de l'Information	EGC	2006	I-Semantec : une plateforme collaborative de capitalisation des connaissances métier en conception de produits industriels		Mohamed-Foued Sriti, Philippe Boutinaud, Nada Matta, Manuel Zacklad	http://editions-rnti.fr/render_pdf.php?p1&p=1000441	http://editions-rnti.fr/render_pdf.php?p=1000441	
1051	Revue des Nouvelles Technologies de l'Information	EGC	2006	Indexation de vues virtuelles dans un médiateur XML pour le traitement de XQuery Text	Intégrer le traitement de requêtes de recherche d'information dans unmédiateur XML est un problème difficile. Ceci est notamment dû au fait quecertaines sources de données ne permettent pas de recherche sur mot-clefs etdistance ni de classer les résultats suivant leur pertinence. Dans cet article nousabordons l'intégration des fonctionnalités principales du standard XQuery Textdans XLive, un médiateur XML/XQuery. Pour cela nous avons choisid'indexer des vues virtuelles de documents. Les documents virtuelssélectionnés sont transformés en objets des sources. L'opérateur de sélectiondu médiateur est étendu pour supporter des recherches d'information sur lesdocuments de la vue. La recherche sur mots-clefs et le classement de résultatsont ainsi supportés. Notre formule de classement de résultats est adaptée auformat de données semi-structurées, basé sur le nombre de mots-clefs dans lesdifférents éléments et la distance entre les éléments d'un résultat.	Clément Jamard, Georges Gardarin	http://editions-rnti.fr/render_pdf.php?p1&p=1000325	http://editions-rnti.fr/render_pdf.php?p=1000325	intégrer traitement requête rechercher dinformation dan unmédiateur xml problème difficile devoir faire quecertaines source donnée permettre rechercher motclef etdistance classer résultat pertinence Dans article nousabordon lintégration fonctionnalité principal standard xquery textdan xlive médiateur xmlxquery Pour celer choisidindexer virtuelle document document virtuelssélectionner transformer objet source Lopérateur sélectiondu médiateur étendre supporter recherche dinformation lesdocument rechercher motsclef classement résultatsont supporter formuler classement résultat adapter auformat donnée semistructurer baser nombre motsclef dan lesdifférent élément distancer entrer élément dun résultat
1052	Revue des Nouvelles Technologies de l'Information	EGC	2006	Interrogation et Vérification de documents OWL dans le modèle des Graphes Conceptuels	OWL est un langage pour la description d'ontologies sur le Web. Cependant,en tant que langage, OWL ne fournit aucun moyen pour interpréter lesontologies qu'il décrit, et étant orienté machine, il reste difficilement compréhensiblepar l'humain. On propose une approche de visualisation, d'interrogationet de vérification de documents OWL, regroupées dans un unique environnementgraphique : le modèle des graphes conceptuels.	Thomas Raimbault, Henri Briand, Rémi Lehn, Stéphane Loiseau	http://editions-rnti.fr/render_pdf.php?p1&p=1000342	http://editions-rnti.fr/render_pdf.php?p=1000342	owl langage description dontologier Web Cependanten langage OWL fournir moyen interpréter lesontologie quil décrire orienter machin rester difficilement compréhensiblepar lhumain proposer approcher visualisation dinterrogationet vérification document OWL regrouper dan environnementgraphiqu   modeler graphe conceptuel
1053	Revue des Nouvelles Technologies de l'Information	EGC	2006	La fouille de graphes dans les bases de données réactionnelles au service de la synthèse en chimie organique	La synthèse en chimie organique consiste à concevoir de nouvellesmolécules à partir de réactifs et de réactions. Les experts de la synthèse s'appuientsur de très grandes bases de données de réactions qu'ils consultent à traversdes procédures d'interrogation standard. Un processus de découverte denouvelles réactions leur permettrait de mettre au point de nouveaux procédés desynthèse. Cet article présente une modélisation des réactions par des graphes etintroduit une méthode de fouille de ces graphes de réaction qui permet de faireémerger des motifs génériques utiles à la prédiction de nouvelles réactions. Enfinl'article fait le point sur l'état actuel de ce travail de recherche en présentantle modèle général dans lequel s'intégrera un nouvel algorithme de fouille deréactions chimiques.	Frédéric Pennerath, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1000398	http://editions-rnti.fr/render_pdf.php?p=1000398	synthèse chimie organique consister concevoir nouvellesmolécule partir réactif réaction expert synthèse sappuientsur grand base donnée réaction quils consulter traversde procédure dinterrogation standard processus découvrir denouvell réaction permettre mettre poindre procédé desynthèse article présenter modélisation réaction graphe etintroduit méthode fouiller graphe réaction permettre faireémerger motif générique utile prédiction réaction Enfinlarticle faire poindre létat actuel travail rechercher présentantle modeler général dan sintégrer nouvel algorithme fouiller deréactions chimique
1054	Revue des Nouvelles Technologies de l'Information	EGC	2006	Le forage distribué des données : une méthode simple, rapide et efficace	Dans cet article nous nous attaquons au problème du forage de trèsgrandes bases de données distribuées. Le résultat visé est un modèle qui soit etprédictif et descriptif, appelé méta-classificateur. Pour ce faire, nous proposonsde miner à distance chaque base de données indépendamment. Puis, il s'agitde regrouper les modèles produits (appelés classificateurs de base), sachant quechaque forage produira un modèle prédictif et descriptif, représenté pour nos besoinspar un ensemble de règles de classification. Afin de guider l'assemblage del'ensemble final de règles, qui sera l'union des ensembles individuels de règles,un coefficient de confiance est attribué à chaque règle de chaque ensemble. Cecoefficient, calculé par des moyens statistiques, représente la confiance que nouspouvons avoir dans chaque règle en fonction de sa couverture et de son taux d'erreurface à sa capacité d'être appliquée correctement sur de nouvelles données.Nous démontrons dans cet article que, grâce à ce coefficient de confiance, l'agrégationpure et simple de tous les classificateurs de base pour obtenir un agrégatde règles produit un méta-classificateur rapide et efficace par rapport aux techniquesexistantes.	Mohamed Aounallah, Guy Mineau	http://editions-rnti.fr/render_pdf.php?p1&p=1000328	http://editions-rnti.fr/render_pdf.php?p=1000328	Dans article attaquer problème forage trèsgrande base donnée distribuer résultat viser modeler etprédictif descriptif appeler métaclassificateur Pour faire proposonsde miner distancer baser donnée indépendammer Puis sagitde regrouper modèle produit appeler classificateur baser savoir quechaqu forage produire modeler prédictif descriptif représenter besoinspar ensemble règle classification Afin guider lassemblage delensembl final règle lunion ensemble individuel règlesun coefficient confiance attribuer régler ensemble Cecoefficient calculer moyen statistique représenter confiance nouspouvon dan régler fonction couverture taux derreurfac capacité dêtre appliquer correctement donnéesnou démontrer dan article grâce coefficient confiance lagrégationpur simple tou classificateur baser obtenir agrégatde régler produire métaclassificateur rapide efficace rapport techniquesexistant
1055	Revue des Nouvelles Technologies de l'Information	EGC	2006	Maintaining an Online Bibliographical Database: The Problem of Data Quality	CiteSeer and Google-Scholar are huge digital libraries which provideaccess to (computer-)science publications. Both collections are operated likespecialized search engines, they crawl the web with little human interventionand analyse the documents to classify them and to extract some metadata fromthe full texts. On the other hand there are traditional bibliographic data baseslike INSPEC for engineering and PubMed for medicine. For the field of computerscience the DBLP service evolved from a small specialized bibliographyto a digital library covering most subfields of computer science. The collectionsof the second group are maintained with massive human effort. On the longterm this investment is only justified if data quality of the manually maintainedcollections remains much higher than that of the search engine style collections.In this paper we discuss management and algorithmic issues of data quality. Wefocus on the special problem of person names	Michael Ley, Patrick Reuther	http://editions-rnti.fr/render_pdf.php?p1&p=1000317	http://editions-rnti.fr/render_pdf.php?p=1000317	citeseer and GoogleScholar are huge digital librari which provideaccess to computerscience publicater both collection are operated likespecialized search engin they crawl the web with little human interventionand analyser the document to classify them and to extract som metadata fromth full text the other hand there are traditional bibliographic dater baseslike inspec for engineering and PubMed for medicine For the field of computerscience the DBLP service evolved from small specialized bibliographyto digital library covering most subfield of computer science The collectionsof the second group are maintained with massif human effort the longterm this investment is only justified if dater quality of the manually maintainedcollection remain much higher than that of the search engine styler collectionsIn this paper we discus management and algorithmic issu of dater quality Wefocus the special problem of person nam
1056	Revue des Nouvelles Technologies de l'Information	EGC	2006	Méthode de récolte de traces de navigation sur interface graphique et visualisation de parcours		Marc Damez	http://editions-rnti.fr/render_pdf.php?p1&p=1000452	http://editions-rnti.fr/render_pdf.php?p=1000452	
1057	Revue des Nouvelles Technologies de l'Information	EGC	2006	Modèle conceptuel pour bases de données multidimensionnelles annotées	Nos travaux visent à proposer une mémoire d'expertises décisionnellespermettant de conserver et de manipuler non seulement les données décisionnellesmais aussi l'expertise analytique des décideurs. Les données décisionnellessont représentées au travers de concepts multidimensionnels etl'expertise associée est matérialisée grâce au concept d'annotation	Guillaume Cabanac, Max Chevalier, Franck Ravat, Olivier Teste	http://editions-rnti.fr/render_pdf.php?p1&p=1000330	http://editions-rnti.fr/render_pdf.php?p=1000330	travail viser proposer mémoire dexpertis décisionnellespermetter conserver manipuler donnée décisionnellesmais lexpertise analytique décideur donnée décisionnellessont représenter travers concept multidimensionnel etlexpertise associer matérialiser grâce concept dannotation
1058	Revue des Nouvelles Technologies de l'Information	EGC	2006	Modèle décisionnel basé sur la qualité des données pour sélectionner les règles d'associations légitimement intéressantes	Dans cet article nous proposons d'exploiter des mesures décrivant laqualité des données pour définir la qualité des règles d'associations résultantd'un processus de fouille. Nous proposons un modèle décisionnel probabilistebasé sur le coût de la sélection de règles légitimement, potentiellement intéressantesou inintéressantes si la qualité des données à l'origine de leur calcul estbonne, moyenne ou douteuse. Les expériences sur les données de KDD-CUP-98 montrent que les 10 meilleures règles sélectionnées d'après leurs mesuresde support et confiance ne sont intéressantes que dans le cas où la qualité deleurs données est correcte voire améliorée.	Laure Berti-Equille	http://editions-rnti.fr/render_pdf.php?p1&p=1000410	http://editions-rnti.fr/render_pdf.php?p=1000410	Dans article proposer dexploiter mesure décrire laqualité donnée définir qualité règle dassociation résultantdun processus fouiller proposer modeler décisionnel probabilistebaser coût sélection règle légitimemer potentiellement intéressantesou inintéressant qualité donnée lorigine calcul estbonn moyen douteux expérience donnée kddcup98 montrer 10 meilleure régler sélectionner daprès mesuresde support confiance intéressant dan cas qualité deleurs donner correct voire améliorer
1059	Revue des Nouvelles Technologies de l'Information	EGC	2006	Modélisation informationnelle : un cadre méthodologique pour représenter des connaissances évolutives spatialisables	Pour comprendre et représenter les évolutions du bâti, question renouvelée avec le développement des NTIC, l'analyste s'appuie sur des connaissances évolutives ayant dans notre champ d'application - le patrimoine architectural – un caractère spatialisable (par l'attachement à un lieu lambda) mais aussi des caractéristiques handicapantes (hétérogénéité, incertitudes et contradictions, etc.). En réponse, nous utilisons ce caractère spatialisable pour intégrer les ressources constituant le jeu de connaissances propre à chaque édifice: théorie, sources documentaires, observations. Cette démarche que nous nommons modélisation informationnelle a pour objectif un gain de compréhension du lieu architectural et des informations qui lui sont associées. Notre contribution introduit les filiations de cette démarche, le cadre méthodologique qui la matérialise, et discute de son application au cas concret de la place centrale de Cracovie (Rynek Glowny) pour en évaluer l'apport potentiel en matière de gestion et de visualisation de connaissances.	Jean-Yves Blaise, Iwona Dudek	http://editions-rnti.fr/render_pdf.php?p1&p=1000368	http://editions-rnti.fr/render_pdf.php?p=1000368	Pour comprendre représenter évolution bâtir question renouveler développement ntic lanalyst sappuie connaissance évolutif dan champ dapplication   patrimoine architectural – caractère spatialisabl lattachement lieu lambda caractéristique handicapant hétérogénéité incertitud contradiction En réponse utiliser caractère spatialisabl intégrer ressource constituer jeu connaissance propre édifice théori source documentaire observation démarcher nommer modélisation informationnel objectif gain compréhension lieu architectural information luire associer contribution introduire filiation démarcher cadrer méthodologique matérialiser discuter application cas concret placer central Cracovie Rynek Glowny évaluer lapport potentiel matière gestion visualisation connaissance
1060	Revue des Nouvelles Technologies de l'Information	EGC	2006	Multi-catégorisation de textes juridiques et retour de pertinence	La fouille de données textuelles constitue un champ majeur dutraitement automatique des données. Une large variété de conférences, commeTREC, lui sont consacrées. Dans cette étude, nous nous intéressons à la fouillede textes juridiques, dans l'objectif est le classement automatique de ces textes.Nous utilisons des outils d'analyses linguistiques (extraction de terminologie)dans le but de repérer les concepts présents dans le corpus. Ces conceptspermettent de construire un espace de représentation de faible dimensionnalité,ce qui nous permet d'utiliser des algorithmes d'apprentissage basés sur desmesures de similarité entre individus, comme les graphes de voisinage. Nouscomparons les résultats issus du graphe et de C4.5 avec les SVM qui eux sontutilisés sans réduction de la dimensionnalité.	Vincent Pisetta, Hakim Hacid, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1000353	http://editions-rnti.fr/render_pdf.php?p=1000353	fouiller donnée textuel constituer champ majeur dutraitement automatique donnée large variété conférence commetrec luire consacrer Dans étude intéresser fouillede texter juridiquer dan lobjectif classement automatique textesnou utiliser outil danalyse linguistique extraction terminologiedan boire repérer concept présent dan corpus conceptspermetter construire espacer représentation faible dimensionnalitéce permettre dutiliser algorithme dapprentissage basé desmesure similarité entrer individu graphe voisinage Nouscomparons résultat issu graphe c45 svm sontutiliser réduction dimensionnalité
1061	Revue des Nouvelles Technologies de l'Information	EGC	2006	Outil de datamining spatial appliqué à l'analyse des risques liés au territoire		Schahrazed Zeghache, Farida Admane, Kamel Elaraba Ziane	http://editions-rnti.fr/render_pdf.php?p1&p=1000442	http://editions-rnti.fr/render_pdf.php?p=1000442	
1062	Revue des Nouvelles Technologies de l'Information	EGC	2006	Prédiction de solubilité de molécules à partir des seules données relationnelles	La recherche de médicaments passe par la synthèse de molécules candidatesdont l'efficacité est ensuite testée. Ce processus peut être accéléré enidentifiant les molécules non solubles, car celles-ci ne peuvent entrer dans lacomposition d'un médicament et ne devraient donc pas être étudiées. Des techniquesont été développées pour induire un modèle de prédiction de l'indice desolubilité, utilisant principalement des réseaux de neurones ou des régressionslinéaires multiples. La plupart des travaux actuels visent à enrichir les donnéesde caractéristiques supplémentaires sur les molécules. Dans cet article, nous étudionsl'intérêt de la construction automatique d'attributs basée sur la structureintrinsèquement multi-relationnelle des données. Les attributs obtenus sont utilisésdans un algorithme d'arbre de modèles, auquel on associe une méthodede bagging. Les tests réalisés montrent que ces méthodes donnent des résultatscomparables aux meilleures méthodes du domaine qui travaillent sur des attributsconstruits par les experts.	Sébastien Derivaux, Agnès Braud, Nicolas Lachiche	http://editions-rnti.fr/render_pdf.php?p1&p=1000424	http://editions-rnti.fr/render_pdf.php?p=1000424	rechercher médicament passer synthèse molécule candidatesdont lefficacité ensuite tester processus pouvoir accélérer enidentifiant molécule soluble cellesci pouvoir entrer dan lacomposition dun médicament devoir étudier techniquesont développer induire modeler prédiction lindice desolubilité utiliser principalement réseau neurone régressionslinéaire travail actuel viser enrichir donnéesde caractéristique supplémentaire molécule Dans article étudionslintérêt construction automatique dattributs baser structureintrinsèquement multirelationnelle donnée attribut obtenir utilisésdan algorithme darbre modèle associer méthodede bagging test réaliser montrer méthode donner résultatscomparable meilleur méthode domaine travailler attributsconstruit expert
1063	Revue des Nouvelles Technologies de l'Information	EGC	2006	Préparation des données Radar pour la reconnaissance/identification de cibles aériennes	La problématique générale présentée dans ce papier concerne lessystèmes intelligents, dédiés pour l'aide à la prise de décision dans le domaineradar. Les premiers travaux ont donc consisté après avoir adapté le processusd'extraction de connaissances à partir de données (ECD) au domaine radar, àmettre en oeuvre les étapes en amont de la phase de fouille de données. Nousnous limitons dans ce papier à la phase de préparation des données (imagesISAR : Inverse Synthetic Aperture Radar). Nous introduisons ainsi la notion dequalité comme moyen d'évaluer l'imperfection dans les données radarsexpérimentales.	Abdelmalek Toumi, Brigitte Hoeltzener, Ali Khenchaf	http://editions-rnti.fr/render_pdf.php?p1&p=1000426	http://editions-rnti.fr/render_pdf.php?p=1000426	problématique général présenter dan papier concerner lessystèm intelligent dédier laid priser décision dan domaineradar travail consister adapter processusdextraction connaissance partir donnée ecd domaine radar àmettre oeuvrer étape amont phase fouiller donnée nousnou limiter dan papier phase préparation donnée imagesisar   Inverse Synthetic Aperture Radar introduire notion dequaliter moyen dévaluer limperfection dan donnée radarsexpérimental
1064	Revue des Nouvelles Technologies de l'Information	EGC	2006	Prétraitement de grands ensembles de données pour la fouille visuelle	Nous présentons une nouvelle approche pour le traitement des ensemblesde données de très grande taille en fouille visuelle de données. Les limitesde l'approche visuelle concernant le nombre d'individus et le nombre dedimensions sont connues de tous. Pour pouvoir traiter des ensembles de donnéesde grande taille, une solution possible est d'effectuer un prétraitement del'ensemble de données avant d'appliquer l'algorithme interactif de fouille visuelle.Pour ce faire, nous utilisons la théorie du consensus (avec une affectationvisuelle des poids). Nous évaluons les performances de notre nouvelle approchesur des ensembles de données de l'UCI et du Kent Ridge Bio MedicalDataset Repository.	François Poulet, Edwige Fangseu Badjio	http://editions-rnti.fr/render_pdf.php?p1&p=1000324	http://editions-rnti.fr/render_pdf.php?p=1000324	présenter approcher traitement ensemblesde donner grand tailler fouiller visuel donnée limitesde lapproch visuel concerner nombre dindividus nombre dedimensions connu tou Pour pouvoir traiter ensemble donnéesde grand tailler solution deffectuer prétraitement delensembl donnée dappliquer lalgorithme interactif fouiller visuellepour faire utiliser théorie consensus affectationvisuelle poids évaluer performance approchesur ensemble donnée luci Kent Ridge Bio MedicalDataset Repository
1065	Revue des Nouvelles Technologies de l'Information	EGC	2006	Recherche de règles non redondantes par vecteurs de bits dans des grandes bases de motifs 		François Jacquenet, Christine Largeron, Cédric Udréa	http://editions-rnti.fr/render_pdf.php?p1&p=1000414	http://editions-rnti.fr/render_pdf.php?p=1000414	
1066	Revue des Nouvelles Technologies de l'Information	EGC	2006	Recherche de sous-structures fréquentes pour l'intégration de schémas XML	La recherche d'un schéma médiateur à partir d'un ensemble de schémasXML est une problématique actuelle où les résultats de recherche issusde la fouille de données arborescentes peuvent être adoptés. Dans ce contexte,plusieurs propositions ont été réalisées mais les méthodes de représentation desarborescences sont souvent trop coûteuses pour permettre un véritable passageà l'échelle. Dans cet article, nous proposons des algorithmes de recherche desous-schémas fréquents basés sur une méthode originale de représentation deschémas XML. Nous décrivons brièvement la structure adoptée pour ensuitedétailler les algorithmes de recherche de sous-arbres fréquents s'appuyant surune telle structure. La représentation proposée et les algorithmes associés ontété évalués sur différentes bases synthétiques de schémas XML montrant ainsil'intérêt de l'approche proposée	Federico Del Razo López, Anne Laurent, Pascal Poncelet, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000393	http://editions-rnti.fr/render_pdf.php?p=1000393	rechercher dun schéma médiateur partir dun ensemble schémasxml problématique actuel résultat rechercher issusde fouiller donnée arborescent pouvoir adopter Dans contexteplusieur proposition réaliser méthode représentation desarborescenc coûteuse permettre véritable passageà léchell Dans article proposer algorithme rechercher desousschémer fréquent baser méthode original représentation deschéma xml décrivon brièvement structurer adopter ensuitedétailler algorithme rechercher sousarbre fréquent sappuyer surune structurer représentation proposer algorithme associé ontété évaluer base synthétique schéma xml montrer ainsilintérêt lapproche proposer
1067	Revue des Nouvelles Technologies de l'Information	EGC	2006	Recherche en temps réel de préfixes massifs hiérarchiques dans un réseau IP à l'aide de techniques de stream mining	Au cours de ces dernières années, de nombreuses techniques de streammining ont été proposées afin d'analyser des flux de données en temps réel.Dans cet article, nous montrons comment nous avons utilisé des techniques destream mining permettant la recherche d'objets massifs hiérarchiques (hierarchicalheavy hitters) dans un flux de données pour identifier en temps réel dans unréseau IP les préfixes dont la contribution au trafic dépasse une certaine proportionde ce trafic pendant un intervalle de temps donné.	Pascal Cheung-Mon-Chan, Fabrice Clérot	http://editions-rnti.fr/render_pdf.php?p1&p=1000323	http://editions-rnti.fr/render_pdf.php?p=1000323	Au cours année technique streammining proposer danalyser flux donnée temps réelDans article montrer utiliser technique destream mining permettre rechercher dobjet massif hiérarchique hierarchicalheavy hitter dan flux donnée identifier temps réel dan unréseau ip préfixe contribution trafic dépasser proportiond trafic pendre intervalle temps donner
1068	Revue des Nouvelles Technologies de l'Information	EGC	2006	Reconnaissance automatique d'évènements survenant sur patients en réanimation à l'aide d'une méthode adaptative d'extraction en ligne d'épisodes temporels	Ce papier présente la version adaptative d'un algorithmed'extraction d'épisodes temporels développé précédemment. Les trois paramè-tres de réglages de l'algorithme ne sont plus fixes. Ils sont modifiés en ligne enfonction de la variance estimée du signal que l'on veut décomposer en épiso-des temporels. La version adaptative de l'algorithme a été utilisée pour recon-naître automatiquement des aspirations trachéales à partir de plusieures varia-bles physiologiques enregistrés sur des patients hospitalisés en réanimation.Des résultats préliminaires sont présentés dans ce papier.	Sylvie Charbonnier	http://editions-rnti.fr/render_pdf.php?p1&p=1000333	http://editions-rnti.fr/render_pdf.php?p=1000333	papier présenter version adaptatif dun algorithmedextraction dépisod temporel développer précédemment paramètre réglage lalgorithme plaire fixer modifier ligne enfonction variance estimer signal lon vouloir décomposer épisode temporel version adaptatif lalgorithme utiliser reconnaître automatiquement aspiration trachéal partir plusieure variable physiologique enregistrer patient hospitaliser réanimationdes résultat préliminaire présenter dan papier
1069	Revue des Nouvelles Technologies de l'Information	EGC	2006	Reconnaissance automatique de concepts à partir d'une ontologie	Ce papier présente une approche qui s'appuie sur une ontologie pourreconnaître automatiquement des concepts spécifiques à un domaine dans uncorpus en langue naturelle. La solution proposée est non-supervisée et peuts'appliquer à tout domaine pour lequel une ontologie a été déjà construite. Uncorpus du domaine est utilisé dans lequel les concepts seront reconnus. Dansune première phase, des connaissances sont extraites de ce corpus en faisantappel à des fouilles de textes. Une ontologie du domaine est utilisée pour étiqueterces connaissance. Le papier donne un aperçu des techniques de fouillesemployées et décrit le processus d ‘étiquetage. Les résultats d‘une premièreexpérimentation dans le domaine de l'accidentologie sont aussi présentés	Valentina Ceausu, Sylvie Desprès	http://editions-rnti.fr/render_pdf.php?p1&p=1000351	http://editions-rnti.fr/render_pdf.php?p=1000351	papier présenter approcher sappuie ontologie pourreconnaître automatiquement concept spécifique domaine dan uncorpu langue solution proposer nonsuperviser peutsappliquer domaine ontologie déjà construire uncorpu domaine utiliser dan concept reconnaître Dansune phase connaissance extraire corpus faisantappel fouille texte ontologie domaine utiliser étiqueterce connaissance papier donner apercevoir technique fouillesemployée décrire processus ‘ étiquetage résultat d‘une premièreexpérimentation dan domaine laccidentologie présenter
1070	Revue des Nouvelles Technologies de l'Information	EGC	2006	Règles d'association avec une prémisse composée : Mesure du gain d'information.	La communauté de fouille de données a développé un grand nombre d'indices permettantde mesurer la qualité des règles d'association (RA) selon diverses sémantiques (Guillet,2004). Cependant ces sémantiques, qui permettent d'interpréter les règles simples, s'avèrentd'utilisation trop complexe pour un expert dans le cas de règles à prémisse composée. Notreobjectif est donc de sélectionner les règles à prémisse composée de type AB&#8594;C quiapportent une information supplémentaire à celle des règles simples A&#8594;C et B&#8594;C. Pourcela nous définissons un indice de gain d'une règle composée par rapport aux règles simples.Dans l'application présentée, nous extrayons des RA de résultats de classifications pouren faciliter l'analyse . Le gain a permis de filtrer des règles d'interprétation simple	Martine Cadot, Pascal Cuxac, Claire François	http://editions-rnti.fr/render_pdf.php?p1&p=1000412	http://editions-rnti.fr/render_pdf.php?p=1000412	communauté fouiller donnée développer grand nombre dindice permettantde mesurer qualité règle dassociation ra sémantique Guillet2004 sémantique permettre dinterpréter règle simple savèrentdutilisation complexe expert dan cas règle prémisse composer Notreobjectif sélectionner règle prémisse composer typer ab8594c quiapportent information supplémentaire règle simple a8594c B8594C Pourcela définir indice gain dune régler composer rapport règle simplesdan lapplication présenter extraire RA résultat classification pouren faciliter lanalyse   gain permettre filtrer règle dinterprétation simple
1071	Revue des Nouvelles Technologies de l'Information	EGC	2006	Représentation d'expertise psychologique sous la forme de graphes orientés, codés en RDF		Yves Fossé, Stéphane Daviet, Henri Briand, Fabrice Guillet	http://editions-rnti.fr/render_pdf.php?p1&p=1000434	http://editions-rnti.fr/render_pdf.php?p=1000434	
1072	Revue des Nouvelles Technologies de l'Information	EGC	2006	Représentation des connaissances appliquées à la géotechnique : une approche		Nicolas Faure	http://editions-rnti.fr/render_pdf.php?p1&p=1000436	http://editions-rnti.fr/render_pdf.php?p=1000436	
1073	Revue des Nouvelles Technologies de l'Information	EGC	2006	Sélection de variables et modélisation d'expression d'émotion dans les dialogues Homme-Machine		Barbara Poulain	http://editions-rnti.fr/render_pdf.php?p1&p=1000438	http://editions-rnti.fr/render_pdf.php?p=1000438	
1074	Revue des Nouvelles Technologies de l'Information	EGC	2006	Sélection supervisée d'instances : une approche descriptive	La classification suivant le plus proche voisin est une règle simple etperformante. Sa mise en oeuvre pratique nécessite, tant pour des raisons de coûtde calcul que de robustesse, de sélectionner les instances à conserver. La partitionde Voronoi induite par les prototypes constitue la structure sous-jacente àcette règle. Dans cet article, on introduit un critère descriptif d'évaluation d'unetelle partition, quantifiant le compromis entre nombre de cellules et discriminationde la variable cible entre les cellules. Une heuristique d'optimisation estproposée, tirant partie des propriétés des partitions de Voronoi et du critère. Laméthode obtenue est comparée avec les standards sur une vingtaine de jeux dedonnées de l'UCI. Notre technique ne souffre d'aucun défaut de performanceprédictive, tout en sélectionnant un minimum d'instances. De plus, elle ne surapprendpas.	Sylvain Ferrandiz, Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1000382	http://editions-rnti.fr/render_pdf.php?p=1000382	classification plaire voisin régler simple etperformante miser oeuvrer pratiquer nécessit raison coûtde calcul robustesse sélectionner instance conserver partitionde Voronoi induire prototype constituer structurer sousjacent àcette régler Dans article introduire critère descriptif dévaluation dunetell partition quantifier compromis entrer nombre cellule discriminationde variable cibl entrer cellule heuristique doptimisation estproposer tirer partir propriété partition Voronoi critère Laméthode obtenir comparer standard vingtaine jeu dedonner luci technique souffrir daucun défaut performanceprédictive sélectionner minimum dinstancer De plaire surapprendpas
1075	Revue des Nouvelles Technologies de l'Information	EGC	2006	SVM incrémental, parallèle et distribué pour le traitement de grandes quantités de données	Nous présentons un nouvel algorithme de SVM (Support VectorMachine ou Séparateur à Vaste Marge) linéaire et non-linéaire, parallèle etdistribué permettant le traitement de grands ensembles de données dans untemps restreint sur du matériel standard. A partir de l'algorithme de Newton-GSVM proposé par Mangasarian, nous avons construit un algorithmeincrémental, parallèle et distribué permettant d'améliorer les performances entemps d'exécution et mémoire en s'exécutant sur un groupe d'ordinateurs. Cenouvel algorithme a la capacité de classifier un million d'individus en 20dimensions et deux classes en quelques secondes sur un ensemble de dix PC	Thanh-Nghi Do, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1000322	http://editions-rnti.fr/render_pdf.php?p=1000322	présenter nouvel algorithme svm Support VectorMachine séparateur Vaste marge linéaire nonlinéair parallèle etdistribué permettre traitement grand ensemble donnée dan untemps restreindre matériel standard A partir lalgorithme newtongsvm proposer Mangasarian construire algorithmeincrémental parallèle distribuer permettre daméliorer performance entemp dexécution mémoire sexécuter grouper dordinateurs cenouvel algorithme capacité classifier million dindividus 20dimension classe seconde ensemble PC
1076	Revue des Nouvelles Technologies de l'Information	EGC	2006	Système d'aide à la décision pour la surveillance de la qualité de l'air intérieur		Zoulikha Heddadji, Nicole Vincent, Séverine Kirchner, Georges Stamon	http://editions-rnti.fr/render_pdf.php?p1&p=1000445	http://editions-rnti.fr/render_pdf.php?p=1000445	
1077	Revue des Nouvelles Technologies de l'Information	EGC	2006	Techniques de fouille de données pour la réécriture de requêtes en présence de contraintes de valeurs	Dans cet article, nous montrons comment les techniques de fouilles de données peuvent résoudre efficacement le problème de la réécriture de requêtes en termes de vues en présence de contraintes de valeurs. A partir d'une formalisation du problème de la réécriture dans le cadre de la logique de description ALN(Ov), nous montrons comment ce problème se rattache à un cadre de découverte de connaissances dans les bases de données. L'exploitation de ce cadre nous permet de bénéficier de solutions algorithmiques existantes pour la résolution du problème de réécriture. Nous proposons une implémentation de cette approche, puis nous l'expérimentons. Les premiers résultats démontrent l'intérêt d'une telle approche en termes de capacité à traiter un grand nombre de sources de données.	Hélène Jaudoin, Frédéric Flouvat	http://editions-rnti.fr/render_pdf.php?p1&p=1000326	http://editions-rnti.fr/render_pdf.php?p=1000326	Dans article montrer technique fouille donnée pouvoir résoudre efficacement problème réécriture requête terme présence contrainte partir dune formalisation problème réécriture dan cadrer logique description alnov montrer problème rattacher cadrer découvrir connaissance dan base donnée lexploitation cadrer permettre bénéficier solution algorithmique existant résolution problème réécriture proposer implémentation approcher pouvoir lexpérimenter résultat démontrer lintérêt dune approcher terme capacité traiter grand nombre source donnée
1078	Revue des Nouvelles Technologies de l'Information	EGC	2006	Teximus Expertise : un logiciel de gestion de connaissances	Le logiciel Teximus Expertise est un outil évolué de gestion dynamiquede connaissances basé sur les notions de référentiel sémantique. Cette suiteintégrée facilite le partage de connaissances et d'informations dans les entreprises.	Olivier Gerbé	http://editions-rnti.fr/render_pdf.php?p1&p=1000453	http://editions-rnti.fr/render_pdf.php?p=1000453	logiciel Teximus Expertise outil évoluer gestion dynamiqued connaissance baser notion référentiel sémantique suiteintégré faciliter partager connaissance dinformation dan entreprise
1079	Revue des Nouvelles Technologies de l'Information	EGC	2006	Typicalité et contribution des sujets et des variables supplémentaires en Analyse Statistique Implicative	L'analyse statistique implicative traite des tableaux sujets xvariables afin d'extraire règles et métarègles statistiques entre les variables.L'article interroge les structures obtenues représentées par graphe et hiérarchieorientés afin de dégager la responsabilité des sujets ou des groupes de sujets(variables supplémentaires) dans la constitution des chemins du graphe ou desclasses de la hiérarchie. On distingue les concepts de typicalité pour signifier laproximité des sujets avec le comportement moyen de la population envers lesrègles statistiques extraites, puis de contribution pour quantifier le rôlequ'auraient les sujets par rapport aux règles strictes associées. Un exemple dedonnées réelles, traité à l'aide du logiciel CHIC, illustre et montre l'intérêt deces deux concepts.	Régis Gras, Jérôme David, Jean-Claude Régnier, Fabrice Guillet	http://editions-rnti.fr/render_pdf.php?p1&p=1000370	http://editions-rnti.fr/render_pdf.php?p=1000370	lanalyse statistique implicatif traire tableau xvariabl dextraire règle métarègl statistique entrer variableslarticle interrog structure obtenu représenter graph hiérarchieorienter dégager responsabilité groupe sujetsvariabl supplémentaire dan constitution chemin graphe desclass hiérarchie distinguer concept typicalité signifier laproximité comportement moyen population lesrègl statistique extraite pouvoir contribution quantifier rôlequauraient rapport règle strict associé exemple dedonné réel traiter laid logiciel CHIC illustre montr lintérêt dece concept
1080	Revue des Nouvelles Technologies de l'Information	EGC	2006	Un automate pour évaluer la nature des textes	On ne peut s'intéresser aux textes sans s'intéresser à leur nature. La nature des textes permet de distinguer les textes d'un point de vue primaire. Elle est utilisée pour identifier les textes artificiels, pour la reconnaissance de la langue, afin d'identifier les SPAMS... En ce sens, la méthode la plus connue reste encore la méthode de Zipf. Cet article propose une nouvelle méthode basée sur un automate. L'automate construit un signal pour chaque texte. L'automate est présenté en détail et des expérimentations montrent son utilité dans les domaines aussi divers que ceux cités précédemment/	Hubert Marteau, Nicole Vincent	http://editions-rnti.fr/render_pdf.php?p1&p=1000355	http://editions-rnti.fr/render_pdf.php?p=1000355	pouvoir sintéresser texter sintéresser nature nature texte permettre distinguer texte dun poindre primaire utiliser identifier texte artificiel reconnaissance langue didentifier spam En sens méthode plaire connaître rester méthode zipf article proposer méthode baser automate Lautomate construire signal texte Lautomate présenter détail expérimentation montrer utilité dan domaine cité précédemment
1081	Revue des Nouvelles Technologies de l'Information	EGC	2006	Un logiciel permettant d'apprendre des règles et leurs exceptions : Area		Sylvain Lagrue, Jérémie Lussiez, Julien Rossit	http://editions-rnti.fr/render_pdf.php?p1&p=1000454	http://editions-rnti.fr/render_pdf.php?p=1000454	
1082	Revue des Nouvelles Technologies de l'Information	EGC	2006	Un modèle de qualité de l'information	Ce travail s'intègre dans la problématique générale de la recherched'information ; et plus particulièrement dans la personnalisation et la qualitéd'information. Dans cet article nous proposons un modèle multidimensionnelde la qualité de l'information décrivant les différents facteurs de qualité influantsur la personnalisation de l'information. Ce modèle permet de structurerles différents facteurs de qualité de l'information dans une hiérarchie afind'assister l'utilisateur dans la construction de son propre profil selon ses besoinset ses exigences en termes de qualité.	Rami Harrathi, Sylvie Calabretto	http://editions-rnti.fr/render_pdf.php?p1&p=1000363	http://editions-rnti.fr/render_pdf.php?p=1000363	travail sintègre dan problématique général recherchedinformation   plaire dan personnalisation qualitédinformation Dans article proposer modeler multidimensionneld qualité linformation décrire facteur qualité influantsur personnalisation linformation modeler permettre structurerle facteur qualité linformation dan hiérarchie afindassister lutilisateur dan construction propre profil besoinset exigence terme qualité
1083	Revue des Nouvelles Technologies de l'Information	EGC	2006	Un modèle métier extensible adapté à la gestion de dépêches d'agences de presse		Frédéric Bertrand, Cyril Faucher, Marie-Christine Lafaye, Jean-Yves Lafaye, Alain Bouju	http://editions-rnti.fr/render_pdf.php?p1&p=1000447	http://editions-rnti.fr/render_pdf.php?p=1000447	
1084	Revue des Nouvelles Technologies de l'Information	EGC	2006	Une approche distribuée pour l'extraction de connaissances : Application à l'enrichissement de l'aspect factuel des BDG	Les systèmes d'informations géographiques (SIG) sont utilisés pouraméliorer l'efficacité des entreprises et des services publics, en associantméthodes d'optimisation et prise en compte de la dimension géographique.Cependant, les bases de données géographiques (BDG) stockées dans les SIGsont restreintes à l'application pour laquelle elles ont été conçues. Souvent, lesutilisateurs demeurent contraints de l'existant et se trouvent dans le besoin dedonnées complémentaires pour une prise de décision adéquate. D'où, l'idée del'enrichissement de l'aspect descriptif des BDG existantes. Pour atteindre cetobjectif, nous proposons une approche qui consiste à intégrer un module defouille de données textuelles au SIG lui même. Il s'agit de proposer uneméthode distribuée de résumé de documents multiples à partir de corpus enligne.L'idée est de faire coopérer un ensemble d'agents s'entraidant afind'aboutir à un résumé optimal.	Khaoula Mahmoudi, Sami Faiz	http://editions-rnti.fr/render_pdf.php?p1&p=1000329	http://editions-rnti.fr/render_pdf.php?p=1000329	système dinformation géographique sig utiliser pouraméliorer lefficacité entreprise service public associantméthod doptimisation priser compter dimension géographiquecepender base donnée géographique BDG stocker dan sigsont restreinte lapplication conçu lesutilisateur demeurer contraint lexistant trouver dan besoin dedonner complémentaire priser décision adéquat Doù lider delenrichissemer laspect descriptif bdg existant Pour atteindre cetobjectif proposer approcher consister intégrer moduler defouille donnée textuel SIG luire sagit proposer uneméthode distribuer résumer document partir corpu enlignelider faire coopérer ensemble dagent sentraider afindaboutir résumer optimal
1085	Revue des Nouvelles Technologies de l'Information	EGC	2006	Une approche multi-agent adaptative pour la simulation de schémas tactiques	Ce papier est consacré à la simulation ou à la réalisation automatiquede schémas tactiques par un groupe d´agents footballeurs autonomes. Son objectifest de montrer ce que peuvent apporter des techniques d'apprentissagepar renforcement à des agents réactifs conçus pour cette tâche. Dans un premiertemps, nous proposons une plateforme et une architecture d'agents capabled'effectuer des schémas tactiques dans des cas relativement simples. Ensuite,nous mettons en oeuvre un algorithme d'apprentissage par renforcementpour permettre aux agents de faire face à des situations plus complexes. Enfin,une série d'expérimentations montrent le gain apporté aux agents réactifs parl'utilisation d'algorithmes d'apprentissage.	Aydano Machado, Yann Chevaleyre, Jean-Daniel Zucker	http://editions-rnti.fr/render_pdf.php?p1&p=1000334	http://editions-rnti.fr/render_pdf.php?p=1000334	papier consacrer simulation réalisation automatiquede schéma tactique grouper d´agent footballeur autonomer objectifest montrer pouvoir apporter technique dapprentissagepar renforcement agent réactif conçu tâcher Dans premiertemp proposer plateforme architecturer dagent capabledeffectuer schéma tactique dan cas simple ensuitenou metton oeuvrer algorithme dapprentissage renforcementpour permettre agent faire face situation plaire complexe enfinune série dexpérimentation montrer gain apporter agent réactif parlutilisation dalgorithme dapprentissage
1086	Revue des Nouvelles Technologies de l'Information	EGC	2006	Une approche simple inspirée des réseaux sociaux pour la hiérarchisation des systèmes autonomes de l'Internet	"Le transit des flux d'information dans le réseau Internet à l'échellemondiale est régi par des accords commerciaux entre systèmes autonomes, accordsqui sont mis en oeuvre via le protocole de routage BGP. La négociationde ces accords commerciaux repose implicitement sur une hiérarchie des systèmesautonomes et la position relative de deux systèmes débouche sur un accordde type client/fournisseur (un des systèmes, le client, est nettement mieuxclassé que l'autre, le fournisseur, et le client paye le fournisseur pour le transitdes flux d'information) ou sur un accord de type ""peering"" (transit gratuit dutrafic entre les deux systèmes). En dépit de son importance, il n'existe pas dehiérarchie officielle de l'Internet (les clauses commerciales des accords entresystèmes autonomes ne sont pas nécessairement publiques) ni de consensus surla façon d'établir une telle hiérarchie. Nous proposons une heuristique simpleinspirée de la notion de ""centralité spectrale"" issue de l'analyse des réseaux sociauxpour analyser la position relative des systèmes autonomes de l'Internet àpartir des informations des seules informations de connectivité entre systèmesautonomes."	Fabrice Clérot, Quang Nguyen	http://editions-rnti.fr/render_pdf.php?p1&p=1000391	http://editions-rnti.fr/render_pdf.php?p=1000391	transir flux dinformation dan réseau Internet léchellemondiale régir accord commercial entrer système autonome accordsqui mettre oeuvrer protocole routage BGP négociationde accord commercial reposer implicitement hiérarchie systèmesautonome position relatif système déboucher accordde typer clientfournisseur système client nettement mieuxclasser lautre fournisseur client payer fournisseur transitd flux dinformation accord typer peering transir gratuit dutrafic entrer système En dépit importance nexiste dehiérarchie officiel lInternet clause commercial accord entresystèm autonome nécessairement public consensus surla détablir hiérarchie proposer heuristique simpleinspiré notion centralité spectral issu lanalyse réseau sociauxpour analyser position relatif système autonome lInternet àpartir information information connectivité entrer systèmesautonom
1087	Revue des Nouvelles Technologies de l'Information	EGC	2006	Une comparaison de certains indices de pertinence des règles d'association	Cet article propose une comparaison graphique de certains indices depertinence pour évaluer l'intérêt des règles d'association. Nous nous sommesappuyés sur une étude existante pour sélectionner quelques indices auxquelsnous avons ajouté l'indice de Jaccard et l'indice d'accords désaccords (IAD).Ces deux derniers nous semblent plus adaptés pour discriminer les règles intéressantesdans le cas où les items sont des événements peu fréquents. Une applicationest réalisée sur des données réelles issues du secteur automobile	Marie Plasse, Ndeye Niang, Gilbert Saporta, Laurent Leblond	http://editions-rnti.fr/render_pdf.php?p1&p=1000405	http://editions-rnti.fr/render_pdf.php?p=1000405	article proposer comparaison graphique indice depertinence évaluer lintérêt règle dassociation sommesappuyer étude existant sélectionner indice auxquelsnous ajouter lindice Jaccard lindice daccord désaccord iadc sembler plaire adapter discriminer règle intéressantesdans cas item événement fréquent applicationest réaliser donnée réel issu secteur automobile
1088	Revue des Nouvelles Technologies de l'Information	EGC	2006	Une mesure de proximité et une méthode de regroupement pour l'aide à l'acquisition d'ontologies spécialisées	Cet article traite du regroupement d'unités textuelles dans une perspectived'aide à l'élaboration d'ontologies spécialisées. Le travail présenté s'inscritdans le cadre du projet BIOTIM. Nous nous concentrons ici sur l'une desétapes de construction semi-automatique d'une ontologie qui consiste à structurerun ensemble d'unités textuelles caractéristiques en classes susceptibles dereprésenter les concepts du domaine. L'approche que nous proposons s'appuiesur la dénition d'une nouvelle mesure non-symétrique permettant d'évaluer laproximité entre lemmes, en utilisant leurs contextes d'apparition dans les documents.En complément de cette mesure, nous présentons un algorithme declassication non-supervisée adapté à la problématique et aux données traitées.Les premières expérimentations présentées sur les données botaniques laissentpercevoir des résultats pertinents pouvant être utilisés pour assister l'expert dansla détermination et la structuration des concepts du domaine.	Guillaume Cleuziou, Sylvie Billot, Stanislas Lew, Lionel Martin, Christel Vrain	http://editions-rnti.fr/render_pdf.php?p1&p=1000339	http://editions-rnti.fr/render_pdf.php?p=1000339	article traire regroupement duniter textuel dan perspectivedaide lélaboration dontologie spécialiser travail présenter sinscritdans cadrer projet BIOTIM concentrer lune desétape construction semiautomatiqu dune ontologie consister structurerun ensembl dunité textuel caractéristiquer classe susceptible dereprésenter concept domaine Lapproche proposer sappuiesur dénition dune mesurer nonsymétriqu permettre dévaluer laproximité entrer lemme utiliser contexte dapparition dan documentsen complémer mesurer présenter algorithme declassication nonsuperviser adapter problématique donnée traitéesl expérimentation présenter donnée botanique laissentpercevoir résultat pertinent pouvoir utiliser assister lexpert dansla détermination structuration concept domaine
1089	Revue des Nouvelles Technologies de l'Information	EGC	2006	Une nouvelle mesure sémantique pour le calcul de la similarité entre deux concepts d'une même ontologie	Les ontologies sont au coeur du processus de gestion des connaissances.Différentes mesures sémantiques ont été proposées dans la littératurepour évaluer quantitativement l'importance de la liaison sémantique entre pairesde concepts. Cet article propose une synthèse analytique des principales mesuressémantiques basées sur une ontologie modélisée par un graphe et restreinte iciaux liens hiérarchiques is-a. Après avoir mis en évidence différentes limites desmesures actuelles, nous en proposons une nouvelle, la PSS (Proportion of SharedSpecificity), qui sans corpus externe, tient compte de la densité des liens dans legraphe reliant deux concepts	Emmanuel Blanchard, Mounira Harzallah, Pascale Kuntz, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1000344	http://editions-rnti.fr/render_pdf.php?p=1000344	ontologie coeur processus gestion connaissancesdifférent mesure sémantique proposer dan littératurepour évaluer quantitativement limportance liaison sémantique entrer pairesde concept article proposer synthèse analytique principal mesuressémantiqu baser ontologie modéliser graphe restreint iciaux lien hiérarchique isa Après mettre évidence limite desmesur actuel proposer PSS Proportion of SharedSpecificity corpus externe compter densité lien dan legraphe relier concept
1090	Revue des Nouvelles Technologies de l'Information	EGC	2006	Utilisation de métadonnées pour l'aide à l'interprétation de classes et de partitions	Les résultats des méthodes de fouille de données sont difficilementinterprétables par un utilisateur n'ayant pas l'expertise requise. Dans ce papiernous proposons un outil permettant aux utilisateurs d'interpréter les résultatsissus des méthodes de classification non supervisée. Cet outil est basé sur desmétadonnées utilisées pour formaliser le processus d'interprétationautomatique. Ces métadonnées vont servir à l'utilisateur pour comprendre dansquelles circonstances les données originales ont été collectées et de quellemanière elles ont été agrégées puis classifiées. L'intérêt de ce travail porte surla souplesse qu'auront les utilisateurs à pouvoir interpréter facilement lesclasses obtenues. Nous développons notre approche basée sur l'utilisation desmétadonnées. Nous traduirons notre méthodologie par un exemple concret.	Abdourahamane Baldé, Yves Lechevallier, Brigitte Trousse, Marie-Aude Aufaure	http://editions-rnti.fr/render_pdf.php?p1&p=1000371	http://editions-rnti.fr/render_pdf.php?p=1000371	résultat méthode fouiller donnée difficilementinterprétabler utilisateur nayer lexpertise requérir Dans papiernou proposer outil permettre utilisateur dinterpréter résultatsissu méthode classification superviser outil baser desmétadonner utilisée formaliser processus dinterprétationautomatiqu métadonnée aller servir lutilisateur comprendre dansquell circonstance donnée original collecter quellemanièr agréger pouvoir classifier Lintérêt travail porter surla soupless quauront utilisateur pouvoir interpréter facilement lesclass obtenu développer approcher basé lutilisation desmétadonner traduire méthodologie exemple concret
1091	Revue des Nouvelles Technologies de l'Information	EGC	2006	Utilisation des réseaux bayésiens dans le cadre de l'extraction de règles d'association	Cet article aborde le problème de l'utilisation d'un modèle de connaissancedans un contexte de fouille de données. L'approche méthodologique proposéemontre l'intérêt de la mise en oeuvre de réseaux bayésiens couplée à l'extractionde règles d'association dites delta-fortes (membre gauche minimal, fréquenceminimale et niveau de confiance contrôlé). La découverte de règles potentiellementutiles est alors facilitée par l'exploitation des connaissances décritespar l'expert et représentées dans le réseau bayésien. Cette approche estvalidée sur un cas d'application concernant la fouille de données d'interruptionsopérationnelles dans l'industrie aéronautique.	Clément Fauré, Sylvie Delprat, Alain Mille, Jean-François Boulicaut	http://editions-rnti.fr/render_pdf.php?p1&p=1000407	http://editions-rnti.fr/render_pdf.php?p=1000407	article aborder problème lutilisation dun modeler connaissancedans contexte fouiller donnée lapproche méthodologique proposéemontre lintérêt miser oeuvrer réseau bayésien coupler lextractionde régler dassociation deltaforte membre gauche minimal fréquenceminimal niveau confiance contrôler découvrir règle potentiellementutil faciliter lexploitation connaissance décritespar lexpert représenter dan réseau bayésien approcher estvalider cas dapplication concerner fouiller donnée dinterruptionsopérationnell dan lindustrie aéronautique
1092	Revue des Nouvelles Technologies de l'Information	EGC	2006	Vers l'extraction de motifs rares	Un certain nombre de travaux en fouille de données se sont intéressés à l'extraction de motifs et à la génération de règles d'association à partir de ces motifs. Cependant, ces travaux se sont jusqu'à présent, centrés sur la notion de motifs fréquents. Le premier algorithme à avoir permis l'extraction de tous les motifs fréquents est Apriori mais d'autres ont été mis au point par la suite, certains n'extrayant que des sous-ensembles de ces motifs (motifs fermés fréquents, motifs fréquents maximaux, générateurs minimaux). Dans cet article, nous nous intéressons aux motifs rares qui peuvent également véhiculer des informations importantes. Les motifs rares correspondent au complémentaire des motifs fréquents. A notre connaissance, ces motifs n'ont pas encore été étudiés, malgré l'intérêt que certains domaines pourraient tirer de ce genre de modèle. C'est en particulier le cas de la médecine, où par exemple, il est important pour un praticien de repérer les symptômes non usuels ou les effets indésirables exceptionnels qui peuvent se déclarer chez un patient pour une pathologie ou un traitement donné.	Laszlo Szathmary, Sandy Maumus, Pierre Petronin, Yannick Toussaint, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1000396	http://editions-rnti.fr/render_pdf.php?p=1000396	nombre travail fouiller donnée intéresser lextraction motif génération règle dassociation partir motif Cependant travail jusquà présent centrer notion motif fréquent algorithme permettre lextraction tou motif fréquent Apriori dautr mettre poindre suite nextrayer sousensemble motif motif fermer fréquent motif fréquent maximal générateur minimal Dans article intéresser motif pouvoir également véhiculer information important motif correspondre complémentaire motif fréquent connaissance motif nont étudier lintérêt domaine pouvoir tirer genre modeler cest cas médecine exemple importer praticien repérer symptôme usuel indésirable exceptionnel pouvoir déclarer patient pathologie traitement donner
1093	Revue des Nouvelles Technologies de l'Information	EGC	2006	Visualisation en Gestion des Connaissances Développement d'un nouveau modèle graphique Graph'Atanor	Les systèmes de gestion des connaissances servent de support pour lacréation et la diffusion de mémoires d'entreprises qui permettent de capitaliser,conserver et enrichir les connaissances des experts. Dans ces systèmes, l'interactionavec les experts est effectuée avec des outils adaptés dans lesquels uneformalisation graphique des connaissances est utilisée. Cette formalisation estsouvent basée au niveau théorique sur des modèles de graphes mais de façonpratique, les représentations visuelles sont souvent des arbres et des limitationsapparaissent par rapport aux représentations basées sur des graphes. Dans cetarticle nous présentons le modèle utilisé par le serveur de connaissances Atanorqui utilise des arbres pour visualiser les connaissances, et nous développons unenouvelle approche qui permet de représenter les mêmes connaissances sous laforme de graphes en niveaux. Une analyse comparative des deux méthodes dansun contexte industriel de maintenance permet de mettre en valeur l'apport desgraphes dans le processus de visualisation graphique des connaissances.	Bruno Pinaud, Pascale Kuntz, Fabrice Guillet, Vincent Philippé	http://editions-rnti.fr/render_pdf.php?p1&p=1000364	http://editions-rnti.fr/render_pdf.php?p=1000364	système gestion connaissance servir support lacréation diffusion mémoire dentrepris permettre capitaliserconserver enrichir connaissance expert Dans système linteractionavec expert effectuer outil adapter dan uneformalisation graphique connaissance utiliser formalisation estsouvent basé niveau théorique modèle graphe façonpratique représentation visuel arbre limitationsapparaissent rapport représentation baser graphe Dans cetarticle présenter modeler utiliser serveur connaissance Atanorqui utiliser arbre visualiser connaissance développer unenouvell approcher permettre représenter connaissance sou laforme graphe niveau analyser comparatif méthode dansun contexte industriel maintenance permettre mettre lapport desgraphe dan processus visualisation graphique connaissance
1094	Revue des Nouvelles Technologies de l'Information	EGC	2006	Visualisation interactive de données avec des méthodes à base de points d'intérêt	Nous présentons dans cet article une méthode de visualisation interactivede données numériques ou symboliques permettant à un utilisateur expertdu domaine d'obtenir des informations et des connaissances pertinentes. Nousproposons une approche nouvelle en adaptant l'utilisation des points d'intérêtsdans un contexte de fouille visuelle de données. A partir d'un ensemble de pointsd'intérêt disposés sur un cercle, les données sont visualisées à l'intérieur de cecercle en fonction de leur similarité à ces points d'intérêt. Des opérations interactivessont alors définies : sélectionner, zoomer, changer dynamiquement lespoints d'intérêts. Nous évaluons les propriétés d'une telle visualisation sur desdonnées aux caractéristiques connues. Nous décrivons une application réelle encours dans le domaine de l'exploration de données issues d'enquêtes de satisfaction.	David Da Costa, Gilles Venturini	http://editions-rnti.fr/render_pdf.php?p1&p=1000367	http://editions-rnti.fr/render_pdf.php?p=1000367	présenter dan article méthode visualisation interactived donnée numérique symbolique permettre utilisateur expertdu domaine dobtenir information connaissance pertinent nousproposon approcher adapter lutilisation point dintérêtsdan contexte fouiller visuel donnée partir dun ensemble pointsdintérêt disposer cercler donnée visualiser lintérieur cecercle fonction similarité point dintérêt opération interactivessont définie   sélectionner zoomer changer dynamiquement lespoint dintérêts évaluer propriété dune visualisation desdonnée caractéristique connu décrire application réel encourir dan domaine lexploration donnée issu denquêt satisfaction
1095	Revue des Nouvelles Technologies de l'Information	EGC	2006	Web sémantique pour la mémoire d'expériences d'une communauté scientifique : le projet MEAT	Cet article décrit le projet MEAT (Mémoire d'Expériences pourl'Analyse du Transcriptome) dont le but est d'assister les biologistes travaillantdans le domaine des puces à ADN, pour l'interprétation et la validation de leursrésultats. Nous proposons une aide méthodologique et logicielle pour construireune mémoire d'expériences pour ce domaine. Notre approche, basée surles technologies du web sémantique, repose sur l'utilisation des ontologies etdes annotations sémantiques sur des articles scientifiques et d'autres sourcesde connaissances du domaine. Notre approche peut être généralisée à d'autresdomaines requérant des expérimentations et traitant un grand flux de données(protéomique, chimie,etc.).	Khaled Khelif, Rose Dieng-Kuntz, Pascal Barbry	http://editions-rnti.fr/render_pdf.php?p1&p=1000340	http://editions-rnti.fr/render_pdf.php?p=1000340	article décrire projet MEAT Mémoire dexpérience pourlanalyse transcriptome boire dassister biologiste travaillantdans domaine puce adn linterprétation validation leursrésultat proposer aider méthodologique logiciel construireune mémoire dexpérience domaine approcher baser surl technologi web sémantique reposer lutilisation ontologie etd annotation sémantique article scientifique dautr sourcesd connaissance domaine approcher pouvoir généraliser dautresdomaine requérir expérimentation traiter grand flux donnéesprotéomique chimieetc
1096	Revue des Nouvelles Technologies de l'Information	EGC	2006	Web Usage Mining : extraction de périodes denses à partir des logs	"Les techniques de Web Usage Mining existantes sont actuellementbasées sur un découpage des données arbitraire (e.g. ""un log par mois"") ou guidépar des résultats supposés (e.g. ""quels sont les comportements des clients pourla période des achats de Noël ? ""). Ces approches souffrent des deux problèmessuivants. D'une part, elles dépendent de cette organisation arbitraire des donnéesau cours du temps. D'autre part elles ne peuvent pas extraire automatiquementdes ""pics saisonniers"" dans les données stockées. Nous proposons d'exploiterles données pour découvrir de manière automatique des périodes ""denses"" decomportements. Une période sera considérée comme ""dense"" si elle contient aumoins un motif séquentiel fréquent pour l'ensemble des utilisateurs qui étaientconnectés sur le site à cette période."	Florent Masseglia, Pascal Poncelet, Maguelonne Teisseire, Alice Marascu	http://editions-rnti.fr/render_pdf.php?p1&p=1000377	http://editions-rnti.fr/render_pdf.php?p=1000377	technique Web Usage Mining existant actuellementbaser découpage donnée arbitraire eg log mois guidépar résultat supposer eg comportement client pourla période achat Noël    approche souffrir problèmessuivant dune partir dépendre organisation arbitraire donnéesau cours temps Dautre partir pouvoir extraire automatiquementd pic saisonnier dan donnée stocker proposer dexploiterl donnée découvrir manière automatique périod dense decomportements période considérer dense contenir aumoins motif séquentiel fréquent lensemble utilisateur étaientconnecter site période
1097	Revue des Nouvelles Technologies de l'Information	EGC	2005	ACKA : Une approche d'acquisition coopérative de connaissances pour la construction d'un modèle de simulation multi-agents	Cet article présente une approche (ACKA an Approach for Cooperative Knowledge Acquisition) participative et coopérative d'acquisition de connaissances nécessaires pour la construction d'un modèle de simulation basé sur des agents. Elle est basée sur le principe de jeu de rôles dans une réunion d'entreprise. Nous proposons de construire un modèle multi-acteurs, représentant un modèle initial du système multi-agents. Dans cette étude, Nous appliquons ACKA pour construire un modèle multi-acteurs pour la compréhension des processus de décision dans les ?rmes de la ?liere avicole. En particulier, nous cherchons à comprendre les impacts des comportements individuels sur la gestion de l'utilisation des matières premières agricoles.	Athmane Hamel, Suzanne Pinson	http://editions-rnti.fr/render_pdf.php?p1&p=1000403	http://editions-rnti.fr/render_pdf.php?p=1000403	article présenter approcher ACKA an Approach for Cooperative Knowledge acquisition participatif coopératif dacquisition connaissance nécessaire construction dun modeler simulation baser agent baser principe jeu rôle dan réunion dentrepris proposer construire modeler multiacteur représenter modeler initial système multiagent Dans étude appliquer ACKA construire modeler multiacteur compréhension processus décision dan rme liere avicole En chercher comprendre impact comportement individuel gestion lutilisation matière agricole
1098	Revue des Nouvelles Technologies de l'Information	EGC	2005	Acquisition et exploitation de connaissances dans un contexte multi-experts pour un système d'aide à la décision	Nous présentons une méthodologie d'extraction, de gestion et d'exploitation de connaissances dans un contexte multi-experts. Elle repose sur trois étapes : extraction des connaissances de chaque expert, gestion des connaissances individuelles afin de constituer une base de connaissances commune et exploitation de cette base afin de fournir une aide à la décision aux experts. La méthodologie proposée a été mise en œuvre au Cameroun avec cinq experts en micro-finance. Elle a donné des résultats en adéquation avec les pratiques des experts. Au-delà, on envisage de mettre en œuvre un système de capitalisation des connaissances. Il doit permettre d'analyser rapidement un plus grand nombre de situations, les experts restant en nombre limité, et contribuer à un transfert de compétences pour former les décideurs locaux. En effet, les experts sont en général membres d'ONG et restent rarement plus de deux ans sur place.	Jean-Pierre Barthélemy, Jean-Robert Kala Kamdjoug, Philippe Lenca	http://editions-rnti.fr/render_pdf.php?p1&p=1000220	http://editions-rnti.fr/render_pdf.php?p=1000220	présenter méthodologie dextraction gestion dexploitation connaissance dan contexte multiexpert reposer étape   extraction connaissance expert gestion connaissance individuel constituer baser connaissance commun exploitation baser fournir aider décision expert méthodologie proposer mettre œuvre Cameroun expert microfinance donner résultat adéquation pratique expert Audelà envisager mettre œuvre système capitalisation connaissance devoir permettre danalyser rapidement plaire grand nombre situation expert rester nombre limité contribuer transfert compétence former décideur local En expert général membre dong ruer plaire an placer
1099	Revue des Nouvelles Technologies de l'Information	EGC	2005	AID : Un framework intégré de conception d'un schéma objet-relationnel	Devant la prolifération des données complexes qui ne cessent de croître, et la diversité des structures qui se multiplient, la conception des schémas de base de données en général et des schémas objet-relationnels en particulier, est devenue une activité difficile et complexe, qui fait appel à des connaissances variées. Lors de la conception d'un schéma, l'utilisateur (non averti) doit connaître la théorie sous-jacente au modèle de données, de façon à énoncer son modèle, syntaxiquement correct lui permettant de construire un schéma de base de données objet-relationnel répondant à ses besoins. Plusieurs outils spécialisés dans la conception de schémas de base de données provenant aussi bien de la communauté académique que du monde industriel, tels Super, Totem, Rational/Rose, etc. ont été développés dans des contextes et avec des buts souvent très différents. Affin de répondre à ce besoin pressant, nous avons proposé une solution consistant en l'élaboration d'environnements intégrés facilitant la cohabitation de plusieurs modèles et techniques utilisés lors de la conception d'un schéma de base de données. Il s'agit d'offrir une plate-forme logicielle appelée AID (Aided Interface for Database design) offrant des mécanismes opératoires uniformes représentant un soutien graphique et interactif pour une conception incrémentale basée sur des manipulations directes et systémiques des graphes au travers d'une palette graphique d'opérateurs. L'innovation d'AID est son approche systémique qui facilite l'expression des besoins par le concepteur averti ou non, en lui automatisant sa tâche.	Etienne Pichat, Hassan Badir	http://editions-rnti.fr/render_pdf.php?p1&p=1000292	http://editions-rnti.fr/render_pdf.php?p=1000292	Devant prolifération donnée complexe cesser croître diversité structure multiplier conception schéma baser donnée général schéma objetrelationnel devenir activité difficile complexe faire appel connaissance varier conception dun schéma lutilisateur avertir devoir connaître théorie sousjacent modeler donnée énoncer modeler syntaxiquemer correct luire permettre construire schéma baser donnée objetrelationnel répondre besoin outil spécialiser dan conception schéma baser donnée provenir communauté académique monder industriel Super Totem rationalrose développer dan contexte but Affin répondre besoin presser proposer solution consister lélaboration denvironnement intégrer faciliter cohabitation modèle technique utiliser conception dun schéma baser donnée sagit doffrir plateforme logiciel appeler AID Aided Interface for Database design offrir mécanisme opératoire uniforme représenter soutien graphique interactif conception incrémental baser manipulation direct systémique graphe travers dune palette graphique dopérateur Linnovation daid approcher systémique faciliter lexpression besoin concepteur avertir luire automatiser tâcher
1100	Revue des Nouvelles Technologies de l'Information	EGC	2005	Amélioration de la performance de l'Analyse de la Sémantique Latente pour des corpus de petite taille		Fadoua Ataa-Allah, Abderrahim El Qadi, Siham Boulaknadel, Driss Aboutajdine	http://editions-rnti.fr/render_pdf.php?p1&p=1000278	http://editions-rnti.fr/render_pdf.php?p=1000278	
1101	Revue des Nouvelles Technologies de l'Information	EGC	2005	Analyse comparative de classifications : apport des règles d'association floues	Notre travail s'appuie sur l'analyse d'un corpus bibliographique dans le domaine de la géotechnique à l'aide de cartes réalisées avec la plateforme Stanalyst®. Celui-ci intègre un algorithme de classification automatique non hiérarchique (les K-means axiales) donnant des résultats dépendant du nombre de classes demandé. Cette instabilité rend difficile toute comparaison entre classifications, et laisse un doute quant au choix du nombre de classes nécessaire pour représenter correctement un domaine. Nous comparons les résultats de classifications selon 3 protocoles : (1) analyse des intitulés des classes ; (2) relations entre les classes à partir des membres communs ; (3) règles d'association floues. Les graphes obtenus présentant des similitudes remarquables, nous privilégions les règles d'association floues : elles sont extraites automatiquement et se basent sur la description des classes et non des membres. Ceci nous permet donc d'analyser des classifications issues de corpus différents.	Pascal Cuxac, Martine Cadot, Claire François	http://editions-rnti.fr/render_pdf.php?p1&p=1000359	http://editions-rnti.fr/render_pdf.php?p=1000359	travail sappuie lanalyse dun corpu bibliographique dan domaine géotechnique laid carte réaliser plateforme Stanalyst ® Celuici intègre algorithme classification automatique hiérarchique kmean axial donner résultat dépendre nombre classe demander instabilité difficile comparaison entrer classification douter choix nombre classe nécessaire représenter correctement domaine comparer résultat classification 3 protocole   1 analyser intitulé classe   2 relation entrer classe partir membre commun   3 règle dassociation flou graphe obtenir présenter similitude privilégier règle dassociation flou   extraire automatiquement baser description classe membre permettre danalyser classification issu corpus
1102	Revue des Nouvelles Technologies de l'Information	EGC	2005	Analyse de données symboliques et graphe de connaissances d'un agent	Dans cet article nous appliquons l'analyse de données symboliques au graphe de connaissances d'un agent. Nous présentons une mesure de similarité entre des données symboliques adaptée à nos graphes de connaissances. Nous utilisons les pyramides symboliques pour extraire un nouvel objet symbolique. Le nouvel objet est ensuite réinséré dans le graphe où il peut être utilisé par l'agent, faisant ainsi évoluer sa sémantique. Il peut alors servir d'individu lors des analyses ultérieures, permettant de découvrir de nouveaux concepts prenant en compte l'évolution de la sémantique.	Philippe Caillou, Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1000408	http://editions-rnti.fr/render_pdf.php?p=1000408	Dans article appliquer lanalyse donnée symbolique graphe connaissance dun agent présenter mesurer similarité entrer donnée symbolique adapter graphe connaissance utiliser pyramide symbolique extraire nouvel objet symbolique nouvel objet ensuite réinsérer dan graphe pouvoir utiliser lagent faire évoluer sémantique pouvoir servir dindividu analyse ultérieur permettre découvrir concept prendre compter lévolution sémantique
1103	Revue des Nouvelles Technologies de l'Information	EGC	2005	Analyse géométrique des données pour l'affinement de la connaissance : cas des données EPGY (Education Program for Gifted Students, Stanford University)		Brigitte Le Roux	http://editions-rnti.fr/render_pdf.php?p1&p=1000256	http://editions-rnti.fr/render_pdf.php?p=1000256	
1104	Revue des Nouvelles Technologies de l'Information	EGC	2005	Analyse stochastique de séquences d'événements discrets pour la découverte de signatures	"Cet article concerne la découverte de signatures (ou modèles de chroniques) à partir d'une séquence d'événements discrets (alarmes) générée par un agent cognitif de surveillance (Monitoring Cognitive Agent ou MCA).Considérant un couple (Processus, MCA) comme un générateur stochastique d'événements discrets, deux représentations complémentaires permettent de caractériser les propriétés stochastiques et temporelles d'un tel générateur : une chaîne de Markov à temps continu et une superposition de processus de Poisson. L'étude de ces deux représentations duales permet de découvrir des ""signatures"" décrivant les relations stochastiques et temporelles entre événements dans une séquence. Ces signatures peuvent alors être utilisées pour reconnaître des comportements spécifiques, comme le montre l'application de l'approche à un outil de production industriel piloté par un système Sachem, le MCA développé et utilisé par le groupe Arcelor pour aider au pilotage de ses outils de production."	Philippe Bouché, Marc Le Goc	http://editions-rnti.fr/render_pdf.php?p1&p=1000219	http://editions-rnti.fr/render_pdf.php?p=1000219	article concerner découvrir signature modèle chronique partir dune séquence dévénement discret alarme générer agent cognitif surveillance Monitoring Cognitive Agent mcaconsidérer coupler Processus MCA générateur stochastique dévénement discret représentation complémentaire permettre caractériser propriété stochastique temporel dun générateur   chaîner Markov temps continu superposition processus Poisson Létude représentation dual permettre découvrir signature décrire relation stochastique temporel entrer événement dan séquence signature pouvoir utiliser reconnaître comportement spécifique montr lapplication lapproche outil production industriel piloter système Sachem MCA développer utiliser grouper Arcelor aider pilotage outil production
1105	Revue des Nouvelles Technologies de l'Information	EGC	2005	Annotation de textes par extraction d'informations lexicosyntaxiques  et acquisition de schémas conceptuels de causalité	Nous présentons la méthode INSYSE (INterface SYntaxe SEmantique) pour l'annotation de documents textuels. Notre objectif est de construire des annotations sémantiques de ces résumés pour interroger le corpus sur la fonction des gènes et leurs relations de causalité avec certaines maladies. Notre approche est semi-automatique, centrée sur (1) l'extraction d'informations lexico-syntaxiques à partir de certaines phrases du corpus comportant des lexèmes de causation, et (2) l'élaboration de règles basées sur des grammaires d'unification permettant d'acquérir à partir de ces informations des schémas conceptuels instanciés. Ceux-ci sont traduits en annotations RDF(S) sur la base desquelles le corpus de textes peut être interrogé avec le moteur de recherche sémantique Corese.	Laurent Alamarguy, Rose Dieng-Kuntz, Catherine Faron-Zucker	http://editions-rnti.fr/render_pdf.php?p1&p=1000258	http://editions-rnti.fr/render_pdf.php?p=1000258	présenter méthode insys interface syntaxe semantiqu lannotation document textuel objectif construire annotation sémantique résumé interroger corpus fonction gène relation causalité maladie approcher semiautomatique centré 1 lextraction dinformation lexicosyntaxiqu partir phrase corpus comporter lexème causation 2 lélaboration règle baser grammaire dunification permettre dacquérir partir information schéma conceptuel instancié Ceuxci traduire annotation RDFS baser desquell corpus texte pouvoir interroger moteur rechercher sémantique Corese
1106	Revue des Nouvelles Technologies de l'Information	EGC	2005	Apprentissage automatique des modèles structurels d'objets cartographiques	Pour reconnaitre les objets cartographiques dans les images satellitales on a besoin d'un modèle d'objet qu'on recherche. Nous avons développé un système d'apprentissage qui construit le modèle structurel d'objets cartographiques automatiquement a partir des images satellitales segmentées. Les images contenants les objets sont décomposées en formes primitives et sont transformées en Graphes Relationnels Attribués (ARGs). Nous avons généré les modèles d'objets a partir de ces graphes, en utilisant des algorithmes d'appariement de graphes. La qualité d'un modèle est évaluée par la distance d'édition des exemples a ce modèle. Nous sommes parvenus a obtenir des modèles de ponts et de ronds-points qui sont compatibles avec les modèles construits manuellement.	Güray Erus, Nicolas Loménie	http://editions-rnti.fr/render_pdf.php?p1&p=1000336	http://editions-rnti.fr/render_pdf.php?p=1000336	Pour reconnaitre objet cartographique dan image satellital besoin dun modeler dobjet quon rechercher développer système dapprentissage construire modeler structurel dobjet cartographique automatiquement partir image satellital segmenter image contenant objet décomposer forme primitif transformer Graphes relationnel Attribués ARGs générer modèle dobjet partir graphe utiliser algorithme dappariemer graphe qualité dun modeler évaluer distancer dédition exemple modeler parvenir obtenir modèle pont rondspoint compatible modèle construit manuellement
1107	Revue des Nouvelles Technologies de l'Information	EGC	2005	Apprentissage de scénarios à partir de séries temporelles multivariées		Thomas Guyet, Catherine Garbay, Michel Dojat	http://editions-rnti.fr/render_pdf.php?p1&p=1000224	http://editions-rnti.fr/render_pdf.php?p=1000224	
1108	Revue des Nouvelles Technologies de l'Information	EGC	2005	Apprentissage de signatures de facteurs de transcription à partir de données d'expression	L'inférence de signatures de facteurs de transcription à partir des données puces à ADN a déjà été étudié dans la communauté bioinformatique. La principale difficulté à résoudre est de trouver un ensemble d'heuristiques pertinentes, afin de contrôler la complexité de résolution de ce problème NP-difficile. Nous proposons dans cet article une solution heuristique alternative à celles utilisées dans les approches bayésiennes, fondée sur la recherche de motifs fréquents maximaux dans une matrice discrétisée issue des données numériques de puces ADN. Notre méthode est appliquée sur des données de cancer de vessie de l'Institut Curie et de l'Hôpital Henri Mondor de Créteil.	Mohamed Elati, Céline Rouveirol, François Radvanyi	http://editions-rnti.fr/render_pdf.php?p1&p=1000416	http://editions-rnti.fr/render_pdf.php?p=1000416	Linférence signature facteur transcription partir donnée puc ADN déjà étudier dan communauté bioinformatiqu principal difficulté résoudre trouver ensemble dheuristiqu pertinent chuter complexité résolution problème npdifficil proposer dan article solution heuristique alternatif utiliser dan approche bayésienn fonder rechercher motif fréquent maximal dan matrice discrétiser issu donnée numérique puce ADN méthode appliquer donnée cancer vessie linstitut Curie lhôpital Henri Mondor Créteil
1109	Revue des Nouvelles Technologies de l'Information	EGC	2005	Apprentissage de structures de réseaux Bayésiens et données incomplètes	"Le formalisme des modèles graphiques connait actuellement un essor dans les domaines du ""machine learning"". En particulier, les réseaux bayésiens sont capables d'effectuer des raisonnements probabilistes à partir de données incomplètes alors que peu de méthodes sont actuellement capables d'utiliser les bases d'exemples incomplètes pour leur apprentissage. En s'inspirant du principe de AMS-EM proposé par (Friedman, 1997) et des travaux de(Chow & Liu, 1968), nous proposons une méthode permettant de faire l'apprentissage de réseaux bayésiens particuliers, de structure arborescente, à partir de données incomplètes. Une étude expérimentale expose ensuite des résultats préliminaires qu'il est possible d'attendre d'une telle méthode, puis montre le gain potentiel apporté lorsque nous utilisons les arbres obtenus comme initialisation d'une méthode de recherche gloutonne comme AMS-EM."	Olivier François, Philippe Leray	http://editions-rnti.fr/render_pdf.php?p1&p=1000222	http://editions-rnti.fr/render_pdf.php?p=1000222	formalisme modèle graphique conner actuellement essor dan domaine machiner learning En réseau bayésien capabler deffectuer raisonnement probabiliste partir donnée incomplet méthode actuellement capabler dutiliser base dexempl incomplet apprentissage En sinspirer principe amsem proposer friedman 1997 travail deChow   Liu 1968 proposer méthode permettre faire lapprentissage réseau bayésien structurer arborescent partir donnée incomplet étude expérimental exposer ensuite résultat préliminaire quil dattendre dune méthod pouvoir montrer gain potentiel apporter utiliser arbre obtenir initialisation dune méthode rechercher glouton amsem
1110	Revue des Nouvelles Technologies de l'Information	EGC	2005	Apprentissage non supervisé de séries temporelles à l'aide des k-Means et d'une nouvelle méthode d'agrégation de séries	"L'utilisation d'un algorithme d'apprentissage non supervisé de type k-Means sur un jeu de séries temporelles amène à se poser deux questions : Celle du choix d'une mesure de similarité et celle du choix d'une méthode effectuant l'agrégation de plusieurs séries afin d'en estimer le centre (i.e. calculer les k moyennes). Afin de répondre à la première question, nous présentons dans cet article les principales mesures de similarité existantes puis nous expliquons pourquoi l'une d'entre elles (appelée Dynamic Time Warping) nous paraît la plus adaptée à l'apprentissage non supervisé. La deuxième question pose alors problème car nous avons besoin d'une méthode d'agrégation respectant les caractéristiques bien particulières du Dynamic Time Warping. Nous pensons que l'association de cette mesure de similarité avec l'agrégation Euclidienne peut générer une perte d'informations importante dans le cadre d'un apprentissage sur la ""forme"" des séries. Nous proposons donc une méthode originale d'agrégation de séries temporelles, compatible avec le Dynamic Time Warping, qui améliore ainsi les résultats obtenus à l'aide de l'algorithme des k-Means."	Nicolas Nicoloyannis, Rémi Gaudin	http://editions-rnti.fr/render_pdf.php?p1&p=1000250	http://editions-rnti.fr/render_pdf.php?p=1000250	lutilisation dun algorithm dapprentissage superviser typer kmean jeu série temporel amène poser question   choix dune mesurer similarité choix dune méthode effectuer lagrégation série den estimer centrer ie calculer moyen Afin répondre question présenter dan article principal mesure similarité existant pouvoir expliquer lune dentre appeler Dynamic Time Warping paraître plaire adapté lapprentissage superviser question poser problème besoin dune méthode dagrégation respecter caractéristique Dynamic Time Warping penser lassociation mesurer similarité lagrégation Euclidienne pouvoir générer perte dinformation important dan cadrer dun apprentissage former série proposer méthode original dagrégation série temporel compatible Dynamic Time Warping améliorer résultat obtenir laid lalgorithme kmean
1111	Revue des Nouvelles Technologies de l'Information	EGC	2005	Apprentissage supervisé pour la classification des images basé sur la structure P-tree	Un problème important de la production automatique de règles de classification concerne la durée de génération de ces règles ; en effet, les algorithmes mis en œuvre produisent souvent des règles pendant un certain temps assez long. Nous proposons une nouvelle méthode de classification à partir d'une base de données images. Cette méthode se situe à la jonction de deux techniques : l'algèbre de P-tree et l'arbre de décision en vue d'accélérer le processus de classification et de recherche dans de grandes bases d'images. La modélisation que nous proposons se base, d'une part, sur les descripteurs visuels tels que la couleur, la forme et la texture dans le but d'indexer les images et, d'autre part, sur la génération automatique des règles de classification à l'aide d'un nouvel algorithme C4.5(P-tree). Pour valider notre méthode, nous avons développé un système baptisé C.I.A.D.P-tree qui a été implémenté et confronté à une application réelle dans le domaine du traitement d'images. Les résultats expérimentaux montrent que cette méthode réduit efficacement le temps de classification.	Rim Faiz, Najeh Naffakhi, Khaled Mellouli	http://editions-rnti.fr/render_pdf.php?p1&p=1000343	http://editions-rnti.fr/render_pdf.php?p=1000343	problème importer production automatique règle classification concerner durer génération règle   algorithme mettre œuvre produire règle pendre temps long proposer méthode classification partir dune baser donnée imager méthode situer jonction technique   lalgèbre Ptree larbre décision daccélérer processus classification rechercher dan grand base dimager modélisation proposer baser dune partir descripteur visuel couleur former texture dan boire dindexer image dautre partir génération automatique règle classification laid dun nouvel algorithme c45ptre Pour valider méthode développer système baptiser CIADPtree implémenter confronter application réel dan domaine traitement dimager résultat expérimental montrer méthode réduire efficacement temps classification
1112	Revue des Nouvelles Technologies de l'Information	EGC	2005	Arbre de décision sur des données de type intervalle : évaluation et comparaison	Le critère de découpage binaire de Kolmogorov-Smirnov nécessite un ordre total des valeurs prises par les variables explicatives. Nous pouvons ordonner des intervalles fermés bornés de nombres réels de différentes façons. Notre contribution dans cet article consiste à évaluer et à comparer des arbres de décision obtenus sur des données de type intervalle à l'aide du critère de découpage binaire de Kolmogorov-Smirnov étendu à ce type de données (Mballo et al. 2004). Pour ce faire, nous axons notre attention sur le taux d'erreur mesuré sur l'échantillon de test. Pour estimer ce paramètre, nous divisons aléatoirement chaque base de données en deux parties égales en terme d'effectif (à un objet près) pour construire deux arbres. Ces deux arbres sont d'abord testés par un même échantillon puis par deux échantillons différents.	Chérif Mballo, Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1000215	http://editions-rnti.fr/render_pdf.php?p=1000215	critère découpage binaire KolmogorovSmirnov nécessiter ordre total prendre variable explicatif pouvoir ordonner intervalle fermer borné nombre réel contribution dan article consister évaluer comparer arbre décision obtenir donnée typer intervall laid critère découpage binaire KolmogorovSmirnov étendre typer donnée Mballo al 2004 Pour faire axer attention taux derreur mesurer léchantillon test Pour estimer paramètre diviser aléatoirement baser donnée party égal terme deffectif objet construire arbre arbre dabord tester échantillon pouvoir échantillon
1113	Revue des Nouvelles Technologies de l'Information	EGC	2005	Caractérisation d'une région d'intérêt dans les images	Une image est un support d'information qui a montré son efficacité. Néanmoins une image comporte souvent plusieurs zones, l'arrière plan et une zone d'intérêt privilégiée. La vision humaine permet la segmentation de manière naturelle et intégrant toute la connaissance que le sujet peut avoir de l'objectif visé par l'image. Nous proposons ici une méthode de détermination des régions d'intérêt d'une image numérique comme zones saillantes. Les lois de Zipf et Zipf inverse sont adaptées au traitement des images et permettent d'évaluer la complexité structurelle d'une image. Une comparaison des modèles locaux évalués sur des imagettes permet de mettre en évidence une région de l'image. Deux méthodes de classification ont été utilisées pour la détermination de la région d'intérêt : la partition d'un nuage de points représentant les caractéristiques associées aux imagettes, et les réseaux de neurones. Cette méthode de détection permet d'obtenir des zones d'intérêt conformes à la perception humaine. On opère une hiérarchisation sur les zones en fonction de la structuration de l'information élémentaire, les pixels.	Yves Caron, Pascal Makris, Nicole Vincent	http://editions-rnti.fr/render_pdf.php?p1&p=1000338	http://editions-rnti.fr/render_pdf.php?p=1000338	imager support dinformation montrer efficacité imager comporter zone larrièr plan zone dintérêt privilégier vision humain permettre segmentation manière intégrer connaissance pouvoir lobjectif viser limage proposer méthode détermination région dintérêt dune imager numérique zone saillant loi zipf zipf inverse adapter traitement image permettre dévaluer complexité structurel dune imager comparaison modèle local évaluer imagette permettre mettre évidence région limage Deux méthode classification utiliser détermination région dintérêt   partition dun nuage point représenter caractéristique associé imagett réseau neuroner méthode détection permettre dobtenir zone dintérêt conforme perception humain opérer hiérarchisation zone fonction structuration linformation élémentaire pixel
1114	Revue des Nouvelles Technologies de l'Information	EGC	2005	CHIC : traitement de données avec l'analyse implicative	Cet article a pour but de montrer les possibilités offertes par le logiciel CHIC (Classification Hiérarchique Implicative et Cohésitive) pour effectuer certaines analyses de données. Il est basé sur la théorie de l'Analyse Statistique Implicative ou A.S.I. développée par Régis Gras et ses collaborateurs. Le principe premier de l'A.S.I. repose sur la problématique d'une mesure des règles d'association du type : «si a alors b» dans une population instanciant les variables a et b. CHIC enrichit sa réponse, établie sur des bases statistiques, en évaluant la responsabilité des sujets dans l'élection de la règle. L'article présent explique la démarche à suivre pour utiliser le logiciel ainsi que les possibilités offertes par celui-ci.	Raphaël Couturier, Régis Gras	http://editions-rnti.fr/render_pdf.php?p1&p=1000423	http://editions-rnti.fr/render_pdf.php?p=1000423	article boire montrer possibilité offert logiciel CHIC classification hiérarchique Implicative cohésitiv effectuer analyse donnée baser théorie lanalyse Statistique Implicative asi développer Régis Gras collaborateur principe lasi reposer problématique dune mesurer règle dassociation typer   « » dan population instancier variable CHIC enrichir réponse établir base statistique évaluer responsabilité dan lélection régler Larticle présent expliquer démarcher utiliser logiciel possibilité offert celuici
1115	Revue des Nouvelles Technologies de l'Information	EGC	2005	Classification 2-3 Hiéarchique de données du Web		Sergiu Theodor Chelcea, Brigitte Trousse	http://editions-rnti.fr/render_pdf.php?p1&p=1000253	http://editions-rnti.fr/render_pdf.php?p=1000253	
1116	Revue des Nouvelles Technologies de l'Information	EGC	2005	Classification d'un tableau de contingence et modèle probabiliste	Les modèles de mélange, qui supposent que l'échantillon est formé de sous-populations caractérisées par une distribution de probabilité, constitue un support théorique intéressant pour étudier la classification automatique. On peut ainsi montrer que l'algorithme des k-means peut être vu comme une version classifiante de l'algorithme d'estimation EM dans un cas particulièrement simple de mélange de lois normales. Lorsque l'on cherche à classifier les lignes (ou les colonnes) d'un tableau de contingence, il est possible d'utiliser une variante de l'algorithme des k-means, appelé Mndki2, en s'appuyant sur la notion de profil et sur la distance du khi-2. On obtient ainsi une méthode simple et efficace pouvant s'utiliser conjointement à l'analyse factorielle des correspondances qui s'appuie sur la même représentation des données. Malheureusement et contrairement à l'algorithme des k-means classique, les liens qui existent entre les modèles de mélange et la classification ne s'appliquent pas directement à cette situation. Dans ce travail, nous montrons que l'algorithme Mndki2 peut être associé, à une approximation près, à un modèle de mélange de lois multinomiales.	Gérard Govaert, Mohamed Nadif	http://editions-rnti.fr/render_pdf.php?p1&p=1000251	http://editions-rnti.fr/render_pdf.php?p=1000251	modèle mélanger supposer léchantillon former souspopulation caractériser distribution probabilité constituer support théorique intéresser étudier classification automatique pouvoir montrer lalgorithme kmean pouvoir voir version classifiant lalgorithme destimation em dan cas simple mélanger loi normal lon chercher classifier ligne colonne dun tableau contingence dutiliser variant lalgorithme kmean appeler mndki2 sappuyer notion profil distancer khi2 obtenir méthode simple efficace pouvoir sutiliser conjointement lanalyse factoriel correspondance sappuie représentation donnée malheureusement contrairement lalgorithme kmean classique lien exister entrer modèle mélanger classification sappliquer situation Dans travail montrer lalgorithm Mndki2 pouvoir associer approximation modeler mélanger loi multinomial
1117	Revue des Nouvelles Technologies de l'Information	EGC	2005	Classification non supervisée et visualisation 3D de documents	Le nombre de documents issus d'une requête sur le Web devient de plus en plus important. Cela nous amène à chercher des solutions pour aider l'utilisateur qui est confronté à cette masse de données. Une alternative possible à un affichage linéaire non triée selon un critère, consiste à effectuer une classification des résultats. C'est dans ce but que l'on s'intéresse aux cartes auto-organisatrices de Kohonen qui sont issues d'un d'algorithme de classification non supervisée. Cependant, il faut ajouter des contraintes à cet algorithme afin qu'il soit adapté à la classification des résultats d'une requête. Par exemple, il doit être déterministe. De plus la classification obtenue dépend fortement de la distance utilisée pour comparer deux documents. On évalue alors l'impact de différentes distances ou dissimilarités, afin de trouver la plus adaptée à notre problème. Un compromis doit également être trouvé entre le temps d'exécution de l'algorithme et la qualité de la classification obtenue. Pour cela, l'utilisation d'un échantillonnage est envisagée. Enfin, ces travaux sont intégrés dans un prototype qui permet de visualiser les résultats en trois dimensions et d'interagir avec eux.	Nicolas Bonnel, Annie Morin, Alexandre Cotarmanac'h	http://editions-rnti.fr/render_pdf.php?p1&p=1000379	http://editions-rnti.fr/render_pdf.php?p=1000379	nombre document issu dune requête web devenir plaire plaire importer celer amener chercher solution aider lutilisateur confronter masser donnée alternatif affichage linéaire trier critère consister effectuer classification résultat cest dan boire lon sintéresse carte autoorganisatrice Kohonen issu dun dalgorithme classification superviser falloir ajouter contrainte algorithme quil adapter classification résultat dune requête Par exemple devoir déterministe De plaire classification obtenir dépendre fortement distancer utiliser comparer document évaluer limpact distance dissimilariter trouver plaire adapté problème compromis devoir également trouver entrer temps dexécution lalgorithm qualité classification obtenir Pour celer lutilisation dun échantillonnage envisager Enfin travail intégrer dan prototype permettre visualiser résultat dimension dinteragir
1118	Revue des Nouvelles Technologies de l'Information	EGC	2005	Classifying XML Materialized Views for their maintenance on distributed Web sources	Ces dernières années ont mis en évidence la croissance et la diversité des informations électroniques accessibles sur le web. C'est ainsi que les systèmes d'intégration de données tels que des médiateurs ont été conçus pour intégrer ces données distribuées et hétérogènes dans une vue uniforme. Pour faciliter l'intégration des données à travers différents systèmes, XML a été adopté comme format standard pour échanger des informations. XQuery est un langage d'intégration des données à travers différents systèmes, XML a été adopté comme format standard pour échanger des informations. XQuery est un langage d'interrogation pour XML qui s'est imposé pour les systèmes basés sur XML. Ainsi XQuery est employé sur des systèmes de médiation pour concevoir des vues définies sur plusieurs sources. Pour optimiser l'évaluation de requêtes, les vues sont matérialisées lors de la mise à jour des sources, car dans le contexte de sources web, très peu d'informations sont fournies par les sources. Les méthodes habituellement proposées ne peuvent pas être appliquées. Cet article étudie comment mettre à jour des vues matérialisées XML sur des sources web, au sein d'une architecture de médiation.	Tuyet-Tram Dang-Ngoc, Virginie Sans, Dominique Laurent	http://editions-rnti.fr/render_pdf.php?p1&p=1000331	http://editions-rnti.fr/render_pdf.php?p=1000331	année mettre évidence croissance diversité information électronique accessible web cest système dintégration donnée médiateur concevoir intégrer donnée distribuer hétérogène dan uniforme Pour faciliter lintégration donnée travers système xml adopter format standard échanger information xquery langage dintégration donnée travers système xml adopter format standard échanger information xquery langage dinterrogation XML sest imposer système baser xml Ainsi xquery employer système médiation concevoir définie source Pour optimiser lévaluation requêt vue matérialiser miser jour source dan contexte source web dinformation fournir source méthode habituellement proposer pouvoir appliquer article étudier mettre jour vue matérialisé xml source web dune architecturer médiation
1119	Revue des Nouvelles Technologies de l'Information	EGC	2005	Combinaison de fonctions de préférence par Boosting pour la recherche de passages dans les systèmes de question/réponse	Nous proposons une méthode d'apprentissage automatique pour la sélection de passages susceptibles de contenir la réponse à une question dans les systèmes de Question-Réponse (QR). Les systèmes de RI ad hoc ne sont pas adaptés à cette tâche car les passages recherchés ne doivent pas uniquement traiter du même sujet que la question mais en plus contenir sa réponse. Pour traiter ce problème les systèmes actuels ré-ordonnent les passages renvoyés par un moteur de recherche en considérant des critères sous forme d'une somme pondérée de fonctions de scores. Nous proposons d'apprendre automatiquement les poids de cette combinaison, grâce à un algorithme de réordonnancement défini dans le cadre du Boosting, qui sont habituellement déterminés manuellement. En plus du cadre d'apprentissage proposé, l'originalité de notre approche réside dans la définition des fonctions allouant des scores de pertinence aux passages. Nous validons notre travail sur la base de questions et de réponses de l'évaluation TREC-11 des systèmes de QR. Les résultats obtenus montrent une amélioration significative des performances en terme de rappel et de précision par rapport à un moteur de recherche standard et à une méthode d'apprentissage issue du cadre de la classification.	Nicolas Usunier, Massih-Reza Amini, Patrick Gallinari	http://editions-rnti.fr/render_pdf.php?p1&p=1000169	http://editions-rnti.fr/render_pdf.php?p=1000169	proposer méthode dapprentissage automatique sélection passage susceptible contenir réponse question dan système questionréponse QR système RI ad hoc adapter tâcher passage rechercher devoir uniquement traiter question plaire contenir réponse Pour traiter problème système actuel réordonner passage renvoyer moteur rechercher considérer critère sou former dune sommer pondérer fonction score proposer dapprendre automatiquement poids combinaison grâce algorithme réordonnancement définir dan cadrer Boosting habituellement déterminé manuellement En plaire cadrer dapprentissage proposer loriginalité approcher résider dan définition fonction allouer score pertinence passage valider travail baser question réponse lévaluation TREC11 système qr résultat obtenir montrer amélioration significatif performance terme rappel précision rapport moteur rechercher standard méthode dapprentissage issu cadrer classification
1120	Revue des Nouvelles Technologies de l'Information	EGC	2005	De la statistique des données à la statistique des connaissances : avancées récentes en analyse des données symboliques		Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1000437	http://editions-rnti.fr/render_pdf.php?p=1000437	
1121	Revue des Nouvelles Technologies de l'Information	EGC	2005	Élagage et aide à l'interprétation symbolique et graphique d'une pyramide	"Le but de ce travail est de faciliter l'interprétation d'une classification pyramidale construite sur un tableau de données symboliques. Alors que dans une hiérarchie binaire le nombre de paliers est égal à n-1, si n est le nombre d'individus à classer, dans le cas d'une pyramide ce dernier peut atteindre n(n-1)/2. Afin de réduire ce nombre, on élague la pyramide et on utilise un critère de sélection de paliers basé sur la hauteur. De plus on décrit tous les paliers retenus par des variables que l'on sélectionne également en utilisant ""le degré de généralité"" ainsi que des mesures de dissimilarités de type symbolique-numérique. L'aide à l'interprétation se sert d'outils graphiques et interactifs grâce à la bibliothèque OpenGL. Enfin une simulation montre comment évoluent ces sélections quand le nombre de classes et de variables croit."	Kutluhan Kemal Pak, Mohamed Cherif Rahal, Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1000226	http://editions-rnti.fr/render_pdf.php?p=1000226	boire travail faciliter linterprétation dune classification pyramidal construire tableau donnée symboliquer dan hiérarchie binaire nombre palier égal n1 nombre dindividus classer dan cas dune pyramid pouvoir atteindre nn12 Afin réduire nombre élaguer pyramide utiliser critère sélection palier baser hauteur De plaire décrire tou palier retenir variable lon sélectionner également utiliser degré généralité mesure dissimilarité typer symboliquenumériqu Laide linterprétation servir doutil graphique interactif grâce bibliothèque opengl simulation montr évoluer sélection nombre classe variable croire
1122	Revue des Nouvelles Technologies de l'Information	EGC	2005	Enrichissement sémantique de documents XML représentant des tableaux	Ce travail a pour objectif la construction automatique d'un entrepôt thématique de données, à partir de documents de format divers provenant du Web. L'exploitation de cet entrepôt est assurée par un moteur d'interrogation fondé sur une ontologie. Notre attention porte plus précisément sur les tableaux extraits de ces documents et convertis au format XML, aux tags exclusivement syntaxiques. Cet article présente la transformation de ces tableaux, sous forme XML, en un formalisme enrichi sémantiquement dont la plupart des tags et des valeurs sont des termes construits à partir de l'ontologie.	Fatiha Saïs, Hélène Gagliardi, Ollivier Haemmerlé, Nathalie Pernelle	http://editions-rnti.fr/render_pdf.php?p1&p=1000309	http://editions-rnti.fr/render_pdf.php?p=1000309	travail objectif construction automatique dun entrepôt thématique donnée partir document format provenir Web Lexploitation entrepôt assurer moteur dinterrogation fonder ontologie attention porter plaire précisément tableau extrait document convertir format xml tag exclusivement syntaxique article présenter transformation tableau sou former xml formalisme enrichir sémantiquemer tag terme construit partir lontologie
1123	Revue des Nouvelles Technologies de l'Information	EGC	2005	Entrepôt de Données Spatiales basé sur GML : Politique  de Gestion de Cache		Lionel Savary, Georges Gardarin, Karine Zeitouni	http://editions-rnti.fr/render_pdf.php?p1&p=1000290	http://editions-rnti.fr/render_pdf.php?p=1000290	
1124	Revue des Nouvelles Technologies de l'Information	EGC	2005	Évaluation des algorithmes LEM et eLEM pour données continues	Très populaire et très efficace pour l'estimation de paramètres d'un modèle de mélange, l'algorithme EM présente l'inconvénient majeur de converger parfois lentement. Son application sur des tableaux de grande taille devient ainsi irréalisable. Afin de remédier à ce problème, plusieurs méthodes ont été proposées. Nous présentons ici le comportement d'une méthode connue, LEM, et d'une variante que nous avons proposée récemment eLEM. Celles-ci permettent d'accélérer la convergence de l'algorithme, tout en obtenant des résultats similaires à celui-ci. Dans ce travail, nous nous concentrons sur l'aspect classification, et nous illustrons le bon comportement de notre variante sur des données continues simulées et réelles.	François-Xavier Jollois, Mohamed Nadif	http://editions-rnti.fr/render_pdf.php?p1&p=1000231	http://editions-rnti.fr/render_pdf.php?p=1000231	populaire efficace lestimation paramètre dun modeler mélanger lalgorithm em présenter linconvénient majeur converger lentement application tableau grand tailler devenir irréalisable Afin remédier problème méthode proposer présenter comportement dune méthode connaître lem dune variant proposer récemment eLEM Cellesci permettre daccélérer convergence lalgorithm obtenir résultat similaire celuici Dans travail concentrer laspect classification illustrer comportement variant donnée continu simuler réel
1125	Revue des Nouvelles Technologies de l'Information	EGC	2005	Expériences de classification d'une collection de documents XML de structure homogène	Cet article présente différentes expériences de classification de documents XML de structure homogène, en vue d'expliquer et de valider une présentation organisationnelle pré-existante. Le problème concerne le choix des éléments et mots utilisés pour la classification et son impact sur la typologie induite. Pour cela nous combinons une sélection structurelle basée sur la nature des éléments XML et une sélection linguistique basée sur un typage syntaxique des mots. Nous illustrons ces principes sur la collection des rapports d'activité 2003 des équipes de recherche de l'Inria en cherchant des groupements d'équipes (Thèmes) à partir du contenu de différentes parties de ces rapports. Nous comparons nos premiers résultats avec les thèmes de recherche officiels de l'Inria.	Thierry Despeyroux, Yves Lechevallier, Brigitte Trousse, Anne-Marie Vercoustre	http://editions-rnti.fr/render_pdf.php?p1&p=1000243	http://editions-rnti.fr/render_pdf.php?p=1000243	article présenter expérience classification document xml structurer homogène dexpliquer valider présentation organisationnel préexistant problème concerner choix élément utiliser classification impact typologie induit Pour celer combiner sélection structurel baser nature élément XML sélection linguistique basé typage syntaxique illustrer principe collection rapport dactivité 2003 équipe rechercher linria chercher groupement déquip thèm partir contenir party rapport comparer résultat thème rechercher officiel linria
1126	Revue des Nouvelles Technologies de l'Information	EGC	2005	Expérimentations sur un modèle de recherche d'information utilisant les liens hypertextes des pages Web	La fonction de correspondance, qui permet de sélectionner et de classer les documents par rapport à une requête est un composant essentiel dans tout système de recherche d'information. Nous proposons de modéliser une fonction de correspondance prenant en compte à la fois le contenu et les liens hypertextes des pages Web. Nous avons expérimenté notre système sur la collection de test TREC-9, et nous concluons que pour certains types de requêtes, inclure le texte ancre associé aux liens hypertextes des pages dans la fonction de similarité s'avère plus efficace.	Bich-Liên Doan, Idir Chibane	http://editions-rnti.fr/render_pdf.php?p1&p=1000264	http://editions-rnti.fr/render_pdf.php?p=1000264	fonction correspondance permettre sélectionner classer document rapport requête composer essentiel dan système rechercher dinformation proposer modéliser fonction correspondance prendre compter contenir lien hypertext page Web expérimenter système collection test TREC9 conclure type requête inclure texte ancrer associer lien hypertext page dan fonction similarité savère plaire efficace
1127	Revue des Nouvelles Technologies de l'Information	EGC	2005	Extension de l'algorithme Apriori et des règles d'association aux cas des données symboliques diagrammes et intervalles	Nous traitons l'extension de l'algorithme Apriori et des règles d'association aux cas des données symboliques diagrammes et intervalles. La méthode proposée nous permet de découvrir des règles d'association au niveau des concepts. Cette extension implique notamment de nouvelles définitions pour le support et la confiance afin d'exploiter la structure symbolique des données. Au fil de l'article l'exemple classique du panier de la ménagère est développé. Ainsi, plutôt que d'extraire des règles entre différents articles appartenant à des mêmes transactions enregistrées dans un magasin comme dans le cas classique, nous extrayons des règles d'association au niveau des clients afin d'étudier leurs comportements d'achat.	Filipe Afonso, Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1000348	http://editions-rnti.fr/render_pdf.php?p=1000348	traiter lextension lalgorithme Apriori règle dassociation cas donnée symbolique diagramm intervalle méthode proposer permettre découvrir règle dassociation niveau concept extension impliquer définition support confiance dexploiter structurer symbolique donnée Au fil larticle lexemple classique panier ménager développer dextraire règle entrer article appartenir transaction enregistrer dan magasin dan cas classique extraire règle dassociation niveau client détudier comportement dachat
1128	Revue des Nouvelles Technologies de l'Information	EGC	2005	Extension des bases de données inductives pour la découverte de chroniques	Les bases de données inductives intègrent le processus de fouille de données dans une base de données qui contient à la fois les données et les connaissances induites. Nous nous proposons d'étendre les données traitées afin de permettre l'extraction de motifs temporels fréquents et non fréquents à partir d'un ensemble de séquences d'évènements. Les motifs temporels visés sont des chroniques qui permettent d'exprimer des contraintes numériques sur les délais entre les occurrences d'évènements.	Alexandre Vautier, Marie-Odile Cordier, Rene Quiniou	http://editions-rnti.fr/render_pdf.php?p1&p=1000314	http://editions-rnti.fr/render_pdf.php?p=1000314	base donnée inductif intégrer processus fouiller donnée dan baser donnée contenir donnée connaissance induit proposer détendre donnée traiter permettre lextraction motif temporel fréquent fréquent partir dun ensemble séquence dévènement motif temporel viser chronique permettre dexprimer contrainte numérique délai entrer occurrence dévènement
1129	Revue des Nouvelles Technologies de l'Information	EGC	2005	Extraction Bayésienne et intégration de patterns représentés suivant les K plus proches voisins pour le go 19x19	Cet article décrit la génération automatique et l'utilisation d'une base de patterns pour le go 19x19. La représentation utilisée est celle des K plus proches voisins. Les patterns sont engendrés en parcourant des parties de professionnels. Les probabilités d'appariement et de jeu des patterns sont également estimées à ce moment là. La base créée est intégrée dans un programme existant, Indigo. Soit elle est utilisée comme un livre d'ouvertures en début de partie, soit comme une extension des bases pré-existantes du générateur de coups du programme. En terme de niveau de jeu, le gain résultant est estimé à 15 points en moyenne.	Bruno Bouzy, Guillaume Chaslot	http://editions-rnti.fr/render_pdf.php?p1&p=1000212	http://editions-rnti.fr/render_pdf.php?p=1000212	article décrire génération automatique lutilisation dune baser pattern go 19x19 représentation utiliser plaire voisin pattern engendrer parcourir party professionnel probabilité dappariemer jeu pattern également estimer moment baser créer intégrer dan programmer exister Indigo utiliser livrer douvertur partir extension base préexistanter générateur coup programmer En terme niveau jeu gain résulter estimer 15 point moyenner
1130	Revue des Nouvelles Technologies de l'Information	EGC	2005	Extraction bilingue de termes médicaux dans un corpus  parallèle anglais/français	Le Catalogue et Index des Sites Médicaux Francophones (CISMeF) recense les principales ressources institutionnelles de santé en français. La description de ces ressources, puis leur accès par les utilisateurs, se fait grâce à la terminologie CISMeF, fondée sur le thésaurus américain Medical Subject Headings (MeSH). La version française du MeSH comprend tous les descripteurs MeSH, mais de nombreux synonymes américains restent à traduire. Afin d'enrichir la terminologie, nous proposons ici une méthode de traduction automatique de ces synonymes. Pour ce faire, nous avons constitué deux corpus parallèles anglais/français du domaine médical. Après alignement semi-automatique des corpus paragraphe à paragraphe, nous avons procédé automatiquement à l'appariement bilingue des termes. Pour cela, le lexique constitué des descripteurs MeSH américains et de leur traduction en français a fourni les couples amorces qui ont servi de point de départ à la propagation syntaxique des liens d'appariement. 217 synonymes ont pu être traduits, avec une précision de 70%.	Aurélie Névéol, Sylwia Ozdowska	http://editions-rnti.fr/render_pdf.php?p1&p=1000413	http://editions-rnti.fr/render_pdf.php?p=1000413	Catalogue Index site médical Francophones CISMeF recenser principal ressource institutionneller santé français description ressource pouvoir accès utilisateur faire grâce terminologie cismef fonder thésauru américain Medical Subject Headings mesh version français MeSH comprendre tou descripteur MeSH synonyme américain ruer traduire Afin denrichir terminologie proposer méthode traduction automatique synonyme Pour faire constituer corpu parallèle anglaisfrançai domaine médical Après alignement semiautomatique corpus paragraphe paragraphe procéder automatiquement lappariement bilingue terme Pour celer lexiqu constituer descripteur mesh américain traduction français fournir couple amorc servir poindre départir propagation syntaxique lien dappariemer 217 synonyme pouvoir traduire précision 70
1131	Revue des Nouvelles Technologies de l'Information	EGC	2005	Extraction de la localisation des termes pour le classement des documents	Trouver et classer les documents pertinents par rapport à une requête est fondamental dans le domaine de la recherche d'information. Notre étude repose sur la localisation des termes dans les documents. Nous posons l'hypothèse que plus les occurrences des termes d'une requête se retrouvent proches dans un document alors plus ce dernier doit être positionné en tête de la liste de réponses. Nous présentons deux variantes de notre modèle à zone d'influence, la première est basée sur une notion de proximité floue et la seconde sur une notion de pertinence locale.	Annabelle Mercier, Michel Beigbeder	http://editions-rnti.fr/render_pdf.php?p1&p=1000269	http://editions-rnti.fr/render_pdf.php?p=1000269	trouver classer document pertinent rapport requête fondamental dan domaine rechercher dinformation étude reposer localisation terme dan document poser lhypothèse plaire occurrence terme dune requête retrouver dan document plaire devoir positionner tête liste réponse présenter variante modeler zone dinfluence baser notion proximité flou second notion pertinence local
1132	Revue des Nouvelles Technologies de l'Information	EGC	2005	Extraction de règles d'association quantitatives application à des données médicales	L'extraction de règles d'association est devenue aujourd'hui une tâche populaire en fouille de données. Cependant, l'algorithme Apriori et ses variantes restent dédiés aux bases de données renfermant des informations catégoriques.Nous proposons dans cet article QuantMiner, qui est un outil que nous avons développé dans le but d'extraire des règles d'association gérant variables catégoriques et numériques. L'outil que nous proposons repose sur un algorithme génétique permettant de découvrir de façon dynamique les intervalles des variables numériques apparaissant dans les règles.Nous présentons également une application réelle de notre outil sur des données médicales relatives à la maladie de l'athérosclérose et donnons des résultats de notre expérience pour la description et la caractérisation de cette maladie.	Cyril Nortet, Ansaf Salleb-Aouissi, Teddy Turmeaux, Christel Vrain	http://editions-rnti.fr/render_pdf.php?p1&p=1000352	http://editions-rnti.fr/render_pdf.php?p=1000352	lextraction règle dassociation devenir aujourdhui tâcher populaire fouiller donnée lalgorithme apriori variante ruer dédier base donnée renfermer information catégoriquesnous proposer dan article QuantMiner outil développer dan boire dextraire règle dassociation gérer variable catégorique numérique Loutil proposer reposer algorithme génétique permettre découvrir dynamique intervalle variable numérique apparaître dan règlesnou présenton également application réel outil donnée médical relatif maladie lathérosclérose donnon résultat expérience description caractérisation maladie
1133	Revue des Nouvelles Technologies de l'Information	EGC	2005	Extraction de termes centrée autour de l'expert	Nous développons un logiciel, Exit, capable d'aider un expert à extraire des termes qu'il trouve pertinents dans des textes de spécialité. Tout est mis en place pour faciliter le travail de l'expert afin qu'il puisse consacrer son temps à la seule reconnaissance des termes pertinents. Pour cela, différentes mesures statistiques et de nombreuses options d'extraction sont disponibles dans Exit. Afin d'utiliser au mieux les connaissances de l'expert, notre approche est semi-automatique. De plus, l'expert construit des termes pouvant inclure des termes précédemment extraits ce qui rend itératif et constructif notre processus de formation des termes. Enfin, l'ergonomie du logiciel a profité des enseignements tirés lors de son utilisation pour une compétition internationale d'extraction de connaissances.	Thomas Heitz, Mathieu Roche, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1000425	http://editions-rnti.fr/render_pdf.php?p=1000425	développer logiciel exit capable daider expert extraire terme quil trouver pertinent dan texte spécialité mettre placer faciliter travail lexpert quil pouvoir consacrer temps reconnaissance terme pertinent Pour celer mesure statistique option dextraction disponible dan Exit Afin dutiliser mieux connaissance lexpert approcher semiautomatique De plaire lexpert construire terme pouvoir inclure terme précédemment extrait itératif constructif processus formation terme Enfin lergonomie logiciel profiter enseignement tirer utilisation compétition international dextraction connaissance
1134	Revue des Nouvelles Technologies de l'Information	EGC	2005	Extraction des connaissances pour l'enrichissement des bases  de données géographiques		Sami Faiz, Khaoula Mahmoudi	http://editions-rnti.fr/render_pdf.php?p1&p=1000394	http://editions-rnti.fr/render_pdf.php?p=1000394	
1135	Revue des Nouvelles Technologies de l'Information	EGC	2005	Fonctions d'oubli et conservation de détail dans les entrepôts de données		Aliou Boly, Georges Hébrail, Marie-Luce Picard	http://editions-rnti.fr/render_pdf.php?p1&p=1000287	http://editions-rnti.fr/render_pdf.php?p=1000287	
1136	Revue des Nouvelles Technologies de l'Information	EGC	2005	Forage distribué des données : une comparaison entre l'agrégation d'échantillons et l'agrégation de règles	Pour nous attaquer au problème du forage de très grandes bases de données distribuées, nous proposons d'étudier deux approches. La première est de télécharger seulement un échantillon de chaque base de données puis d'y effectuer le forage. La deuxième approche est de miner à distance chaque base de données indépendamment, puis de télécharger les modèles résultants, sous forme de règles de classification, dans un site central où l'agrégation de ces derniers est réalisée. Dans cet article, nous présentons une vue d'ensemble des techniques d'échantillonnage les plus communes. Nous présentons ensuite cette nouvelle technique de forage distribué des données où la mécanique d'agrégation est basée sur un coefficient de confiance attribué à chaque règle et sur de très petits échantillons de chaque base de données. Le coefficient de confiance d'une règle est calculé par des moyens statistiques en utilisant le théorème limite centrale. En conclusion, nous présentons une comparaison entre les meilleures techniques d'échantillonnage que nous avons trouvées dans la littérature, et notre approche de forage distribué des données (FDD) basée sur l'agrégation de modèles.	Mohamed Aounallah, Sébastien Quirion, Guy W. Mineau	http://editions-rnti.fr/render_pdf.php?p1&p=1000211	http://editions-rnti.fr/render_pdf.php?p=1000211	Pour attaquer problème forage grand base donnée distribuer proposer détudier approche télécharger échantillon baser donnée pouvoir dy effectuer forage approcher miner distancer baser donnée indépendammer pouvoir télécharger modèle résultant sou former règle classification dan site central lagrégation réaliser Dans article présenter densembl technique déchantillonnage plaire commune présenter ensuite technique forage distribuer donnée mécanique dagrégation baser coefficient confiance attribuer régler petit échantillon baser donnée coefficient confiance dune régler calculer moyen statistique utiliser théorèm limiter central En conclusion présenter comparaison entrer meilleure technique déchantillonnage trouver dan littérature approcher forage distribuer donnée FDD basé lagrégation modèle
1137	Revue des Nouvelles Technologies de l'Information	EGC	2005	Fouille de Données Relationnelles dans les SGBD		Cédric Udréa, Fadila Bentayeb	http://editions-rnti.fr/render_pdf.php?p1&p=1000288	http://editions-rnti.fr/render_pdf.php?p=1000288	
1138	Revue des Nouvelles Technologies de l'Information	EGC	2005	Fouille de graphes et découverte de règles d'association : application à l'analyse d'images de document	Cet article présente une méthode permettant la découverte non supervisée de motifs fréquents représentatifs de symboles sur des images de documents. Les symboles sont considérés comme des entités graphiques porteurs d'information et les images de document sont représentées par des graphes relationnels attribués. Dans un premier temps, la méthode réalise la découverte de sous-graphes disjoints fréquents et fait correspondre pour chacun d'eux un symbole différent. Une recherche des règles d'association entre ces symboles permet alors d'accéder à une partie des connaissances du domaine décrit par ces symboles. L'objectif à terme est d'utiliser les symboles découverts pour la classification ou la recherche d'images dans un flux hétérogène de document là ou une approche supervisée n'est pas envisageable.	Eugen Barbu, Pierre Héroux, Sébastien Adam, Éric Trupin	http://editions-rnti.fr/render_pdf.php?p1&p=1000341	http://editions-rnti.fr/render_pdf.php?p=1000341	article présenter méthode permettre découvrir superviser motif fréquent représentatif symbole image document symbole considérer entité graphique porteur dinformation image document représenter graphe relationnel attribuer Dans temps méthode réalis découvrir sousgraphe disjoint fréquent faire correspondr symbole rechercher règle dassociation entrer symbole permettre daccéder partir connaissance domaine décrire symbole Lobjectif terme dutiliser symbole découvert classification rechercher dimager dan flux hétérogène document approcher superviser nest envisageable
1139	Revue des Nouvelles Technologies de l'Information	EGC	2005	Fouille de textes pour orienter la construction d'une ressource terminologique	La finalité de ce papier est d'analyser l'apport de techniques de fouille de données textuelles à une méthodologie de construction d'ontologie à partir de textes. Le domaine d'application de cette expérimentation est celui de l'accidentologie routière. Dans ce contexte, les résultats des techniques de fouille de données textuelles sont utilisés pour orienter la construction d'une ressource terminologique à partir de procès-verbaux d'accidents. La méthode TERMINAE et l'outil du même nom offrent le cadre général pour la modélisation de la ressource. Le papier présente les techniques de fouille employées et l'intégration des résultats des fouilles dans les différentes étapes du processus de construction de la ressource.	Valentina Ceausu, Sylvie Desprès	http://editions-rnti.fr/render_pdf.php?p1&p=1000261	http://editions-rnti.fr/render_pdf.php?p=1000261	finalité papier danalyser lapport technique fouiller donnée textuel méthodologie construction dontologie partir texte domaine dapplication expérimentation laccidentologie routier Dans contexte résultat technique fouiller donnée textuel utiliser orienter construction dune ressource terminologique partir procèsverbaux daccident méthode terminae loutil nom offrir cadrer général modélisation ressource papier présenter technique fouiller employer lintégration résultat fouille dan étape processus construction ressource
1140	Revue des Nouvelles Technologies de l'Information	EGC	2005	Hiérarchisation des règles d'association en fouille de textes	L'extraction de règles d'association est souvent exploitée comme méthode de fouille de données. Cependant, une des limites de cette approche vient du très grand nombre de règles extraites et de la difficulté pour l'analyste à appréhender la totalité de ces règles. Nous proposons donc de pallier ce problème en structurant l'ensemble des règles d'association en hiérarchies. La structuration des règles se fait à deux niveaux. Un niveau global qui a pour objectif de construire une hiérarchie structurant les règles extraites des données. Nous définissons donc un premier type de subsomption entre règles issue de la subsomption dans les treillis de Galois. Le second niveau correspond à une analyse locale des règles et génère pour une règle donnée une hiérarchie de généralisation de cette règle qui repose sur des connaissances complémentaires exprimées dans un modèle terminologique. Ce niveau fait appel à un second type de subsomption inspiré de la subsomption en programmation logique inductive. Nous définissons ces deux types de subsomptions, développons un exemple montrant l'intérêt de l'approche pour l'analyste et étudions les propriétés formelles des hiérarchies ainsi proposées.	Rokia Bendaoud, Yannick Toussaint, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1000268	http://editions-rnti.fr/render_pdf.php?p=1000268	lextraction règle dassociation exploiter méthode fouiller donnée limite approcher venir grand nombre règle extrait difficulté lanalyste appréhender totalité règle proposer pallier problème structurer lensembl règle dassociation hiérarchie structuration règle faire niveau niveau global objectif construire hiérarchie structurer règle extrait donnée définir typer subsomption entrer règle issu subsomption dan treillis Galois second niveau correspondre analyser local règle génèr régler donner hiérarchie généralisation régler reposer connaissance complémentaire exprimer dan modeler terminologique niveau faire appel second typer subsomption inspirer subsomption programmation logique inductif définir type subsomption développon exemple montrer lintérêt lapproche lanalyste étudion propriété formel hiérarchie proposer
1141	Revue des Nouvelles Technologies de l'Information	EGC	2005	Intégration efficace des arbres de décision dans les SGBD : utilisation des index bitmap	Nous présentons dans cet article une nouvelle approche de fouille qui permet d'appliquer des algorithmes de construction d'arbres de décision en répondant à deux objectifs : (1) traiter des bases volumineuses, (2) en des temps de traitement acceptables. Le premier objectif est atteint en intégrant ces algorithmes au cœur des SGBD, en utilisant uniquement les outils fournis par ces derniers. Toutefois, les temps de traitement demeurent longs, en raison des nombreuses lectures de la base. Nous montrons que, grâce aux index bitmap, nous réduisons à la fois la taille de la base d'apprentissage et les temps de traitements. Pour valider notre approche, nous avons implémenté la méthode ID3 sous forme d'une procédure stockée dans le SGBD Oracle.	Cécile Favre, Fadila Bentayeb	http://editions-rnti.fr/render_pdf.php?p1&p=1000281	http://editions-rnti.fr/render_pdf.php?p=1000281	présenter dan article approcher fouiller permettre dappliquer algorithme construction darbr décision répondre objectif   1 traiter base volumineux 2 temps traitement acceptabler objectif atteindre intégrer algorithme cœur sgbd utiliser uniquement outil fournir temps traitement demeurer long raison lectur baser montrer grâce index bitmap réduire tailler baser dapprentissage temps traitement Pour valider approcher implémenter méthode id3 sou former dune procédure stocké dan sgbd oracl
1142	Revue des Nouvelles Technologies de l'Information	EGC	2005	L'automate textuel pour la prise en compte de l'évolution du texte	Il n'est plus à rappeler que le corpus textuel, est tel qu'il est actuellement, intraitable à l'échelle que sa croissance nous confirme l'obligation d'utiliser des outils automatique de traitement. Cet article s'intéresse plus particulièrement à la caractérisation de textes et par là même à celle d'auteurs. A l'heure actuelle, toutes les méthodes existant travaillent sur le document fini, sans admettre qu'un cheminement existe entre le début du document et sa fin. Nous proposons une méthode tentant d'apporter cette notion d'évolution textuelle en traitant le texte par un automate et l'évaluation choisie. Puis nous présenterons des résultats validés par des experts, obtenus sur un corpus d'entretiens sociologiques.	Hubert Marteau, Nicole Vincent	http://editions-rnti.fr/render_pdf.php?p1&p=1000276	http://editions-rnti.fr/render_pdf.php?p=1000276	nest plaire rappeler corpus textuel quil actuellement intraitable léchelle croissance confirmer lobligation dutiliser outil automatique traitement article sintéresse plaire caractérisation texte dauteur lheure actuel méthode exister travailler document finir admettre quun cheminement exister entrer document fin proposer méthode tenter dapporter notion dévolution textuel traiter texte automate lévaluation choisir Puis présenter résultat valider expert obtenir corpus dentretien sociologique
1143	Revue des Nouvelles Technologies de l'Information	EGC	2005	La démarche ontologique pour la gestion des compétences et des connaissances	"La gestion des ressources humaines repose d'une part sur la connaissance des individus et de leurs compétences et d'autre part sur la connaissance de l'organisation et de ses métiers. C'est par la ""mise en correspondance"" de ces connaissances qu'il est possible d'améliorer l'emploi, de valoriser les connaissances et les compétences individuelles et de mieux gérer l'organisation. Cette mise en correspondance nécessite une représentation explicite des connaissances, ce qui permet de répondre à de nouveaux besoins : annuaire de compétences, gestion des projets et des retours d'expériences, identification des connaissances à risques, etc.Nous verrons dans le cadre de cet article l'intérêt de l'approche ontologique tant d'un point de vue méthodologique pour la clarification des notions mises en jeu dans le cadre de la GPECC (Gestion Prévisionnelle des Emplois des Compétences et des Connaissances) que pour la construction, la représentation et la maintenance des référentiels des compétences, des connaissances et des métiers. Elle permet en particulier une gestion de l'information par la terminologie et le sens métier propre à l'organisation."	Christophe Roche, Charles Foveau, Samah Reguigui	http://editions-rnti.fr/render_pdf.php?p1&p=1000299	http://editions-rnti.fr/render_pdf.php?p=1000299	gestion ressource humain reposer dune partir connaissance individu compétence dautre partir connaissance lorganisation métier Cest miser correspondance connaissance quil daméliorer lemploi valoriser connaissance compétence individuel mieux gérer lorganisation miser correspondance nécessiter représentation expliciter connaissance permettre répondre besoin   annuaire compétence gestion projet dexpérience identification connaissance risque etcNous voir dan cadrer article lintérêt lapproche ontologique dun poindre méthodologique clarification notion mettre jeu dan cadrer gpecc Gestion prévisionnel emploi compétence connaissance construction représentation maintenance référentiel compétence connaissance métier permettre gestion linformation terminologie sens métier propre lorganisation
1144	Revue des Nouvelles Technologies de l'Information	EGC	2005	La réussite universitaire : prédictions par génération de règles		Nadine Meskens, Jean-Philippe Vandamme, Abdelhakim Artiba	http://editions-rnti.fr/render_pdf.php?p1&p=1000361	http://editions-rnti.fr/render_pdf.php?p=1000361	
1145	Revue des Nouvelles Technologies de l'Information	EGC	2005	Les NTIC au services de la capitalisation des connaissances		Jean-Paul Barthès	http://editions-rnti.fr/render_pdf.php?p1&p=1000439	http://editions-rnti.fr/render_pdf.php?p=1000439	
1146	Revue des Nouvelles Technologies de l'Information	EGC	2005	Logiciel d'aide à l'étiquetage morpho-syntaxique de  textes de spécialité	La compréhension de textes de spécialité nécessite un étiquetage morpho-syntaxique de bonne qualité. Or, lorsque les textes étudiés sont issus de domaines spécifiques et peu usités, il est rare de disposer de dictionnaires et autres ressources lexicales fiables. Le logiciel que nous proposons permet d'utiliser un étiquetage réalisé par un étiqueteur généraliste, puis d'améliorer cet étiquetage en intégrant des connaissances d'experts du domaine étudié. Grâce au logiciel développé, il est relativement aisé pour un expert du domaine de détecter des erreurs d'étiquetage et de mettre en place des règles de ré-étiquetage. Ces règles peuvent être obtenues de deux manières différentes : (1) soit en utilisant un langage de programmation permettant d'exprimer des règles complexes de ré-étiquetage, (2) soit par apprentissage automatique des règles à partir d'exemples corrigés au moyen d'une interface dédiée. Cet apprentissage propose de nouvelles règles à l'expert, acquises automatiquement.	Ahmed Amrani, Jérôme Azé, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1000421	http://editions-rnti.fr/render_pdf.php?p=1000421	compréhension texte spécialité nécessiter étiquetage morphosyntaxique qualité Or texte étudié issu domaine spécifique usité disposer dictionnaire ressource lexical fiable logiciel proposer permettre dutiliser étiquetage réaliser étiqueteur généraliste pouvoir daméliorer étiquetage intégrer connaissance dexperts domaine étudier grâce logiciel développer aiser expert domaine détecter erreur détiquetage mettre placer règle réétiquetage règle pouvoir obtenu manière   1 utiliser langage programmation permettre dexprimer règle complexe réétiquetage 2 apprentissage automatique règle partir dexempl corrigé moyen dune interface dédier apprentissage proposer règle lexpert acquérir automatiquement
1147	Revue des Nouvelles Technologies de l'Information	EGC	2005	Manipulation et fusion de données multidimensionnelles	Cet article définit une algèbre permettant de manipuler des tables dimensionnelles extraites d'une base de données multidimensionnelles. L'algèbre intègre un noyau minimum d'opérateurs unaires permettant d'effectuer les analyses décisionnelles par combinaison d'opérateurs. Cette algèbre intègre un opérateur binaire permettant la fusion de tables dimensionnelles facilitant les corrélations des sujets analysés.	Franck Ravat, Olivier Teste, Gilles Zurfluh	http://editions-rnti.fr/render_pdf.php?p1&p=1000286	http://editions-rnti.fr/render_pdf.php?p=1000286	article définir algèbre permettre manipuler table dimensionnel extraite dune baser donnée multidimensionnel Lalgèbre intègre noyau minimum dopérateurs unaire permettre deffectuer analyse décisionnel combinaison dopérateur algèbre intègre opérateur binaire permettre fusion table dimensionnel faciliter corrélation analyser
1148	Revue des Nouvelles Technologies de l'Information	EGC	2005	Méthode de construction d'ontologie de termes à partir du treillis de l'iceberg de Galois	"L'approche présentée dans cet article a pour objectif la construction d'une ontologie à partir du treillis de l'iceberg de Galois. Nous entendons par ontologie un ensemble de termes structurés entre eux par un ensemble de liens de divers types. Dans notre cas d'étude, cette ontologie constitue un support de connaissances ""documentaires"". En effet, elle peut être utilisée dans diverses applications en Recherche d'Information (RI), telles que l'indexation automatique et l'expansion de requêtes ainsi qu'en text-mining. La méthode de construction que nous proposons est fondée sur l'analyse formelle de concepts (AFC) et plus précisément, la structure du treillis de l'iceberg de Galois. En utilisant cette structure hiérarchique partiellement ordonnée, nous présentons une translation directe des relations laticielles vers celles ontologiques. Nous proposons ainsi d'enrichir l'ontologie dérivée par des règles associatives génériques entre termes, découvertes dans le cadre d'un processus de text-mining."	Cherif Chiraz Latiri, Mehdi Mtir, Sadok Ben Yahia	http://editions-rnti.fr/render_pdf.php?p1&p=1000304	http://editions-rnti.fr/render_pdf.php?p=1000304	lapproche présenter dan article objectif construction dune ontologie partir treillis liceberg Galois entendre ontologie ensemble terme structuré entrer ensemble lien type Dans cas détude ontologie constituer support connaissance documentaire En pouvoir utiliser dan application rechercher dinformation RI lindexation automatique lexpansion requête quen textmining méthode construction proposer fonder lanalyse formel concept AFC plaire précisément structurer treillis liceberg Galois En utiliser structurer hiérarchique partiellement ordonner présenter translation direct relation laticiell ver ontologique proposer denrichir lontologie dérivé règle associatif générique entrer terme découvrir dan cadrer dun processus textmining
1149	Revue des Nouvelles Technologies de l'Information	EGC	2005	Microarray data mining : recent advances		Gregory Piatetsky-Shapiro	http://editions-rnti.fr/render_pdf.php?p1&p=1000443	http://editions-rnti.fr/render_pdf.php?p=1000443	
1150	Revue des Nouvelles Technologies de l'Information	EGC	2005	Mining Frequent Queries in Star Schemes	L'extraction de toutes les requêtes fréquentes dans une base de données relationnelle est un problème di±cile, même si l'on ne considère que des requêtes conjonctives. Nous montrons que ce problème devient possible dans le cas suivant : le schéma de la base est un schéma en étoile, et les données satisfont un ensemble de dépendances fonctionnelles et de contraintes référentielles. De plus, les schémas en étoile sont appropriés pour les entrepôts de données et que les dépendances fonctionnelles et les contraintes référentielles sont les contraintes les plus usuelles dans les bases de données. En considérant le modèle des instances faibles, nous montrons que les requêtes fréquentes exprimées par sélection-projection peuvent être extraites par des algorithmes de type Apriori.	Tao-Yuan Jen, Dominique Laurent, Nicolas Spyratos, Oumar Sy	http://editions-rnti.fr/render_pdf.php?p1&p=1000283	http://editions-rnti.fr/render_pdf.php?p=1000283	lextraction requêt fréquent dan baser donnée relationnel problème di±cile lon considérer requête conjonctive montrer problème devenir dan cas   schéma baser schéma étoiler donnée satisfaire ensemble dépendance fonctionnel contrainte référentiel De plaire schéma étoiler approprié entrepôt donnée dépendance fonctionnel contrainte référentiel contraint plaire usuel dan base donnée En considérer modeler instance faible montrer requêt fréquent exprimer sélectionprojection pouvoir extraire algorithme typer Apriori
1151	Revue des Nouvelles Technologies de l'Information	EGC	2005	Modélisation d'objets mobiles dans un entrepôt de données	La gestion d'objets mobiles a connu un regain d'intérêt ces dernières années, particulièrement dans le but de gérer et de prédire la localisation d'objets mobiles. Cependant, il y a peu de recherches sur l'exploitation d'historiques de bases d'objets mobiles. La première étape dans ce processus est la mise en œuvre d'un entrepôt d'objets mobiles. Seulement, les modèles d'entrepôts existants ne permettent pas de traiter directement ce type de données complexes. Cet article présente une approche originale pour pallier ce problème. Cette approche offre la puissance de l'algèbre OLAP sur toute combinaison de données classiques, spatiales et/ou temporelles et mobiles. Elle a été validée par un prototype et appliquée à l'analyse de la mobilité urbaine1. Les résultats de l'expérimentation montrent la validité de l'approche et les tests de performances son efficacité.	Tao Wan, Karine Zeitouni	http://editions-rnti.fr/render_pdf.php?p1&p=1000284	http://editions-rnti.fr/render_pdf.php?p=1000284	gestion dobjet mobile connaître regain dintérêt année dan boire gérer prédire localisation dobjet mobile yu recherche lexploitation dhistoriqu base dobjet mobile étape dan processus miser œuvre dun entrepôt dobjet mobile modèle dentrepôt existant permettre traiter typer donnée complexe article présenter approcher original pallier problème approcher offrir puissance lalgèbre olap combinaison donnée classique spatiale etou temporel mobile valider prototype appliquer lanalyse mobilité urbaine1 résultat lexpérimentation montrer validité lapproche test performance efficacité
1152	Revue des Nouvelles Technologies de l'Information	EGC	2005	Modélisation d'un agent émotionnel en UML et RDF	Pouvoir extraire de la connaissance à partir d'une plate-forme de simulation est aujourd'hui envisageable en conjuguant les avancées obtenues en Intelligence Artificielle autour des systèmes multi-agents et les méthodes de formalisation et d'extraction des connaissances. C'est donc dans un cadre général de gestion des connaissances que nous proposons de modéliser un agent artificiel doté de connaissances et d'émotions. Pour cela, une expertise psychologique a été recueillie et formalisée de manière à être stockée dans une base de connaissances sous forme de règles et de classes en UML et RDF. L'implémentation du modèle permet d'entrevoir les perspectives d'une telle simulation : enrichissement par des données issues de simulations, découverte de nouvelles connaissances par l'application de processus d'ECD.	Hélène Desmier, Fabrice Guillet, Adina Magda Florea, Henri Briand, Vincent Philippé	http://editions-rnti.fr/render_pdf.php?p1&p=1000406	http://editions-rnti.fr/render_pdf.php?p=1000406	pouvoir extraire connaissance partir dune plateforme simulation aujourdhui envisageable conjuguer avancée obtenu Intelligence Artificielle autour système multiagent méthode formalisation dextraction connaissance cest dan cadrer général gestion connaissance proposer modéliser agent artificiel doter connaissance démotion Pour celer expertiser psychologique recueillir formaliser manière stocker dan baser connaissance sou former règle classe uml RDF Limplémentation modeler permettre dentrevoir perspective dune simulation   enrichissemer donnée issu simulation découvrir connaissance lapplication processus decd
1153	Revue des Nouvelles Technologies de l'Information	EGC	2005	Modélisation de connaissances pour un système de médiation	Travaillant sur l'élaboration d'une méthodologie de développement de systèmes de médiation intégrés dans des systèmes coopératifs, nous avons proposé une architecture à 3 composants : le premier concerne la coopération, le second l'assistance et le troisième est relatif aux connaissances nécessaires aux 2 précédents. Dans cet article nous présentons plus particulièrement le point de vue des connaissances. Ces connaissances sont de 2 natures : des connaissances statiques, sur le domaine par exemple, et des connaissances acquises pendant l'utilisation coopérative du système, notamment la mémoire des activités et les descriptions des actes de résolutions de problèmes. Pour illustrer cette modélisation de connaissances, nous nous intéresserons aux activités coopératives de suivi, de gestion et d'évaluation de projets d'étudiants, assistées par l'outil iPédagogique.	Victoria Eugenia Ospina, Alain-Jérôme Fougères, Manuel Zacklad	http://editions-rnti.fr/render_pdf.php?p1&p=1000383	http://editions-rnti.fr/render_pdf.php?p=1000383	travailler lélaboration dune méthodologie développement système médiation intégrer dan système coopératif proposer architecturer 3 composant   concerner coopération second lassistance relatif connaissance nécessaire 2 précédent Dans article présenter plaire poindre connaissance connaissance 2 nature   connaissance statique domaine exemple connaissance acquérir pendre lutilisation coopératif système mémoire activité description acte résolution problème Pour illustrer modélisation connaissance intéresser activité coopératif gestion dévaluation projet détudiant assister loutil ipédagogiqu
1154	Revue des Nouvelles Technologies de l'Information	EGC	2005	Modélisation de la cognition sociale – propositions autour de  l'utilisation de schémas cognitifs		Jorge Louçã	http://editions-rnti.fr/render_pdf.php?p1&p=1000395	http://editions-rnti.fr/render_pdf.php?p=1000395	
1155	Revue des Nouvelles Technologies de l'Information	EGC	2005	Modélisation des individus et de leurs relations pour l'aide à l'intégration des individus dans l'organisation	L'objectif de ce papier est de présenter une contribution à la modélisation des individus et de leurs relations pour permettre l'aide à l'intégration des acteurs dans une organisation. Nous étudions en particulier le cas du remplacement d'un acteur (« turn-over »). Dans ce cadre, nous proposons un modèle regroupant un ensemble de données relatives à un individu, aux relations que celui-ci entretient avec les autres acteurs et à son espace informationnel. L'étude porte sur la mise en oeuvre de mécanismes d'aide fournissant à un acteur les moyens de son intégration : la mise à disposition d'une image des espaces informationnels et relationnels de son prédécesseur ainsi que la mise en relation de l'acteur avec les autres acteurs de l'organisation. Cette étude est menée en partenariat avec des experts en GRH.	Marie-Françoise Canut, Max Chevalier, André Péninou, Florence Sedes	http://editions-rnti.fr/render_pdf.php?p1&p=1000392	http://editions-rnti.fr/render_pdf.php?p=1000392	Lobjectif papier poster contribution modélisation individu relation permettre laid lintégration acteur dan organisation étudier cas remplacement dun acteur « turnover » Dans cadrer proposer modeler regrouper ensemble donnée relatif individu relation celuici entretenir acteur espacer informationnel Létude porter miser oeuvrer mécanisme daid fournir acteur moyen intégration   miser disposition dune imager espace informationnel relationnel prédécesseur miser relation lacteur acteur lorganisation étude mener partenariat expert GRH
1156	Revue des Nouvelles Technologies de l'Information	EGC	2005	Modélisation des interactions entre individus avec AgentUML	Pour faciliter l'étude de certains phénomènes, des outils de simulation ont été créés dans de nombreux domaines. L'étude du comportement humain à jusque là échappé à cette tendance. Aujourd'hui, les systèmes multi-agents couplés aux avancées des sciences humaines fournissent les bases nécessaires à l'élaboration de ce type d'outil. Cet article s'inscrit ainsi dans cette dynamique avec l'objectif de développer un outil de simulation du comportement d'individus traumatisés crâniens sur une chaîne de production. Cet outil doit permettre la collecte de la connaissance relative au système étudié et fournir une aide à la décision pour les responsables de l'entreprise. Cet article propose une modélisation des interactions entre individus dans le formalisme AgentUML. Une implémentation du modèle au sein d'un outil de simulation fonctionnel et les résultats obtenus seront également présentés. A terme, le but est la production de données de simulation exploitables par des techniques d'ECD.	Stéphane Daviet, Fabrice Guillet, Henri Briand, Adina Magda Florea, Vincent Philippé	http://editions-rnti.fr/render_pdf.php?p1&p=1000399	http://editions-rnti.fr/render_pdf.php?p=1000399	Pour faciliter létude phénomène outil simulation créer dan domaine Létude comportement humain échapper tendance Aujourdhui système multiagent coupler avancer science humain fournir base nécessaire lélaboration typer doutil article sinscrit dan dynamique lobjectif développer outil simulation comportement dindividus traumatiser crânien chaîner production outil devoir permettre collecter connaissance relatif système étudier fournir aider décision responsable lentreprise article proposer modélisation interaction entrer individu dan formalisme AgentUML implémentation modeler dun outil simulation fonctionnel résultat obtenu également présenter A terme boire production donnée simulation exploitable technique decd
1157	Revue des Nouvelles Technologies de l'Information	EGC	2005	Motifs séquentiels flous : un peu, beaucoup, passionément	La plupart des bases de données issues du monde réel sont constituées de données numériques et historiées (données de capteurs, données scientifiques, données démographiques). Dans ce cadre les algorithmes d'extraction de motifs séquentiels, s'ils sont adaptés au caractère temporel des données ne permettent pas le traitement de données numériques. es données sont alors pré-traitées pour les transformer en données binaire, ce qui entraîne une perte d'information. Des algorithmes ont donc été proposés pour traiter les données numériques sous forme d'intervalles et d'intervalles flous notamment. En ce qui concerne la recherche de motifs séquentiels fondée sur des intervalles flous, les deux méthodes de la littérature ne sont pas satisfaisantes car incomplètes soit dans le traitement des séquences soit dans le calcul du support. Dans cet article, nous proposons donc trois méthodes d'extraction de motifs séquentiels flous {SPEEDYFUZZY, MINIFUZZY et TOTALLYFUZZY) et en détaillons les algorithmes sous-jacents en soulignant les différents niveaux de fuzzification. Ces algorithmes sont implémentés et évalués à travers différentes expérimentations menées sur des jeux de tests synthétiques.	Céline Fiot, Anne Laurent, Maguelonne Teisseire	http://editions-rnti.fr/render_pdf.php?p1&p=1000357	http://editions-rnti.fr/render_pdf.php?p=1000357	base donnée issu monder réel constituer donnée numérique historier donner capteur donnée scientifique donnée démographique Dans cadrer algorithme dextraction motif séquentiel sil adapter caractère temporel donnée permettre traitement donnée numérique donnée prétraiter transformer donnée binaire entraîner perte dinformation algorithme proposer traiter donnée numérique sou former dintervall dintervall flou En concerner rechercher motif séquentiel fonder intervalle flou méthode littérature satisfaisanter incomplète dan traitement séquence dan calcul support Dans article proposer méthode dextraction motif séquentiel flou speedyfuzzy minifuzzy totallyfuzzy détaillon algorithme sousjacent souligner niveau fuzzification algorithme implémenter évaluer travers expérimentation mener jeu test synthétique
1158	Revue des Nouvelles Technologies de l'Information	EGC	2005	Notion de sémantiques bien-formées pour les règles	La notion de règles entre attributs est très générale, allant des règles d'association en fouille de données aux dépendances fonctionnelles (DF) en bases de données. Malgré cette diversité, la syntaxe des règles est toujours la même, seule leur sémantique diffère. Pour une sémantique donnée, en fonction des propriétés induites, des techniques algorithmiques sont mises en oeuvre pour découvrir les règles à partir des données. A partir d'un ensemble de règles, il est aussi utile en pratique de raisonner sur ces règles, comme cela est le cas par exemple avec les axiomes d'Armstrong pour les dépendances fonctionnelles. Dans cet article, nous proposons un cadre qui permet de s'assurer qu'une sémantique donnée pour les règles est bien-formée, i.e. les axiomes d'Armstrong sont justes et complets pour cette sémantique. Les propositions faites dans ce papier proviennent du contexte applicatif de l'analyse de données de biopuces. A partir de plusieurs sémantiques pour les données d'expression de gènes, nous montrons comment ces sémantiques s'intègrent dans le cadre présenté.	Marie Agier, Jean-Marc Petit	http://editions-rnti.fr/render_pdf.php?p1&p=1000207	http://editions-rnti.fr/render_pdf.php?p=1000207	notion règle entrer attribut général aller règle dassociation fouiller donnée dépendanc fonctionnel DF base donnée Malgré diversité syntaxe règle sémantique diffèr Pour sémantique donner fonction propriété induit technique algorithmique mettre oeuvrer découvrir règle partir donnée A partir dun ensemble règle utile pratiquer raisonner règle celer cas exemple axiome darmstrong dépendance fonctionnel Dans article proposer cadrer permettre sassurer quune sémantique donné règle bienformer ie axiome dArmstrong complet sémantique proposition dan papier provenir contexte applicatif lanalyse donnée biopuce A partir sémantique donnée dexpression gène montrer sémantique sintègrent dan cadrer présenter
1159	Revue des Nouvelles Technologies de l'Information	EGC	2005	Outil de classification et de visualisation de grands volumes de données mixtes	Nous avons conçu un outil de classification de données original que nous détaillons dans le présent article. Cet outil comporte un module de création de résumés et un module d'affichage. Le module de visualisation permet une lecture aisée des résumés grâce à une interface graphique évoluée permettant la présentation et l'exploration des résumés sous forme d'une hiérarchie de profils ou d'un tableau de profils. Chaque profil donne de manière claire les informations relatives au résumé de données correspondant. La lecture de la hiérarchie et du tableau est aussi grandement facilitée par les choix d'un ordre optimal pour la présentation des variables et des résumés.	Christophe Candillier, Noureddine Mouaddib	http://editions-rnti.fr/render_pdf.php?p1&p=1000369	http://editions-rnti.fr/render_pdf.php?p=1000369	concevoir outil classification donnée original détailler dan présent article outil comporter moduler création résumé moduler daffichage moduler visualisation permettre lecture aisé résumé grâce interface graphique évoluer permettre présentation lexploration résumé sou former dune hiérarchie profil dun tableau profil profil donner manière clair information relatif résumer donnée correspondre lecture hiérarchie tableau grandement faciliter choix dun ordre optimal présentation variable résumé
1160	Revue des Nouvelles Technologies de l'Information	EGC	2005	Prise en compte des « Points de Vue » pour l'annotation d'un processus d'Extraction de Connaissances à partir de Données	Dans cet article on propose une nouvelle approche qui rend explicite la notion de point de vue dans une analyse multivues issue d'un processus d'Extraction de Connaissances à partir de Données (ECD). Par point de vue, nous entendons la vision particulière d'un analyste lors de son processus ECD, vision référant à un corps de connaissances qui lui est spécifique. On cherche, d'une part, à faciliter la réutilisabilité et l'adaptabilité du processus, et d'autre part à garder une trace des points de vues sous-jacents aux analyses faites. Le processus d'ECD sera vu comme un processus de génération et de transformation de vues qui seront annotées par des métadonnées pour garder la sémantique de la connaissance extraite. Un positionnement de notre approche vis-à-vis des travaux méthodologiques du processus d'ECD sera donné. Des éléments de modélisation du processus ECD basé sur les points de vue seront décrits au niveau ontologique. Enfin, on illustrera notre approche sur l'analyse des usages d'un site web à partir des fichiers log, selon le point de vue fiabilité.	Hicham Behja, Brigitte Trousse, Abdelaziz Marzak	http://editions-rnti.fr/render_pdf.php?p1&p=1000263	http://editions-rnti.fr/render_pdf.php?p=1000263	Dans article proposer approcher expliciter notion poindre dan analyser multivu issu dun processus dextraction connaissance partir donnée ecd Par poindre entendre vision dun analyste processus ECD vision référer corps connaissance luire spécifique chercher dune partir faciliter réutilisabilité ladaptabilité processus dautre partir garder tracer point sousjacent analys faite processus decd voir processus génération transformation vue annoter métadonnée garder sémantique connaissance extraire positionnement approcher visàvis travail méthodologique processus decd donner élément modélisation processus ecd baser point décrire niveau ontologique illustrer approcher lanalyse usage dun site web partir fichier log poindre fiabilité
1161	Revue des Nouvelles Technologies de l'Information	EGC	2005	Problématiques de gestion de connaissances dans le cadre de l'enseignement à distance sur l'Internet.	Le développement des réseaux à haut-débit et de l'Internet fournit un nouveau support à l'enseignement à distance. Aujourd'hui, de nombreux acteurs dans le domaine de l'enseignement ont mis en place des dispositifs de formation en ligne. Ceux-ci se composent généralement d'une sélection de matériaux organisés et présentés de manière à suivre un programme pédagogique particulier, de mécanismes de communication entre apprenants et enseignants, et d'outils de suivi des apprenants. Les plates-formes d'enseignement à distance devenant de plus en plus génériques, des nouveaux modèles ont été définis, standardisés ou normalis és, permettant la formalisation de méta-données pédagogiques ou tentant d'évaluer les connaissances acquises par les apprenants. En nous appuyant sur ces modèles, nous proposons de construire une base de connaissances, associant notamment les termes des domaines enseignés en relations à sémantique pédagogique. L'exploitation de cette base de connaissances fournit un premier niveau d'aide à l'ingénierie pédagogique, en particulier lorsque le volume de contenus en ligne est important. Des inférences mettant en jeu ces connaissances permettent alors un meilleur suivi du dispositif d'enseignement.	Romain Dailly, Christian Chervet, Rémi Lehn, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1000389	http://editions-rnti.fr/render_pdf.php?p=1000389	développement réseau hautdébit lInternet fournir support lenseignement distancer Aujourdhui acteur dan domaine lenseignement mettre placer dispositif formation ligne Ceuxci composer généralement dune sélection matériau organiser présenter manière programmer pédagogique mécanisme communication entrer apprenant enseignant doutil apprenant platesforme denseignemer distancer devenir plaire plaire générique modèle définir standardiser normalis és permettre formalisation métadonnée pédagogique tenter dévaluer connaissance acquérir apprenant En appuyer modèle proposer construire baser connaissance associer terme domaine enseigner relation sémantique pédagogique lexploitation baser connaissance fournir niveau daid lingénierie pédagogique volume contenir ligne importer inférence mettre jeu connaissance permettre meilleur dispositif denseignemer
1162	Revue des Nouvelles Technologies de l'Information	EGC	2005	Processus de traitement de données radar pour la reconnaissance/identification de cibles aériennes		Abdelmalek Toumi, Brigitte Hoeltzener, Ali Khenchaf	http://editions-rnti.fr/render_pdf.php?p1&p=1000345	http://editions-rnti.fr/render_pdf.php?p=1000345	
1163	Revue des Nouvelles Technologies de l'Information	EGC	2005	Raisonnement en gestion des compétences	Nous nous intéressons au raisonnement sur les compétences des ressources humaines pour simplifier leur gestion. Dans cet article, nous proposons une méthode de raisonnement pour l'aide à l'identification des compétences d'un individu. Un processus de knowledge-mining défini par analogie avec l'extraction de règles d'association en data-mining est proposé afin d'induire une base de règles à partir d'une base de connaissances sur le domaine. De plus, un prototype a été développé pour expérimenter notre approche sur un exemple académique.	Emmanuel Blanchard, Mounira Harzallah, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1000385	http://editions-rnti.fr/render_pdf.php?p=1000385	intéresser raisonnemer compétence ressource humain simplifier gestion Dans article proposer méthode raisonnement laid lidentification compétence dun individu processus knowledgemining définir analogie lextraction règle dassociation datamining proposer dinduire baser règle partir dune baser connaissance domaine De plaire prototype développer expérimenter approcher exemple académique
1164	Revue des Nouvelles Technologies de l'Information	EGC	2005	RASMA : Une approche Multi-Agent pour l'amélioration de l'Algorithme des Règles d'Associations Spatiales		Hajer Baazaoui Zghal, Radhia Ben Hamed, Sami Faiz, Henda Ben Ghézala	http://editions-rnti.fr/render_pdf.php?p1&p=1000365	http://editions-rnti.fr/render_pdf.php?p=1000365	
1165	Revue des Nouvelles Technologies de l'Information	EGC	2005	Réécriture de requêtes multimédias : approche basée sur l'usage d'une ontologie	Nous proposons dans cet article une stratégie de réécriture de requêtes sur des données multimédias décrites moyennant le standard MPEG-7. Ce standard se base sur XML schéma qui permet de décrire la structure des données. Cependant, aucune sémantique n'est assignée à cette structure. Nous proposons d'étendre ce standard d'une ontologie permettant d'exprimer les connaissances du domaine. Ainsi, l'ontologie sera utilisée durant l'indexation des données multimédias et la réécriture de requêtes. Le but de la réécriture de requêtes est de transformer une requête initiale en une ou plusieurs requêtes équivalentes ou sémantiquement proches compte tenu des connaissances représentées dans l'ontologie.	Samira Hammiche, Salima Benbernou, Mohand-Said Hacid	http://editions-rnti.fr/render_pdf.php?p1&p=1000306	http://editions-rnti.fr/render_pdf.php?p=1000306	proposer dan article stratégie réécriture requête donnée multimédia décrit moyenner standard MPEG7 standard baser xml schéma permettre décrire structurer donnée sémantique nest assigner structurer proposer détendre standard dune ontologie permettre dexprimer connaissance domaine lontologie utiliser durer lindexation donnée multimédia réécriture requête boire réécriture requête transformer requête initial requête équivalente sémantiquement compter connaissance représenter dan lontologie
1166	Revue des Nouvelles Technologies de l'Information	EGC	2005	Règles de propagation pour la création d'ontologies d'annotation de ressources	L'annotation se distingue de l'indexation automatique par l'utilisation d'une ou plusieurs ontologies qui définissent un domaine global de référence permettant de cadrer et de normaliser les annotations effectuées, par ailleurs une ressource annotée doit l'être non pas par une liste de mots clefs, mais bien par une ou plusieurs ontologies. Malheureusement, il est peu réaliste de penser que les centaines de millions de ressources mises à disposition sur le Web puissent être annotées par leurs auteurs. Pour résoudre ce problème, notre démarche consiste à indexer les documents en se basant sur l'ontologie globale et ensuite propager les annotations en utilisant des documents déjà annotés pour annoter d'autres documents référencés par ceux-ci. La propagation des annotations suit des règles que nous proposons dans cet article. L'illustration est effectuée sur un corpus de livres dont le thème relève de l'informatique.	Lylia Abrouk, Pierre Pompidor, Danièle Hérin, Michel Sala	http://editions-rnti.fr/render_pdf.php?p1&p=1000305	http://editions-rnti.fr/render_pdf.php?p=1000305	lannotation distinguer lindexation automatique lutilisation dune ontologie définir domaine global référence permettre cadrer normaliser annotation effectuer ressource annoter devoir lêtr liste clef ontologie malheureusement réaliste penser centaine million ressource mettre disposition web pouvoir annoter auteur Pour résoudre problème démarcher consister indexer document baser lontologie global ensuite propager annotation utiliser document déjà annoter annoter dautr document référencer ceuxci propagation annotation règle proposer dan article Lillustration effectuer corpus livre thème relever linformatique
1167	Revue des Nouvelles Technologies de l'Information	EGC	2005	Réponses coopératives dans l'interrogation de documents RDF	Le développement du Web Sémantique a conduit à l'élaboration de standards pour la représentation des connaissances sur le Web. RDF, comme un de ces standards, est devenu une recommandation du W3C. Même s'il a été conçu pour être interprétable par l'homme et la machine (encodage XML, triplets, graphes étiquetés), RDF n'a pas été fourni avec des services d'interrogation et de raisonnement. La plupart des travaux concernant l'interrogation de documents RDF se sont concentrés sur l'usage de techniques issues de la programmation logique et sur des extensions de SQL. Nous portons un nouveau regard sur les techniques d'interrogation et de raisonnement sur les documents RDF et nous montrons que la sémantique des termes OSF (Order Sorted Features) est compatible avec la représentation isomorphique (triplets) des propositions RDF. Cette transformation permet l'ordonnancement des ressources en ontologies et, à travers ceci, des meilleurs mécanismes de réponses (par approximation et recouvrement) aux interrogations de documents RDF.	Adrian Tanasescu, Mohand-Said Hacid	http://editions-rnti.fr/render_pdf.php?p1&p=1000307	http://editions-rnti.fr/render_pdf.php?p=1000307	développement web Sémantique conduire lélaboration standard représentation connaissance web RDF standard devenir recommandation w3c sil concevoir interprétable lhomme machiner encodage xml triplet graph étiqueté RDF fournir service dinterrogation raisonnement travail concerner linterrogation document RDF concentrer lusage technique issu programmation logique extension SQL porter regard technique dinterrogation raisonnement document RDF montrer sémantique terme osf Order Sorted Features compatible représentation isomorphiqu triplet proposition RDF transformation permettre lordonnancement ressource ontologie travers meilleur mécanism réponse approximation recouvrement interrogation document RDF
1168	Revue des Nouvelles Technologies de l'Information	EGC	2005	Restructuration automatique de documents dans les corpus semi-structurés hétérogènes	L'interrogation de grandes bases de documents semi-structurés (type XML) est un problème ouvert important. En effet, pour interroger un document dont le schéma est nouveau, un système doit pouvoir soit adapter la requête posée au document, soit adapter le document pour pouvoir lui appliquer la requête. Nous nous positionnons ici dans le cadre de la restructuration de documents qui consiste à transformer des documents semi-structurés issus de diverses sources dans un schéma de médiation connu. Nous proposons un cadre statistique général à la problématique de la restructuration de documents et détaillons une instance d'un modèle stochastique de documents structurés appliquée à cette problématique. Nous détaillons enfin un ensemble d'expériences effectuées sur les documents du corpus INEX afin de mesurer la capacité de notre modèle.	Guillaume Wisniewski, Ludovic Denoyer, Patrick Gallinari	http://editions-rnti.fr/render_pdf.php?p1&p=1000260	http://editions-rnti.fr/render_pdf.php?p=1000260	linterrogation grand base document semistructuré typer xml problème ouvrir importer En interroger document schéma système devoir pouvoir adapter requête poser document adapter document pouvoir luire appliquer requête positionner dan cadrer restructuration document consister transformer document semistructuré issu source dan schéma médiation connaître proposer cadrer statistique général problématique restructuration document détaillon instance dun modeler stochastique document structuré appliquer problématique détailler ensemble dexpérience effectuer document corpus INEX mesurer capacité modeler
1169	Revue des Nouvelles Technologies de l'Information	EGC	2005	Sélection de modèles par des méthodes à noyaux pour la classification de données séquentielles	Ce travail concerne le développement de méthodes de classification discriminantes pour des données séquentielles. Quelques techniques ont été proposées pour étendre aux séquences les méthodes discriminantes, comme les machines à vecteurs supports, par nature plus adaptées aux données en dimension fixe. Elles permettent de classifier des séquences complètes mais pas de réaliser la segmentation, qui consiste à reconnaître la séquence d'unités, phonèmes ou lettres par exemple, correspondant à un signal. En utilisant une correspondance donnée / modèle nous transformons le problème de l'apprentissage des modèles à partir de données par un problème de sélection de modèles, qui peut être attaqué via des méthodes du type machines à vecteurs supports. Nous proposons et évaluons divers noyaux pour cela et fournissons des résultats expérimentaux pour deux problèmes de classification.	Trinh Minh Tri Do, Thierry Artières, Patrick Gallinari	http://editions-rnti.fr/render_pdf.php?p1&p=1000236	http://editions-rnti.fr/render_pdf.php?p=1000236	travail concerner développement méthode classification discriminant donnée séquentiel technique proposer étendre séquence méthode discriminanter machine vecteur support nature plaire adapter donnée dimension fixer permettre classifier séquence complet réaliser segmentation consister reconnaître séquence duniter phonèm lettre exemple correspondre signal En utiliser correspondance donné   modeler transformer problème lapprentissage modèle partir donnée problème sélection modèle pouvoir attaquer méthode typer machine vecteur support proposer évaluon noyau celer fournisson résultat expérimental problème classification
1170	Revue des Nouvelles Technologies de l'Information	EGC	2005	Semi-supervised incremental clustering of categorical data	"Le clustering semi-supervisé combine l'apprentissage supervisé et non-supervisé pour produire meilleurs clusterings. Dans la phase initiale supervisée de l'algorithme, un échantillon d'apprentissage est produit par sélection aléatoire. On suppose que les exemples de l'échantillon d'apprentissage sont étiquetés par un attribut de classe. Puis, un algorithme incrémentiel développé pour les données catégoriques est utilisé pour produire un ensemble de clusters pur (tels que les exemple de chaque cluster ont la même étiquette), qui servent de ""seeding clusters"" pour la deuxième phase non-supervisée de l'algorithme. Dans cette phase, l'algorithme incrémentiel est appliqué aux données non étiquetées. La qualité du clustering est évaluée par l'index de Gini moyen des clusters. Les expériences démontrent que des très bons clusterings peuvent être obtenus avec des petits échantillons d'apprentissage."	Dan A. Simovici, Natima Singla	http://editions-rnti.fr/render_pdf.php?p1&p=1000246	http://editions-rnti.fr/render_pdf.php?p=1000246	clustering semisuperviser combiner lapprentissage superviser nonsuperviser produire meilleur clusterings Dans phase initial superviser lalgorithme échantillon dapprentissage produire sélection aléatoire supposer exemple léchantillon dapprentissage étiqueter attribut classer Puis algorithme incrémentiel développer donnée catégorique utiliser produire ensemble cluster exemple cluster étiqueter servir seeding cluster phase nonsuperviser lalgorithme Dans phase lalgorithm incrémentiel appliquer donnée étiqueter qualité clustering évaluer lindex Gini moyen cluster expérience démontrer trè clusterings pouvoir obtenir petit échantillon dapprentissage
1171	Revue des Nouvelles Technologies de l'Information	EGC	2005	SEQTREE, un outil de fouille de données séquentielles par visualisation	Dans cet article, nous présentons un outil de visualisation de séquences modélisées par des arbres de suffixes probabilistes (Prediction suffix trees - PST). Ce type d'arbre permet de représenter une chaîne de Markov d'ordre variable. Dans différentes application, il s'est avéré plus efficace qu'une chaîne de Markov d'ordre fixe avec un coût calculatoire moindre. Pour ces raisons, il nous a paru intéressant d'exploiter le caractère arborescent de ce mode de représentation non seulement d'un point de vue algorithmique, mais aussi d'un point de vue visuel.	Christine Largeron	http://editions-rnti.fr/render_pdf.php?p1&p=1000430	http://editions-rnti.fr/render_pdf.php?p=1000430	Dans article présenter outil visualisation séquence modéliser arbre suffixe probabilister prediction suffix treer   pst typer darbre permettre représenter chaîner Markov dordre variable Dans application sest avérer plaire efficace quune chaîner Markov dordre fixer coût calculatoir moindre Pour raison paraître intéresser dexploiter caractère arborescent mode représentation dun poindre algorithmique dun poindre visuel
1172	Revue des Nouvelles Technologies de l'Information	EGC	2005	SSC : Statistical Subspace Clustering	Cet article se place dans le cadre du subspace clustering, dont la problématique est double : identifier simultanément les clusters et le sous-espace spécifique dans lequel chacun est défini, et caractériser chaque cluster par un nombre minimal de dimensions, permettant ainsi une présentation des résultats compréhensible par un expert du domaine d'application. Les méthodes proposées jusqu'à présent pour cette tâche ont le défaut de se restreindre à un cadre numérique. L'objectif de cet article est de proposer un algorithme de subspace clustering capable de traiter des données décrites à la fois par des attributs continus et des attributs catégoriels. Nous présentons une méthode basée sur l'algorithme classique EM mais opérant sur un modelé simplifié des données et suivi d'une technique originale de sélection d'attributs pour ne garder que les dimensions pertinentes de chaque cluster. Les expérimentations présentées ensuite, menées sur des bases de données aussi bien artificielles que réelles, montrent que notre algorithme présente des résultats robustes en termes de qualité de la classification et de compréhensibilité des clusters obtenus.	Laurent Candillier, Isabelle Tellier, Fabien Torre, Olivier Bousquet	http://editions-rnti.fr/render_pdf.php?p1&p=1000241	http://editions-rnti.fr/render_pdf.php?p=1000241	article placer dan cadrer subspace clustering problématique double   identifier simultanément cluster sousespace spécifique dan définir caractériser cluster nombre minimal dimension permettre présentation résultat compréhensible expert domaine dapplication méthode proposer jusquà présent tâcher défaut restreindre cadrer numérique Lobjectif article proposer algorithme subspace clustering capable traiter donnée décrit attribut continu attribut catégoriel présenter méthode baser lalgorithme classique em opérer modeler simplifier donnée dune technique original sélection dattribut garder dimension pertinent cluster expérimentation présenter ensuite mener base donnée artificiel réel montrer algorithme présenter résultat robuste terme qualité classification compréhensibilité cluster obtenir
1173	Revue des Nouvelles Technologies de l'Information	EGC	2005	SVM et visualisation pour la fouille de grands ensembles de données	Nous présentons un algorithme de SVM et des méthodes graphiques pour le traitement de grands ensembles de données. Pour pouvoir traiter de tels ensembles de données, nous utilisons une représentation des données de plus haut niveau (sous forme symbolique). L'algorithme de séparateur à vaste marge (SVM) est adapté pour pouvoir traiter ce nouveau type de données. Nous construisons un nouveau noyau RBF (Radial Basis Function) que l'algorithme utilise à la fois pour la classification, la régression et la détection d'individus atypiques dans des données de type intervalle. Nous utilisons ensuite des méthodes de visualisation interactive (elles aussi adaptées au cas des variables de type intervalle) pour expliquer les résultats obtenus par les SVM. La méthode est évaluée sur des ensembles de données symboliques existant ou créés artificiellement.	Thanh-Nghi Do, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1000372	http://editions-rnti.fr/render_pdf.php?p=1000372	présenter algorithme SVM méthode graphique traitement grand ensemble donnée Pour pouvoir traiter ensembl donnée utiliser représentation donnée plaire niveau sou former symbolique Lalgorithme séparateur vaste marge SVM adapter pouvoir traiter typer donnée construire noyau RBF Radial Basis Function lalgorithme utiliser classification régression détection dindividus atypiquer dan donnée typer intervall utiliser ensuite méthode visualisation interactif adapter cas variable typer intervall expliquer résultat obtenir svm méthode évaluer ensemble donnée symbolique exister créer artificiellement
1174	Revue des Nouvelles Technologies de l'Information	EGC	2005	Tableau de Bits Indexé (TBI)  pour la Recherche de Séquences Fréquentes		Lionel Savary, Karine Zeitouni	http://editions-rnti.fr/render_pdf.php?p1&p=1000279	http://editions-rnti.fr/render_pdf.php?p=1000279	
1175	Revue des Nouvelles Technologies de l'Information	EGC	2005	TANAGRA : un logiciel gratuit pour l'enseignement et la recherche	TANAGRA est un logiciel « open source » librement accessible sur le web, il tente de concilier deux types d'utilisation. D'une part, en proposant une interface suffisamment conviviale, il est accessible aux utilisateurs non spécialistes qui veulent effectuer des études sur des données réelles. D'autre part, en définissant une architecture simplifiée à l'extrême, les efforts de développement portent sur l'essentiel, à savoir la mise au point et l'intégration d'algorithmes de fouille de données, les chercheurs peuvent ainsi mener des expérimentations sur les méthodes. Dans cet article, nous présentons les principales fonctionnalités du logiciel en essayant de le positionner sur l'échiquier des (très) nombreux logiciels diffusés actuellement.	Ricco Rakotomalala	http://editions-rnti.fr/render_pdf.php?p1&p=1000435	http://editions-rnti.fr/render_pdf.php?p=1000435	tanagra logiciel « open source » librement accessible web tenter concilier type dutilisation dune partir proposer interface suffisamment convivial accessible utilisateur spécialiste vouloir effectuer étude donnée réel dautre partir définir architecturer simplifier lextrême effort développement porter lessentiel savoir miser poindre lintégration dalgorithme fouiller donnée chercheur pouvoir mener expérimentation méthode Dans article présenter principal fonctionnaliter logiciel essayer positionner léchiquier trè logiciel diffuser actuellement
1176	Revue des Nouvelles Technologies de l'Information	EGC	2005	Tendances dans les expressions de gènes :  application à l'analyse du transcriptome  de Plasmodium Falciparum	L'étude de l'expression des gènes est depuis quelques années révolutionnée par les puces à ADN. Les méthodes habituellement mises en oeuvre pour analyser ces données s'appuient sur des algorithmes de partitionnement, comme les clustering hiérarchiques, et sur une hypothèse communément admise qui associe à un ensemble de profils d'expression similaires, une fonction identique. Cette analyse étudie l'ensemble des gènes sans distinction. L'approche que nous proposons deux catégories de gènes : connus ou putatifs. Pour chaque gène n'ayant pas d'information rattachée, nous étudions son voisinage afin d'y trouver des motifs fréquents (itemsets). Ensuite, l'Analyse est guidée par l'interprétation biologique afin de faire émerger des propriétés intéressantes. Un premier jeu de test sur Plasmodium Falciparum (agent de la Malaria) nous a permis de mettre en évidence, en nous intéressant aux items relatifs à la glycolyse, un transporteur de nucléosides qui intervient au niveau énergétique dans la phase ring (précoce) du parasite.	Philippe Collet, Vincent Derozier, Gérard Dray, François Trousset, Pascal Poncelet, Michel Crampes	http://editions-rnti.fr/render_pdf.php?p1&p=1000411	http://editions-rnti.fr/render_pdf.php?p=1000411	Létude lexpression gène année révolutionner puce ADN méthode habituellement mise oeuvrer analyser donnée sappuient algorithme partitionnement clustering hiérarchique hypothèse communément admis associer ensemble profil dexpression similaire fonction identique analyser étudier lensembl gène distinction Lapproche proposer catégorie gène   connu putatif Pour gène nayer dinformation rattaché étudier voisinage dy trouver motif fréquent itemset ensuite lanalyse guider linterprétation biologique faire émerger propriété intéressant jeu test Plasmodium falciparum agent Malaria permettre mettre évidence intéresser item relatif glycolyse transporteur nucléoside intervenir niveau énergétique dan phase ring précoce parasiter
1177	Revue des Nouvelles Technologies de l'Information	EGC	2005	Un automate pour la génération complète ou partielle des concepts du treillis de Galois	"Cet article se situe dans le domaine de l'analyse formelle de concepts et du treillis de concepts (treillis de Galois) lequel est un cadre théorique intéressant pour le regroupement conceptuel des données et la génération des règles d'association. Puisque la prospection de données (data mining) est utilisée comme support à la prise de décision par des analystes rarement intéressés par la liste exhaustive (souvent très longue) des concepts et des règles, l'élaboration d'une solution approximative sera dans la plupart des cas un compromis satisfaisant et relativement moins coûteux qu'une solution exhaustive. Dans cet article, on propose une approche appelée CIGA (Closed Itemset Generation using an Automata) de génération partielle ou complète de concepts par la construction et le parcours d'un automate à états finis. La génération des concepts permet l'identification des ""itemsets"" fermés fréquents, étape cruciale pour l'extraction des règles d'association."	Ganaël Jatteau, Rokia Missaoui, Madenda Sarifuddin	http://editions-rnti.fr/render_pdf.php?p1&p=1000229	http://editions-rnti.fr/render_pdf.php?p=1000229	article situer dan domaine lanalyse formel concept treillis concept treillis Galois cadrer théorique intéresser regroupement conceptuel donnée génération règle dassociation Puisque prospection donnée dater mining utiliser support priser décision analyste intéresser liste exhaustif long concept règle lélaboration dune solution approximatif dan cas compromis satisfaire coûteux quune solution exhaustif Dans article proposer approcher appeler CIGA Closed Itemset generation using an automata génération partiel complet concept construction parcours dun automate finir génération concept permettre lidentification itemset fermer fréquent étape crucial lextraction règle dassociation
1178	Revue des Nouvelles Technologies de l'Information	EGC	2005	Un critère d'évaluation pour la sélection de variables	Cet article aborde le problème de la sélection de variables dans le cadre de la classification supervisée. Les méthodes de sélection reposent sur un algorithme de recherche et un critère d'évaluation pour mesurer la pertinence des sous-ensembles potentiels de variables. Nous présentons un nouveau critère d'évaluation fondé sur une mesure d'ambigüité. Cette mesure est fondée sur une combinaison d'étiquettes représentant le degré de spécificité ou d'appartenance aux classes en présence. Les tests menés sur de nombreux jeux de données réels et artificiels montrent que notre méthode est capable de sélectionner les variables pertinentes et d'augmenter dans la plupart des cas les taux de bon classement.	Dahbia Semani, Carl Frélicot, Pierre Courtellemont	http://editions-rnti.fr/render_pdf.php?p1&p=1000218	http://editions-rnti.fr/render_pdf.php?p=1000218	article aborder problème sélection variable dan cadrer classification superviser méthode sélection reposer algorithme rechercher critère dévaluation mesurer pertinence sousensemble potentiel variable présenter critère dévaluation fonder mesurer dambigüité mesurer fonder combinaison détiquettes représenter degré spécificité dappartenance classe présence test mené jeu donnée réel artificiel montrer méthode capable sélectionner variable pertinent daugmenter dan cas taux classement
1179	Revue des Nouvelles Technologies de l'Information	EGC	2005	Un système d'aide à la navigation dans des hypermédias	Avec le développement d'Internet et d'applications hypermédias, la construction et l'exploitation de profils ou modèles des utilisateurs deviennent capitaux dans de nombreux domaines. Pouvoir cibler un utilisateur d'un hypermédia ou d'un site web afin de lui proposer ce qu'il attend devient essentiel, par exemple lorsque l'on veut lui présenter les produits qu'il est le plus susceptible d'acheter, ou bien plus généralement à chaque fois que l'on veut éviter de noyer l'utilisateur dans un flot d'informations. Nous présentons un système d'aide à la navigation, intégrant un système de modélisation du comportement de navigation et un stratège qui met en œuvre, en fonction du comportement détecté, une aide visant à recommander des liens particuliers.	Julien Blanchard, Bertrand Petitjean, Thierry Artières, Patrick Gallinari	http://editions-rnti.fr/render_pdf.php?p1&p=1000272	http://editions-rnti.fr/render_pdf.php?p=1000272	Avec développement dInternet dapplication hypermédier construction lexploitation profil modèle utilisateur devenir capital dan domaine pouvoir cibler utilisateur dun hypermédier dun site web luire proposer quil attendre devenir essentiel exemple lon vouloir luire poster produit quil plaire susceptible dacheter plaire généralement lon vouloir éviter noyer lutilisateur dan flot dinformation présenter système daid navigation intégrer système modélisation comportement navigation stratège mettre œuvre fonction comportement détecter aider viser recommander lien
1180	Revue des Nouvelles Technologies de l'Information	EGC	2005	Une approche filtre pour la sélection de variables en apprentissage non supervisé	"La Sélection de Variable (SV) constitue une technique efficace pour réduire la dimension des espaces d'apprentissage et s'avère être une méthode essentielle pour le pré-traitement de données afin de supprimer les variables bruitées et/ou inutiles. Peu de méthodes de SV ont été proposées dans le cadre de l'apprentissage non supervisé, et, la plupart d'entre elles, sont des méthodes dites ""enveloppes"" nécessitant l'utilisation d'un algorithme d'apprentissage pour évaluer les sous ensembles de variables. Or, l'approche ""enveloppe"" est largement mal adaptée à une utilisation lors de cas ""réels"". En effet, d'une part ces méthodes ne sont pas indépendantes vis à vis des algorithmes d'apprentissage non supervisé qui nécessitent le plus souvent de fixer un certain nombre de paramètres ; mais surtout, il n'existe pas de critères bien adaptés à l'évaluation de la qualité d'apprentissage non supervisé dans des sous espaces différents. Nous proposons et évaluons dans ce papier une méthode ""filtre"" et donc indépendante des algorithmes d'apprentissage non supervisé. Cette méthode s'appuie sur deux indices permettant d'évaluer l'adéquation entre deux ensembles de variables (entre deux sous espaces)."	Pierre-Emmanuel Jouve, Nicolas Nicoloyannis	http://editions-rnti.fr/render_pdf.php?p1&p=1000209	http://editions-rnti.fr/render_pdf.php?p=1000209	sélection Variable sv constituer technique efficace réduire dimension espace dapprentissage savère méthode essentiel prétraitement donnée supprimer variable bruiter etou inutile méthode sv proposer dan cadrer lapprentissage superviser dentre méthode enveloppe nécessiter lutilisation dun algorithm dapprentissage évaluer sou ensemble variable Or lapproche envelopper largement mal adapter utilisation cas réel En dun partir méthode indépendant vis vis algorithme dapprentissage superviser nécessiter plaire fixer nombre paramètre   nexiste critère adapter lévaluation qualité dapprentissage superviser dan sou espace proposer évaluon dan papier méthode filtrer indépendant algorithme dapprentissage superviser méthode sappuie indice permettre dévaluer ladéquation entrer ensemble variable entrer sou espace
1181	Revue des Nouvelles Technologies de l'Information	EGC	2005	Une méthode d'évaluation de la pertinence des pages Web dans WebSum	Dans cet article nous présentons une méthode d'évaluation de la pertinence des pages Web retournées par un moteur de recherche.	Olfa Jenhani el Jed	http://editions-rnti.fr/render_pdf.php?p1&p=1000312	http://editions-rnti.fr/render_pdf.php?p=1000312	Dans article présenter méthode dévaluation pertinence page Web retourner moteur rechercher
1182	Revue des Nouvelles Technologies de l'Information	EGC	2005	Usage non classificatoire d'arbres de classification : enseignements d'une analyse de la participation féminine à l'emploi en Suisse	Cet article présente une application en grandeur réelle des arbres de classification dans un contexte non classificatoire. Les arbres générés visent à mettre en lumière les différences régionales dans la façon dont les femmes décident de leur participation au marché du travail. L'accent est donc mis sur la capacité descriptive plutôt que prédictive des arbres. L'application porte sur des données relatives à la participation féminine au marché du travail issues du Recensement Suisse de la Population de l'an 2000. Ce vaste ensemble de données a été analysé en deux phases. Un premier arbre exploratoire a mis en évidence la nécessité de procéder à des études séparées pour les non mères, les mères mariées ou veuves, et les mères célibataires ou divorcées. Nous nous limitons ici aux résultats de ce dernier groupe, pour lequel nous avons généré un arbre séparé pour chacune des trois régions linguistiques principales. Les arbres obtenus font apparaître des différences culturelles fondamentales entre régions. Du point de vue méthodologique, la principale difficulté de cet usage non classificatoire des arbres concerne leur validation, puisque le taux d'erreur de classification généralement retenu perd tout son sens dans ce contexte. Nous commentons cet aspect et illustrons l'usage d'alternatives plus pertinentes et facilement calculables.	Gilbert Ritschard, Pau Origoni, Fabio B. Losa	http://editions-rnti.fr/render_pdf.php?p1&p=1000172	http://editions-rnti.fr/render_pdf.php?p=1000172	article présenter application grandeur réel arbre classification dan contexte classificatoire arbre généré viser mettre lumière différence régional dan femme décider participation marcher travail Laccent mettre capacité descriptif prédictif arbre lapplication porter donnée relatif participation féminin marcher travail issu recensement Suisse population lan 2000 vaste ensemble donnée analyser phase arbre exploratoire mettre évidence nécessiter procéder étude séparer mère mère marié veuve mère célibataire divorcer limiter résultat grouper générer arbre séparer région linguistique principal arbre obtenir faire apparaître différence culturel fondamentale entrer région poindre méthodologique principal difficulté usage classificatoire arbre concerner validation taux derreur classification généralement retenir perdre sens dan contexte commenter aspect illustron lusage dalternativ plaire pertinent facilement calculable
1183	Revue des Nouvelles Technologies de l'Information	EGC	2005	Utilisation des technologies XML pour la formalisation de l'ontologie de modèles e-business	Notre travail de recherche consiste à représenter l'ontologie des modèles e-business e-BMO par le langage BM²L spécifié sur la base d'un méta-modèle XML. BM²L est comparé à d'autres langages de définition d'ontologie à savoir, RDF(S), DAML + OIL et OWL et ce, selon un framework établis sur les spécificités de cette ontologie. Aussi, introduisons nous une application Web e-BMH pour la conception et l'exploitation des modèles e-business conformément à l'ontologie.	Rim Djedidi Hannachi, Sarra Ben Lagha, Mohamed Ben Ahmed	http://editions-rnti.fr/render_pdf.php?p1&p=1000311	http://editions-rnti.fr/render_pdf.php?p=1000311	travail rechercher consister représenter lontologie modèle ebusiness ebmo langage BM²L spécifier baser dun métamodèle XML BM²L comparer dautre langage définition dontologie savoir rdf DAML   OIL OWL framework établir spécificité ontologie introduison application Web ebmh conception lexploitation modèle ebusiness conformément lontologie
1184	Revue des Nouvelles Technologies de l'Information	EGC	2005	Validation statistique des cartes de Kohonen en apprentissage supervisé	En apprentissage supervisé, la prédiction de la classe est le but ultime. Plus largement, on attend d'une bonne méthodologie d'apprentissage qu'elle permette une représentation des données susceptible de faciliter la navigation de l'utilisateur dans la base d'exemples et d'aider au choix des exemples et des variables pertinents tout en assurant une pré-diction de qualité dont on comprenne les ressorts. Différents travaux ont montré l'aptitude des graphes de voisinage issus des prédicteurs à fonder une telle méthodologie, ainsi le graphe des voisins relatifs de Toussaint. Cependant, la complexité de leur construction, en O(n3), reste élevée. Dans le cas de données volumineuses, nous proposons de substituer aux graphes de voisinage les cartes de Kohonen construites sur les prédicteurs. Après un bref rappel du principe des cartes de Kohonen en apprentissage non supervisé, nous montrons comment celles-ci peuvent fonder une stratégie d'apprentissage optimisée. Nous proposons ensuite d'évaluer la qualité de cette stratégie par une statistique originale qui est étroitement corrélée au taux d'erreur en généralisation. Différentes expérimentations montrent la faisabilité de cette approche. On dispose alors d'un critère fiable pour sélectionner les individus et les attributs pertinents.	Elie Prudhomme, Stéphane Lallich	http://editions-rnti.fr/render_pdf.php?p1&p=1000217	http://editions-rnti.fr/render_pdf.php?p=1000217	En apprentissage superviser prédiction classer boire ultime plaire largement attendre dune méthodologie dapprentissage permettre représentation donnée susceptible faciliter navigation lutilisateur dan baser dexempl daider choix exemple variable pertinent assurer prédiction qualité comprendre ressort travail montrer laptitude graphe voisinage issu prédicteur fonder méthodologie graphe voisin relatif Toussaint complexité construction on3 rester élevé Dans cas donnée volumineuser proposer substituer graphe voisinage carte Kohonen construire prédicteur Après bref rappel principe carte Kohonen apprentissage superviser montrer cellesci pouvoir fonder stratégie dapprentissage optimiser proposer ensuite dévaluer qualité stratégie statistique original étroitement corréler taux derreur généralisation expérimentation montrer faisabilité approcher disposer dun critèr fiable sélectionner individu attribut pertinent
1185	Revue des Nouvelles Technologies de l'Information	EGC	2005	Visualisation de la perception d'un site web par ses utilisateurs.	Nous proposons dans cet article une méthode de visualisation de l'activité des utilisateurs d'un site web qui permet d'évaluer qualitativement l'adéquation entre son architecture logique et la perception de celle-ci par les internautes. Nous travaillons sur les parcours des internautes sur le site étudié, après reconstruction de ceux-ci grâce aux fichiers logs des serveurs concernés. Nous utilisons la structure logique des sites étudiés pour simplifier la représentation des parcours, en ne tenant pas compte de l'ordre de visite des catégories sémantiques du site. Les parcours simplifiés sont utilisés pour calculer une dissimilarité entre les catégories sémantiques qui sont ensuite représentées dans un plan par Multi Dimensional Scaling. Nous complétons cette visualisation d'ensemble par une représentation de l'arbre couvrant minimal des catégories sémantiques qui permet de mieux appréhender certaines interactions. Nous illustrons l'intérêt de la méthode en l'appliquant au site de l'INRIA.	Fabrice Rossi, Yves Lechevallier, Aïcha El Golli	http://editions-rnti.fr/render_pdf.php?p1&p=1000381	http://editions-rnti.fr/render_pdf.php?p=1000381	proposer dan article méthode visualisation lactivité utilisateur dun site web permettre dévaluer qualitativement ladéquation entrer architecturer logique perception celleci internaute travailler parcours internaute site étudier reconstruction ceuxci grâce fichier logs serveur concerner utiliser structurer logique site étudier simplifier représentation parcours compter lordre visiter catégorie sémantique site parcours simplifier utiliser calculer dissimilarité entrer catégorie sémantique ensuite représenter dan plan multi Dimensional Scaling compléter visualisation densembl représentation larbre couvrir minimal catégorie sémantique permettre mieux appréhender interaction illustrer lintérêt méthode lappliquer site linria
1186	Revue des Nouvelles Technologies de l'Information	EGC	2005	« La connaissance de la connaissance » : une réflexion sur la triangulation des analyses textuelles à partir d'un corpus spécialisé en gouvernance d'entreprise	Suite à la survenue récente de scandales financiers, la synthèse des idées mobilisables en gouvernance d'entreprise semble désormais essentielle si l'on veut sécuriser les investisseurs. Dans cette perspective, le présent projet de recherche consiste à mettre en œuvre un panel d'outils d'analyse de données textuelles (Alceste, Syntex, Tropes-Zoom/Decision Explorer, Wordmapper, Weblex) afin d'évaluer les moyens dont peut disposer un analyste désireux d'extraire des connaissances contenues dans un ensemble d'articles académiques. La qualité de représentation du corpus dans sa globalité est tout d'abord testée. L'étude est ensuite centrée sur le concept même de connaissance, mobilisé dans la théorie de la gouvernance des entreprises. La convergence et la complémentarité des approches méthodologiques sont alors explicitées. Il en est de même pour ce qui concerne la capacité d'extraction d'une connaissance pertinente à partir des textes étudiés.	Stéphane Trébucq	http://editions-rnti.fr/render_pdf.php?p1&p=1000274	http://editions-rnti.fr/render_pdf.php?p=1000274	suite survenir récent scandale financier synthèse idée mobilisable gouvernance dentrepris sembler essentiel lon vouloir sécuriser investisseur Dans perspectif présent projet rechercher consister mettre œuvre panel doutil danalyse donnée textuel alcest Syntex tropeszoomdecision Explorer Wordmapper Weblex dévaluer moyen pouvoir disposer analyste désireux dextraire connaissance contenu dan ensemble darticl académique qualité représentation corpus dan globalité dabord tester Létude ensuite centrer concept connaissance mobiliser dan théorie gouvernance entreprise convergence complémentarité approche méthodologique expliciter concerner capacité dextraction dune connaissance pertinent partir texte étudier
1187	Revue des Nouvelles Technologies de l'Information	EGC	2004	A Galois connecion semantics-based approach for deriving generic bases of association rules	"L'augmentation vertigineuse de la taille des données (textuelles ou transactionnelles) est un défi constant pour la ""scalabilité"" des techniques d'extraction des connaissances. Dans ce papier, on présente une approche pour la dérivation des bases génériques de règles associatives. Les principales caract éristiques de cette approches sont les suivantes. D'une part, l'introduction d'une structure de données appelée ""Trie-itemset"" pour le stockage de la relation en entrée. D'autre part, on utilise une méthode ""Diviser pour régner"" pour réduire le coût de construction de structures partiellement ordonnées, à partir desquelles les bases génériques de règles sont directement extraites."	Sadok Ben Yahia, Narjes Doggaz, Yahya Slimani, Jihem Rezgui	http://editions-rnti.fr/render_pdf.php?p1&p=1001033	http://editions-rnti.fr/render_pdf.php?p=1001033	laugmentation vertigineux tailler donnée textuel transactionnel défi constant scalabilité technique dextraction connaissance Dans papier présenter approcher dérivation base générique règle associatif principal caract éristique approche dune partir lintroduction dune structurer donnée appeler trieitemset stockage relation entrer dautre partir utiliser méthode diviser régner réduire coût construction structure partiellement ordonner partir desquelle base générique règle extraite
1188	Revue des Nouvelles Technologies de l'Information	EGC	2004	A metric approach to supervised discretization	Nous présentons une nouvelle approche à la discrétisation supervisée des attributs continues qui se sert de l'espace métrique des partitions d'un ensemble fini. Nous discutons deux nouvelles idées fondamentales : une généralisation des techniques de discrétisation de Fayyad-Irani basée sur une distance sur des partitions, dérivée de l'entropie généralisée de Daroczy, et un nouveau critère géométrique pour arrêter l'algorithme de discrétisation. Les arbres de décision résultants sont plus petits, ont moins de feuilles, et montrent des niveaux plus élevés d'exactitude établis par la validation croisée stratifiée.	Dan A. Simovici, Richard Butterworth	http://editions-rnti.fr/render_pdf.php?p1&p=1000966	http://editions-rnti.fr/render_pdf.php?p=1000966	présenter approcher discrétisation superviser attribut continu servir lespace métrique partition dun ensemble finir discuter idée fondamental   généralisation technique discrétisation fayyadirani baser distancer partition dérivé lentropie généralisé Daroczy critère géométrique arrêter lalgorithme discrétisation arbre décision résultant plaire petit feuille montrer niveau plaire élevé dexactitude établir validation croisé stratifier
1189	Revue des Nouvelles Technologies de l'Information	EGC	2004	A robust method for partitioning the values of categorical attributes	Dans le domaine de l'apprentissage supervisé, les méthodes de groupage des modalités d'un attribut symbolique permettent de construire un nouvel attribut synthétique conservant au maximum la valeur informationnelle de l'attribut initial et diminuant le nombre de modalités. Nous proposons ici une généralisation de l'algorithme de discrétisation Khiops pour le problème du groupage des modalités. L'algorithme proposé permet de contrôler a priori le risque de sur-apprentissage et d'améliorer significativement la robustesse des groupages produits. Cette caractéristique de robustesse a été obtenue en étudiant la statistique des variations du critère du Khi2 lors de regroupements de lignes d'un tableau de contingence et en modélisant le comportement statistique de l'algorithme Khiops. Des expérimentations intensives ont permis de valider cette approche et ont montré que la méthode de groupage Khiops aboutit à des groupages performants, à la fois en terme de qualité prédictive et de faible nombre de groupes.	Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1000950	http://editions-rnti.fr/render_pdf.php?p=1000950	Dans domaine lapprentissage superviser méthode groupage modalité dun attribut symbolique permettre construire nouvel attribut synthétique conserver maximum informationnel lattribut initial diminuer nombre modalité proposer généralisation lalgorithme discrétisation Khiops problème groupage modalité Lalgorithme proposer permettre chuter priori risquer surapprentissage daméliorer significativement robustesse groupage produire caractéristique robustesse obtenir étudier statistique variation critère khi2 regroupement ligne dun tableau contingence modéliser comportement statistique lalgorithm Khiops expérimentation intensif permettre valider approcher montrer méthode groupage Khiops aboutir groupage performant terme qualité prédictif faible nombre groupe
1190	Revue des Nouvelles Technologies de l'Information	EGC	2004	Accélération de EM pour données qualitatives : études comparative de différentes versions	L'algorithme EM est très populaire et très efficace pour l'estimation de paramètres d'un modèle de mélange. L'inconvénient majeur de cet algorithme est la lenteur de sa convergence. Son application sur des tableaux de grande taille pourrait ainsi prendre énormément de temps. Afin de remédier à ce problème, nous étudions ici le comportement de plusieurs variantes connus de EM, ainsi qu'une nouvelle méthode. Celles-ci permettent d'accélérer la convergence de l'algorithme, tout en obtenant des résultats similaires à celui-ci. Dans ce travail, nous nous concentrons sur l'aspect classification. Nous réalisons une étude comparative entre les différentes variantes sur des données simulées et réelles et proposons une stratégie d'utilisation de notre méthode qui s'avère très efficace.	Mohamed Nadif, François-Xavier Jollois	http://editions-rnti.fr/render_pdf.php?p1&p=1001019	http://editions-rnti.fr/render_pdf.php?p=1001019	Lalgorithme EM populaire efficace lestimation paramètre dun modeler mélanger linconvénient majeur algorithme lenteur convergence application tableau grand tailler pouvoir prendre énormément temps Afin remédier problème étudier comportement variante connu em quune méthode Cellesci permettre daccélérer convergence lalgorithm obtenir résultat similaire celuici Dans travail concentrer laspect classification réaliser étude comparatif entrer variante donnée simuler réel proposon stratégie dutilisation méthode savère efficace
1191	Revue des Nouvelles Technologies de l'Information	EGC	2004	Acquisition de données vs gestion de connaissances  patrimoniales : le cas des vestiges du théâtre antique d'Arles	Qu'y a t'il de commun aujourd'hui entre l'acquisition de données 3D, la gestion d'informations patrimoniales, ou encore la modélisation tridimensionnelle en temps réel ? Bien peu, force est de le constater, si ce n'est que l'édifice patrimonial sert là souvent de terrain d'expérimentation. Pourtant, il ne saurait être réduit à ce seul statut : il est objet de connaissances dont l'étude doit bénéficier de différents jeux de technologies. Notre proposition, expérimentée sur des vestiges du théâtre antique d'Arles, place cet édifice au centre d'un dispositif visant à intégrer, au sein d'un système d'informations architecturales 3D en devenir, les résultats de différentes phases de son étude. Un jeu de connaissances formalisé sur l'édifice sert de dénominateur commun depuis l'acquisition de données 3D jusqu'à la représentation dans une maquette temps réel pour la toile. Cette maquette devient outil de navigation dans le jeu d'informations et de savoirs qui caractérise l'édifice.	Jean-Yves Blaise, Francesca De Domenico, Livio De Luca, Iwona Dudek	http://editions-rnti.fr/render_pdf.php?p1&p=1001149	http://editions-rnti.fr/render_pdf.php?p=1001149	quy til commun aujourdhui entrer lacquisition donnée 3d gestion dinformation patrimoniale modélisation tridimensionnel temps réel   forc constater nest lédifice patrimonial servir terrain dexpérimentation pourtant savoir réduire statut   objet connaissance létude devoir bénéficier jeu technologie proposition expérimenter vestige théâtre antique darl placer édifice centrer dun dispositif viser intégrer dun système dinformation architectural 3d devenir résultat phase étude jeu connaissance formaliser lédifice servir dénominateur commun lacquisition donnée 3D jusquà représentation dan maquette temps réel toile maquette devenir outil navigation dan jeu dinformation savoir caractériser lédific
1192	Revue des Nouvelles Technologies de l'Information	EGC	2004	Analyse d'information relationnelle par des graphes interactifs de grandes tailles	La découverte de connaissances à partir d'importantes masses de données hétérogènes débouche le plus souvent sur l'analyse relationnelle. La recherche d'informations stratégiques s'appuie en effet sur les liens fonctionnels et sémantiques entre documents, acteurs, terminologie et concepts d'un domaine sans oublier le paramètre temps. De nombreuses méthodes sont proposées pour identifier, analyser et visualiser les mécanismes mis à jour : analyse relationnelle, classifications supervisées et non supervisées, analyse factorielle, analyse sémantique, cartes, dendogrammes, … Mais ces approches demandent souvent une expertise non négligeable pour être comprises et ne s'adressent donc pas aux non initiés. Par contre, la vue d'un graphe mettant en relation une ou deux classes d'éléments interdépendants est directement assimilable par tout le monde. Nous proposons donc un ensemble de visualisations interactives de graphes dont la manipulation doit permettre une découverte de connaissances intuitive et basée sur un langage graphique naturel. Nous illustrons notre propos de nombreux exemples tirés de cas réels d'analyses stratégiques qui ont permis d'évaluer cette approche sur un panel très large de données.	Saïd Karouach, Bernard Dousset	http://editions-rnti.fr/render_pdf.php?p1&p=1001086	http://editions-rnti.fr/render_pdf.php?p=1001086	découvrir connaissance partir dimportante mass donnée hétérogène déboucher plaire lanalyse relationnel rechercher dinformation stratégique sappuie lien fonctionnel sémantique entrer document acteur terminologie concept dun domaine oublier paramètre temps méthode proposer identifier analyser visualiser mécanisme mettre jour   analyser relationnel classification superviser superviser analyser factoriel analyser sémantique cart dendogramm … Mais approche demander expertiser négligeable sadresser initier Par contrer dun graph mettre relation classe déléments interdépendant assimilable monder proposer ensemble visualisation interactif graphe manipulation devoir permettre découvrir connaissance intuitif baser langage graphique illustrer propos exemple tirer cas réel danalyse stratégique permettre dévaluer approcher panel large donnée
1193	Revue des Nouvelles Technologies de l'Information	EGC	2004	Annotation automatique de documents XML	Nous proposons dans cet article un mécanisme automatique d'annotation de documents. Ce mécanisme s'appuie sur une opération de composition permettant de créer de nouveaux documents à partir de documents existants et sur un algorithme permettant d'inférer l'annotation d'un document composé à partir d'annotation de ses parties. Notre modèle est illustré par une étude de cas consacrée à la mise en commun de documents pédagogiques au format XML, dans un environnement coopératif d'enseignement à distance. Nous décrivons un prototype permettant d'annoter ces documents, et d'engendrer une description RDF contenant les annotations.	Birahim Gueye, Philippe Rigaux, Nicolas Spyratos	http://editions-rnti.fr/render_pdf.php?p1&p=1001138	http://editions-rnti.fr/render_pdf.php?p=1001138	proposer dan article mécanisme automatique dannotation document mécanisme sappuie opération composition permettre créer document partir document existant algorithme permettre dinférer lannotation dun document composer partir dannotation party modeler illustrer étude cas consacrer miser commun document pédagogique format XML dan environnement coopératif denseignemer distancer décrire prototype permettre dannoter document dengendrer description RDF contenir annotation
1194	Revue des Nouvelles Technologies de l'Information	EGC	2004	Apprentissage des réseaux bayésiens avec des graphes chaînés maximaux		Paul Munteanu, Mohamed Bendou	http://editions-rnti.fr/render_pdf.php?p1&p=1001093	http://editions-rnti.fr/render_pdf.php?p=1001093	
1195	Revue des Nouvelles Technologies de l'Information	EGC	2004	Apprentissage et optimisation conjoints : extraction de connaissances pertinentes sur les systèmes de production		Anne-Lise Huyet, Jean-Luc Paris	http://editions-rnti.fr/render_pdf.php?p1&p=1001169	http://editions-rnti.fr/render_pdf.php?p=1001169	
1196	Revue des Nouvelles Technologies de l'Information	EGC	2004	Apprentissage incrémental des profils dans un système de filtrage d'information	Cet article présente une méthode d'apprentissage des profils dans les systèmes de filtrage d'information. Le processus d'apprentissage est effectué d'une manière incrémentale au fur et à mesure que les informations sont filtrées et jugées par l'utilisateur. Des expérimentations effectuées sur une collection de test de référence TREC, montrent que la méthode permet effectivement l'amélioration des profils.	Mohand Boughanem, Hamid Tebri, Mohamed Tmar	http://editions-rnti.fr/render_pdf.php?p1&p=1001123	http://editions-rnti.fr/render_pdf.php?p=1001123	article présenter méthode dapprentissage profil dan système filtrage dinformation processus dapprentissage effectuer dune manière incrémental fur mesurer information filtrer juger lutilisateur expérimentation effectuer collection test référence trec montrer méthode permettre effectivement lamélioration profil
1197	Revue des Nouvelles Technologies de l'Information	EGC	2004	Approche binaire pour la génération de fortes règles d'association	Dans ce papier, nous proposons une nouvelle méthode d'extraction des règles d'association dans des bases de données relationnelles basée sur la technologie des arbres de Peano (Ptree). La structure de données utilisée pour représenter la base de données est un ensemble de Ptrees de base représentant chacun un vecteur binaire et tous ces Ptrees sont stockés dans des fichiers binaires. Nous montrons que la structure Ptree combinée avec la technique de réduction appelée élagage par support minimum produisent des règles d'association fortes et réduisent considérablement le temps de construction de l'association. En effet, notre approche présente l'avantage de ne pas effectuer des parcours coûteux de la base de données. Cette approche a été testée à travers un prototype que nous avons implémenté. Les résultats expérimentaux montrent que les règles d'association fortes sont générées dans un temps minimum comparativement à d'autres travaux.	Thabet Slimani, Boutheina Ben Yaghlane, Khaled Mellouli	http://editions-rnti.fr/render_pdf.php?p1&p=1001052	http://editions-rnti.fr/render_pdf.php?p=1001052	Dans papier proposer méthod dextraction règle dassociation dan base donnée relationnel baser technologie arbre Peano Ptree structurer donnée utiliser représenter baser donnée ensemble Ptrees baser représenter vecteur binaire tou Ptrees stocker dan fichier binaire montrer structurer Ptree combiner technique réduction appeler élagage support minimum produire règle dassociation fort réduire considérablemer temps construction lassociation En approcher présent lavantage effectuer parcours coûteux baser donnée approcher tester travers prototype implémenter résultat expérimental montrer règle dassociation fort générer dan temps minimum comparativement dautre travail
1198	Revue des Nouvelles Technologies de l'Information	EGC	2004	Approche innovante pour la recherche et l'extractin coopérative et dynamique d'informations sur Internet	Il existe de nombreuses techniques qui permettent de classifier les documents textuels en fonction de l'intérêt d'un utilisateur (kNN, SVM, ...). Malheureusement, l'intégration de ces méthodes dans les plates-formes de textmining est souvent très statique au cours du temps. Le but de cet article est de présenter une plate-forme de webmining dans laquelle les données hétérogènes sont représentées uniformément selon un formalisme XML/TEI et où l'utilisateur peut interagir sur les processus de récupération et d'analyse de ces données. Pour cela, les modules de traitements sont représentés par des agents fonctionnant sur la plate-forme MadKit et l'apprentissage se fait par une méthode dérivée de VSM et TFIDF utilisant un principe de listes noires pondérées permettant la reconnaissance de documents indésirables. La dynamique de la plate-forme repose principalement sur la possibilité d'ajouter à la volée des agents de traitement et de pouvoir modifier l'ordre et les paramètres d'analyse des documents.	Xavier Denis, Gaële Simon, Nicolas Chanchevrier	http://editions-rnti.fr/render_pdf.php?p1&p=1001129	http://editions-rnti.fr/render_pdf.php?p=1001129	exister technique permettre classifier document textuel fonction lintérêt dun utilisateur knn svm   malheureusement lintégration méthode dan platesforme textmining statique cours temps boire article poster plateforme webmining dan donnée hétérogène représenter uniformément formalisme xmltei lutilisateur pouvoir interagir processus récupération danalyse donnée Pour celer module traitement représenter agent fonctionner plateforme madkit lapprentissage faire méthode dérivé VSM tfidf utiliser principe liste noir pondérer permettre reconnaissance document indésirable dynamique plateforme reposer principalement possibilité dajouter voler agent traitement pouvoir modifier lordre paramètre danalyse document
1199	Revue des Nouvelles Technologies de l'Information	EGC	2004	BELUGA : un outil pour l'analyse dynamique des connaissances de la littérature scientifique d'un domaine. Première application au cas des maladies à prions	Un projet ciblé sur l'étude du domaine des maladies à prions à permis de formaliser une méthodologie commune, sociologique et informatique, de compréhension de sa dynamique par l'analyse thématique. Nous avons créé une plate forme d'indexation de notices bibliographiques dont le but est d'extraire des associations évoluant à travers des intervalles de temps. Beluga propose une chaîne de traitement basée sur l'indexation des documents en unités de base : références, auteurs, termes simples et composés, organismes. L'outil est fondé sur une double approche d'apprentissage et de visualisation qui automatise les processus d'extraction de groupes d'auteurs et de termes, et permet à l'utilisateur de revenir aux données documentaires sources. L'analyse diachronique de corpus de documents électroniques nous permet d'analyser comment la terminologie est structurée en thématiques émergentes.	Nicolas Turenne, Marc Barbier	http://editions-rnti.fr/render_pdf.php?p1&p=1001096	http://editions-rnti.fr/render_pdf.php?p=1001096	projet cibler létude domaine maladie prion permettre formaliser méthodologie commun sociologique informatique compréhension dynamique lanalyse thématique créer plat former dindexation notice bibliographique boire dextraire association évoluer travers intervalle temps Beluga proposer chaîner traitement baser lindexation document unité baser   référence auteur terme simple composer organism Loutil fonder doubler approcher dapprentissage visualisation automatise processus dextraction groupe dauteur terme permettre lutilisateur revenir donnée documentaire source Lanalyse diachronique corpus document électronique permettre danalyser terminologie structurer thématique émergent
1200	Revue des Nouvelles Technologies de l'Information	EGC	2004	BooLoader : un chargeur efficace dédié aux bases de transactions denses	Nous nous intéressons à la représentation et au chargement de bases de transactions en mémoire. Pour cela, nous proposons d'utiliser un format condensé fondé sur les diagrammes de décision binaires et nous présentons un algorithme que nous avons implanté en un système baptisé BooLoader, pour charger des bases de transactions. Nous donnons également des résultats expérimentaux de notre système sur des bases éparses et denses.	Zahir Maazouzi, Ansaf Salleb-Aouissi, Christel Vrain	http://editions-rnti.fr/render_pdf.php?p1&p=1000898	http://editions-rnti.fr/render_pdf.php?p=1000898	intéresser représentation chargemer base transaction mémoire Pour celer proposer dutiliser format condenser fonder diagramme décision binaire présenter algorithme implanter système baptiser booloader charger base transaction donner également résultat expérimental système base épars dense
1201	Revue des Nouvelles Technologies de l'Information	EGC	2004	Caractérisation de signatures complexes dans des familles de protéines distantes	L'identification de signatures de protéines est un problème majeur pour la découverte de nouveaux membres dans des familles de protéines connues. Le concept de signature qui permet de caractériser ces familles est généralement basé sur la définition de motifs communs. Il s'avère que les familles de protéines connues. Le concept de signature qui permet de caractériser ces familles est généralement basé sur la définition de motifs communs. Il s'avère que les familles distantes sont trop hétérogènes pour qu'on puisse identifier les régions conservées à partir des algorithmes classiques de la bioinformatique. Nous proposons une approche génétique pour la découverte de motifs hiérarchiques; l'algorithme suit une démarche descendante en s'appuyant dans une première phase sur les classes physico-chimiques des acides aminés. Les signatures sont ensuite définies par des séquences des motifs ainsi obtenus. Elles sont extraites au moyen d'un algorithme de découverte d'itemsets séquentiels où les motifs jouent le rôle d'items. Une dernière étape consiste à fouiller dans cette base d'itemsets pour n'en retenir qu'un ensemble réduit de signatures. Plusieurs stratégies sont proposés pour déterminer un ensemble optimal de signatures qui respecte des contraintes de complétude, de cardinalité et de spécificité. Nous appliquons notre démarche sur la famille des cytokines. L'analyse de la base de protéines SCOP a montré que les groupes de signatures que nous avons extrait cible spécifiquement cette famille d'intérêt.	Jérôme Mikolajczak, Gérard Ramstein, Yannick Jacques	http://editions-rnti.fr/render_pdf.php?p1&p=1001049	http://editions-rnti.fr/render_pdf.php?p=1001049	lidentification signature protéine problème majeur découvrir membre dan famille protéine connu concept signature permettre caractériser famille généralement baser définition motif commun savère famille protéine connu concept signature permettre caractériser famille généralement baser définition motif commun savère famille distant hétérogène quon pouvoir identifier région conserver partir algorithme classique bioinformatique proposer approcher génétique découvrir motif hiérarchique lalgorithm démarcher descendant sappuyer dan phase classe physicochimique acide aminé signature ensuite définir séquence motif obtenir extraire moyen dun algorithme découvrir ditemset séquentiel motif jouer rôle ditems étape consister fouiller dan baser ditemset nen retenir quun ensemble réduire signature stratégie proposer déterminer ensemble optimal signature respecter contrainte complétude cardinalité spécificité appliquer démarcher famille cytokine lanalyse baser protéine SCOP montrer groupe signature extraire cibl spécifiquement famille dintérêt
1202	Revue des Nouvelles Technologies de l'Information	EGC	2004	Caractérisation globale de l'exécution de jobs	La caractérisation globale de l'exécution de jobs passe par l'exploitation de mesures recueillies sur les machines en production. Afin de répondre à la problématique, il est nécessaire de tenir compte des différents types de données, ainsi que de la dualité de la caractérisation : statique et dynamique. Une solution technique répondant aux contraintes est proposée. Elle repose sur l'utilisation de SVM afin de détecter des phases, et à un niveau supérieur, à un réseau bayésien afin d'automatiser l'analyse de modèles de Markov enrichis. Ceux-ci sont introduits comme la base formelle et synthétique de description du comportement du job, aussi bien sur un système batch, que parallèle. Enfin, les résultats obtenus à l'aide d'un prototype sont discutés.	Fabrice Gadaud, Guillaume Duquesnay	http://editions-rnti.fr/render_pdf.php?p1&p=1001161	http://editions-rnti.fr/render_pdf.php?p=1001161	caractérisation global lexécution job passer lexploitation mesure recueillir machine production Afin répondre problématique nécessaire compter type donnée dualité caractérisation   statique dynamique solution technique répondre contraint proposer reposer lutilisation svm détecter phase niveau supérieur réseau bayésien dautomatiser lanalyse modèle Markov enrichir Ceuxci introduire baser formel synthétique description comportement job système batch parallèle résultat obtenir laid dun prototype discuter
1203	Revue des Nouvelles Technologies de l'Information	EGC	2004	Cartographie sémantique des connaissances à la carte		Christophe Tricot, Cristophe Roche	http://editions-rnti.fr/render_pdf.php?p1&p=1000916	http://editions-rnti.fr/render_pdf.php?p=1000916	
1204	Revue des Nouvelles Technologies de l'Information	EGC	2004	Classer pour découvrir : une nouvelle méthode d'analyse du comportement de tous les utilisateurs d'un site Web	"L'analyse du comportement des utilisateurs d'un site Web est un domaine riche et complexe. Le grand nombre de méthodes d'extraction de connaissances appliquées aux logs Web, ainsi que la diversité du type de ces méthodes en est une preuve. Cependant, compte tenu de cette complexité, nous posons dans cet article la question suivante : Est-il possible de combiner des méthodes existantes pour proposer une analyse qui tire profit des résultats de plusieurs spécialités et extraire par exemple des comportements fréquents minoritaires ?Notre étude à donc porté sur une nouvelle approche hybride (issue de la classification neuronale et de la recherche de motifs séquentiels) visant à classer les navigations des utilisateurs d'un site (à l'aide de leurs résumés sémantiques) puis, pour chaque classe de navigations, d'en extraire les comportements fréquents. Notre objectif est 1) de pallier les limites de l'extraction de motifs fréquents par rapport à la quantité de données à traiter et aussi par rapport à la qualité des résultats et 2) de pallier les limites d'une première méthode d'analyse du comportement appelée ""Diviser pour Découvrir"", que nous avons proposé en 2003. Nous avons mené des expérimentations sur les logs HTTP des sites INRIA. Les résultats obtenus confirment le bien fondé de notre approche vis à vis de l'état de l'art."	Doru Tanasa, Brigitte Trousse, Florent Masseglia	http://editions-rnti.fr/render_pdf.php?p1&p=1001146	http://editions-rnti.fr/render_pdf.php?p=1001146	Lanalyse comportement utilisateur dun site Web domaine riche complexe grand nombre méthode dextraction connaissance appliquer log Web diversité typer méthode preuve compter complexité poser dan article question   Estil combiner méthode existant proposer analyser tir profit résultat spécialité extraire exemple comportement fréquent minoritaire étude porter approcher hybride issu classification neuronal rechercher motif séquentiel viser classer navigation utilisateur dun site laid résumé sémantique pouvoir classer navigation den extraire comportement fréquent objectif 1 pallier limite lextraction motif fréquent rapport quantité donnée traiter rapport qualité résultat 2 pallier limite dune méthod danalyse comportement appeler diviser découvrir proposer 2003 mener expérimentation log http site INRIA résultat obtenir confirmer fonder approcher vis vis létat lart
1205	Revue des Nouvelles Technologies de l'Information	EGC	2004	Classification automatique d'images		Mohamed Hammami, Boulbaba Ben Amor, Liming Chen	http://editions-rnti.fr/render_pdf.php?p1&p=1001027	http://editions-rnti.fr/render_pdf.php?p=1001027	
1206	Revue des Nouvelles Technologies de l'Information	EGC	2004	Construction de variables et arbre de décision		Gaëlle Legrand, Nicolas Nicoloyannis	http://editions-rnti.fr/render_pdf.php?p1&p=1000996	http://editions-rnti.fr/render_pdf.php?p=1000996	
1207	Revue des Nouvelles Technologies de l'Information	EGC	2004	Contrôle du risque multiple pour la sélection de règles d'association significatives	Les algorithmes d'extraction de règles d'association parcourent efficacement le treillis des itemsets pour constituer une base de règles admissibles à des seuils de support et de confiance, mais donnent une multitude de règles peu exploitables. Nous suggérons d'épurer de telles bases en éliminant les règles non statistiquement significatives. La multitude de tests pratiqués conduit mécaniquement à multiplier les règles sélectionnées à tort. après avoir présenté des procédures issues de la biostatistique qui contrôlent non pas le risque, mais le nombre de fausses découvertes, nous proposons BS_DF, un algorithme original fondé sur le bootstrat qui sélectionne les règles significatives en contrôlant le nombre de fausses découvertes. Des expérimentations montrer l'efficacité de ces procédures.	Elie Prudhomme, Stéphane Lallich, Olivier Teytaud	http://editions-rnti.fr/render_pdf.php?p1&p=1001046	http://editions-rnti.fr/render_pdf.php?p=1001046	algorithme dextraction règle dassociation parcourir efficacement treillis itemset constituer baser règle admissible seuil support confiance donner multitude règle exploitable suggérer dépurer base éliminer règle statistiquemer significatif multitude test pratiquer conduire mécaniquement multiplier règle sélectionner tort présenter procédure issu biostatistique contrôler risquer nombre faux découverte proposer BSDF algorithme original fonder bootstrat sélectionner règle significatif contrôler nombre faux découverte expérimentation montrer lefficacité procédure
1208	Revue des Nouvelles Technologies de l'Information	EGC	2004	Découverte de régularités pour l'intégration de  données semi structurées	Cet article présente l'utilisation d'une technique de fouille de données pour aider à la spécification de vues sur des sources XML. Notre langage de vues permet d'intégrer des données XML provenant de sources hétérogènes. Cependant, la définition de motifs sur les sources permettant de spécifier les données à extraire est souvent difficile, car la structure des données n'est pas toujours connue. Nous proposons d'extraire les structures fréquentes dans les données des sources pour spécifier des motifs pertinents à utiliser dans la spécification des vues.	Pierre-Alain Laur, Xavier Baril	http://editions-rnti.fr/render_pdf.php?p1&p=1001139	http://editions-rnti.fr/render_pdf.php?p=1001139	article présent lutilisation dune technique fouiller donnée aider spécification vue source XML langage permettre dintégrer donnée xml provenir source hétérogène définition motif source permettre spécifier donnée extraire difficile structurer donnée nest connaître proposer dextraire structure fréquent dan donnée source spécifier motif pertinent utiliser dan spécification vue
1209	Revue des Nouvelles Technologies de l'Information	EGC	2004	Détection par SVM - Application à la détection de Churn en téléphonie mobile prépayée		Cédric Archaux, Arnaud Martin, Ali Khenchaf	http://editions-rnti.fr/render_pdf.php?p1&p=1001165	http://editions-rnti.fr/render_pdf.php?p=1001165	
1210	Revue des Nouvelles Technologies de l'Information	EGC	2004	ETIQ, un étiqueteur inductif convivail pour les corpus de spécialité		Ahmed Amrani, Oriane Matte-Tailliez, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1001120	http://editions-rnti.fr/render_pdf.php?p=1001120	
1211	Revue des Nouvelles Technologies de l'Information	EGC	2004	Étude de textes par leur image	Nous proposons une méthode automatique de comparaison de textes reposant sur une technique de transformation d'un texte en une image de taille donnée et l'analyse à l'aide des outils de la géométrie fractale. Nous présentons une application à l'étude d'un corpus de 90 textes longs.	Hubert Marteau, Alexandre Lefevre, Nicole Vincent	http://editions-rnti.fr/render_pdf.php?p1&p=1001095	http://editions-rnti.fr/render_pdf.php?p=1001095	proposer méthode automatique comparaison texte reposer technique transformation dun texte imager tailler donner lanalyse laid outil géométrie fractal présenter application létude dun corpu 90 texte long
1212	Revue des Nouvelles Technologies de l'Information	EGC	2004	Étude expérimentale de mesures de qualité de règles d'association	La validation des connaissances extraites d'un processus d'ECD par un expert métier nécessite de filtrer ces connaissances. Pour ce faire, de nombreuses mesures ont été proposées, chacune répondant à des besoins spécifiques. Ces mesures présentent des caractéristiques variées et parfois contradictoires qu'il convient alors d'examiner. Arguant du fait que la sélection des bonnes connaissances passe aussi par l'utilisation d'un ensemble de mesures adaptées au contexte, nous présentons dans cet article une étude expérimentale de différentes mesures. Cette étude est mise en regard d'une étude formelle synthétisant les qualités des mesures.	Benoît Vaillant, Philippe Lenca, Stéphane Lallich	http://editions-rnti.fr/render_pdf.php?p1&p=1001056	http://editions-rnti.fr/render_pdf.php?p=1001056	validation connaissance extrait dun processus decd expert métier nécessit filtrer connaissance Pour faire mesure proposer répondre besoin spécifique mesure présenter caractéristique varier contradictoire quil convier dexaminer arguer faire sélection connaissance passer lutilisation dun ensemble mesure adapter contexte présenter dan article étude expérimental mesure étude mettre regard dune étude formel synthétiser qualité mesure
1213	Revue des Nouvelles Technologies de l'Information	EGC	2004	EXIT : EXtraction Itérative de la Terminologie		Mathieu Roche, Thomas Heitz, Oriane Matte-Tailliez, Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1001121	http://editions-rnti.fr/render_pdf.php?p=1001121	
1214	Revue des Nouvelles Technologies de l'Information	EGC	2004	Extraction de connaissances grâce à un outil de text-mining. Application à la veille informationnelle dans le cadre policier		Marc Borry, Annick Castiaux	http://editions-rnti.fr/render_pdf.php?p1&p=1001167	http://editions-rnti.fr/render_pdf.php?p=1001167	
1215	Revue des Nouvelles Technologies de l'Information	EGC	2004	Extraction de processus fonctionnels en génétique des  microbes à partir de résumés MEDLINE	Après l'ère du décodage des génomes, les biologistes sont de plus en plus confrontés à l'intégration de myriades de connaissances parcellaires, stockées majoritairement sous forme textuelle. Nous montrons, à travers un exemple concret, que la conjonction de deux chaînes de traitement faisant appel de façon modérée à l'expertise humaine offre au biologiste une aide utile pour parcourir cette littérature, à partir d'une structuration sans a priori de son corpus ; il s'agit ici de résumés Medline indexés par les gènes et protéines qu'ils citent, et que l'algorithme structure (sans superviseur) en principales voies métaboliques et de régulation présentes dans le corpus choisi. 1) Une chaîne d'indexation par les noms de gènes et protéines inclut un expert pour valider, 2) Un environnement interactif de clustering thématique attribue des valeurs graduées de centralité dans chaque thème aux résumés comme aux noms, comme à toute autre variable illustrative (autres termes bio., MeSH, …).	Alain Lelu, Philippe Bessières, Alain Zasadzinski, Dominique Besagni	http://editions-rnti.fr/render_pdf.php?p1&p=1001107	http://editions-rnti.fr/render_pdf.php?p=1001107	Après lère décodage génome biologist plaire plaire confronter lintégration myriade connaissance parcellaire stocker majoritairement sou former textuel montrer travers exemple concret conjonction chaîne traitement faire appel modéré lexpertise humain offrir biologiste aider utile parcourir littérature partir dune structuration priori corpus   sagit résumé Medline indexer gène protéine quil citer lalgorithme structurer superviseur principal voir métabolique régulation présenter dan corpu choisir 1 chaîner dindexation nom gène protéine inclure expert valider 2 environnement interactif clustering thématique attribuer gradué centralité dan thème résumer nom variable illustrative terme bio mesh …
1216	Revue des Nouvelles Technologies de l'Information	EGC	2004	Extraction of text summary using latent semantic indexing and information retrieval technique : comparison of four strategies	In this paper, we present four generic text summarization techniques. Each technique extracts a text summary by ranking and extracting sentences from an original document. The first method, SUMMARIZER 1, uses standard information retrieval (IR) methods to rank sentences. The second method, SUMMARIZER 2, uses the Latent Semantic Analysis (LSA) technique to identify semantically important sentences, for summary creations. The third method, SUMMARIZER 3, uses a combination of the latent semantic analysis technique, reduction and relevance measure. The fourth method simply uses the TF*IDF (Term frequency * Inverse Document frequency) weighting scheme. Evaluations of the four methods are conducted using Document Understanding Conferences (DUC) datasets from NIST. We have compared the summary of each method with the manual summaries. Summarizer 4, with its lowest overhead, has comparable performance to summarizer 1. Analysis shows that a combination of LSA technique and the relevance measure (Summarizer 3) has the best performance on an average.	Abdelghani Bellaachia, Anand Mahajan	http://editions-rnti.fr/render_pdf.php?p1&p=1001111	http://editions-rnti.fr/render_pdf.php?p=1001111	in this paper we preser four generic text summarization technique each technique extract text summary by ranking and extracting sentence from an original document The first method summarizer 1 use standard information retrieval IR method to rank sentence The second method summarizer 2 use the Latent Semantic Analysis lsa technique to identify semantically importer sentence for summary creation The third method summarizer 3 use combination of the latent semantic analysi technique reduction and relevance measur The fourth method simply user the TFIDF Term frequency   Inverse Document frequency weighting scheme Evaluations of the four method are conducted using document Understanding Conferences DUC dataset from nist We hav compared the summary of each method with the manual summarie summarizer 4 with it lowest overhead has performance to summarizer 1 Analysis show that combination of LSA technique and the relevance measur Summarizer 3 the best performance an average
1217	Revue des Nouvelles Technologies de l'Information	EGC	2004	Fonctionnement d'un Système Informatique d'Aide à la Décision (SIAD)	Cet article présente le fonctionnement d'un SIAD : alimentation, traitement des données qui l'alimentent, production des résultats, outils de consultation mis à la disposition des utilisateurs, exploitation éditoriale.	Michel Volle	http://editions-rnti.fr/render_pdf.php?p1&p=1000886	http://editions-rnti.fr/render_pdf.php?p=1000886	article présenter fonctionnement dun SIAD   alimentation traitement donnée lalimenter production résultat outil consultation mettre disposition utilisateur exploitation éditorial
1218	Revue des Nouvelles Technologies de l'Information	EGC	2004	Fonctions d'oubli dans les entrpôts de données	Les entrepôts de données stockent des quantités de données de plus en plus massives, en particulier du fait de la constitution d'historiques. Nous proposons ici une solution pour éviter la saturation des entrepôts de données. Nous définissons un langage de spécifications de fonctions d'oubli des données les plus anciennes, permettant de déterminer ce qui doit être présent dans l'entrepôt de données à chaque instant. Ces spécifications de fonctions d'oubli se traduisent par des opérations de résumé par agrégation, et par des opérations de suppression des données anciennes réalisées de façon mécanique à chaque pas de mise à jour. La communication présente tout d'abord une description syntaxique du langage de spécifications des fonctions d'oubli. Les contraintes à vérifier pour assurer la cohérence du langage sont ensuite décrites. Enfin, nous proposons des structures de données adaptées au stockage des données nécessaires à la gestion des fonctions d'oubli.	Aliou Boly, Georges Hébrail, Marie-Luce Picard	http://editions-rnti.fr/render_pdf.php?p1&p=1000891	http://editions-rnti.fr/render_pdf.php?p=1000891	entrepôt donnée stocker quantité donnée plaire plaire massif faire constitution dhistoriqu proposer solution éviter saturation entrepôt donnée définir langage spécification fonction doubli donnée plaire ancien permettre déterminer devoir présent dan lentrepôt donnée instant spécification fonction doubli traduire opération résumer agrégation opération suppression donnée ancien réalisée mécanique miser jour communication présenter dabord description syntaxique langage spécification fonction doubli contrainte vérifier cohérence langage ensuite décrire proposer structure donnée adapter stockage donnée nécessaire gestion fonction doubli
1219	Revue des Nouvelles Technologies de l'Information	EGC	2004	Fouille dans la structure de documents XML	La prolifération des documents XML appelle des techniques appropriées pour extraire et exploiter l'information contenue dans ces documents. On distingue deux approches de fouille : XML Content Mining portant sur le contenu et XML Structure Lining qui a trait à la structure des documents. Combiner ces deux approches est très intéressant. Les informations contenues dans la structure orientent la fouille sur le contenu. Nous présentons la première étape de cette démarche : une nouvelle méthode d'extraction des règles d'association à partir de la structure des documents XML qui permet de gérer les aspects hiérarchiques de ces documents tout en améliorant les mécanismes d'extraction grâce à la création d'une structure spéciale représentant la hiérarchie des balises rencontrées.	Amandine Duffoux, Omar Boussaid, Stéphane Lallich, Fadila Bentayeb	http://editions-rnti.fr/render_pdf.php?p1&p=1001134	http://editions-rnti.fr/render_pdf.php?p=1001134	prolifération document xml appeler technique approprier extraire exploiter linformation contenir dan document distinguer approche fouiller   xml Content Mining porter contenir xml Structure Lining traire structurer document combiner approche intéresser information contenu dan structurer orienter fouiller contenir présenter étape démarcher   méthod dextraction règle dassociation partir structurer document xml permettre gérer aspect hiérarchique document améliorer mécanisme dextraction grâce création dune structurer spécial représenter hiérarchie balise rencontrer
1220	Revue des Nouvelles Technologies de l'Information	EGC	2004	Fouille de grands ensembles de données avec un boosting proximal SVM	Les SVM (support vector machines) ont montré leur efficacité dans plusieurs domaines d'application. L'apprentissage des SVM se ramène à résoudre un programme quadratique, dont la mise en oeuvre est en général coûteuse en temps. Une reformulation plus récente des SVM (proximal SVM), proposée par Fung et Mangasarian, ne nécessite que la résolution d'un système linéaire, cet algorithme de PSVM est plus efficace et permet de traiter des données dont le nombre d'individus est très important (109) et le nombre d'attributs plus restreint (104). Nous proposons d'utiliser la formule de Sherman-Morrison-Woodbury pour adapter le PSVM à la fouille d'ensembles de données dont le nombre d'attributs est très important et le nombre d'individus plus restreint sur un matériel standard. Puis nous présentons un algorithme de boosting de PSVM pour classifier des données de très grandes tailles en nombre d'individus et d'attributs. Nous évaluons les performances du nouvel algorithme sur les ensembles de données de l'UCI, Twonorm, Ringnorm, Reuters-21578 et Ndc.	Thanh-Nghi Do, François Poulet	http://editions-rnti.fr/render_pdf.php?p1&p=1001011	http://editions-rnti.fr/render_pdf.php?p=1001011	svm support vector machine montrer efficacité dan domaine dapplication lapprentissage svm ramener résoudre programmer quadratique miser oeuvrer général coûteux temps reformulation plaire récent svm proximal svm proposer fung Mangasarian nécessiter résolution dun système linéaire algorithme PSVM plaire efficace permettre traiter donnée nombre dindividus importer 109 nombre dattributs plaire restreindre 104 proposer dutiliser formuler shermanmorrisonwoodbury adapter PSVM fouiller densemble donnée nombre dattribut importer nombre dindividus plaire restreindre matériel standard Puis présenter algorithme boosting PSVM classifier donnée grand taille nombre dindividus dattribut évaluer performance nouvel algorithme ensemble donnée luci Twonorm Ringnorm Reuters21578 Ndc
1221	Revue des Nouvelles Technologies de l'Information	EGC	2004	Gestion de données hétérogènes dans un entrepôt de données		Laurence Duval	http://editions-rnti.fr/render_pdf.php?p1&p=1000907	http://editions-rnti.fr/render_pdf.php?p=1000907	
1222	Revue des Nouvelles Technologies de l'Information	EGC	2004	GVSR : un annuaire de logiciels de manipulation et d'édition de graphes		Bruno Pinaud, Pascale Kuntz, Magalie Delépine, Julien Barberet	http://editions-rnti.fr/render_pdf.php?p1&p=1001094	http://editions-rnti.fr/render_pdf.php?p=1001094	
1223	Revue des Nouvelles Technologies de l'Information	EGC	2004	How well go Lattice algorithms on currently used machine leaning TestBeds ?	Many research papers in classification or association rules increase the interest of Concept lattices structures for data mining (DM) and machine learning (ML). To increase the efficiency of concept lattice-bases algorithms in ML, it is necessary to make us of an efficient algorithms to build concept lattices. In fact, more than ten algorithms for generating concept lattices were published. As real data sets for data mining are very large, concept lattice structure suffers form its complexity issues on such data. The efficiency and performance of concept lattices algorithms are very different from one to another. So we need to compare the existing lattice algorithms with large data. We implemented the four first algorithms in Java environment and compared these algorithms on about 30 datasets of the UCI repository that are well established to be used to compare ML algorithms. Preliminary results give preference to Ganter's algorithm, and then to Bordat's algorithm, which do not fil well with the recommendations of Kuznetsov and Obiedkov. Furthermore, we analyzed the duality of lattice-based algorithms.	Huaiguo Fu, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1001078	http://editions-rnti.fr/render_pdf.php?p=1001078	Many research papers in classification or association ruler increase the interest of Concept lattice structur for dater mining DM and machiner learning ML To increase the efficiency of concept latticebases algorithm in ML it is necessary to make us of an efficient algorithms to build concept lattice In fact more than ten algorithms for generating concept lattice were published real dater set for dater mining are very large concept lattice structurer suffer form it complexity issu such dater The efficiency and performance of concept lattice algorithms are very from one to another so we need to comparer the existing lattice algorithms with large dater We implemented the four first algorithm in Java environmer and compared these algorithm about 30 dataset of the UCI repository that are well established to be used to comparer ml algorithm Preliminary results giv preference to Ganters algorithm and then to Bordats algorithm which do not fil well with the recommendation of Kuznetsov and Obiedkov furthermore we analyzed the duality of latticebased algorithm
1224	Revue des Nouvelles Technologies de l'Information	EGC	2004	Identification de blocs homogènes sur des données continues	Contrairement aux méthodes usuelles de classification ne cherchant généralement qu'une seule partition, soit des instances, soit des attributs, les méthodes de classification croisée et de classification directe fournissent des blocs de données liant des instances à des attributs. Les premières consistent à chercher simultanément une partition en lignes et une partition en colonnes. Les secondes, elles, s'appliquent directement sur les données, et permettent d'obtenir des blocs de données homogènes de toutes tailles, ainsi que des hiérarchies de classes en lignes et en colonnes. Combinant les avantages des deux méthodes, nous présentons ici une méthodologie permettant de travailler sur de grandes bases de données.	François-Xavier Jollois, Mohamed Nadif	http://editions-rnti.fr/render_pdf.php?p1&p=1001015	http://editions-rnti.fr/render_pdf.php?p=1001015	contrairement méthode usuel classification chercher généralement quune partition instance attribut méthode classification croisé classification direct fournir bloc donnée lier instance attribut consister chercher simultanément partition ligne partition colonne seconde sappliquer donnée permettre dobtenir bloc donnée homogène taille hiérarchie classe ligne colonne combiner avantage méthode présenter méthodologie permettre travailler grand base donnée
1225	Revue des Nouvelles Technologies de l'Information	EGC	2004	Induction extensionnelle : définition et application à l'acquisition de concepts à partir de textes	"Lorsque des outils inductifs sont inclus dans un système d'acquisition des connaissances, on dit que l'on construit un système apprenti. C'est dans le but de soulager la charge de travail de l'expert du domaine que cette forme d'apprentissage comporte des outils inductifs. La difficulté tient en ce que l'énumération des connaissances expertes produit des données peu bruitées mais très incomplètes que les itérations successives d'induction vont compléter, toutefois en y ajoutant de grandes quantités de bruit. Il en résulte qu'on doit utiliser des procédures inductives spéciales, adaptées à l'apprentissage par croissance de noyaux de connaissance supervisée. En particulier, pour résoudre le problème difficile de la reconnaissance de concepts dans les textes, nous avons défini une forme d'apprentissage qui intègre l'apprentissage à partir d'instances et les systèmes apprentis, que nous nommons ""Induction Extensionnelle"", un oxymoron qui souligne que malgré l'absence de création d'un modèle explicite, une induction prend effectivement place."	Yves Kodratoff	http://editions-rnti.fr/render_pdf.php?p1&p=1001017	http://editions-rnti.fr/render_pdf.php?p=1001017	Lorsque outil inductif inclure dan système dacquisition connaissance lon construire système apprenti cest dan boire soulager charger travail lexpert domaine former dapprentissage comporter outil inductif difficulté lénumération connaissance expert produire donnée bruiter incomplet itération successif dinduction aller compléter yu ajouter grand quantité bruire résulter quon devoir utiliser procédure inductif spécial adapter lapprentissage croissance noyau connaissance superviser En résoudre problème difficile reconnaissance concept dan texte définir former dapprentissage intégrer lapprentissage partir dinstance système apprenti nommer induction extensionnelle oxymoron souligner labsence création dun modeler expliciter induction prendre effectivement placer
1226	Revue des Nouvelles Technologies de l'Information	EGC	2004	Intégration efficace de méthodes  de fouille de données dans les SGBD	Cet article présente une nouvelle approche permettant d'appliquer des algorithmes de fouille, en particulier d'apprentissage supervisé, à de grandes bases de données et en des temps de traitement acceptables. Cet objectif est atteint en intégrant ces algorithmes dans un SGBD. Ainsi, nous ne sommes limités que par la taille du disque et plus par celle de la mémoire. Cependant, les entrées-sorties nécessaires pour accéder à la base engendrent des temps de traitement longs. Nous proposons donc dans cet article une méthode originale pour réduire la taille de la base d'apprentissage en construisant sa table de contingence. Les algorithmes d'apprentissage sont alors adaptés pour s'appliquer à la table de contingence. Afin de valider notre approche, nous avons implémenté la méthode de construction d'arbre de décision ID3 et montré que l'utilisation de la table de contingence permet d'obtenir des temps de traitements équivalents à ceux des logiciels classiques.	Cédric Udréa, Fadila Bentayeb, Jérôme Darmont, Omar Boussaid	http://editions-rnti.fr/render_pdf.php?p1&p=1000899	http://editions-rnti.fr/render_pdf.php?p=1000899	article présenter approcher permettre dappliquer algorithme fouiller dapprentissage superviser grand base donnée temps traitement acceptable objectif atteindre intégrer algorithme dan sgbd limité tailler disqu plaire mémoire entréessortie nécessaire accéder baser engendrer temps traitement long proposer dan article méthode original réduire tailler baser dapprentissage construire tabler contingence algorithme dapprentissage adapter sappliquer tabler contingence Afin valider approcher implémenter méthode construction darbre décision id3 montrer lutilisation tabler contingence permettre dobtenir temps traitement équivalent logiciel classique
1227	Revue des Nouvelles Technologies de l'Information	EGC	2004	Interrogation de sources biomédicales : prise en compte des préférences de l'utilisateur	Nous nous plaçons dans le cadre d'un projet de constitution d'une plate-forme intégrative de données biomédicales pour l'étude génomique des cancers. La plate-forme comporte, entre autres, un certain nombre de scénarios d'analyse qui sont proposés à l'utilisateur. A chaque étape d'un scénario qu'il a choisi de réaliser pour les besoins de son étude, l'utilisateur peut être amené à poser une requête nécessitant d'accéder à différentes sources et il doit alors choisir les sources pertinentes. Nous proposons un guide à l'utilisateur sous forme d'un algorithme de sélection de sources adapté à sa requête et à ses préférences. Pour cela, nous explorons quelques spécificités des banques de données biomédicales et définissons différents critères de préférence utiles pour les biologistes. Nous illustrons notre démarche avec un exemple de requête biomédicale.	Sarah Cohen Boulakia, Christine Froidevaux, Séverine Lair	http://editions-rnti.fr/render_pdf.php?p1&p=1000893	http://editions-rnti.fr/render_pdf.php?p=1000893	placer dan cadrer dun projet constitution dune plateforme intégratif donnée biomédical létude génomique cancer plateforme comporter entrer nombre scénario danalyse proposer lutilisateur étape dun scénario quil choisir réaliser besoin étude lutilisateur pouvoir amener poser requête nécessiter daccéder source devoir choisir source pertinent proposer guider lutilisateur sou former dun algorithme sélection source adapter requête préférence Pour celer explorer spécificité banque donnée biomédical définisson critère préférence utile biologiste illustrer démarcher exemple requête biomédical
1228	Revue des Nouvelles Technologies de l'Information	EGC	2004	Le e-lien une solution pour l'extraction et le partage de connaissances structurées dans les documents hypertextuels		Gilles Verley, Jean-Pierre Asselin de Beauville	http://editions-rnti.fr/render_pdf.php?p1&p=1000917	http://editions-rnti.fr/render_pdf.php?p=1000917	
1229	Revue des Nouvelles Technologies de l'Information	EGC	2004	Les règles d'association comme outil de catégorisation textuelle		Simon Jaillet, Maguelonne Teisseire, Jacques Chauché	http://editions-rnti.fr/render_pdf.php?p1&p=1001127	http://editions-rnti.fr/render_pdf.php?p=1001127	
1230	Revue des Nouvelles Technologies de l'Information	EGC	2004	Maintenance de bases de connaissances terminologiques	L'acquisition des connaissances terminologiques de l'entreprise se fait souvent à partir des textes qu'elle utilise. Dans le cadre de ce travail, la base de connaissances terminologiques repose sur la modélisation des concepts-métier sous la forme d'une ontologie. Le problème de la maintenance de cette base et de cette ontologie doit alors être traité.Dans cet article, après avoir donné une définition d'une base de connaissances terminologiques (BCT) et des problèmes de diachronie, nous présentons notre modèle et notre méthode d'acquisition des connaissances terminologiques de l'entreprise. Nous exposons alors notre proposition pour maintenir au cours du temps la base de connaissances terminologiques ainsi construite.Nous illustrons ce travail sur une base de connaissance terminologique sur le cinéma d'animation en décrivant le problème de la maintenance dans une reconstitution historique de différents états de cette base lors de l'apparition des techniques numériques d'animation.	Daniel Beauchêne, Christophe Roche, Cécile Million-Rousseau	http://editions-rnti.fr/render_pdf.php?p1&p=1000910	http://editions-rnti.fr/render_pdf.php?p=1000910	lacquisition connaissance terminologique lentreprise faire partir texte utiliser Dans cadrer travail baser connaissance terminologique reposer modélisation conceptsmétier sou former dune ontologie problème maintenance baser ontologie devoir traitéDans article donner définition dune baser connaissance terminologique BCT problème diachronie présenter modeler méthode dacquisition connaissance terminologique lentreprise exposer proposition maintenir cours temps baser connaissance terminologique construitenous illustrer travail baser connaissance terminologique cinéma danimation décrire problème maintenance dan reconstitution historique baser lapparition technique numérique danimation
1231	Revue des Nouvelles Technologies de l'Information	EGC	2004	Manipulation de représentations de cubes de données		Arnaud Giacometti, Patrick Marcel, Hassina Mouloudi	http://editions-rnti.fr/render_pdf.php?p1&p=1000908	http://editions-rnti.fr/render_pdf.php?p=1000908	
1232	Revue des Nouvelles Technologies de l'Information	EGC	2004	Mediating the Semantic Web	Cet article développe une extension d'une architecture de médiation pour intégrer le Web sémantique. Plus précisément, XLive est un médiateur tout XML développé à PRiSM. Il permet d'exécuter des XQuery sur des sources de données hétérogènes. Après une rapide présentation de XLive et du Web sémantique, une architecture à trois niveaux d'ontologies et de schémas est introduite pour connecter des adaptateurs pour le Web sémantique. Cette architecture vise à intégrer des sources de type Web service d'information conformément à une ontologie globale de référence. Elle conduit à étendre XLive avec le support de vues, un outil de conception de vues et de mappings, et des adaptateurs pour les Web services.	Georges Gardarin, Tuyet-Tram Dang-Ngoc	http://editions-rnti.fr/render_pdf.php?p1&p=1000883	http://editions-rnti.fr/render_pdf.php?p=1000883	article développer extension dune architecturer médiation intégrer web sémantique plaire précisément xlive médiateur xml développer PRiSM permettre dexécuter xquery source donnée hétérogène Après rapide présentation xlive web sémantique architecturer niveau dontologie schéma introduire connecter adaptateur web sémantique architecturer vis intégrer source typer Web service dinformation conformément ontologie global référence conduire étendre xliv support outil conception mapping adaptateur Web service
1233	Revue des Nouvelles Technologies de l'Information	EGC	2004	Mesurer la qualité des règles et de leurs contraposées avec le taux informationnel TIC	La validation des connaissances est l'une des étapes les plus problématiques d'un processus de découverte de règles d'association. Pour que le décideur (expert des données) puisse trouver des connaissances intéressantes dans les grandes quantités de règles produites par les algorithmes de fouille de données, il est nécessaire de mesurer la qualité des règles. Nous insérant dans le cadre de l'analyse statistique implicative, nous proposons dans cet article d'évaluer les règles en considérant leur contenu informationnel à travers un nouvel indice de qualité fondé sur l'entropie de Shannon : TIC (Taux Informationnel modulé par la Contraposée). Cet indice a l'avantage d'être bien adapté à la sémantique des règles, puisque d'une part il respecte leur caractère asymétrique et d'autre part il tire profit de leurs contraposées. Par ailleurs, c'est à notre connaissance la seule mesure de qualité de règles qui intègre à la fois indépendance et déséquilibre, c'est-à-dire qui permette de rejeter simultanément les règles entre variables corrélées négativement et les règles qui possèdent plus de contre-exemples que d'exemples. Des comparaisons de TIC avec la J-mesure, l'information mutuelle, l'indice de Gini, et la confiance sont réalisées sur des simulations numériques.	Julien Blanchard, Fabrice Guillet, Régis Gras, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1001035	http://editions-rnti.fr/render_pdf.php?p=1001035	validation connaissance lune étape plaire problématique dun processus découvrir règle dassociation Pour décideur expert donnée pouvoir trouver connaissance intéressant dan grand quantité règle produire algorithme fouiller donnée nécessaire mesurer qualité règle insérer dan cadrer lanalyse statistique implicatif proposer dan article dévaluer règle considérer contenir informationnel travers nouvel indice qualité fonder lentropie Shannon   TIC taux Informationnel moduler Contraposée indice lavantage dêtre adapter sémantique règle dune partir respecter caractère asymétrique dautre partir tir profit contraposée Par ailleur cest connaissance mesurer qualité règle intégrer indépendanc déséquilibrer cestàdir permettre rejeter simultanément règle entrer variable corréler négativement règle posséder plaire contreexemple dexempl comparaison TIC Jmesure linformation mutuel lindice Gini confiance réaliser simulation numérique
1234	Revue des Nouvelles Technologies de l'Information	EGC	2004	Mesurer les usages d'internet	Nous rendons compte d'une démarche mise en place pour construire une représentation fine des usages d'internet et de leur évolution, en procédant à du traitement secondaire de données de trafic, provenant de panels représentatifs d'internautes. Après avoir présenté les caractéristiques des cohortes étudiées et les différents modes d'enrichissement des données de trafic mis en place, nous présentons quelques résultats construits à partir de ces données enrichies, et en particulier une segmentation des internautes construite sur la base de l'entrelacement des pratiques de communication et de navigation.	Valérie Beaudouin	http://editions-rnti.fr/render_pdf.php?p1&p=1000887	http://editions-rnti.fr/render_pdf.php?p=1000887	compter dune démarcher mettre placer construire représentation fin usage dinternet évolution procéder traitement secondaire donnée trafic provenir panel représentatif dinternaut Après présenter caractéristique cohorte étudier mode denrichissemer donnée trafic mettre placer présenter résultat construit partir donnée enrichie segmentation internaute construire baser lentrelacement pratique communication navigation
1235	Revue des Nouvelles Technologies de l'Information	EGC	2004	Mise en oeuvre des méthodes de fouille de données spatiales alternatives et performances	La fouille de données spatiales nécessite l'analyse des interactions dans l'espace. Ces interactions peuvent être matérialisées dans des tables de distances, ramenant ainsi la fouille de données spatiales à l'analyse multitables. Or, les méthodes de fouilles de données traditionnelles considèrent une seule table en entrée où chaque tuple est une observation à analyser. De simples jointures entre ces tables ne résoud pas le problème et fausse les résultats en raison du comptage multiple des observations. Nous proposons trois alternatives de fouille de données multi-tables dans le cadre de la fouille des données spatiales. La première consiste à interroger à la volée les différentes tables et modifie en dur les algorithmes existants. La seconde est une optimisation de la première qui pré -calcule les jointures et adapte les algorithmes existants. La troisième réorganise les données dans une table unique en complétant - et non en joignant- la table d'analyse par les données présentes dans les autres tables, ensuite applique un algorithme standard sans modification. Cet article présente ces trois alternatives. Il décrit leur implémentation pour la classification supervisée et compare leur performance.	Nadjim Chelghoum, Karine Zeitouni	http://editions-rnti.fr/render_pdf.php?p1&p=1001003	http://editions-rnti.fr/render_pdf.php?p=1001003	fouiller donnée spatiale nécessit lanalyse interaction dan lespace interaction pouvoir matérialiser dan table distance ramener fouiller donnée spatial lanalyse multitabl Or méthode fouille donnée traditionnel considérer tabler entrer tuple observation analyser simple jointure entrer table résoud problème faux résultat raison comptage observation proposer alternative fouiller donnée multitabl dan cadrer fouiller donnée spatial consister interroger voler table modifier dur algorithme existant second optimisation pré calculer jointure adapter algorithme existant réorganiser donnée dan tabler compléter   joindre tabler danalyse donnée présenter dan table ensuite appliquer algorithme standard modification article présenter alternative décrire implémentation classification superviser comparer performance
1236	Revue des Nouvelles Technologies de l'Information	EGC	2004	Modèle de gestion intégrée des compétences et connaissances	La compétence et la connaissance sont deux concepts qui nous semblent fortement conjoints, cependant, ils sont rarement étudiés et gérés ensemble. Nous cherchons donc à identifier les liens et frontières qui peuvent exister entre eux. Ceci a pour objectif de développer un modèle de représentation et de gestion, intégré aux connaissances et aux compétences. Dans cet article, est tout d'abord présentée, une synthèse sur les concepts de compétence et de connaissance. Ensuite, les modèles et outils de gestion de ces concepts sont exposés. Puis, le modèle CKIM (Competency and Knowledge Integrated Model) développé, est défini. Les utilités de ce modèle et son exploitation sont discutées en quatrième partie. La dernière partie représente un prototype d'implantation du modèle CKIM réalisé sur le serveur de connaissances ATHANOR.	Nathalie Vergnaud, Mounira Harzallah, Henri Briand	http://editions-rnti.fr/render_pdf.php?p1&p=1000915	http://editions-rnti.fr/render_pdf.php?p=1000915	compétence connaissance concept sembler fortement conjoindre étudier géré ensemble chercher identifier lien frontière pouvoir exister entrer objectif développer modeler représentation gestion intégré connaissance compétence Dans article dabord présenter synthèse concept compétence connaissance ensuite modèle outil gestion concept exposer Puis modeler CKIM Competency and Knowledge Integrated Model développer définir utilité modeler exploitation discuter partir partir représenter prototype dimplantation modeler ckim réaliser serveur connaissance athanor
1237	Revue des Nouvelles Technologies de l'Information	EGC	2004	Modèle topologique pour l'interrogation des bases d'images	Nous proposons dans cet article un modèle topologique de représentation de bases d'images. Chaque image est représentée à l'aide d'un vecteur de caractéristiques dans R^p et figure comme noeud dans un graphe de voisinage. L'exploration du graphe correspond à la navigation dans la base de données, les voisins d'un noeud représentent des images similaires. Afin de pouvoir traiter des requêtes, nous définissons un modèle topologique. L'image requête est représentée par un vecteur de caractéristiques dans R^p et insérée dans le graphe en mettant à jour localement les relations de voisinage. Ce travail se positionne dans le domaine de la fouille de données complexes.	Mihaela Scuturici, Jérémy Clech, Vasile-Marian Scuturici, Djamel Abdelkader Zighed	http://editions-rnti.fr/render_pdf.php?p1&p=1001092	http://editions-rnti.fr/render_pdf.php?p=1001092	proposer dan article modeler topologique représentation base dimager imager représenter laid dun vecteur caractéristique dan rp figurer noeud dan graphe voisinage lexploration graphe correspondre navigation dan baser donnée voisin dun noeud représenter image similaire Afin pouvoir traiter requête définir modeler topologique limage requête représenter vecteur caractéristique dan rp insérer dan graphe mettre jour localement relation voisinage travail positionner dan domaine fouiller donnée complexe
1238	Revue des Nouvelles Technologies de l'Information	EGC	2004	Modélisation dynamique et temporelle de l'utilisateur pour un filtrage personnalisé de documents textuels	L'apprentissage efficace du profil utilisateur est un challenge car il évolue sans cesse. Dans cet article nous proposons une nouvelle approche pour l'apprentissage du profil long-terme de l'utilisateur pour le filtrage de documents textuels. Dans ce cadre, les documents consultés sont classés de manière dynamique et nous analysons la répartition dans le temps des classes de documents afin de déterminer le mieux possible les classes d'intérêts de l'utilisateur. L'étude empirique confirme la pertinence de notre approche pour une meilleure personnalisation de documents.	Rachid Arezki, Abdenour Mokrane, Pascal Poncelet, Gérard Dray, David William Pearson	http://editions-rnti.fr/render_pdf.php?p1&p=1001122	http://editions-rnti.fr/render_pdf.php?p=1001122	lapprentissage efficace profil utilisateur challenge évoluer cesser Dans article proposer approcher lapprentissage profil longterme lutilisateur filtrage document textuel Dans cadrer document consulter classer manière dynamique analyser répartition dan temps classe document déterminer mieux classe dintérêts lutilisateur Létude empirique confirmer pertinence approcher meilleur personnalisation document
1239	Revue des Nouvelles Technologies de l'Information	EGC	2004	MUSETTE : a framework for knowledge capture from experience	Nous présentons dans cet article une nouvelle approche de modélisation de l'expérience d'utilisation d'un système informatique, avec pour objectif de réutiliser cette expérience en contexte pour assister l'utilisateur à effectuer sa tâche. Quatre scénarios illustrent cette approche.	Pierre-Antoine Champin, Yannick Prié, Alain Mille	http://editions-rnti.fr/render_pdf.php?p1&p=1000912	http://editions-rnti.fr/render_pdf.php?p=1000912	présenter dan article approcher modélisation lexpérience dutilisation dun système informatique objectif réutiliser expérience contexte assister lutilisateur effectuer tâcher Quatre scénario illustrer approcher
1240	Revue des Nouvelles Technologies de l'Information	EGC	2004	OpAC : Opérateur d'analyse en ligne basé sur une technique de fouilles de données	L'analyse en ligne OLAP (On-Line Analysis Processing) et la fouille de données (Data Mining) sont deux champs de recherche qui ont connu, depuis quelques années, des évolutions parallèles et indépendantes. De récentes études ont montré l'importance et l'intérêt de l'association entre ces deux domaines scientifiques. A l'heure actuelle, on assiste à l'accroissement du besoin d'une analyse en ligne plus élaborée. Nous pensons que le couplage entre OLAP et la fouille de données pourra apporter des réponses à ce besoin. Dans cet article, nous proposons d'adopter ce couplage en vue de créer un nouvel opérateur, baptisé OpAC (Opérateur d'Agrégation par Classification), d'analyse en ligne des données multidimensionnelles. OpAC consiste particulièrement en l'agrégation sémantique des modalités d'une dimension d'un cube de données en se basant sur la technique de la classification ascendante hiérarchique.	Riadh Ben Messaoud, Sabine Rabaseda, Omar Boussaid, Fadila Bentayeb	http://editions-rnti.fr/render_pdf.php?p1&p=1000889	http://editions-rnti.fr/render_pdf.php?p=1000889	lanalyse ligne OLAP onlin Analysis Processing fouiller donnée Data Mining champ rechercher connaître année évolution parallèle indépendant récent étude montrer limportance lintérêt lassociation entrer domaine scientifique lheure actuel assister laccroissement besoin dune analyser ligne plaire élaborer penser couplage entrer olap fouiller donnée pouvoir apporter réponse besoin Dans article proposer dadopter couplage créer nouvel opérateur baptiser OpAC Opérateur dagrégation classification danalyse ligne donnée multidimensionnel OpAC consister lagrégation sémantique modalité dune dimension dun cube donnée baser technique classification ascendant hiérarchique
1241	Revue des Nouvelles Technologies de l'Information	EGC	2004	Optimisation des requêtes temporelles sur le web		Rim Faiz, Nizar Khayati, Khaled Mellouli	http://editions-rnti.fr/render_pdf.php?p1&p=1001126	http://editions-rnti.fr/render_pdf.php?p=1001126	
1242	Revue des Nouvelles Technologies de l'Information	EGC	2004	Outil de représentation des évolutions de communautés d'intérêts	Cet article présente un système de visualisation permettant l'observation des comportements collectifs implicites. Il s'agit de reconnaître et de représenter des communautés à partir des connexions Internet des utilisateurs : les utilisateurs sont répartis en communautés en fonction des similarités entre des listes de termes établies sur l'analyse des documents consultés par chacun d'eux. L'étude est rendue dynamique par la comparaison des communautés reconnues sur des périodes de temps connexes. L'outil décrit ci après offre deux représentations différentes de ces communautés : une vision des liaisons thématiques entre les utilisateurs sur chaque période étudiée et une vue comparative des communautés reconnues sur toute la durée de l'étude.	Anne Lavallard, Luigi Lancieri	http://editions-rnti.fr/render_pdf.php?p1&p=1001140	http://editions-rnti.fr/render_pdf.php?p=1001140	article présenter système visualisation permettre lobservation comportement collectif implicite sagit reconnaître représenter communauté partir connexion Internet utilisateur   utilisateur répartir communauter fonction similarité entrer liste terme établir lanalyse document consulter Létude rendu dynamique comparaison communauté reconnaître période temps connexe Loutil décrire offrir représentation communauté   vision liaison thématique entrer utilisateur période étudié comparatif communauté reconnaître durer létude
1243	Revue des Nouvelles Technologies de l'Information	EGC	2004	PoBOC : un algorithme de 	"Nous décrivons l'algorithme PoBOC (Pole-Based Overlapping Clustering) qui génère un ensemble de clusters non-disjoints (ou ""softclusters"") présentés sous forme d'une hiérarchie de concepts à partir de la seule matrice de similarités sur les données considérées. Nous évaluons l'approche sur deux situations d'apprentissage : la classification par apprentissage de règles et l'organisation de données plus complexes et peu structurées telles que les données textuelles.La validation des méthodes de clustering est une étape difficile résolue le plus souvent par une évaluation d'experts. Les deux applications proposées permettent de valider la méthode d'organisation selon deux points de vue : d'une part quantitativement en évaluant l'influence de la méthode pour la classification, d'autre part en permettant une analyse ""humaine"" du résultat dans le cas des données textuelles. Nous mettons en évidence l'intérêt de PoBOC comparativement à d'autres approches d'apprentissage non-supervisé."	Guillaume Cleuziou, Lionel Martin, Christel Vrain	http://editions-rnti.fr/render_pdf.php?p1&p=1001007	http://editions-rnti.fr/render_pdf.php?p=1001007	décrire lalgorithm poboc PoleBased Overlapping Clustering générer ensemble cluster nondisjoint softclusters présenter sou former dune hiérarchie concept partir matrice similarité donnée considérer évaluer lapproch situation dapprentissage   classification apprentissage règle lorganisation donnée plaire complexe structurer donnée textuellesla validation méthode clustering étape difficile résoudre plaire évaluation dexperts application proposer permettre valider méthode dorganisation point   dune partir quantitativement évaluer linfluence méthode classification dautr partir permettre analyser humain résultat dan cas donnée textuel mettre évidence lintérêt poboc comparativement dautr approche dapprentissage nonsupervisé
1244	Revue des Nouvelles Technologies de l'Information	EGC	2004	Positionnement multidimensionnel et partitionnement pour la visualisation de données multivariées		Antoine Naud	http://editions-rnti.fr/render_pdf.php?p1&p=1001030	http://editions-rnti.fr/render_pdf.php?p=1001030	
1245	Revue des Nouvelles Technologies de l'Information	EGC	2004	Qualité et datawarehouse dans le milieu hospitalier		Mireille Cosquer, François Gros, Alain Livartowski	http://editions-rnti.fr/render_pdf.php?p1&p=1000905	http://editions-rnti.fr/render_pdf.php?p=1000905	
1246	Revue des Nouvelles Technologies de l'Information	EGC	2004	Recherche ciblée de documents sur le web	Les langages de requêtes mots-clés pour le web manquent souvent de précision lorsqu'il s'agit de rechercher des documents particuliers difficilement caractérisables par de simples mots-clés (exemple : des cours java ou des photos de formule 1). Nous proposons un langage multi-critères de type attribut-valeur pour augmenter la précision de la recherche de documents sur le web.Nous avons expérimentalement montré le gain de précision de la recherche de documents basé sur ce langage.	Amar-Djalil Mezaour	http://editions-rnti.fr/render_pdf.php?p1&p=1001124	http://editions-rnti.fr/render_pdf.php?p=1001124	langage requête motsclé web manquer précision lorsquil sagit rechercher document difficilement caractérisabl simple motsclé exemple   cours java photo formuler 1 proposer langage multicritère typer attributvaleur augmenter précision rechercher document webnou expérimentalement montrer gain précision rechercher document baser langage
1247	Revue des Nouvelles Technologies de l'Information	EGC	2004	Recherche dans de grandes bases d'images fixes : une nouvelle approche guidée par les règles d'association	"Une base d'images fixes peut être décrite de plusieurs façons, notamment par des descripteurs visuels globaux de couleur, de texture, ou de forme. Les requêtes les plus fréquentes impliquent et combinent les résultats de plusieurs types de descripteurs : par exemple, ""retrouver toutes les images ayant une couleur et une texture semblables à celles d'une image requête donnée"". Pour retrouver plus efficacement et plus rapidement une image dans une grande base, nous exploitons des combinaisons appropriées de descripteurs et étudions l'intérêt des règles d'association entre clusters de descripteurs pour accélérer le temps de réponse à des requêtes sur de grandes bases d'images fixes."	Anicet Kouomou Choupo, Annie Morin, Laure Berti-Equille	http://editions-rnti.fr/render_pdf.php?p1&p=1000895	http://editions-rnti.fr/render_pdf.php?p=1000895	baser dimag fixe pouvoir décrire descripteur visuel global couleur texture former requête plaire fréquent impliquer combiner résultat type descripteur   exemple retrouver image couleur texture dune imager requêt donner Pour retrouver plaire efficacement plaire rapidement imager dan grand baser exploiter combinaison approprier descripteur étudion lintérêt règle dassociation entrer cluster descripteur accélérer temps réponse requête grand base dimag fixe
1248	Revue des Nouvelles Technologies de l'Information	EGC	2004	Recherche de règles d'association hiérarchiques par une approche anthropocentrée	L'Extraction de Connaissances dans la Bases de Données est devenue, pour les banques, une alternative au problème lié à la quantité de données qui sont stockées et qui ne cessent d'augmenter. Ceci aboutit à un paradoxe puisqu'il faut mieux cibler la clientèle susceptible d'être intéressée par une offre en utilisant des méthodes qui ne permettent plus de traiter le nombre croissant d'enregistrements des bases de données. Nos travaux se situent dans la continuité d'une étude que nous avons réalisée sur la recherche de règles d'association appliquée au marketing bancaire. En effet, des premiers résultats encourageants nous ont conduit à approfondir nos travaux vers une recherche de règles d'association hiérarchiques utilisant non plus une approche automatique mais une approche anthropocentrée. Il s'agit d'une approche dans laquelle l'expert fait partie intégrante du processus en jouant le rôle d'heuristique évolutive. Cet article présente les résultats de notre démarche de recherche.	Olivier Couturier, Engelbert Mephu Nguifo, Brigitte Noiret	http://editions-rnti.fr/render_pdf.php?p1&p=1001154	http://editions-rnti.fr/render_pdf.php?p=1001154	lextraction connaissance dan Bases donnée devenir banque alternatif problème lier quantité donnée stocker cesser daugmenter aboutir paradoxe puisquil falloir mieux cibler clientèle susceptible dêtre intéressé offrir utiliser méthode permettre plaire traiter nombre croître denregistrement base donnée travail situer dan continuité dune étude réaliser rechercher règle dassociation appliquer marketing bancaire En résultat encourageant conduire approfondir travail ver rechercher règle dassociation hiérarchique utiliser plaire approcher automatique approcher anthropocentrer sagit dune approcher dan lexpert faire partir intégrant processus jouer rôle dheuristiqu évolutif article présenter résultat démarcher rechercher
1249	Revue des Nouvelles Technologies de l'Information	EGC	2004	Réduction d'un jeu de règles d'association par des méta-règles issues de la logique du 		Martine Cadot, Joseph Di Martino, Amedeo Napoli	http://editions-rnti.fr/render_pdf.php?p1&p=1001058	http://editions-rnti.fr/render_pdf.php?p=1001058	
1250	Revue des Nouvelles Technologies de l'Information	EGC	2004	Réduction du coût d'évaluation d'une règle relationnelle	De nombreuses tâches en Fouille de Données visent à extraire des connaissances exprimées sous la forme d'un ensemble de règles. Les algorithmes dédiés à ces tâches engendrent des règles dont l'adéquation aux données doit être évaluée. On se place dans le cadre où cette évaluation est réalisée directement en lançant des requêtes de dénombrement sur la base de données, et où cette base est relationnelle. Les requêtes comptent les données qui s'apparient avec la règle, calcul qui peut être extrêmement coûteux. Dans cet article, nous étudions l'impact d'une approche d'échantillonnage visant à réduire le coût de l'évaluation des règles relationnelles en tenant compte des spécificités structurelles des requêtes induites.	Agnès Braud, Teddy Turmeaux	http://editions-rnti.fr/render_pdf.php?p1&p=1001039	http://editions-rnti.fr/render_pdf.php?p=1001039	tâche Fouille donnée viser extraire connaissance exprimer sou former dun ensemble règle algorithme dédier tâche engendrer règle ladéquation donnée devoir évaluer placer dan cadrer évaluation réaliser lancer requête dénombremer baser donnée baser relationnel requête compter donnée sapparier régler calcul pouvoir extrêmement coûteux Dans article étudier limpact dune approcher déchantillonnage viser réduire coût lévaluation règle relationnel compter spécificité structurel requête induit
1251	Revue des Nouvelles Technologies de l'Information	EGC	2004	Règles d'identification et méthodes de visualisation d'objets architecturaux	Dans l'étude du patrimoine bâti, la gestion d'informations pose aujourd'hui des problèmes d'interfaçage non triviaux, notamment par la masse, la diversité, la complexité et le caractère hétérogène des contenus. La représentation tridimensionnelle du tissu urbain à différentes échelles (de la ville au corpus architectural), parce qu'elle localise spatialement l'information à délivrer et l'attache à la morphologie de l'édifice, apparaît comme une des réponses possibles. Cette réponse semble par ailleurs bien adaptée aux problématiques spécifiques de l'analyse architecturale du patrimoine que sont par exemple la restitution d'édifices disparus (et les notions d'incertitude qui s'y attachent) ou le réemploi d'éléments de corpus. Pourtant, la représentation tridimensionnelle dans notre champ d'application est aujourd'hui loin de remplir ce rôle. Notre contribution vise à discuter quelques uns des pré-requis qui nous semblent s'imposer à la lumière de nos expériences pour faire de la maquette 3D un outil d'investigation des connaissances sur l'édifice.	Iwona Dudek, Jean-Yves Blaise	http://editions-rnti.fr/render_pdf.php?p1&p=1001157	http://editions-rnti.fr/render_pdf.php?p=1001157	Dans létude patrimoine bâtir gestion dinformation poser aujourdhui problème dinterfaçage trivial masser diversité complexité caractère hétérogène contenu représentation tridimensionnel tissu urbain échelle ville corpus architectural localiser spatialement linformation délivrer lattache morphologie lédifice apparaître réponse réponse sembler adapter problématique spécifique lanalyse architectural patrimoine exemple restitution dédific disparu notion dincertitude sy attacher réemploi déléments corpus pourtant représentation tridimensionnel dan champ dapplication aujourdhui loin remplir rôle contribution vis discuter prérequis sembler simposer lumière expérience faire maquette 3D outil dinvestigation connaissance lédifice
1252	Revue des Nouvelles Technologies de l'Information	EGC	2004	Régression linéaire symbolique avec variables taxonomiques	Le présent papier concerne l'extension des méthodes classiques de régression linéaire aux cas des données symboliques et fait suite à de précédents travaux de Billard et Diday sur la régression linéaire avec variables intervalles et histogrammes. Dans ce papier, nous présentons des méthodes de régression avec variables taxonomiques. Les variables taxonomiques sont des variables organisées en arbre exprimant plusieurs niveaux de généralité (les villes sont regroupées en régions qui sont elles-mêmes regroupées en pays). La méthode proposée sera testée sur données simulées. Finalement, nous observerons que ces méthodes nous permettent d'utiliser la régression linéaire pour étudier des concepts et pour réduire le nombre de données afin d'améliorer les résultats obtenus par rapport à une régression classique.	Filipe Afonso, Lynne Billard, Edwin Diday	http://editions-rnti.fr/render_pdf.php?p1&p=1001000	http://editions-rnti.fr/render_pdf.php?p=1001000	présent papier concerner lextension méthode classique régression linéaire cas donnée symbolique faire suite précédent travail billard diday régression linéaire variable intervalle histogramme Dans papier présenter méthode régression variable taxonomique variable taxonomique variable organiser arbre exprimer niveau généralité ville regrouper région ellesmêm regrouper pays méthode proposer tester donnée simuler finalement observer méthode permettre dutiliser régression linéaire étudier concept réduire nombre donnée daméliorer résultat obtenir rapport régression classique
1253	Revue des Nouvelles Technologies de l'Information	EGC	2004	Relations entre gènes impliqués dans les cancers de la thyroïde	Des relations entre gènes et protéines impliqués dans les cancers de la thyroïde ont été mises en évidence par l'analyse d'un important corpus de résumés de  la base de données bibliographique Medline. Une approche pluridisciplinaire (biologistes, cliniciens, linguistes et chercheurs en sciences de l'information) a permis l'indexation automatique et l'analyse de ce corpus. L'indexation contrôlée, structurée en classes sémantiques, à partir de vastes ressources hétérogènes (les bases biomédicales et génétiques UMLS et LocusLink), prend en compte la spécificité des termes : nomenclatures biochimiques, acronymes de gènes, aberrations chromosomiques ou encore variantes linguistiques de termes. Les deux méthodes de classification complémentaires appliquées révèlent un réseau lexical dense de gènes concurrents autour de trois principales pathologies de la thyroïde : les cancers médullaires, papillaires et des dysfonctionnements du système immunitaire. Les développements apportés aux outils de visualisation interactifs du serveur VISA de l'INIST facilitent lecture et navigation au sein des documents.	Jean Royauté, Claire François, Alain Zasadzinski, Dominique Besagni, Philippe Dessen, Sylvaine Le Minor, Marie-Thérèse Maunoury	http://editions-rnti.fr/render_pdf.php?p1&p=1001119	http://editions-rnti.fr/render_pdf.php?p=1001119	relation entrer gène protéine impliquer dan cancer thyroïde mettre évidence lanalyse dun importer corpu résumé   baser donnée bibliographique medline approcher pluridisciplinaire biologist clinicien linguist chercheur science linformation permettre lindexation automatique lanalyse corpus lindexation contrôler structurer classe sémantique partir vaste ressourc hétérogène base biomédical génétique uml LocusLink prendre compter spécificité terme   nomenclatur biochimique acronym gène aberration chromosomique varianter linguistique terme méthode classification complémentaire appliquer révéler réseau lexical dense gène concurrent autour principal pathologi thyroïde   cancer médullaire papillaire dysfonctionnement système immunitaire développement apporter outil visualisation interactif serveur viser linist faciliter lectur navigation document
1254	Revue des Nouvelles Technologies de l'Information	EGC	2004	Représentation condensée de motifs émergents	Les motifs émergents sont des associations de caractéristiques fortement présentes dans une classe et rares dans les autres. Ils font ressortir les distinctions entre classes et se révèlent particulièrement efficaces pour construire des classifieurs et apporter une aide au diagnostic. À cause de la forte combinatoire du problème, la recherche et la représentation des motifs émergents restent des tâches complexes pour de grandes bases de données. Nous proposons ici une représentation condensée exacte des motifs émergents (i.e., les motifs et leurs taux de croissance sont directement obtenus depuis la représentation condensée). L'idée principale est de s'appuyer sur les récents résultats relatifs aux représentations condensées de motifs fermés fréquents. À partir de cette représentation, nous donnons aussi une méthode aisée à mettre en oeuvre pour obtenir les motifs émergents ayant les meilleurs taux de croissance. Ces motifs, appelés motifs émergents forts, ont été exploités avec succès dans une collaboration avec la société Philips.	Arnaud Soulet, Bruno Crémilleux, François Rioult	http://editions-rnti.fr/render_pdf.php?p1&p=1001022	http://editions-rnti.fr/render_pdf.php?p=1001022	motif émergent association caractéristique fortement présenter dan classer dan faire ressortir distinction entrer classe révéler efficacer construire classifieur apporter aider diagnostic À causer fort combinatoire problème rechercher représentation motif émergent ruer tâche complexe grand base donnée proposer représentation condenser exact motif émergent ie motif taux croissance obtenir représentation condenser Lidée principal sappuyer récent résultat relatif représentation condenser motif fermer fréquent À partir représentation donner méthode aisé mettre oeuvrer obtenir motif émergent meilleur taux croissance motif appeler motif émergent fort exploiter succès dan collaboration société Philips
1255	Revue des Nouvelles Technologies de l'Information	EGC	2004	Représentation de graphes par ACP granulaire	"L'extraction d'information de grands graphes repose le plus souvent sur leur représentation dans des espaces de dimension réduite et on utilise généralement des méthodes factorielles appliquées à des mesures de dissimilarités calculées à partir des matrices associée du graphe ou l'analyse spectrale de leur Laplacien discret. Efficaces pour dégager les structures globales, ces représentations sont parfois peu exploitables dès lors que l'on s'intéresse à une perspective du graphe à partir de certains sommets privilégiés. Or l'information recherchée a souvent un caractère ""local"". Pour représenter le graphe du point de vue d'un ou plusieurs sommets sélectionnés, nous proposons une méthode d'Analyse en Composantes Principales ""Granulaire"" consistant à appliquer une A.C.P. ""filtrée"" à un tableau de proximités. La visualisation d'un graphe de dictionnaire dont la mesure de proximité est obtenue à partir d'un algorithme original illustre notre propos."	Bruno Gaume, Louis Ferré	http://editions-rnti.fr/render_pdf.php?p1&p=1001083	http://editions-rnti.fr/render_pdf.php?p=1001083	lextraction dinformation grand graphe reposer plaire représentation dan espace dimension réduit utiliser généralement méthode factoriel appliquer mesure dissimilarité calculer partir matrice associer graphe lanalyse spectral Laplacien discret Efficaces dégager structure global représentation exploitable lon sintéresse perspectif graphe partir sommet privilégié Or linformation rechercher caractère local Pour représenter graphe poindre dun sommet sélectionner proposer méthode danalyse composante Principales Granulaire consister appliquer ACP filtrer tableau proximiter visualisation dun graph dictionnair mesurer proximité obtenir partir dun algorithm original illustrer propos
1256	Revue des Nouvelles Technologies de l'Information	EGC	2004	Résumé de cubes de données multidimensionnelles à l'aide de règles floues	Dans le contexte des entrepôts de données, et des magasins de données multidimensionnelles, les outils OLAP fournissent des moyens aux utilisateurs de naviguer dans leur données afin d'y découvrir des informations pertinentes. cependant, les données à traiter sons souvent très volumineuses et ne permettent pas une exploration systématique et exhaustive. Il s'agit donc de développer des traitements automatisés facilitant la visualisation et la navigation dans les données. Dans cet article, nous étudions une méthode originale permettant de construire et d'identifier de manière automatique et efficace des blocs de données similaires présents dans les cubes de données pouvant être exprimés sous la forme de règles. Cette méthode est fondée sur l'utilisation combinée d'un algorithme par niveaux (de type Apriori) et de la théorie des sous-ensembles flous. Cette théorie nous permet en effet de pallier les problèmes posés par le fait que les blocs de données calculés par notre algorithme peuvent se recouvrir.	Yeow Wei Choong, Anne Laurent, Dominique Laurent, Pierre Maussion	http://editions-rnti.fr/render_pdf.php?p1&p=1000904	http://editions-rnti.fr/render_pdf.php?p=1000904	Dans contexte entrepôt donnée magasin donnée multidimensionnel outil olap fournir moyen utilisateur naviguer dan donnée dy découvrir information pertinent donnée traiter volumineux permettre exploration systématique exhaustif sagit développer traitement automatiser faciliter visualisation navigation dan donnée Dans article étudier méthode original permettre construire didentifier manière automatique efficace bloc donnée similaire présent dan cube donnée pouvoir exprimer sou former règle méthode fonder lutilisation combiner dun algorithme niveau typer Apriori théorie sousensemble flou théorie permettre pallier problème poser faire bloc donnée calculer algorithme pouvoir recouvrir
1257	Revue des Nouvelles Technologies de l'Information	EGC	2004	Sélection d'attributs et classification d'objets complexes		Alexandre Blansché, Pierre Gançarski	http://editions-rnti.fr/render_pdf.php?p1&p=1000994	http://editions-rnti.fr/render_pdf.php?p=1000994	
1258	Revue des Nouvelles Technologies de l'Information	EGC	2004	Sélection rapide en apprentissage supervisé	La sélection de variables (SdV) permet de réduire l'espace de représentation des données. Ce processus est de plus en plus critique en raison de l'augmentation de la taille des bases de données. Traditionnellement, les méthodes de SdV nécessitent plusieurs accès au jeu de données, ce qui peut représenter une part relativement importante du temps d'exécution de ces algorithmes. Nous proposons une nouvelle méthode efficiente et rapide (ne nécessitant qu'un unique accès aux données). Cette méthode utilise les algorithmes génétiques ainsi que des mesures de validité de classification non supervisée (cns).	Nicolas Nicoloyannis, Gaëlle Legrand, Pierre-Emmanuel Jouve	http://editions-rnti.fr/render_pdf.php?p1&p=1000953	http://editions-rnti.fr/render_pdf.php?p=1000953	sélection variable SdV permettre réduire lespace représentation donnée processus plaire plaire critique raison laugmentation tailler base donnée traditionnellemer méthode SdV nécessiter accès jeu donnée pouvoir représenter partir important temps dexécution algorithme proposer méthode efficient rapide nécessiter quun accès donnée méthode utiliser algorithme génétique mesure validité classification superviser cns
1259	Revue des Nouvelles Technologies de l'Information	EGC	2004	Sous-ensembles flous définis sur une ontologie	"Les sous-ensembles flous peuvent être utilisés pour représenter des valeurs imprécises, comme un intervalle aux limites mal définies. Ils peuvent également servir à l'expression de préférences dans les critères de sélection de requêtes en bases de données. En représentation des connaissances, l'utilisation de hiérarchies de types est largement répandue afin de modéliser les relations existant entre les types d'objets d'un domaine donné. Nous nous intéressons aux sous-ensembles flous dont le domaine de définition est une hiérarchie d'éléments partiellement ordonnés par la relation ""sorte de"", que nous appelons ontologie. Nous introduisons la notion de sous-ensemble flou défini sur une partie de l'ontologie, puis sa forme développée définie sur l'ensemble de l'ontologie, que nous appelons extension du sous-ensemble flou. Des classes d'équivalence de sous-ensembles flous définis sur une ontologie peuvent être caractérisées par un représentant unique que nous appelons sous-ensemble flou minimal. Nous concluons par un exemple d'application dans un système d'information relatif à la prévention du risque micro-biologique en sécurité alimentaire."	Rallou Thomopoulos, Patrice Buche, Ollivier Haemmerlé	http://editions-rnti.fr/render_pdf.php?p1&p=1000914	http://editions-rnti.fr/render_pdf.php?p=1000914	sousensemble flou pouvoir utiliser représenter imprécis intervalle limite mal définie pouvoir également servir lexpression préférence dan critère sélection requête base donnée En représentation connaissance lutilisation hiérarchie type largement répandre modéliser relation exister entrer type dobjet dun domaine donner intéresser sousensembl flou domaine définition hiérarchie déléments partiellement ordonner relation sort appeler ontologie introduire notion sousensembl flou définir partir lontologie pouvoir former développer définir lensembl lontologie appeler extension sousensembl flou classe déquivalence sousensemble flou définir ontologie pouvoir caractériser représenter appeler sousensembl flou minimal conclure exemple dapplication dan système dinformation relatif prévention risquer microbiologique sécurité alimentaire
1260	Revue des Nouvelles Technologies de l'Information	EGC	2004	Uitliation de connaissances pour l'aide à la recherche documentaire fondée sur le contenu		Amedeo Napoli, Rim Al Hulou	http://editions-rnti.fr/render_pdf.php?p1&p=1001125	http://editions-rnti.fr/render_pdf.php?p=1001125	
1261	Revue des Nouvelles Technologies de l'Information	EGC	2004	Un algorithme de génération des itemsets fermés pour la fouille de données	Le traitement de grand volume de données est un problème pour l'extraction de connaissances. La fouille de données nécessite des méthodes de résolution efficaces. Le treillis de concepts (treillis de Galois) est un outil utile pour l'analyse de données. Des travaux en classification et sur les règles d'association ont permis d'accroître son intérêt. Plusieurs algorithmes de génération on été proposés, parmi lesquels NextClosure est l'un des meilleurs pour traiter des données de grande taille.Mais la complexité de NextClosure reste malgré tout très élevé. Aussi nous proposons un nouvel algorithme efficace nommé ScalingNextClosure, et basé sur une méthode de partitionnement de données pour générer de manière indépendante les itemsets fermés de chaque partition. Les résultats expérimentaux montrer que cette technique de partitionnement améliore efficacement NextClosure.	Engelbert Mephu Nguifo, Huaiguo Fu	http://editions-rnti.fr/render_pdf.php?p1&p=1001062	http://editions-rnti.fr/render_pdf.php?p=1001062	traitement grand volume donnée problème lextraction connaissance fouiller donnée nécessiter méthode résolution efficace treillis concept treillis Galois outil utile lanalyse donnée travail classification règle dassociation permettre daccroître intérêt algorithme génération proposer nextclosure lun meilleur traiter donnée grand tailleMais complexité nextclosure rester élever proposer nouvel algorithme efficace nommer scalingnextclosure baser méthode partitionnement donnée générer manière indépendant itemset fermer partition résultat expérimental montrer technique partitionnement améliorer efficacement nextclosur
1262	Revue des Nouvelles Technologies de l'Information	EGC	2004	Une approche probabiliste pour le classement d'objets incomplets dans un arbre de décision		Lamis Hawarah, Ana Simonet, Michel Simonet	http://editions-rnti.fr/render_pdf.php?p1&p=1001029	http://editions-rnti.fr/render_pdf.php?p=1001029	
1263	Revue des Nouvelles Technologies de l'Information	EGC	2004	Une étude d'algorithmes de classification supervisée basée sur les treillis de Galois		Huaiyu Fu, Huaiguo Fu, Patrick Njiwoua, Engelbert Mephu Nguifo	http://editions-rnti.fr/render_pdf.php?p1&p=1001025	http://editions-rnti.fr/render_pdf.php?p=1001025	
1264	Revue des Nouvelles Technologies de l'Information	EGC	2004	Une méthode pour l'appropriation de savoir-faire, capitalisé avec MASK	La gestion explicite des savoirs et savoir-faire occupe une place de plus en plus importante dans les organisations. La construction de mémoires d'entreprise dans un but de préservation et de partage est devenu une pratique assez courante. Cependant, on oublie trop suivent que l'efficacité de ces activités est étroitement liée aux capacités d'appropriation et d'apprentissage des acteurs de l'organisation.Dans cet article, nous proposons des démarches générales d'accompagnement permettant de faciliter le processus d'appropriation des mémoires d'entreprise construits avec la méthode MASK, en exploitant des techniques d'ingénierie pédagogique.	Oswaldo Castillo, Nada Matta, Jean-Louis Ermine	http://editions-rnti.fr/render_pdf.php?p1&p=1000911	http://editions-rnti.fr/render_pdf.php?p=1000911	gestion expliciter savoir savoirfair occuper placer plaire plaire important dan organisation construction mémoire dentreprise dan boire préservation partager devenir pratiquer courant oublier lefficacité activité étroitement lier capacité dappropriation dapprentissage acteur lorganisationdans article proposer démarche général daccompagnemer permettre faciliter processus dappropriation mémoire dentreprise construit méthode mask exploiter technique dingénierie pédagogique
1265	Revue des Nouvelles Technologies de l'Information	EGC	2004	Utilisation des graphes de proximité dans le cadre de l'apprentissage basé sur les voisins	"La classification suivant les plus proches voisins est une règle simple et attractive, basée sur une définition paramétrique du voisinage. Les graphes des proximité, quand à eux, induisent des notions plus souples de voisinage. Il s'agit ici d'effectuer la substitution.Les variantes obtenues, peu testées dans la bibliographie, ont été soumises à une expérimentation intensive, sur bases de données de l'UCI et de France Télécom. On a ainsi considéré divers types de prétraitement des données et plusieurs catégories de graphes. De plus, on a caractérisé les effets du ""piège de la dimension"" sur le comportement théorique de tous les graphes présentés, une quantification empirique du phénomène ayant été réalisée.Il ressort de notre étude que l'utilisation du voisinage de Gabriel provoque une amélioration en moyenne et que le prétraitement basé sur la statistique de rang est le plus adéquate. Quoiqu'il arrive, des précautions doivent être prises en grande dimension."	Sylvain Ferrandiz, Marc Boullé	http://editions-rnti.fr/render_pdf.php?p1&p=1001061	http://editions-rnti.fr/render_pdf.php?p=1001061	classification plaire voisin régler simple attractif baser définition paramétrique voisinage graphe proximité induire notion plaire souple voisinage sagit deffectuer substitutionle variante obtenu tester dan bibliographie soumettre expérimentation intensif base donnée luci France Télécom considérer type prétraitement donnée catégorie graphe De plaire caractériser piéger dimension comportement théorique tou graphe présenter quantification empirique phénomène réaliséeil ressortir étude lutilisation voisinage Gabriel provoquer amélioration moyenner prétraitement baser statistique rang plaire adéquat quoiquil arriver précaution devoir prendre grand dimension
1266	Revue des Nouvelles Technologies de l'Information	EGC	2004	Validation de graphes conceptuels	"Les travaux menés en validation des connaissances visent à améliorer la qualité des bases de connaissances. Le modèle des graphes conceptuels est un modèle de représentation des connaissances de la famille des réseaux sémantiques, fondé sur la théorie des graphes et sur la logique du premier ordre. Nous proposons une solution pour valider sémantiquement une base de connaissances composée de graphes conceptuels. La validation sémantique d'une base de connaissance consiste à confronter ses connaissances à des contraintes certifiées fiables. Nous proposons d'utiliser des contraintes descriptives, exprimées sous forme de graphes conceptuels, qui permettent de poser des conditions sur la représentation de certaines connaissance dans la base. Ces contraintes introduisent une notion de cardinalités, et sont soit minimales, soit maximales. Elles permettent respectivement d'exprimer ""si A, alors au moins ou au plus n fois B"". La satisfaction de ces contraintes par une base de connaissances repose sur l'utilisation de l'opération de base du modèle des graphes conceptuels : la projection."	Juliette Dibie-Barthélemy, Ollivier Haemmerlé, Eric Salvat	http://editions-rnti.fr/render_pdf.php?p1&p=1000913	http://editions-rnti.fr/render_pdf.php?p=1000913	travail mener validation connaissance viser améliorer qualité base connaissance modeler graphe conceptuel modeler représentation connaissance famille réseau sémantique fonder théorie graphe logique ordre proposer solution valider sémantiquement baser connaissance composer graphe conceptuel validation sémantique dune baser connaissance consister confronter connaissance contrainte certifier fiable proposer dutiliser contrainte descriptif exprimer sou former graphe conceptuel permettre poser condition représentation connaissance dan baser contrainte introduire notion cardinalité minimal maximal permettre respectivement dexprimer plaire satisfaction contrainte baser connaissance reposer lutilisation lopération baser modeler graphe conceptuel   projection
1267	Revue des Nouvelles Technologies de l'Information	EGC	2004	Veille technologique assistée par la Fouille de Textes	Le domaine de la veille technologique vise à récolter, traiter, et analyser des informations scientifiques et techniques utiles aux acteurs économiques. Dans cet article nous proposons d'utiliser des techniques de fouille de textes pour automatiser le processus de traitement des données issues de bases de textes scientifiques. Toutefois, la veille introduit une difficulté inhabituelle par rapport aux domaines d'application classiques des techniques de fouille de textes, puisqu'au lieu de rechercher de la connaissances fréquente cachée dans les données, il faut rechercher de la connaissance inattendue. Les mesures usuelles d'extraction de la connaissance à partir de textes doivent de ce fait être revues. Pour ce faire, nous avons développé le système UnexpectedMiner dans lequel de nouvelles mesures permettent d'estimer le caractère inattendu d'un document. Notre système est évalué sur une base d'articles dans le domaine de l'apprentissage automatique.	François Jacquenet, Christine Largeron, Stéphanie Chapaux	http://editions-rnti.fr/render_pdf.php?p1&p=1001097	http://editions-rnti.fr/render_pdf.php?p=1001097	domaine veiller technologique vis récolter traiter analyser information scientifique technique utile acteur économique Dans article proposer dutiliser technique fouiller texte automatiser processus traitement donnée issu base texte scientifique veiller introduire difficulté inhabituel rapport domaine dapplication classique technique fouiller texte puisquau lieu rechercher connaissance fréquent cacher dan donnée falloir rechercher connaissance inattendu mesure usuel dextraction connaissance partir texte devoir faire revoir Pour faire développer système unexpectedminer dan mesure permettre destimer caractère inattendu dun document système évaluer baser darticl dan domaine lapprentissage automatique
1268	Revue des Nouvelles Technologies de l'Information	EGC	2004	Vers un entrepôt de données pour la gestion des risques naturels	Les entrepôts de données sont l'un des plus importants développements dans le domaine des systèmes d'informations. Ils permettent d'intégrer des données de plusieurs sources, souvent très volumineux, distribuées et hétérogènes. Dans cet article, nous examinons la possibilité d'utiliser la technique d'entrepôt de données dans la gestion des risques naturels. Nous présentons un modèle conceptuel pour l'entrepôt proposé, avec la présence de formats et types variés de données tel que des données géographiques et multimédia. Nous proposons également des opérations OLAP pour la navigation des informations stockées dans le cube de données.	Hicham Hajji, Nourdine Badji, Jean-Pierre Asté	http://editions-rnti.fr/render_pdf.php?p1&p=1001163	http://editions-rnti.fr/render_pdf.php?p=1001163	entrepôt donnée lun plaire important développement dan domaine système dinformation permettre dintégrer donnée source volumineux distribuée hétérogène Dans article examiner possibilité dutiliser technique dentrepôt donnée dan gestion risque présenter modeler conceptuel lentrepôt proposer présence format type varier donnée donnée géographique multimédier proposer également opération OLAP navigation information stocker dan cuber donnée
